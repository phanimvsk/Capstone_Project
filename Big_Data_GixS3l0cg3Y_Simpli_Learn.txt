data has evolved in the last decade like never before and currently data
generation has increased by Leaps and Bounds and all this data is collectively
termed as Big Data businesses require Big Data Engineers who can handle this data and Big Data
Engineers do this with the help of Frameworks like Hadoop and Spark on that note hi everyone welcome to this video
on Big Data engineer full course in this video we will help you understand how
you can become a big data engineer and we will also cover the fundamentals of Big Data Hadoop and Spark We Begin this
because by understanding how to become a big data engineer we will then understand the top skills
required for big data jobs here we will also take a close look at other top job roles in the Big Data domain we will
then understand big data analytics and understand how big data is used across several domains by learning a few
crucial Big Data applications we will then gain a deep understanding of Big Data its Evolution and its
challenges following which we will learn about Hadoop and how it solves big data problems we will also take an individual
look at components of Hadoop like hdfs mapreduce and yarn we will also be
learning about school high big and headspace the next topic we will be covering is spark here we will
understand the architecture of spark and look at spark streaming finally we will conclude this full
course by looking into a set of top Hadoop interview questions and answers so let's start off with our full course
but before we begin make sure to subscribe to our YouTube channel and hit the Bell icon to never miss an update
from Simply learn we all use smartphones but have you ever wondered how much data
it generates in the form of texts phone calls emails photos videos searches and
music approximately 40 exabytes of data gets generated every month by a single
smartphone user now imagine this number multiplied by 5 billion smartphone users
that's a lot for our mind even process isn't it in fact this amount of data is
quite a lot for traditional Computing systems to handle and this massive amount of data is what we term as Big
Data let's have a look at the data generated per minute on the internet 2.1 million Snaps are shared on Snapchat
3.8 million search queries are made on Google one 1 million people log on to
Facebook 4.5 million videos are watched on YouTube 188 million emails are sent that's a lot
of data so how do you classify any data as Big Data this is possible with the
concept of five V's volume velocity variety veracity and value let us
understand this with an example from the healthcare industry Hospitals and Clinics across the world generate
massive volumes of data 2 314 exabytes of data are collected annually in the
form of patient records and test results all this data is generated at a very high speed which attributes to the
velocity of Big Data variety refers to the various data types such as
structured semi-structured and unstructured data examples include Excel
records log files and x-ray images accuracy and trustworthiness of the
generated data is termed as veracity analyzing all this data will benefit the
medical sector by enabling faster disease detection better treatment and
reduced cost this is known as the value of Big Data but how do we store and
process this big data to do this job we have various Frameworks such as Cassandra Hadoop and Spark let us take
Hadoop as an example and see how Hadoop stores and processes Big Data
Hadoop uses a distributed file system known as Hadoop distributed file system
to store Big Data if you have a huge file your file will be broken down into
smaller chunks and stored in various machines not only that when you break the file you also make copies of it
which goes into different nodes this way you store your big data in a distributed way and make sure that even if one
machine fails your data is safe on another mapreduce technique is used to process
Big Data a lengthy task a is broken into smaller tasks b c and d now instead of one machine
three machines take up each task and complete it in a parallel fashion and
assemble the results at the end thanks to this the processing becomes easy and fast this is known as parallel
processing now that we have stored and processed our big data we can analyze this data
for numerous applications in games like Halo 3 and Call of Duty designers
analyze user data to understand at which stage most of the users pause restart or
quit playing this Insight can help them rework on the storyline of the game and improve the user experience which in
turn reduces the customer churn rate similarly Big Data also helped with
disaster management during Hurricane Sandy in 2012. it was used to gain a better understanding of the storm's
effect on the east coast of the U.S and necessary measures were taken it could predict the Hurricane's landfall five
days in advance which wasn't possible earlier these are some of the clear indications of how valuable big data can
be once it is accurately processed and analyzed today I'm going to tell you how
you can become a big data engineer now it should come as no surprise for you guys to know that organizations generate
as well as use a whole lot of data this vast volume of data is called Big Data companies use big data to draw
meaningful insights and take business decisions and Big Data Engineers are the people who can make sense out of this
enormous amount of data now let's find out who is a big data engineer a big data engineer is a professional Who
develops maintains tests and evaluates the company's Big Data infrastructure in
other words developing Data Solutions based on a company's requirements they maintain these solutions they test out
these Solutions as to the company's requirements they integrate this solution with various tools and systems
of the organization and finally they evaluate how well the solution is working to fulfill the company's
requirements next up let's have a look at the responsibilities of a big data engineer now they need to be able to
design Implement verify and maintain software systems now for the process of
ingesting data as well as processing it they need to be able to build highly scalable as well as robust systems they
need to be able to extract data from one database transform it as well as load it into another data store with the process
of ETL or the extract transform load process and they need to research as well as propose new ways to acquire data
improve the overall data quality and the efficiency of the system now to ensure that all the business requirements are
met they need to build a suitable data architecture they need to be able to integrate several programming languages
and tools to together so that they can generate a structured solution they need to build models that reduce the overall
complexity and increase the efficiency of the whole system by mining data from various sources and finally they need to
work well with other teams ones that include data Architects data analysts and data scientists next up let's have a
look at the skills required to become a big data engineer the first step is to have programming knowledge one of the
most important skills required to become a big data engineer is that of experience with programming languages
especially hands-on experience now the Big Data Solutions that organizations would want you to create will not be
possible without experience in programming languages I can even tell you an easy way through which you can get experience with programming
languages practice and more practice some of the most commonly used programming languages used for Big Data
engineering are python Java and C plus the second skill that you require is to
have in-depth knowledge about dbms and SQL you need to know how data is maintained as well as managed in a data
base you need to know how SQL can be used to transform as well as perform actions on a database and by extension
know how to write SQL queries for any relational database Management Systems some of the commonly used database
Management systems for Big Data engineering mySQL rackle database and the Microsoft SQL Server the third skill
that you require is to have experience working with ETL and warehousing tools now you need to know how to construct as
well as use a data warehouse so that you can perform the ETL operation or the extract transform and load operations
now as a big data engineer you'll be constantly tasked with extracting unstructured data from a number of
different sources transforming it into meaningful information and loading it into other data storages databases or
data warehouses now what this is is basically aggregating unstructured data from multiple sources analyzing it so
that you can take Better Business decisions some of the tools used for this purpose are Talent IBM data stage
pentahoe and Informatica next up we have the fourth skill that you require which is knowledge about operating systems now
since most Big Data tools have unique demands such as root access to operating system functionality as well as Hardware
having a strong understanding of operating systems like Linux and Unix is absolutely mandatory some of the
operating systems used by Big Data Engineers are Unix Linux and Solaris now the fifth skill that you require to have
experience with Hadoop based analytics since Hadoop is one of the most commonly used tools when it comes to Big Data
engineering it's understood that you need to have experience with Apache Hadoop based Technologies Technologies
like hdfs Hadoop mapreduce Apache Edge base Hive and pig the sixth skill that
you require is to have worked with real-time processing Frameworks like Apache spark now as a big data engineer
you'll have to deal with vast volumes of data so for this data you need an analytics engine like spark which can be
used for large-scale real-time data processing now spark can process live streaming data from a number of
different sources like Facebook Instagram Twitter and so on it can also perform interactive analysis and data
integration and now we're at our final skill requirement which is to have experience with data mining and modeling
so as a data engineer you'll have to examine massive pre-existing data so that you can discover patterns as well
as new information with this you will create predictive models for your business so that you can make better
informed decisions some of the tools used for this are are rapidminer weka and 9. now let's talk about a big data
engineer's salary as well as other roles they can look forward to now the average salary of a big data engineer in the
United States is approximately ninety thousand dollars per year now this ranges from sixty six thousand dollars
all the way to 130 000 per annum in India the average salary is around 7
lakh rupees and ranges from 4 lakhs to 13 lakhs per annum after you've become a big data engineer some of the job roles
that you can look forward to are that of a senior Big Data engineer a business intelligence detect and a data architect
now let's talk about certifications that a big data engineer can opt for first off we have the Cloudera CCP data
engineer a cloud data certified professional data engineer possesses the skills to develop reliable and scalable
data pipelines that result in optimized data sets for a variety of workloads it is one of the industry's most demanding
Performance Based certification CCP evaluates and recognizes the candidate's Mastery of the technical skills most
sought after by employers the time limit for this exam is 240 minutes and it costs 400 next we have the IBM certified
data architect Big Data certification an IBM certified Big Data architect understands the complexity of data and
can Design Systems and models to handle different data variety including structured semi-structured unstructured
volume velocity veracity and so on a big data architect is also able to effectively address information
governance and security challenges associated with the system this exam is 75 min it's long and finally we have the
Google Cloud certified data engineer a Google Certified data engineer enables data-driven decision making by
collecting transforming and Publishing data they should also be able to leverage deploy and continuously train
pre-existing machine learning models the length of the certification exam is two hours and its registration fee is 200
now let's have a look at how simply learn can help you become a big data engineer simply learn provides the Big
Data architect Masters program this includes a number of different courses like big data Hadoop and Spark developer
Apache spark and Scala mongodb developer and administrator big data and Hadoop
administrator and so much more this course goes through 50 plus in demand skills and tools 12 plus real life
projects and the possibility of an annual average salary of 19 to 26 lakh rupees per annum it will also help you
get noticed by the top hiring companies this course will also go through some major tools like Kafka Apache spark
Flume edgebase DB Hive big map produce Java Scala and much more now why don't
you head over to simplylearn.com and get started on your journey to get certified and get ahead back in the early 2000s
there was relatively less data generated but with the rise of various social media platforms and multinational
companies across the globe the generation of data has increased by Leaps and Bounds
according to the IDC the total volume of data is expected to reach 175 zettabytes
in 2025. that's a lot of data so we can Define Big Data as massive
amount of data which cannot be stored processed and analyzed using the traditional ways let's now have a look
at the challenges with respect to Big Data the first challenge with big data is its
storage storing big data is not easy as the data generation is endless in
addition to this storing unstructured data in our traditional databases is a great challenge unstructured data refers
to data such as photographs and videos the Second Challenge was processing Big
Data data is only useful to us if it is processed and analyzed processing Big
Data consumes a lot of time due to its size and structure to overcome these challenges of big data
we have various Frameworks such as Hadoop Cassandra and Spark let us have a
quick look at Hadoop and Spark so what is Hadoop it is a framework that
stores big data in a distributed way and processes it parallely how do you think Hadoop does this
well Hadoop users are distributed file system known as Hadoop distributed file system to store Big Data if you have a
huge file your file will be broken out into smaller chunks and stored in various machines this is why it is
termed as distributed storage map reduce is the processing unit of
Hadoop here we have multiple Machines working parallely to process Big Data thanks to this technique the processing
becomes easy and fast let's now move on to spark spark is a
framework that is responsible for processing data both in batches and in real time spark is used to analyze data
across various clusters of computers now that you have understood Big Data
Hadoop and Spark let's look into few of the different job roles in this domain
career opportunities in this field are Limitless as organizations are using big data to enhance their products business
decisions and marketing effectiveness so here we will understand in depth as
to how to become a big data engineer in addition to this we will also look into the various skills required to become a
Hadoop developers Park developer and a big data architect starting off with a big data engineer
role do you know who a big data engineer is well Big Data Engineers are
professionals who develop maintain test and evaluate a company's Big Data infrastructure
they have several responsibilities firstly they are responsible for Designing and implementing software
systems they verify and maintain these systems for their ingestion and processing of data they built robust
systems extract transform load operations known as the ETL process is
carried out by Big Data Engineers they also research various new methods to obtain data and improve its quality
Big Data Engineers are also responsible for building data architectures that meet the business requirements they
provide a solution by integrating various tools and programming languages in addition to the about
responsibilities their primary responsibility is to mine data from Plenty of different sources to build
efficient business models lastly they also work closely with data Architects data scientists and data analysts those
are quite a lot of responsibilities let us now have a look at the skills required to achieve these
responsibilities as you can see on your screens we have listed the top top seven skills needed
to be possessed by a big data engineer starting off the essential skill is
programming a big data engineer needs to have hands-on experience in any one of the programming languages such as Java C
plus plus or python as a big data engineer you should also
have in-depth knowledge of bbms and SQL this is because you have to understand how data is managed and maintained in a
database hence you need to know how to write SQL queries for any rdbms systems
some of the commonly used database Management systems for Big Data engineering are MySQL Oracle database
and the Microsoft SQL Server as mentioned earlier carrying out an Eep
carrying out an ETL operation is a big data engineer's responsibility now you need to know how to construct as well as
use a data warehouse so that you can perform these CTL operations as a big data engineer you will be continuously
tasked with extracting data from various sources transforming them into meaningful information and loading it
into other data storages some of the tools used for this purpose are Talent IBM data stage pentaho and Informatica
next up we have the fourth skill that you require which is knowledge about operating systems
Big Data tools run on operating systems hence a sound understanding of Unix
Linux windows and Solaris is mandatory the fifth skill that you require is to
have experience with Hadoop based Analytics since Hadoop is one of the most commonly used Big Data engineering tools it's
understood that you need to have experience with Apache hadoop-based Technologies like hdfs mapreduce Apache
Pig Hive and Apache headspace the sixth skill that you require is to
have worked with real-time processing Frameworks like Apache spark as a big data engineer you would deal with large
volumes of data so for this you need an analytics engine like spark which can be
used for both batch and real-time processing spark can process live streaming data from several different
sources like Facebook Instagram Twitter and so on and now we are at a final skill
requirement which is to have experience with data mining and data modeling as a
big data engineer you would have to examine Master pre-existing data so that you can discover patterns and new
information this will help you create predictive models that will help you make various business business decisions
some of the tools used for this are are rapidminer Becca and nime
now let's talk about a big data engineer's salary in the US a big data engineer's average annual salary is
around 102 000 dollars per annum in India a big data engineer makes about 7
lakh rupees per annum that was all about our first role Big Data engineer moving on to our second
role we have Hadoop developer as the name suggests a Hadoop developer
looks into the coding and programming of the various Hadoop applications this job
role is more or less similar to that of a software developer
moving on to the skills required to become a Hadoop developer at first it is a knowledge of the Hadoop ecosystem a
Hadoop developer should have in-depth knowledge about the Hadoop ecosystem and its components which include headspace
pink scoop Flume Uzi Etc you should also have data modeling experience with olap
and oltp knowledge of SQL is required like a big data engineer a Hadoop
developer must also know popular tools like pentaho Informatica and talent finally you should also be well versed
in writing Pig Latin scripts and mapreduce jobs the average annual salary of a Hadoop
developer in the US is nearly 76 000 per annum in India a Hadoop developer makes
approximately 4 lakh 57 000 rupees per annum moving to our third job role we have
spark developer we saw What spark is now let's understand what a spark developer
does spark developers create spark jobs using python or Scala for data aggregation and
transformation they also write analytics code and design data processing Pipelines
a spark developer needs to have the following skills they need to know spark and its components such as spark core
spark streaming spark machine learning library Etc they should also know the scripting
languages like python or Perl just like the previous job roles a spark developer
should also have basic knowledge of SQL queries and a database structure
in addition to the above a spark developer is also expected to have a fairly good understanding of Linux and
its commands moving to the average annual salary of a
spark developer it is nearly eighty one thousand dollars per annum in the US in
India a spark developer earns nearly 5 lakh 87 000 rupees per annum
let's now move on to our final job role that is big data architect so let's
understand who are big data architect is well a big data architect is a
professional who is responsible for Designing and planning Big Data Systems they also manage large-scale development
and deployment of Hadoop applications moving on to the skills required to
become a big data architect first up the individual must have Advanced Data
Mining and data analysis skills Big Data Architects should also be able to implement and use a nosql database and
cloud computing techniques they must also have an idea of various Big Data Technologies like Hadoop
mapreduce hbase Hive and so on finally Big Data Architects must hold experience
with agile and scrum Frameworks let's now have a look at the average
annual salary of a big data architect in the United States and India in the U.S a
big data architect earns a whopping hundred and eighteen thousand dollars per annum meanwhile in India a big data
architect makes nearly 19 lakh rupees per annum these are huge numbers right
so now that we have understood the job roles of a big data engineer Hadoop developer spark developer and a big data
architect let us have a look at the companies hiring these professionals
as you all can see on your screens we have IBM Amazon American Express Netflix
Microsoft and Bosch to name a few let us now understand why big data analytics is
required with an example so all of you listen to music online isn't it here we
will take an example of Spotify which is a Swedish audio streaming platform and
see how big data analytics is used here Spotify has nearly 96 plus million users
and all these users generate a tremendous amount of data data like the
songs which are played repeatedly the numerous likes shares and the user's search history all these data can be
termed as Big Data here with respect to Spotify have you ever wondered what
Spotify does with all this big data well Spotify analyzes this big data for
suggesting songs to its users I'm sure all of you might have come across the
recommendation list which is made available able to each one of you by Spotify each one of you will have a
totally different recommendation list this is based on your likes your past history like the songs you like
listening to and your playlists this works on something known as the recommendation system recommendation
systems are nothing but data filtering tools they collect all the data and then
filter them using various algorithms this system has the ability to accurately predict what a user would
like to listen to next with the help of big data analytics this prediction helps
the users stay on the page for a longer time and by doing so Spotify engages all
its users the users don't have the need to go on searching for different songs this is because Spotify readily provides
them with a variety of songs according to their taste well this is how big data
analytics is used by Spotify now that we have understood by big data analytics is
required let us know on to our next topic that is what is big data analytics
but before that there is another term we need to understand that is big data we
all hear the term Big Data many a times but do you know what exactly Big Data means well we will understand the term
Big Data clearly right now big data is a term for data sets that cannot be
handled by traditional computers or tools due to their value volume velocity
and veracity it is defined as massive amount of data which cannot be stored
process and analyzed using various traditional methods do you know how much
data is being generated every day every second even as I talk right now there
are millions of data sources which generate data at a very rapid rate these
data sources are present across the world as you know social media sites generate a lot of data let's take an
example of Facebook Facebook generates over 500 plus terabytes of data every
day this data is mainly generated in terms of your photographs videos messages
Etc Big Data also contains data of different formats like structured data
semi-structured data and unstructured data data like your Excel sheets all fall under structured data this data has
a definite format your emails fall under semi-structured and your pictures and videos all fall under unstructured data
all this data together make up for Big Data it is very tough to store process
and analyze big data using rdbms if you have looked into our previous videos you
would know that Hadoop is the solution to this Hadoop is a framework that stores and processes Big Data it stores
big data using the distributed storage system and it processes big data using
the parallel processing method hence storing and processing big data is no more a problem using Hadoop big data in
its raw form is of no use to us we must try to derive meaningful information
from it in order to benefit from this big data do you know Amazon uses big
data to monitor its items that are in its fulfillment centers across the globe
how do you think Amazon does this well it is done by analyzing big data which
is known as big data analytics what is big data analytics in simple terms big
data analytics is defined as the process which is used to extract meaningful
information from Big Data this information could be your hidden patterns are known correlations market
trends and so on by using big data analytics there are many advantages it
can be used for better decision making to prevent fraudulent activities and many others we will look into four of
them step by step I will first start off with big data analytics which is used for risk management mint BDO which is a
Philippine banking company uses big data analytics for risk management risk management is an important aspect for
any organization especially in the field of banking risk management analysis comprises a series of measures which are
employed to prevent any sort of unauthorized activities identifying fraudulent activities with a main
concern for BDO it was difficult for the bank to identify the fraud still from a
long list of suspects BDO adopted big data analytics which helped the bank to
narrow down the entire list of suspects thus the organization was able to identify the fraudster in a very short
time this is how big data analytics is used in the field of banking for risk management let us now see how big data
analytics is used for product development and Innovations with an example all of you are aware of
Rolls-Royce cars right do you also know that they manufacture jet engines which are used across the world what is more
interesting is that they use big data analytics for developing and innovating this engine a new product is always
developed by trial and error method big data analytics is used here to analyze
if an engine design is good or bad it is also used to analyze if there can be
more scope for improvement based on the previous models and on the future demands this way big data analytics is
used in designing a product which is of higher quality using big data analytics
the company can save a lot of time if the team is struggling to arrive at the right conclusion big data can be used to
zero in on the right data which have to be studied and thus the time spent on the product development is less big data
analytics helps in quicker and better decision making in organizations the
process of selecting a course of action from various other Alternatives is known as decision making lot of organizations
take important decisions based on the data that they have data driven business decisions can make or break a company
hence it is important to analyze all the possibilities thoroughly and quickly
before making important decisions let us now try to understand this with an use
case Starbucks uses big data analytics for making important decisions they decide the location of their new outlet
using big data analytics choosing the right location is an important factor for any organization the wrong location
will not be able to attract the required amount of customers positioning of a new
outlet a few miles here or there can always make a huge difference for an outlet especially a one like Starbucks
various factors are involved in choosing the right location for a new outlet for
example parking adequacy has to be taken into consideration it would be inconvenient for people to go to a store
which has no parking facility similarly the other factors that have to be considered are the visibility of the
location the accessibility the economic factors the population of that particular location and also we would
have to look into the competition in the vicinity all these factors have to be thoroughly analyzed before making a
decision as to where the new outlet must be started without analyzing these
factors it would be impossible for us to make a wise decision using big data analytics we can consider all these
factors and analyze them quickly and thoroughly thus Starbucks makes use of
big data analytics to understand if their new location would be fruitful or not finally we will look into how big
data analytics is used to improve customer experience using an example Delta and American Airline uses big data
analytics to improve its customer experiences with the increase in global air travel it is necessary that an
airline does every everything they can in order to provide good service and experience to its customers Delta
Airlines improves its customer experience by making use of big data analytics this is done by monitoring
tweets which will give them an idea as to how their customers Journey was if the airline comes across a negative
tweet and if it is found to be the airline's fault the airline goes ahead and upgrades that particular customers
ticket when this happens the customer is able to trust the Airlines and without a
doubt the customer will choose Delta for their next journey by doing so the customer is happy and the airlines will
be able to build a good brand recognition thus we see here that by using analysis Delta Airlines was able
to improve its customer experience moving on to our next topic that is life
cycle of big data analytics here we will look into the various stages as to how data is analyzed from scratch the first
stage is the business case evaluation stage here the motive behind the analysis is identified we need to
understand why we are analyzing so that we know how to do it and what are the different parameters that have to be
looked into once this is done it is clear for us and it becomes much easier
for us to proceed with the rest after which we will look into the various data
sources from where we can gather all the data which will be required for analysis
once we get the required data we will have to see if the data that we received
is fit for analysis or not not all the data that we receive will have
meaningful information some of it will surely just be corrupt data to remove
this corrupt data we will pass this entire data through a filtering stage in
this stage all the corrupt data will be removed now we have the data minus the
corrupt data do you think our data data is now fit for analysis well it is not
we still have to figure out which data will be compatible with the tool that we
will be using for analysis if we find data which is incompatible we first extract it and then transform it to a
compatible form depending on the tool that we use in the next stage all the
data with the same Fields will be integrated this is known as the data aggregation stage the next stage which
is the analysis stage is a very important stage in the life cycle of big data analytics right here in this step
the entire process of evaluating your data using various analytical and
statistical tools to discover meaningful information is done like we have
discussed before the entire process of deriving meaningful information from data which is known as analysis is done
here in this stage the result of the data analysis stage is then graphically
communicated using tools like Tableau power bi click view this analysis result
will then be made available to different business stakeholders for various decision making this was the entire life
cycle of big data analytics we just saw how data is analyzed from scratch now we
will move on to a very important topic that is the different types of big data analytics well we have four different
types of big data analytics as you see here these are the types and below this
are the questions that each type tries to answer we have descriptive analytics which asks the question what has
happened then we have diagnostic analytics which asks why did it happen Predictive Analytics asking what will
happen and prescriptive analytics which questions by asking what is the solution
we will look into all these four one by one with an use case for each we we will
first start off with descriptive analytics as mentioned earlier descriptive analytics asks the question
what has happened it can be defined as the type that summarizes past data into
a form that can be understood by humans in this type we will look into the past
data and arrive at various conclusions for example an organization can review
its performance using descriptive analytics that is it analyzes its past
data such as Revenue over the years and arrives at a conclusion with the profit
by looking at this graph we can understand if the company is running at a profit or not thus descriptive
analytics helps us understand this easily we can simply say that descriptive analytics is used for
creating various reports for companies and also for tabulating various social
media metrics like Facebook likes tweets Etc now that we have seen what is descriptive analytics let us look into
and use case of descriptive analytics the Dow Chemical Company analyzed all
its past data using descriptive analytics and by doing so they were able to identify the underutilized space in
their facility descriptive analytics helped them in this space consolidation on the whole the company was able to
save nearly 4 million dollars annually so we now see here that descriptive
analytics not only helps us derive meaningful information from the past data but it can also help companies in
cost reduction if it is used wisely let us now move on to our next type that is
diagnostic analytics diagnostic analytics asks the question why a
particular problem has occurred as you can see it will always ask the question why did it happen it will look into the
root cause of a problem and try to understand why it has occurred diagnostic analytics makes use of
various techniques such as data mining data Discovery and drill down companies
benefit from this type of analytics because it helps them look into the root cause of a problem by doing so the next
time the same problem will not arise as the company already knows why it has happened and they will arrive at a
particular solution for it init solves bi query tool is an example of diagnostic analytics we can use Query
tool for Diagnostic analytics now that you know why diagnostic analytics is
required and what diagnostic analytics is I will run you through an example which shows where diagnostic analytics
can be used all of us shop on e-commerce sites right have you ever added items to
your cart but ended up not buying it yes all of us might have done that at some
point an organization tries to understand why its customers don't end
up buying their products although it has been added to their cuts and this understanding is done with the help of
diagnostic analytics an e-com homicide wonders why they have made few online
sales although they have had a very good marketing strategy there could have been various factors as to why this has
happened factors like the shipping fee which was too high or the page that didn't load correctly not enough payment
options available and so on all these factors are analyzed using diagnostic
analytics and the company comes to a conclusion as to why this has happened thus we see here that the root cost is
identified so that in future the same problem doesn't occur again let us now move on to the third type that is
Predictive Analytics as the name suggests Predictive Analytics makes predictions of the Future IT analyzes
the current and historical facts to make predictions about future it always asks
the question what will happen next Predictive Analytics is used with the help of artificial intelligence machine
learning data mining to analyze the data it can be used for predicting custom customer Trends market trends customer
Behavior Etc it solely works on probability it always tries to understand what can happen next with the
help of all the past and current information a company like PayPal which has 260 plus million accounts always has
the need to ensure that their online fraudulent and unauthorized activities are brought down to nil fear of constant
fraudulent activities have always been a major concern for PayPal when a
fraudulent activity occurs people lose trust in the company and this brings in
a very bad name for the brand it is inevitable that fraudulent activities
will happen in a company like PayPal which is one of the largest online payment processes in the world but
PayPal uses analytics wisely here to prevent such fraudulent activities and
to minimize them it uses Predictive Analytics to do so the organization is
able to analyze pass data which includes a customer's historical payment data a
customer's Behavior Trend and then it builds an algorithm which works on predicting what is likely to happen next
with respect to their transaction with the use of big data and algorithms the system can gauge which of the
transactions are valid and which could be potentially a fraudulent activity by
doing so PayPal is always ready with precautions that they have to take to protect all their clients against
fraudulent transactions we will now move on to our last type that is prescriptive
analytics prescriptive analytics as the name suggests always prescribes a
solution to a particular problem the problem can be something which is happening currently hence it can be
termed as the type that always asks a question what is the solution prescriptive analytics is related to
both predictive and descriptive analytics as we saw earlier descriptive analytics always asks the question what
has happened and Predictive Analytics helps you understand what can happen next with the help of artificial
intelligence and machine learning prescriptive analytics helps you arrive at the best solution for a particular
problem various business rules algorithms and computational modeling procedures are used in prescriptive
analytics let us now have a look at how and bear prescriptive analytics is used
with an example here we will understand how prescriptive analytics is used by an
airline for its profit do you know that when you book a flight ticket the price of it depends on various factors both
internal and external factors apart from taxes seed selection there are other
factors like oil prices customer demand which are all taken into consideration before the flights fare is displayed
prices change due to availability and demand holiday seasons are a time when the rates are much higher than the
normal days Seasons like Christmas and and school vacations also weakens the rates will be much higher than weekdays
another factor which determines a flight's fair is your destination depending on the place where you are
traveling to the flight Fair will be adjusted accordingly this is because there are quite a few places where the
air traffic is less and in such places the flights fare will also be less so
prescriptive analytics analyzes all these factors that are discussed and it builds an algorithm which will
automatically adjust a flight's fare by doing so the airline is able to maximize
its profit these were the four types of analytics now let us understand how we
achieve these with the use of Big Data tools our next topic will be the various
tools used in big data analytics these are few of the tools that I will be talking about today we have Hadoop
mongodb talindi Kafka Cassandra spark and storm we will look into each one of
them one by one we will first start off with Hadoop when you speak of Big Data
the first framework that comes into everyone's mind is Hadoop isn't it as I
mentioned earlier Apache Hadoop is used to store and process big data in a
distributed and parallel fashion it allows us to process data very fast Hadoop uses mapreduce big and high for
analyzing this big data Hadoop is easily one of the most famous Big Data tools
now let us move on to the next one that is mongodb mongodb is a cross-platform
document oriented database it has the ability to deal with large amount of
unstructured data processing of data which is unstructured and processing of data sets that change very frequently is
done using mongodb Talon D provides software and services for data
integration data management and cloud storage it specializes in Big Data
integration talente opens to studio is a free open source tool for processing
data easily on a big data environment Cassandra is used widely for an
effective management of large amounts of data it is similar to Hadoop in its feature of fall tolerance where data is
automatically replicated to multiple nodes Cassandra is preferred for real-time processing spark is another
tool that is used for data processing this data processing engine is developed to process data way faster than Hadoop
mapreduce this is done in a way because spark does all the processing in the
main memory of the data nodes and thus it prevents unnecessary input output
overheads with the disk whereas map reduce is disk based and hence spark
proves to be faster than Hadoop mapreduce storm is a free big data
computational system which is done in real time it is one of the easiest tools for Big Data analysis it can be used
with any programming language this feature makes storm very simple to use
finally we will look into another big data tool which is known as Kafka Kafka is a distributed streaming platform
which was developed by LinkedIn and later given to Apache software Foundation it is used to provide
real-time analytics result and it is also used for fault tolerant storage
these were few of the big data analytics tools now let us move on to our last topic for today that is big data
application domains here we will look into the various sectors where big data analytics is actively used the first
sector is e-commerce nearly 45 percent of the world is online and they create a
lot of data every second big data can be used smartly in the field of e-commerce
by predicting customer Trend forecasting demands adjusting the price and so on
online retailers have the opportunity to create better shopping experience and
generate higher sales if big data analytics is used correctly having big
data doesn't automatically lead to a better marketing strategy meaningful insights need to be derived from it in
order to make right decisions by analyzing big data we can have personalized marketing campaigns which
can result in better and higher sales in the field of Education depending on the
market requirements new courses are developed the market requirement needs to be analyzed correctly with respect to
the scope of a course and accordingly a scope needs to be developed there is no
point in developing a course which has no scope in the future hence to analyze
the market requirement and to develop new courses we use big data analytics here there are a number of uses of big
data analytics in the field of healthcare and one of it is to predict a
patient's health issue that is with the help of the their previous medical history big data analytics can determine
How likely they are to have a particular health issue in the future the example of Spotify that we saw previously showed
how big data analytics is used to provide a personalized recommendation list to all its users similarly in the
field of media and entertainment big data analytics is used to understand the demands of shows songs movies and so on
to deliver personalized recommendation lists as we saw with Spotify big data
analytics is used in the field of banking as we saw previously with a few use cases big data analytics was used
for risk management in addition to risk management it is also used to analyze a
customer's income and spend patterns and to help the bank predict if a particular
customer is going to choose any of the bank offers such as loans credit cards schemes and so on this way the bank is
able to identify the right customer who is interested in its offers it has
noticed that telecom companies have begun to embrace big data to gain profit big data analytics helps in analyzing
Network traffic and call data records it can also improve its service quality and
improve its customer experience let us now look into how big data analytics is
used by governments across the world in the field of law enforcement big data analytics can be applied to analyze all
the available data to understand crime patterns intelligent Services can use Predictive Analytics to forecast the
crime which could be committed in Durham the police department was able to reduce the crime rate using big data analytics
with the help of data police could identify whom to Target where to go when
to petrol and how to investigate crimes big data analytics helped them to
discover patterns of crime emerging in the area before we move on to the application locations let's have a quick
look at the big data market revenue forecast worldwide from 2011 to 2027. so
here's a graph in which the y-axis represents the revenue in billion US Dollars and the x-axis represents the
years as it is seen clearly from the graph big data has grown until 2019 and
statistics predict that this growth will continue even in the future this growth is made possible as numerous companies
use big data in various domains to boost their revenue we will look into few of such applications the first Big Data
application we will look into is weather forecast imagine there is a sudden storm and you're not even prepared that would
be a terrifying situation isn't it dealing with any calamities such as hurricane storms floods would be very
inconvenient if we are caught off guard the solution is to have a tool that predicts the weather of the coming days
well in advance this tool needs to be accurate and to make such a tool big data is used so how does big data help
here well it allows us to gather all the information required to predict the weather information such as the climate
change details wind direction precipitation previous weather reports and so on after all this data is
collected it becomes easier for us to spot a trend and identify what's going to happen next by analyzing all of this
big data a weather prediction engine works on this analysis it predicts the
weather of every region across the world for any given time by using such a tool
we can be well prepared to face any climate change or any natural Calamity let's take an example of a landslide and
try to understand how big data is used to tackle such a situation predicting a landslide is very difficult with just
the basic warning signs lack of this prediction can cause a huge damage to life and property this challenge was
studied by the University of Melbourne and they developed a tool which is capable of predicting a landslide this
tool predicts the boundary where a landslide is likely to occur two weeks before this magical tool works on both
big data and Applied Mathematics an accurate prediction like this which is made two weeks before can save lives and
help in relocating people in that particular region it also gives us an insight into the magnitude of the
upcoming destruction this is how big data is used in weather forecasts and in predicting any natural calamities across
the world let us now move on to our next application that is big data application in the field of media and entertainment
the media and the entertainment industry is a massive one leveraging big data
here can produce sky-high results and boost the revenue for any company let us
see the different ways in which big data is used in this industry have you ever noticed that you come across relevant
advertisements in your social media sites and in your mailboxes well this is done by analyzing all your data such as
your pre previous browsing history and your purchase data Publishers then display what you like in the form of ads
which will in turn catch your interest in looking into it next up is customer sentiment analysis customers are very
important for a company the happier the customer the greater the company's Revenue Big Data helps in gathering all
the emotions of a customer through their posts messages conversations Etc these
emotions are then analyzed to arrive at a conclusion regarding the customer satisfaction if the customer is unhappy
the company strives to do better the next time and provides their customers a better experience while purchasing an
item from an e-commerce site or while watching videos on an entertainment site you might have noticed a segment which
says most recommended list for you this list is a personalized list which is made Available to You by analyzing all
the data such as your previous watch History your subscriptions your likes and so on recommendation engine is a
tool that filters and analyzes all this data and provides you with a list that
you would most likely be interested in by doing so the site is able to retain
and engage its customer for a longer time next is customer churn analysis in
simple words customer churn happens when a customer stops a subscription with a service predicting and preventing this
is of Paramount importance to any organization by analyzing the behavioral
patterns of previously churned customers an organization can identify which of
their current customers are likely to churn by analyzing all of this data the
organization can then Implement effective programs for customer retention let us now look into an use
case of Starbucks big data is effectively used by the Starbucks app 17
million users use this app and you can imagine how much data they generate data
in the form of their coffee buying habits the the store they visit and to the time they purchase all of this data
is fed into the app so when a customer enters a new Starbucks location the
system analyzes all their data and they are provided with their preferred order this app also suggests new products to
the customer in addition to this they also provide personalized offer and discounts on special occasions moving on
to our next sector which is Healthcare it is one of the most important sectors
big data is widely used here to save lives with all the available Big Data
medical researchers are done very effectively they are performed accurately by analyzing all the previous
medical histories and new treatments and medicines are discovered cure can be
found out even for few of the incurable diseases there are cases when one
medication need not be effective for every patient hence Personal Care is
very important person care is provided to each patient depending on their past medical history and individuals medical
history along with their body parameters are analyzed and personal attention is
given to each of them as we all know Medical Treatments are not very pocket friendly every time a medical treatment
is taken the amount increases this can be reduced if readmissions are brought
down analyzing all the data precisely will deliver a long-term efficient
result which will in turn prevent a patient's readmission frequently with
globalization came an increase in the ease for infectious diseases to spread
widely based on geography and demographics Big Data helps in predicting where an outbreak of epidemic
viruses are most likely to occur an American Healthcare Company United Healthcare uses big data to detect any
online medical fraud activities such as payment of unauthorized benefits intentional misrepresentation of data
and so on the Healthcare company runs disease management programs the success
rates of these programs are predicted using big data depending on how patients
respond to it the next sector we will look into is logistics Logistics looks
into the process of transportation and storage of goods the movement of a
product from its supplier to a consumer is very important big data is used to
make this process faster and efficient the most important factor in logistics
is the time taken for the products to reach their destination to achieve minimum time sensors within the vehicle
analyze the fastest route this analysis is based on various data such as the
weather traffic the list of orders and so on by doing so the fastest route is
obtained and the delivery time is reduced capacity planning is another factor which needs to be taken into
consideration details regarding the workforce and the number of vehicles are analyzed thoroughly and each vehicle is
allocated a different route this is done as there is no need for many trucks to
travel in the same direction which will be pointless depending on the analysis of the available Workforce and resources
this decision is taken big data analytics also Finds Its use in managing
warehouses efficiently this analysis along with tracking sensors provide
information regarding the underutilized space which results in efficient resource allocation and eventually
reduces the cost customer satisfaction is important in logistics just like it
is in any other sector customer reactions are analyzed from the available data which will eventually
create an instant feedback loop a happy customer will always help the company gain more customers let us now look into
our use case of UPS as you know UPS is one of the biggest shipping company in
the world they have a huge customer database and they work on data every minute UPS uses big data to gather
different kinds of data regarding the weather the traffic jams the geography the locations and so on after collecting
all this data they analyze it to discover the best and the fastest route to the destination in addition to this
they also use big data to change the routes in real time this is how efficiently UPS leverages Big Data next
up we have a very interesting sector that is the travel and tourism sector the global tourism Market is expected to
grow in the near future big data is used in various ways in this sector let us
look into a few of them hotels can increase their revenue by adjusting the room tariffs depending on the peak
Seasons such as holiday seasons festive seasons and so on the tools industry
uses all of this data to anticipate the demand and maximize their revenue big
data is also used by Resorts and hotels to analyze various details regarding
their competitors this analysis result helps them to incorporate all the good
facilities their competitors are providing and by doing so the hotel is
able to flourish further a customer always comes back if they are offered good packages which are more than just
the basic ones looking into a customer's past travel history likes and preferences hotels can provide its
customers with personalized experiences which will interest them highly investing in an area which could be the
Hub of Tourism is very wise few countries use big data to examine the
tourism activities in their country and this in turn helps them discover new and
fruitful investment opportunities let us look into one of the best online Homestay networks Airbnb and see how big
data is used by them Airbnb undoubtedly provides its customers with the best
accommodation across the world big data is used by it to analyze the different
kinds of available properties depending on the customer's preferences the pricing the keywords previous customers
ratings and experiences Airbnb filters out the best result Big Data Works its
magic yet again now we will move on to our final sector which is the government and law enforcement sector maintaining
Law and Order is of utmost importance to any government it is a huge task by
itself Big Data plays an active role here and in addition to this it also
helps governments bring in new policies and schemes for the welfare of its citizens the police department is able
to predict criminal activities way before it happens by analyzing big data
information such as the previous crime records in a particular region this safety aspect in that region and so on
by analyzing these factors they are able to predict any activity which breaks the
law and order of the region governments are able to tackle unemployment to a
great extent by using big data by analyzing the number of students graduating every year to the number of
relevant job openings the government can have an idea of the unemployment rate in
the country and then take necessary measures to tackle it our next factor is
poverty in large countries it is difficult to analyze which area requires
attention and development big data analytics makes it easier for governments to discover such areas
poverty gradually decreases once these areas begin to develop governments have
to always be on the lookout for better development a public survey voices the
opinion of a country's citizens analyzing all the data collected from such service can help governments build
better policies and services which will benefit its citizens let us now move on
to our use case did you know that the New York Police Department uses big data
analytics to protect its citizens the department prevents and identifies Crimes by analyzing a huge amount of
data which includes fingerprints certain emails and records from previous police
investigations and so on after analyzing all of this data meaningful insights are
drawn from it which will help the police in taking the required preventive measures against crimes now when we talk
about evolution of Big Data we have known that data has evolved in last five
years like never before now in fact before going to Big Data or before understanding these Solutions
and why there is a rush towards Big Data technology and solution I would like to ask a question take a couple of minutes
and think why are organizations interested in why is there certain rush in Industry
where everyone would want to ramp up their current infrastructure or would
want to be working on Technologies which allow them to use the spectators think about it what is happening and why are
organizations interested in this this and if you think on this you will start thinking about what organizations have
been doing in past what organizations have not done and why are organizations
interested in living now before we learn on big data we can always look into internet and
check for use cases where organizations have failed to use Legacy systems or
relational databases to work on their data requirements now over in recent or
over past five years or in recent decade what has happened is organizations have
started understanding the value of data and they have decided not to ignore any
data as being uneconomical now we can talk about different platforms through which data is generated take an example
of social media like Twitter Facebook Instagram WhatsApp YouTube you have
e-commerce and various portals say eBay Amazon Flipkart alibaba.com and then you
have various Tech Giants such as Google Oracle sap Amazon Microsoft and so on so
lots of data is getting generated every day in every business sector the point here is that organizations have slowly
started realizing that they would be interested in working on all the data now the question which I asked was why
are the organizations interested in big data and some of you might have already answered or thought about that
organizations are interested in doing precise analysis or they want to work on
different formats of data such has structured unstructured semi-structured data organizations are interested in
gaining insights or finding the hidden treasure in the so-called big data and
this is the main reason where organizations are interested in big now
there are various use cases there are various use cases we can compare that organizations from past 50 or more than
50 years have been handling huge amount of data they have been working on huge volume of data but the question here is
have they worked on all the data or have they worked on some portion of it what
have they used to store this data and if they have used something to store this data what is happening what is what is
changing now when we talk about the businesses we cannot avoid talking about
the dynamism involved now any organization would want to have a solution which allows them to store data
and store huge amount of data capture it process it analyze it and also look into
the data to give more value to the data organizations have then been looking for
Solutions now let's look at some facts that can convince you or that would convince you that data is exploding and
needs your attention right 55 billion messages and 4.5 billion photos are sent
each day on WhatsApp 300 hours of video are uploaded every minute on YouTube did you guys know that YouTube is the second
largest search engine after Google every minute users send 31.25 million messages
and watch 2.77 million videos on Facebook Walmart handles more than 1
million customer transactions every hour Google 40 000 search queries are
performed on Google per second that is 3.46 million searches a day in fact you
could also say that a lot of times people when they are loading up the Google page is basically just to check
their internet connection however that is also generating data IDC reports that
by 2025 real-time data will be more than a quarter of all the data and by 2025
the volume of Digital Data will increase to 163 Zeta bytes that is we are not
even talking about gigabytes terabytes anymore we are talking about petabytes exabytes and zetabytes and zetabytes
means 10 to the power 21 bytes so this is how data has evolved now you can talk
about different companies which would want to use their data to take business decisions they would want to collect the
data store it and analyze it and that's how they would be interested in drawing insights for the business now this is
just a simple example about Facebook and what it does to work on the data now
before we go to Facebook you could always check in Google by just typing in
companies using big data and if we say companies using big data we should be
able to find a list of different companies which are using big data for different use cases there are various
sources from where you can find we could also search for solution that is Hadoop
which we'll discuss later but you could always say companies using Hadoop and
that should take you to the Wiki page which will basically help you know what
are the different companies which are using this so-called solution called Hadoop Ado okay now coming back to what
we were discussing about so organizations are interested in Big Data as we discussed in gaining insights they
would want to use the data to find hidden information which probably they
ignored earlier now take an example of rdbms what is biggest drawback
using an rdbms now you might think that rdbms is known for stability and
consistency and organizations would be interested in storing their data in Oracle or db2 or MySQL or Microsoft SQL
server and they have been doing that for many years now so what has changed now now when we talk about rdbms the first
question which I would ask is do we have access to 100 of data online in rdbms
the answer is no we would only have 10 or 20 or 30 percent of data online and
rest of the data would be archived which means that if an organization is interested in working on all the data
they would have to move the data from the archived storage to the processing layer and that would involve bandwidth
consumption now this is one of the biggest drawbacks of rdbms you do not
have access to 100 of data online in many of the cases organizations started
realizing that the data which they were ignoring as being uneconomical had
hidden value which they had never exploited I had read a presentation
somewhere which said torture the data and it will confess to anything now that's the value of data which
organizations have realized in recent past take an example of Facebook now
this shows what Facebook does with its big and we'll come to what is Big Data but
let's understand the use case now Facebook collects huge volumes of user data whether that is SMS whether that is
likes whether that is advertisements whether that is features which people
are liking or photographs or even user profiles Now by collecting this data and
providing a portal which people can use to connect Facebook is also accumulating
huge volume of data and that's way Beyond petabytes they would also be
interested in analyzing this data and one of the reasons would be they would want to personalize The Experience take
an example of personalized news feed depending on a user Behavior depending
on what a user likes what a user would want to know about they can recommend a
personalized news feed to every particular user that's just one example of what Facebook does with its data take
an example of photo tax now when you log into Facebook account you could also get suggestions on
different friends whom you would like to connect to or you would want to tag so that they could be known by others some
more examples which show how Facebook uses its data are as follows so the flashback collection of photos and posts
that receive the most comments and likes okay there was something called as I voted that was used for 2016 elections
with reminders and directions to tell users their time and place of polling also something called as safety checks
in incidents such as earthquake hurricane or mass shooting Facebook gives you safety checks now these are
some examples where Facebook is using big data and that brings us to the
question what is Big Data this was just an example where we discussed about one
company which is making use of that data which has been accumulated and it's not
only for companies which are social media area entered like Facebook where data is important take an example of IBM
take an example of JPMorgan Chase the SE example of G of G or any other
organization which is collecting huge amount of data they would all want to gather insights they would want to
analyze the data they would want to be more precise in building their services or Solutions which can take care of
their customers so what is Big Data big data is basically a term it is used to describe the data that is too large and
complex to store in traditional databases and as I gave an example it's not just about storing the data it is
also about what you can do with the data it also means that if there is a lot of
dynamism involved can you change the underlying storage and handle any kind
of data that comes in now before we get into that let's just understand data so big data is basically a term
which has been given to categorize the data if it has different characteristics
organizations would want to have the big data stored processed and then analyzed
to get whatever useful information they can get from this data now there are five V's of Big Data volume velocity
variety value velocity although these are five V's but then there are other
V's which also categorize the data as Big Data such as volatility validity
viscosity virality of data okay so these are five V's of big data and if the data
has one or all of these characteristics then it can be considered as big
including the other ways which I just mentioned so volume basically means incredible amount of data huge volumes
of data data generated every second now that could be used for batch processing that could be used for real-time stream
processing okay you might have data being generated from different kind of devices like your cell phones your
social media websites online transactions variable devices servers
and these days with iot we are also talking about data of getting generated via internet of things that is you could
have different devices which could be communicated each other you could be getting data from Radars or leaders or
even camera sensors so there is a huge volume of data which is getting generated and if we are talking about
data which has huge volume which is getting generated constantly or has been
accumulated over a period of time we would say that is big data velocity now this is one more important aspect of big
data speed with which the data is getting generated think about stock markets think about social media
websites think about online surveys or marketing campaigns or airline industry
so if the data is getting generated with a lot of speed where it becomes
difficult to capture collect process cure mine or analyze the data then we
are certainly talking about Big Data the next aspect of big data is variety now this is where we talk about structured
data semi-structured data or unstructured data and here I would like to ask a question what is the difference
when do you call the data is structured semi-structured or unstructured now
let's look at an example before we theoretically discuss about this I always would like to use some examples
let's look at a log file and let's see what is it so if I look at this log file
and if I would say what kind of data is this which is the highlighted one the
answer would be it is structured data it has specific delimiters such as space it
has data which is separated by space and if I had a hundred or thousand or
million rows which had similar kind of data I could certainly store that in a table I could have a predefined schema
to store this data so I would call the one which is highlighted as structured but if I look at this portion where I
would look at a combination of this kind of data where some data has a pattern
and some data doesn't now this is an example of semi-structured data so if I would have a predefined structure to
store this data probably the pattern of data would break the structure and if I look at all the data then I would
certainly call it unstructured data because there is no clear schema which can Define this now this is what I mean
by variety of data that is structured data which basically has a schema or has
a format which could be easily understood you have semi-structured which could be like in XML or Json or
even your Excel sheets where you could have some data which is structured and the other is unstructured and when we
talk about unstructured we are talking about absence of schema it does not have a format it does not have a schema and
it is hard to analyze which brings its own challenges the next aspect is value
now value refers to the ability to turn your data useful for business you would have a lot of data which is being
collected as we mentioned in slides right there would be lot of data wrangling or data pre-processing or
cleaning up of data happening and then finally you would want to draw value
from that data but from all the data collected what percentage of data gives
us value and if all my data can give me value then why wouldn't I use it this is
an aspect of big data right veracity now this means the quality of data billions
of dollars are lost every year by organizations because the data which was
collected was not of good quality or probably they collected a lot of data and then it was erroneous take an
example of autonomous driving projects which are happening in Europe or us
where there are car fleets which are on the road collecting data via radar
sensors and Camera sensors and when this data has to be processed to train algorithms it has realized that
sometimes the data which was collected was missing in some values might be was not appropriate or had a lot of errors
and all this process of collecting the data becomes a repetitive task because
the quality of data was not good this is just one example we can take example from healthcare industry or stock
markets or financial institutions and so on so extracting loads of data is not
useful if the data is messy or poor in quality and that basically means that
velocity is a very important V of big data now apart from velocity volume
variety velocity and value we have the other v such as viscosity how dense the
data is validity is the data still valid volatility is my data volatile or
virality is the data viral now all of these different V's categorize the data as big data now here we would like to
talk on a big data case study and we have taken an example of Google which
obviously is one of the companies which is churning and working on
YouTube data now it's actually said that if you compare one grain of sand with
one byte of data then Google is processing or Google is handling Whole
World's sand every that is the kind of data which Google is now in early 2000 and since then when
the number of Internet users started growing Google also faced a lot of pro
storing increasing user data and a using the traditional servers to manage that
now that was a challenge which Google started facing could they use traditional data server to store the
data well yes they could Right storage devices have been getting cheaper day by
day but then how much time does it take to retrieve that data what is the seek
time what is the time taken to read and process that data thousands of search queries were raised per second No Doubt
now we could say millions and billions of queries are raised per second every query read 100 MBS
data and consumed tens of billions of CPU Cycles based on these queries so the
requirement was that they wanted to have a large distributed highly fault tolerant file system large
to store to capture process huge amount of data attributed because they could not rely
just on one server even if that had multiple disks stacked up that was not an efficient Choice what would happen if
this particular machine failed what would happen if the whole server was down so they needed a distributed
storage and it distributed computing environment they needed something which can be
highly fault tolerant right so this was the requirement which Google had and the
solution which came out as a result was GFS Google file system now let's look at
how GFS works so normally in any particular Linux system or Linux server
you would have a file system you would have set of processes you would have set of files and directories which could
store the data GFS was different so to facilitate GFS we could store huge
amount of data there was an architecture an architecture which had
multiple chunk servers or you could say slave servers slave machines Master
machine was to contain metadata was to contain data about
we say metadata we are talking about information about data and then you have the chunk servers or the slave machines
which could be storing data in a distributed fashion now any client or an
API or an application which would want to read the data would first Contact the
master server and contact the machine where the master process was running and client would
place a request of reading the data or showing an interest of reading the data
internally what it is doing is it is requesting for metadata your API or an application would want to know from
where it can read the data Master server which has metadata whether that is in
Ram or disk we can discuss that later but then Master server would have the metadata and it would know which are the
chunk servers or the slave machines where the data was stored in a distributed fashion Master would respond
back with the metadata information to the client and then client could use that information to read
two days slave machines where actually the data was stored now this is what the
process or set of processes work together to make GFS So when you say chunk server we would basically have the
files getting divided into fixed size chunks now how would they get divided so
there would be some kind of Chunk size or a block size which would determine that if the file is bigger than the
pre-decided chunk size then it would be split into smaller chunks and be distributed across the chunk servers or
the slave machines if the file was smaller then it would still use one chunk or a block to get stored on the
underlying slave machines so these chunk servers or slave machines are the ones
which actually store the data on local disks as your Linux files client which
is interacting with Master for metadata and then interacting with chunk servers for redride operations would be the one
which would be externally connecting to the cluster so this is how it would look
so you have a master which would obviously be receiving some kind of heartbeats from the chunks that wants to
know their status and receive information in the form of packets which would let the master know which machines
were available for storage which machines already had data and master would build up the metadata within
itself the files would be broken down into chunks for example we can look at file one it is broken down into chunk
one and chunk two to file 2 has one chunk which is one portion of it and
then you have file to resigning on some other chunk server which also lets us know that there is some kind of Auto
replication for this file system right and the data which is getting stored in
the chunk could have a data of 64 MB now that chunk size could be changed based
on the data size but Google file system had the basic size of the chunk as 64 MB
each chunk would be replicated on multiple servers the default replication was three and that could again be
increased or decrease requirement this would also mean that if a particular slave machine or a chunk
server would die or would get killed or would crash there would never be any
data loss because a replica of data residing on the failed machine would
still be available on some other slave server chunk server or slave machine now
this helped Google to store and process huge volumes of data in a distributed Manner and does have a fault tolerant
distributed scalable storage which could allow them to store huge amount of data
now that was just one example which actually led to the solution which today
we call now when we talk about Big Data here I
would like to ask you some questions that if we were talking about the rdbms
case take an example of uh something like NASA which was working on a project
called seti search of extraterrestrial intelligence now this was a project where they were looking for a solution
to take care of their problem the problem was that they would roughly send
some waves in space capture those waves back and then analyze this data to find
if there was any extraterrestrial object in space have two options for it they could either have a huge server built
which could take care of storing the data and processing it or they could go for volunteer computing now volunteer
Computing basically means that you could have a lot of people volunteering and be
part of this project and what they would in turn do is they would be donating their RAM and storage from their
machines when they are not using it how would that happen basically download some kind some kind of patch on their
machine which would run as a screen saver and if the user is not using his machine some portion of data could be
transferred to these machines for intermittent storage and processing using lab now this sounds very
interesting and this sounds very easy however it would have its own challenges right think about security think about
integrity but those those problems are not bigger as much as is the requirement
of bandwidth and this is the same thing which happens in rdbms if you would have to move data from archived solution to
the processing layer that would consume huge amount bandwidth Big Data brings
its own challenges huge amount of data is getting generated every day now the biggest challenge is storing this huge
volume of data and especially when this data is getting generated with a lot of variety when it can have different kind
of formats where it could be viral it could be having a lot of value and nobody has looked into the veracity of
data but the primary problem would be handling this huge volume of data variety of the data would bring in
challenges of storing it in Legacy systems if processing of the data was
required now here again I would suggest you need to think what is the difference between reading a data and process
So reading might just mean bringing in the data from disk and doing some i o operations and processing would mean
reading the data probably doing some Transformations on it extracting some useful information from it and then
storing it in the same format or probably in a different format so processing this massive volume of data
is the Second Challenge organizations don't just store their big data they would eventually want to use it to
process it to gather some insights now processing and extracting insights from Big Data would take huge amount of time
unless and until there was an efficient solution handle and process this big data
securing the data that's again a concern for organizations encryption of big data is difficult to
perform if you would think about different compression mechanisms then that would also mean decompressing of
data which would also mean that you could take a hit on the CPU Cycles or on
disk usage providing user authentication for every team member now that could also be dangerous so that led to Hadoop
as a solution so big data brings its own challenges Big Data brings its own
benefits and here we have a solution which is a Hadoop now what is Hadoop it's a open
source framework for storing data and running applications on clusters of
commodity Hardware Hadoop is an open source framework and before we discuss on two main components
of Hadoop it would be good to look into the link which I was suggesting earlier that is companies using Hadoop and any
person who would be interested in learning Big Data should start somewhere here where you could list down different
companies what kind of setup they have why are they having Hadoop what kind of
processing they are doing and how are they using these so-called Hadoop clusters to process and in fact store
capture and process huge amount of data another link which I would suggest is looking at different distributions of
Hadoop any person who is interested in learning in Big Data should know about different distributions of Hadoop now in
Linux we have different Windows Centos red hat Suzy dbn in the
same way you have different distributions of Hadoop which we can
key page and this is the link which talks about products that include Apache Hadoop or derivative works and
Commercial support which basically means that Apache Hadoop the sole products
that can be called a release of Apache Hadoop come from apache.org that's an open source community and then you have
various vendor-specific distributions like Amazon web services you have cloud
data you have hot and works you have IBM's big Insight you have map R all
these are different distributions of so basically all of these vendor-specific distributions are
depending or using on core Apache Hadoop in brief we can say that these are the
vendors which take up the Apache Hadoop package it within a cluster management solution so that users who intend to use
Apache Hadoop would not have difficulties of setting up a cluster setting up a framework they could just
use a vendor-specific distribution with its cluster installation Solutions cluster management solution and easily
plan deploy install and let's rewind to the days before the
world turn digital back then minuscule amounts of data were generated at a relatively sluggish Pace all the data
was mostly documents and in the form of rows and columns storing or processing
this data wasn't much trouble as a single storage unit and processor combination would do the job but as
years passed by the internet took the World by storm giving rise to tons of data generated in a multitude of forms
and formats every microsecond semi-structured and unstructured data was available now in the form of emails
images audio and video to name a few all this data became collectively known as
Big Data although fascinating it became nearly impossible to handle this big
data and a storage unit processor combination was obviously not enough so
what was the solution multiple storage units and processors were undoubtedly the need of the hour
this concept was incorporated in the framework of Hadoop that could store and process vast amounts of any data
efficiently using a cluster of commodity Hardware Hadoop consisted of three components that were specifically
designed to work on big data in order to capitalize on data the first step is
storing it the first component of Hadoop is its storage unit the Hadoop distributed file system or hdfs
storing massive data on one computer is unfeasible hence data is distributed
amongst many computers and stored in blocks so if you have 600 megabytes of
data to be stored hdfs splits the data into multiple blocks of data that are then stored on several data nodes in the
cluster 128 megabytes is the default size of each block hence 600 megabytes
will be split into four blocks a b c and d of 128 megabytes each and the
remaining 88 megabytes in the last block e so now you might be wondering what if
one data node crashes do we lose that specific piece of data well no that's
the beauty of hdfs hdfs makes copies of the data and stores it across multiple
systems for example when block a is created it is replicated with a replication factor of 3 and stored on
different data nodes this is termed the replication method by doing so data is
not lost at any cost even if one data node crashes making hdfs fault tolerant
after storing the data successfully it needs to be processed this is where the second component of Hadoop mapreduce
comes into play in the traditional data processing method entire data would be processed on a single machine having a
single processor this consumed time and was inefficient especially when processing large volumes of a variety of
data to overcome this mapreduce splits data into parts and processes each of
them separately on different data nodes the individual results are then aggregated to give the final output
let's try to count the number of occurrences of words taking this example
first the input is split into five separate parts based on full stops the next step is the mapper phase where the
occurrence of each word is counted and allocated a number after that depending on the words similar words are shuffled
sorted and grouped following which in the reducer phase all the grouped words are given a count
finally the output is displayed by aggregating the results all this is done by writing a simple program similarly
mapreduce processes each part of Big Data individually and then sums the result at the end this improves load
balancing and saves a considerable amount of time now that we have our mapreduce job ready
it is time for us to run it on the Hadoop cluster this is done with the help of a set of resources such as RAM
Network bandwidth and CPU multiple jobs are run on Hadoop simultaneously and
each of them needs some resources to complete the tasks successfully to efficiently manage these resources we
have the third component of Hadoop which is yarn yet another resource negotiator
or yarn consists of a resource manager node Manager application master and
containers the resource manager assigns resources node managers handle the nodes
and monitor the resource usage in the node the containers hold a collection of physical resources
suppose we want to process the mapreduce job we had created first the application Master requests the container from the
node manager once the node manager gets the resources it sends them to the resource manager this way yarn processes
job requests and manages cluster resources in Hadoop in addition to these components Hadoop also has various Big
Data tools and Frameworks dedicated to managing processing and analyzing data the Hadoop ecosystem comprises several
other components like Hive Pig Apache spark Flume and scoop to name a few the
Hadoop ecosystem works together on big data management before we dive into the
technical side of Hadoop we're going to take a little detour to try to give you a visual understanding and relate it to
maybe a more life setup we're going to go to the farm in this case so we have in a farm far away I almost wish they'd
put far far away it does remind me a little bit of a Star Wars theme so we're going to look at fruit at a farm we have
Jack who harvestses grapes and then sells it in the nearby town after harvesting he stores his produce in a
storage shed or a storage room in this case which we found out though is there was a high demand for other fruits so he
started harvesting apples and oranges as well hopefully he has a couple fills with these different fruit trees growing
and he said up there and you can see that he's working hard to harvest all these different fruits but he has a problem here because there's only one of
him so he can't really do more work so what he needs to do then is hire two
more people to work with him with this harvesting is done simultaneously so instead of him trying to harvest all
this different fruit he now has two more people in there who are putting their food away and harvesting a forum now the
storage room becomes a bottleneck to store and access all the fruits in a single storage area so they can't fit
all the fruit in one place so Jack decides to distribute the storage area and give each one of them a separate
storage and you can look at this computer terms we have our people that are the processors we have our fruit
that's a data and you can see it's storing it in the different storage rooms so you can see me popping up there
getting my uh hello I want fruit basket of three grapes two apples and three oranges I'm getting ready for a
breakfast with family a little large family my family's not that large to complete the order on time all of them
work parallely with their own storage space so here we have a process of
retrieving or querying the data and you can see from the one storage space he pulls out three grapes she pulls out two
apples and then another storage room he pulls out three oranges and we complete a nice fruit basket and this solution
helps them to complete the order on time without any hassle all of them are happy they're prepared for an increase in
demand in the future so they now have this growth system where you can just keep hiring on new people they can
continue to grow and develop a very large farm so how does this story relate to Big Data and I hinted at that a
little bit earlier the limited data only one processor one storage unit was
needed I remember back in the 90s they would just upgrade the computer instead of having a small computer you would
then spend money for a huge Mainframe with all the flat flashing lights on it then the cray computers were really
massive nowadays A lot of the computers it sits on our desktop are powerful as the main frames they had back then so
it's pretty amazing how time has changed but used to be able to do everything on one computer and you had structured data
and a database you stored your structured data in so most of the time you're acquiring databases SQL queries
just think of it as a giant spreadsheet with rows and columns where everything has a very specific size and fits neatly
in that rows and columns and back in the 90s this was a nice setup you just upgraded your computer you would get
yourself a nice big sun computer or Mainframe if you had a lot of data and a lot of stuff going on and it was very
easy to do soon though the data generation increased leading to high
volume of data along with different data formats and so you can imagine in today's world this year we will generate
more data than all the previous years summed together we will generate more
data just this year than all the previous years sum together and that's the way it's been going for some time
and you can see we have a variety of data we have our structured data which is what we're you think about a database with rows and columns and easy to look
at nice spreadsheet we have our semi-structured data they have emails as an example here that would be one
example your XML your HTML web pages and we have unstructured data if you ever look through your folder on photos I
have photos that were taken on my phone with high quality I've got photos from a long time ago I got web photos low
quality so just in my pictures alone none of them are the same you know there certainly are groups of them that are
but overall there are a lot of variety in size and setup so a single processor
was not enough to process such high volume of different kinds of data as it was very time consuming you can imagine
that if you are Twitter with millions of Twitter feeds you're not going to be
able to do a query across one server there's just no way that's going to happen unless people don't mind waiting
a year to get the the history of their tweets or look something up hence we start doing multiple processors so
they're used to process high volume of data and this saved time so we're moving forward we've got multiple processors
the single storage unit became the bottlenecked due to which network overhead was generated so now you have
your network coming in and each one of these servers has to wait before it can grab the data from the single stored
unit maybe you have a SQL Server there with a nice setup or a file system going the solution was to use distributed
storage for each processor this enabled easy access to storage and access to data so this makes a lot of sense you
have multiple workers multiple storage units just like we had our storage room and the different fruit coming in your
variety you can see that nice parallel to working on a farm now we're dealing with a lot of data it's all about the
data now this method worked and there were no network overhead generated you're not getting a bottleneck
somewhere where people are just waiting for data being pulled they're being processed this is known as parallel
processing with distributed storage so parallel processing distributed storage
and you can see here the parallel processing is your different computers running the processes and distributed
storage here is a quick demo on setting up Cloudera quick start VM in case you
are interested in working on a standalone cluster you can download the
Cloudera quick start VM so you can just type in download cloud data quick start
VM and you can search for package now this can be used to set up a quick start
VM which would be a single node Cloudera based cluster so you can click on this
link and then basically based on the platform which you would be choosing to
install such as using a VM box or which version of Cloudera you would install so
here I can select a platform so I can choose virtual box and then you can click on get it now so give your details
and basically then it should allow you you to download the quick start VM which would look something like this and once
you have the zip file which is downloaded you can unzip it which can then be used to set up a single node
Cloudera cluster so once you have downloaded the zip file that would look something like this so you would have a
quick start virtual box and then a virtual boss disk now this can be used
to set up a cluster ignore these files which are related to Amazon machines and
we you don't need to have that would just have this and this can be used to set up a cloud data cluster so for this
to be set up you can click on file import Appliance and here you can choose
your quick start VM by looking into downloads quick start VM select this and
click on open now you can click on next and that shows you the specifications of
CPU Ram which we can then change later and click on import this will start
importing virtual disk image dot vmdk file into your VM box once this is done
we will have to change the specifications or machines to use two CPU cores minimum and give a little more
RAM because Cloudera quick start VM is very CPU intensive and it needs good
amount of ram so to survive I will give two CPU cores and 5gb RAM and that
should be enough for us to bring up a quick start VM which gives us a Cloudera
distribution of Hadoop in a single node cluster setup which can be used for
working learning about different distributions in Cloudera cluster working with sdfs and other Hadoop
ecosystem components let's just wait for this importing to finish and then we
will go ahead and set up a quick start VM for our practice here the importing of Appliance is done and we see clouder
a quick start machine is added to my list of machines I can click on this and
click on settings as mentioned I would like to give it more RAM and more CPU cores so click on system and here let's
increase the ram to at least five and click on processor and let's give it two
CPU cores which would at least be better than using one CPU core Network it goes
for Nat and that's fine click on OK and we would want to start this machine so
that it uses two CPU cores 5gb RAM and it should bring up my Cloudera quick
start VM now let's go ahead and start this machine which has our quick start VM it might take initially some time to
start up because internally there will be various Cloud error Services which will be starting up and those Services
need to be up for our Cloudera quick start VM to be accessible so unlike your Apache Hadoop cluster where we start our
cluster and we and we will be starting all our processes in case of Cloudera it is your
Cloudera SCM server and agents which take care of starting up of your
services and starting up of your different roles for those Services I
explained in my previous session that for a Cloudera cluster it would be these
Services let me show you that so in case of Apache cluster we start our services
that is we start our cluster by running script and then basically those scripts
will individually start the different processes on different nodes in case of
cloud era we would always have a Cloudera SCM server which would be running on one machine and then
including that machine we would have cloud and ICM agents which would on multiple machines similarly if we had
a hortonworks cluster we would have ambari server starting up on the first machine and the number agents running on
other machines so pure server component knows what are the services which are
set up what are their configurations and agents running on every node are
responsible to send heartbeats to the server receive instructions and then take care of starting and stopping off
of individual roles on different machines in case of our single node cluster setup in quick start VM we would
just have one SCM server and one ICM agent which will start on the machine which will then take care of all the
roles which need to be started for your different services so we will just wait for our machine to
come up and basically have clouded SCM server and agent running and once we have that we need to follow few steps so
that we can have the Cloudera admin console accessible which allows you to
browse the cluster look at different Services look at the roles for different services and also work with your cluster
either using command line or using the web interface that is Hue now that my
machine has come up and it all is connected to the internet which we can see here we need to do certain things so
that we can have our admin console accessible at this point of time you can
click on Terminal and check if you have access to the cluster so here type in
hostname and that shows you your host name which is quickstart.cloud data we can also type in hdfs command to see if
we have access and if my cluster is working these commands are same as you
would give them in a Apache Hadoop cluster or in any other distribution of Hadoop sometimes when your cluster is up
and you have access to the terminal it might take few seconds or few minutes before there is a connection established
between cloud and ICM server and Cloudera CM agent running in background
which takes care of your cluster I have given hdf sdfs list command which basically should show me what by default
exists on my sdfs let's just give it a couple of seconds before it shows us the
output we can also check by giving a service Cloud era SCM server status and
here it tells me that if you would want to use Cloudera Express free run this
command it needs 8GB of RAM and it gives two virtual CPU cores and it also
mentions it may take several minutes before cloud manager has I can login as root here and then give
the command service Cloudera SCM server status remember the password for root is
is clouded out so it basically says that if you would want to check the settings it is good to have Express Edition
running so we can close this my sdfs access is working fine let's close the
terminal and here we have launch Cloudera Express click on this and that
will give you that you need to give a command which is force let's copy this
command let's open a different terminal and let's give this command like this
which will then go ahead and shut down your Cloudera Based Services and then it
will restart it only after which you will be able to access your h
so let's just give it a couple of minutes before it does this and then we will have access to our admin console
here if you see it is starting the Cloudera manager server again it is waiting for Cloudera manager API then
starting the Cloudera manager agents and then configuring the deployment as per
the new settings which we have given as to use the express edition of Cloudera
once all this is done it will say the cluster has been restarted and the admin
console can be accessed by ID and password as Cloud error we'll give it a couple of more minutes and once this is
done we are ready to use our admin console now that deployment has been
configured client configurations have also been deployed and it has restarted the Cloudera Management Service
gives you an access to Quick Start admin console using username and password as
Cloud error let's try accessing it so we can open up the browser here and let's
change this to 7180 that's the default port and that shows the admin console
which is coming up now here we can log in as Cloud era Cloud error and then
let's click on login now as I said Cloudera is very CPU intensive and memory intensive so it would slow down
since we have not given enough GB Ram to our Cloud error cluster and thus it will
be advisable to stop or even remove the services which we don't need now as of
now if we look at the services all of them look in a stop status and that's
good in one way because we can then go ahead and remove the services which we will not use in the beginning and later
we can anytime add services to the cluster so for example I can click on
key value store here and then I can scroll down where it says delete to
remove this service from the admin console now anytime you are removing a particular service it will only remove
the service from the management by Cloud electric manager all the role groups under this service will be removed from
host templates so we can click on delete now if this service was depending on
some other service it would have prompted me with a message that removed the relevant services on which this
particular service depends if the service was already running then it would have given me a message that the
service has to be stopped before it can be deleted from the Cloudera admin
so this is my admin console which allows you to click on Services look at the different roles and processes which are
running for this service we anyways have access to our Cloud error cluster from the terminal using our regular sdfs or
yarn or mapred commands now I removed a service I will also remove solar which
we will not be using for the beginning but then it depends on your choice so we can here scroll down to delete it and
that says that before deleting the solar service you must remove the dependencies on this service from the configuration
of following services that is Hue now Hue is a web interface which allows you to work with your sdfs and that is
depending on this so click on configure service dependency and here we can make sure that our Hue service does not
depend on a particular service we are removing so that then we can have a clean removal of the service so I'll
click on none and I will say save changes once this is done then then we can go ahead and try removing the solar
service from our admin console which will reduce some load on my Management
console which will also allow me to work faster on my cluster now here we have
removed the dependency of hue on solar so we can click on this and then we can
delete remember I am only doing this so that my cluster becomes little lighter and I can
work on my focus services at any point of time if you want to add more services to your cluster you can anytime do that
you can fix different configuration issues like what we see here with different warning messages and here we
have these Services which are already existing now if we don't need any of the service I can click on the drop down and
click on delete again this says that's group 2 also has relevance to Hue so Hue
as a web interface also depends on scope 2. as of now we'll make it none at any point of time later you can add the
services by clicking the add service option now this is a cluster to which you have admin access and this is a
quick start VM which gives you a single node Cloudera cluster which you can use for Learning and practicing so here
we'll click on scope 2 and then we will say delete as we have configured the
dependency now and we will remove scope 2 also from the list of services which
your admin console right so once this is done we have removed three services which we did not
need we can even remove scoop as a client and if we need we can add that
later now there are various other alerts which your cloud data admin console shows and we can always fix them by
clicking on the health issues or configuration issues we can click here and see what is the health issue it is
pointing to if that is a critical one or if that can be ignored so it says there is an issue with a clock offset which
basically relates to an ntp service network time protocol which makes sure
that one or multiple machines are in the same time zone and are in sync so for
now we can click on suppress and we can just say suppress for all hosts and we
can say look into it later and confirm so now we will not have that health
issue reported that probably the ntp service and the machines might not be in
sync now that does not have an impact for our use case as of now but if we
have a Kerberos kind of setup which is for security then basically this offset
and time zone becomes important so we can ignore this message and we are still good to use the cluster cluster we also
have other configuration issues and you can click on this which might talk about the Heap size or the ram which is
available for machines it talks about zookeeper should be in odd numbers Hue
does not have a load balancer sdfs only has one data node but all of these
issues are not to be worried upon because this is a single node cluster setup so if you want to avoid all of
these warnings you can always click on suppress and you can avoid and let your
cluster be in but that's nothing to worry so we can click on cluster and basically we can
look at the services so we have removed some Services which we don't intend to use now I have also suppressed a offset
warning which is not very critical for my use case and basically I am good to start the cluster at any point of time
as I said if you would want to add services this is the actions button which you can use services we will just
say restart my cluster which will restart all the services one by one
starting from zookeeper as the first service to come up we can always click on this Arrow Mark and see what is
happening in the services what services are coming up and in which order if you have any issues you can always click on
the link next to it which will take you to the logs and we can click on close to let it happen in the background so this
will basically let my services restart one by one and my cluster will then
become completely accessible either using Hue as a web interface or quick
start terminal which allows you to give your commands now while my machines are coming up you can click on host and you
can have a look at all the hosts we have as of now only one which will also tell you how many rolls or processes are
running on this machine so that is 25 rolls it tells you what is the disk usage it tells you what is the physical
memory being used and using this host tab we can add new host to the cluster
we can check the configuration we can check all the hosts in Diagnostics you
can look at the logs which will give you access to all the logs you can even select the sources from which you would
want to have the logs or you can give the host name you can click on search you can build your own charts you can
also do the admin stuff by adding different users or enabling security using the administration tab so since we
have clicked on restart of a cluster we will slowly start seeing all the services one by one coming up starting
with zookeeper to begin with and once we have our cluster up and running whether that is showing all services in green or
in a different status we still should be able to access the service now as we saw
in Apache Hadoop cluster even here we can click on hdfs and we can access the
web UI once our stfs service is up by clicking on quick links so the service
is not yet up once it is up we should be able to see the web UI link which will
allow you to check things from sdfs web interface similarly yarn as a service
also has a web interface so as soon as the service comes up under your quick
links we will have access to the yarn UI and similarly once the service comes up
we will have access to Hue which will give you the web interface which allows
you to work with your stfs which allows you to work with your different other components within the cluster without
even using the command line tools or command line options so we will have to give it some time while the Cloudera SCM
agent on every machine start the roles which are responsible for your cluster to come up we can
always click here which tells that there are some running commands in the background which are trying to start my
go to the terminal and we can switch as hdfs user remember sdfs user is the
admin user and it does not have a password unless you have set one so you can just log in as hdfs which might ask
you for a password initially which we do not have so the best way to do this is by logging in as root where the password
is cloud error and then you can log in as hdfs so that then onwards you can
give your sdfs commands to work with your file system now since my services
are coming up right now when I try to give a sdfsdfs command it might not work
or it might also say that it is trying to connect name node which is not up yet so we will have to give it some time and
only once the name node is up we will be able to access our sdfs using commands
so this is how you can quickly set up your quick start and then you can be
working using the command line options from the terminal like what you would do in Apache Hadoop cluster you could use
the web interfaces which allow you to work with your cluster now if this usually takes more time so you will have
to give it some time before your services are up and running and for any
reason if you have issues it might require you to restart your cluster
several times in the beginning before it gets accustomed to the settings what you
have given and it starts up the services at any point of time if you have any error message then you can always go
back and look in logs and see what is happening and try starting your cluster so this is how we set up a quick start
VM and you can be using this to work with your Cloud error cluster we'll
start with the big data challenges and the first thing with the big data is you can see here we have a nice chaotic image with all these different inputs
server racks all over the place graphs being generated just about everything you can imagine and so the problems that
come up with big data is one storing it how do you store this massive amount of data and we're not talking about a
terabyte or 10 terabytes we're talking a minimum of 10 terabytes up to petabytes
of data and then the next question is processing and so the two go hand in hand because when you're storing the
data that might take up a huge amount of space or you might have a small amount of data that takes a lot of processing
and so either one will drive a series of data or processing into the Big Data
Arena so with storing data storing Big Data was a problem due to its massive volume just straight up people would
have huge backup tapes and then you'd have to go through the backup tapes for hours to go find your data and a simple
query could take days and then processing processing Big Data consumed more time and so the Hadoop came up with
a cheap way to process the data it used to be like I've had some processes that if I ran on my computer without trying
to use multiple cores and multiple threads would take years to process just a simple data analysis can get that
heavy in the data processing and so processing can be as big of a problem as the size of the data itself so Hadoop as
a solution this is a solution to Big Data and big data storage storing Big Data was a problem due to its massive
volume so we take the Hadoop file system or the hdfs and now we're able to store huge data across a large number of
machines and access it like this one file system and processing Big Data consumed more time we talked about some
processes you can't even do on your computer so would take years now the Hadoop with the map reduce processing
Big Data was faster and I'm going to add a little notation right here that's really important to note that Hadoop is
the beginning when we talk about data processing they've added new processes on top of the map reduce that even
accelerate it your spark set up and some other different functionalities but really the basis of all of it where it
all starts with the most basic concept is your Hadoop mapreduce let us now look
into the hdfs in detail in the traditional approach all the data was stored in a single Central database with
the rise of Big Data a single database was not enough for storage and I remember the old Sun computers or the
huge IBM machines with all the flashing lights now all the data on one of those can be stored on your phone it's almost
a bit of humor how much this has accelerated over the years the same thing with our rise of Big Data no
longer can you just store it on one machine no longer can you go out buy a sun computer and put it on that one Sun
computer no more can you buy a Craig machine or an Enterprise IBM server it's
not going to work you're not going to fit it all into one server the solution was to use distributed approach to store
the massive amount of data data was divided and distributed amongst many individual databases and you can see
here where we have three different databases going on so you might actually saw this in one where they divided up
the user accounts a through G so on by the letter and so the first query would
say what's the first letter of this whatever ID it was and then it would go into that database to find it so it had
a database telling it which database to look for stuff that was a long time ago and to be honest it didn't work really
well nowadays so that was a distributed database you'd have to track which database you put it in so what is hdfs
Hadoop distributed file system hdfs is a specially designed file system for storing huge data sets in commodity
hardware and commodity is an interesting term because I mentioned Enterprise versus commodity and I'll touch back
upon that it has two core components name node and data node name node is the
master Daemon there's only one active name node it manages the data nodes and stores all the metadata so it stores all
the mapping of where the data is on your Hadoop file system now the name node is usually an Enterprise machine you spend
a lot of extra money on it so you have a very solid name known machine and so then we have our data nodes our data
node data node data node we have three here the data node is the slave there can be multiple data nodes and it stores
the actual data and this is where your commodity Hardware comes in the best definition I've heard of commodity
Hardware is the cheap knockoffs this is where you buy you can buy 10 of these and you expect one of them not to work
because you know when they come in they're going to break right away so you're looking at Hardware that's not as
high-end so where you might have your main Master node is your Enterprise server then your data nodes are just as
cheap as you can get them with all the different features you need on them as said earlier name node stores the
metadata metadata gives information regarding the file location block size and so on so our metadata in the hdfs is
maintained by using two files it has the edit log and the fs image the edit log
keeps track of the recent changes made on the Hadoop file system only recent changes are tracked here the fs image
keeps track of every change made on the hdfs since the beginning now what happens when the edit log file size
increases the name node fails these are big ones so what happens we are at a log
it just keeps so big until it's too big or our main Enterprise computer that we spend all that money on so it would
break actually fails because they still fail the solution we make copies of the edit log and the fs image files so
that's pretty straightforward you just copy them over so you have both the most recent edits going on and the long term
image of your file system and then we also create a secondary name node is a node that maintains the copies of the
edit log and the fs image it combines them both to get an updated version of the fs image now the secondary name node
only came in the last oh I guess two three years where it became as part of the main system and usually your
secondary name node is also an Enterprise computer and you'll put them on separate racks so if you have three
racks of computers you would have maybe the first two racks would have the name node and the second rack would have the
secondary name node and the reason you put them on different racks is you can have a whole rack go down you could have
somebody literally trip over the power cable or the switch that goes between the racks is most common goes down well
if the switch goes down you can usually switch to the secondary name node and while you're getting your switch
replaced and replacing that Hardware because of the way the hdfs works it still is completely functional so let's
take a look at the name node we have our edit log our FS image and we have our secondary name node and you take that it
copies the edit log over and it copies the fs image and you can see right here you have all the different contents on
your main name node now also on your secondary name node and then your secondary name node will actually take
these two your edit log and your FS image and it will make a copy so you have a full FS image that contains its
current it's up to date the secondary nade node creates a periodic checkpoint of the files and then it updates the new
FS image into the name node now it used to be this all occurred on the name node before you had a secondary name node so
now you can use your secondary node to both back up everything going on the name node and it does that lifting in the back where you're combining your
edit log and bring your FS image so it's current and then you end up with a new edit log and a new you have your FS
image updated and you start a fresh edit log this process of updating happens every hour and that's how it's scheduled
you can actually change those schedules but that is the standard is to update every hour so let's we took a look at the the master node and the you know the
name node and the secondary name node let's take a look at the cluster architecture of our Hadoop file system
the hdfs cluster architecture so we have our name node in this store orders of
metadata and we have our block location so we have our FS image plus our edit log and we have the backup FS image and
edit log and then we have so you have your rack we have our switch on top remember I was talking about the switch that's the most common thing to go in
the rack is the switches and underneath the rack you have your different data nodes you have your data node one two
three four five maybe you have 10 15 on this rack you can stack them pretty high nowadays uh it used to be you'd only get
about 10 servers on there but now you see racks that contain a lot more and then you have multiple racks so we're
not talking about just one rack we also have you know rack two rack three four five six and so on until you have Rack
in so if you had a hundred data nodes we would be looking at 10 racks of 10 data nodes each and that is literally 10
commodity server computers hardware and we have a core switch which maintains
the network bandwidth and connects the name node to the data nodes so just like each rack has a switch that connects all
your nodes on the rack you now have core switches that can all the racks together and these also Connect into our name
node setup so now we can look up your FS image and your edit log and pull that information your metadata out so we've
looked at the architecture from the name node coming down and you have your metadata your Block locations this then
sorts it out you have your core switches which connect everything all your different racks and then each individual rack has their own switches which
connect all the different nodes and to the core switches so now let's talk about the actual data blocks what's
actually sitting on those commodity machines and so the Hadoop file system
splits massive files into small chunks these chunks are known as data blocks each file in the Hadoop file system is
stored as a data block and we have a nice picture here where it looks like a Lego if you ever played with the Legos as a kid it's a good example we just
stack that data right on top of each other but each block has to be the same symmetry has to be the same size so that
it can track it easily and the default size of one data block is usually 128 megabytes now you can go in and change
that this standard is pretty solid as far as most data is concerned when we're
loading up huge amounts of data and there's certainly reasons to change it the 128 megabytes is pretty standard block so why 128 megabytes if the block
size is smaller then there will be two mini data blocks along with lots of metadata which will create overhead so
that's why you don't really want to go smaller on these data blocks unless you have a very certain kind of data similarly if the block size is very
large then the processing time for each block increases then as I pointed out
earlier each block is the same size just like your Lego blocks are all the same but the last block can be the same size
or less so you might only be storing 100 megabytes in the last block and you can
think of this as if you had a terabyte of data that you're storing on here it's not going to be exactly divided into 128
megabytes we just store all of it 128 megabytes except for the last one which could have anywhere between 1 and 128
megabytes depending on how evenly your data is divided now let's look into how files are stored in Hadoop file system
so we have a file a text let's see it's 520 megabytes we have block a so we take 128 megabytes out of the 520 and we
store it in block a and then we have Block B again we're taking 128 megabytes out of our 520 storing it there and so
on Block C block D and then block e we only have eight megabytes left so when
you add up 128 plus 128 plus 128 plus 128. you only get 512 and so at last
eight megabytes goes into its own block the final block uses only the remaining space for storage data node failure and
replication and this is really where Hadoop shines this is what makes it this is why you can use it with commodity
computers this is why you can have multiple racks and have something go down all the data blocks are stored in
various data nodes you take each block you store it at 128 megabytes and then we're going to put it on different nodes
so here's our block a Block B Block C from our last example and we have node one node two node 3 node four node 5
node 6. and so each one of these represents a different computer it literally splits the data up into
different machines so what happens if node 5 crashes well that's a big deal I mean we might not even have just node
five you might have a whole rack go down and if you're a company that's building your whole business off of that you're
going to lose a lot of money so what does happen when node 5 crashes or the first rack goes down the data stored in
node 5 will be unavailable as there's no copy stored elsewhere in this particular image so the Hadoop file system
overcomes the issue of data node failure by creating copies of the data this is known as the replication method and you
can see here we have our six nodes here's our block a but instead of storing it just on the first machine
we're actually going to store it on the second and fourth notes so now it's spread across three different computers
and in this if these are on a rack one of these is always on a different rack
so you might have two copies on the same rack but you never have all three on the same rack and you always have you never
have more than two copies on one node there's no reason to have more than one copy per node and you can see we do the
same thing with Block B Block C is then also spread out across the different machines and same with block D and
blocky node 5 crashes will the data blocks b d and e be lost well in this
example no because we have backups of all three of those on different machines the blocks have their copies and the
other nodes due to which the data is not lost even if the node 5 crashes and again because they're also stored on
different racks even if the whole rack goes down you are still up and live with your Hadoop file system the default
replication factor is three and total we'll have three copies of each data block now that can be changed for
different reasons or purposes but you got to remember when you're looking at a data center this is all in one huge room
these switches are connecting all these servers so they can Shuffle the data back and forth really quick and that is
very important when you're dealing with big data and you can see here each block is by default replicated three times
that's the standard there is very very occasions to do four and there's even fewer reasons to do two blocks I've only
seen four used once and it was because they had two data centers and so each
data center kept two different copies rack awareness in the Hadoop file system rack is a collection of 30 to 40 data
nodes rack awareness is a concept that helps to decide where the replica of the data block should be stored so here we
have rack one we have our data node one to four remember I was saying they used to be you'd only put 10 machines and
then it went to 20 now it's 30 to 40. so you can have a rack with 40 servers on it then we have rack 2 and rack three
and we put block one on there replicas a block a cannot be in the same rack and
so I'll put the replicas onto a different rack and notice that these are actually these two are on the same rack
but you'll never have all three stored on the same Rack in case the whole rack goes down and replicas of block a are
created in rack two and they actually do they do by default create the replicas onto the same rack and that has to do
with the data exchange and maximizing your processing time and then we have of course our Block B and it's replicated
onto rack 3 and Block C which will then replicate onto rack one and so on for
all of your data all the way up to block D or whatever how much every data you have on there hdfs architecture so let's
look at where the architecture is a bigger picture we looked at the name node and we store some metadata names
replicas home food data three so it has all your different info your metadata
stored on there and then we have our data nodes and you can see our data nodes are each on different racks with
our different machines and we have our name node and you're going to see we have a heartbeat or pulse here and a lot
of times one of the things that confuses people sometimes in class is they talk about nodes versus machines so you could
have a data node that's a Hadoop data node and you could also have a spark node on there Sparks a different
architecture and these are each daemons that are running on these computers that's why you refer to them as nodes
and not just always as server servers and machines so even though I use them interchangeable be aware that these
nodes you can even have virtual machines if you're testing something out it doesn't make sense to have 10 virtual
nodes on one machine and deploy it because you might as well just run your code on the machine and so we have our heartbeat going on here and the
heartbeat is a signal that data nodes continuously send to the name nodes this
signal shows the status of the data node so there's a continual pulse going up and saying hey I'm here I'm ready for
whatever instructions or data you want to send me and you can see here we've divided up into rack one and rack two
and our different data nodes and it also have the replications we talked about how to replicate data and replicates it
in three different locations and then we have a client machine and the client first requests the name node to read the
data now if you're not familiar with the client machine the client is you the programmer the client is you've logged
in external to this Hadoop file system and you're sending it instructions and
so the client whatever instructions or script you're saying ending is first request the name node to read the data
the name node allows a client to read the requested data from the data nodes the data is read from the data nodes and
sent to the client and so you can see here that basically the the name node connects the client up and says here
here's a data stream and now you have the query that you've sent out returning the data you asked for and then of
course it goes and finishes it and says oh metadata operations and it goes in there and finalizes your request the
other thing the name node does is you're sending your information because the client sends the information in there is your block operations so your block
operations are performs creation of data so you're going to create new files and folders you're going to delete the
folders and also it covers the replication of the folders which goes on in the background and so we can see here
that we have a nice full picture you can see where the client machine comes in it cues the metadata the metadata goes into
it stores the metadata and then it goes into block operations and maybe you're sending the data to the Hadoop file so
system maybe you're querying maybe you're asking it to delete if you're sending data in there it then does the
replication on there it goes back so you have your data client which is writing the data into the data node and of
course replicating it and that's all part of the block operations and so let's talk a little bit about read mechanisms in the Hadoop file system the
Hadoop file system read mechanism we have our Hadoop file system client that's you on your computer and the
client jvm on the client node and so we have our client jvm or the Java virtual
machine that is going through and then your client node and we're zooming in on the read mechanism so we're looking at
this picture here as you can guess your client is reading the data and I also have another client down here writing
data we're going to look a little closer at that and so we have our name node up here we have our racks of your data
nodes and your racks of computers down here and so our client the first thing it does is it opens a connection up with
the distributed file system to the hdfs and it goes hey can I get the Block locations and so it goes by using the
RPC see remote procedure call it gets those locations and the name node first
checks if the client is authorized to access the requested file and if yes it then provides a block location and a
token to the client which is shown to the slave for authentication so here's the name node it tells a client hey
here's the token the client's going to come in and get this information from you and it tells the client oh hey here's where the information is so this
is what is telling your script you sent to query your data whether you're writing your script in one of the many
setups that you have available through the Hadoop file system or a connection through your code and so you have your
client machine at this point then reads so your FS data input stream comes
through and you have and you can see right here we did one and two which is verify who you are and give you all the information you need then three you're
going to read it from the input stream and then the input stream is going to grab it from the different nodes where it's at and it'll Supply the tokens to
those machines saying hey this client needs this data here's a token for that we're good to go let me have the data
the client will show the Authentication token to the data nodes for the read process to begin so after reaching the
end of the data block the connection is closed and we can see here where we've gone through the different steps get
Block locations we have step one you open up your connection you get the Block locations by using the RPC then
you actively go through the fs data input stream to grab all those different data brings it back into the client and
then once it's done it closes down that connection and then once the client or
in this case the programmer you know manager's gone in and pig script you can do that there's an actual coding in each
in Hadoop or pulling data called pig or Hive once you get that data back we close the connection delete all those
randomly huge series of tokens so they can't be used anymore and then it's done with that query and we'll go ahead and
zoom in just a little bit more here and let's look at this even a little closer here's our Hadoop file system client and
our client jvm our Java virtual machine on the client node and we have the data to be read block a Block B and so we
request to read block A and B it goes into the name node two then it sends the location in this case IP addresses of
the blocks for the dn1 and dn2 where those blocks are stored then the client interacts with the data nodes through
the switches and so you have here the core switch so your client node comes in three and it goes to the core switch and
that then goes to rec switch one rack switch 2 and rack switch 3. now if you're looking at this you'll automatically see a point of failure in
the core switch and certainly you want a high-end switch mechanism for your core switch you want to use Enterprise
hardware for that and then when you get to the racks that's all commodity all your rack switches so one of those goes
down you don't care as much you just have to get in there and swap it in and out really quick you can see here we have block a which is replicated three
times and so is Block B and it will pull from there so we come in here and here the data is read from the dn1 and dn2 as
they are the closest to each other and so you can see here that it's not going to read from two different racks it's
going to read from one rack whatever the closest setup is for that query the reason for this is if you have 10 other
queries going on you want this one to pull all the data through one setup and it minimizes that traffic so the
response from the data nodes to the client that read the operation was successful it says Ah we've read the data we're successful which is always
good we like to be successful I don't know about you I like to be successful so if we look at the read mechanism
let's go ahead and zoom in and look at the right mechanism for the Hadoop file system sorry hdfs rate mechanism and so
when we have the hdfs write mechanism here's our client machine this is again the programmer on their in computer and
it's going through the client Java machine or Java virtual machine the jvm this is all occurring on the client node
somebody's office or maybe it's on the local server for the office so we have our name node we have data nodes and we
have the distributed file system so the client first executes create file on distributed file system says hey I'm
going to create this file over here then it goes through the RPC call just like our read did The Client First executes
crate file in the distributed file system then the DFS interacts with the name node to create a file name node
then provides a location to write the data and so here we have our hdfs client and the fs data output Stream So this
time instead of the data going to the client it's coming from the client and so here the client writes the data
through the fsdata output stream keep in mind that this output stream the client could be a streaming code it could be I
mean you know I always refer to the client as just being this computer that your programmer is writing on it could
be you have your SQL Server there where your data is it's current with all your current sales and it's archiving all
that information through scoop one of the tools in the Hadoop file system it could be streaming data it could be a
connection to the stock servers and you're pulling stock data down from those servers at a regular time and
that's all controlled you can actually set that code up to be controlled old in many of the different features in the
Hadoop file system and some of the different resources you have that sit on top of it so here the client writes the
data through the fs data output state and the fs data output stream as you can see goes into right packet so it divides
it up into packets 128 megabytes and the data is written and the slave further replicates it so here's our data coming
in and then if you remember correctly that's part of the fs data setup is it tells it where to replicate it out but
the data node itself is like oh hey okay I've got the data coming in and it's also given the tokens of where to send
the replications to and then acknowledgment is sent after the required replicas are made so then that
goes back up saying hey successful I've written data made three replications on this as far as our you know going
through the pipeline of the data nodes and then that goes back and says after the data is written the client performs
the close method so the client's done and says okay I'm done here's the end of the data we're finished and after the
data is written the client performs a close method and we can see here just a quick reshape we go in there just like
we did with the read we create the connection this creates a name node which lets it know what's going on step
two step three that also includes the tokens and everything and then we go into step three where we're now writing
through the fs data output stream and that sorts it out into whatever data node it's going to go to and which also
tells it how to replicate it so that data node then sends it to other data nodes so we have a replication and of
course you finalize it and close everything up marks it complete to the name node and it deletes all those
magical tokens in the background so that they can't be reused and we can go ahead and just do this with an example the
same setups and whether you're doing a read or write they're very similar as we come in here from our client and our
name node and you can see right here we actually depicting these as the actual rack and the switch is going on and so
as the data comes in you have your request like we saw earlier it sends the location of the data nodes and this
actually turns your your IP addresses your Dynamic node connections they come
back to your client your hdfs client and at that point with the tokens it then goes into the core switch and the core
switch hey here it goes the client interacts with the data nodes through the switches and you can see here where
you're writing in block a replication or block a and a second replication of block a on the second server so blocking
is replicated on the second server and the third server at this point you now have three replications of block a and
it comes back and says it acknowledges and says hey we're done and that goes back into your Hadoop file system to the
client it says okay we're done and finally the success written to the name node and it just closes everything down
a quick recap of the Hadoop file system and the advantages and so the first one
is probably one of the all of these are huge one you have multiple data copies are available so it's very fault
tolerant whole racks can go down switches can go down even your main name
node could go down if you have a secondary name node something we didn't talk too much about is how scalable it
is as it uses distributed storage you run into there and you're oh my gosh I'm out of space or I need to do some
heavier processing let me just add another rack of computers so you can scale it up very quickly and it's a
linear scalability where it used to be if you bought a server you would have to pay a lot of money to get that the Craig
computer remember the big Craig computers coming out in the Craig computer runs 2 million a year just the
maintenance to liquid cool it that's very expensive compared to just adding more racks of computer and extending
your data center so it's very cost effective since commodity Hardware is used we're talking cheap knockoff
computers we still need your high-end Enterprise for the name node but the rest of them literally it is a tenth of
the cost of storing data on more traditional high-end computers and then the data is secure so it has a very
high-end data security and provides data security for your data so all Theory and
no play doesn't make for much fun so let's go ahead and show you what it looks like as far as some of the
dimensions when you're getting into pulling data or putting data into the Hadoop file system and you can see here
here I have Oracle virtual machine virtual box manager I have a couple different things loaded on there
Cloudera is one of them so we should probably explain some of these things if you're new to Virtual machines the
Oracle virtual box allows you to spin up a machine as if it is a separate
computer so in this case this is running I believe Centos a Linux and it creates
like a box on my computer so the Centos is running on my machine while I'm running my windows 10. it happens to be
a Windows 10 computer and then underneath here I can actually go under let me just open up General might be
hard to see there and you can see I can actually go down to system processor I happen to be on an eight core it has 16
dedicated threads registers of 16 CPUs but eight cores and I've only designated this for one CPU so it's only going to
use one of my dedicated threads on my computer and this the Oracle virtual
machine is open source you can see right here we're on the Oracle www.oracle.com I usually just do a
search for downloading virtualbox if you search for virtualbox all one word it
will come up with this page and then you can download it for whatever operating system you're working with there certainly are a number of different
options and let me go and point those out if you're setting this up as a for demoing for yourself the first thing to
note for doing a virtual box and doing a Cloudera or Horton setup on that virtual
box for doing the Hadoop system to try it out you need a minimum of 12 gigabytes it cannot be a Windows 10 home
edition because you'll have problems with your virtual setup and sometimes you have to go turn on the virtual
settings so it knows it's in there so if you're on a home setup there are other sources there's Cloudera and we'll talk
a little bit about Cloudera here in just a second but they have the Cloudera online live we can go try the Cloudera
setup I've never used it but Cloudera is a pretty good company Cloudera and hortonworks are two of the common ones
out there and we'll actually be running a Cloudera Hadoop cluster on on our demo here so you have Oracle virtual machine
you also have the option of doing it on different VMware that's another one like virtual machine this is more of a paid
service there is a free setup for just doing it for yourself which will work fine for this and then in the Cloudera
like again they have the new online setup where you can go in there to the online and for Cloudera you want to go
underneath the Cloudera quick start if you type in a search for Cloudera quick start it'll bring you to this website
and then you can select your platform in this case I did virtual box there's VMware we just talked about Docker
Docker is a very high-end virtual setup unless you already know it you really don't want to mess with it then your KVM
is if you're on a Linux computer that sets up multiple systems on that computer so the two you really want to
use are usually the virtual box or do an online setup and you can see here with the download if you're going into the
Horton versions called hortonworks and they call it sandbox so you'll see the term hortonworks sandbox and these are
all test demos you're not going to deploy a single node Hadoop systems that
would just be kind of ridiculous and defeat the whole purpose of having a Horton or having a Hadoop system if it's only installed on one computer in a
virtual node so a lot of different options if you're not on a professional Windows version or you don't have at
least 12 gigabytes Ram to run this uh you'll want to try and see if you can find an online version and of course
simply learn has our own Labs if you sign up for our classes we set you up I don't know what it is now but last time
I was in there was a five node setup so you could get around and see what's going on whether you're studying for the
admin side or for the programming side in script writing and if I go into my
Oracle virtual box and I go under my Cloudera and I start this up and each one has their own flavors Horton uses
just a login so you log everything in through a local host through your Internet Explorer or like you use Chrome
Cloudera actually opens up a full interface so you actually are in that setup and you can see when I start it
let me go back here once I downloaded this this is a big download by the way I had to import The Appliance in
virtualbox the first time I ran it it takes a long time to configure the setup
and the second time it comes up pretty quick and with the cloud Dera quick start again this is a pretend single
node it opens up and you'll see that it actually has Firefox here so here's my web browser I don't have to go to a
local host I'm actually already in the quick start for Cloudera and if we come down here you can see getting started I
have some information analyze your data manage your cluster your general information on there and what I always
want to start to do is to go ahead and open up a terminal window so open up a terminal widen this a little bit let me
just maximize this out here so you can see so we are now in a virtual machine this virtual machine is Centos Linux so
I'm on a Linux computer on my Windows computer and so when I'm on this terminal window this is your basic
terminal if I do list you'll see documents eclipse these are the different things that are installed with
the quick start guide on the Linux system so this is a Linux computer and then Hadoop is running on here so now I
have Hadoop single node so it has both the name node and the data node and everything squished together in one
virtual machine we can then do let's do hdfs telling it that it's a Hadoop file
system DFS minus LS now notice the ls is the same I have LS for list and LS for
list and I click on here it'll take you just a second reading the Hadoop file system and it comes up with nothing so a
quick recap let's go back over this three different environments I have this one out here let's just put this in a
bright red so you can actually see it I have this environment out here which is my slides I have this environment here
where I did a list that's looking at the files on the Linux Centos computer and
then we have this system here which is looking at the files on the Hadoop file
system so three complete separate environments and then we connect them and so right now we have I have whatever
files I have my personal files and the um of course we're also looking at the screen for my Windows 10 and then we're
looking at the screen here here's our list there let's look at the files and this is the screen for Centos Linux and
then this is looking at the files right here for the Hadoop file system so three completely separate files this one here
which is the Linux is running in a virtual box so this is a virtual box I'm using one core to run it or one CPU and
everything in there is it has its own file system you can see we have our desktop and documents and whatever in
there and then you can see here we right now have no files and our Hadoop file system and this Hadoop file system
currently is stored on the Linux machine but it could be stored across 10 Linux machines 20 100 this could be stored
across in petabytes I mean it could be really huge or it could just be in this case just a demo where we're putting it
on just one computer and then once we're in in here let me just see real quick if I can go under view zoom in view zoom in
this is just a standard browser so I could use any like the Control Plus and stuff like that to zoom in and this is
very common to be in a browser window with the Hadoop file system so right now I'm in a Linux and I'm going to do oh
let's just create a file go file my new file and I'm going to use VI and this is
a VI editor just a basic editor and we go ahead and type something in here one two three four maybe it's columns 44 66
77 of course I do file system just like your regular computer can also so in our
value editor you hit your colon I actually work with a lot of different other editors and we'll write quit VI so
let's take a look and see what happened here I'm in my Linux system I type in LS
for list and we should see my new file and sure enough we do over here there it
is let me just highlight that my new file and if I then go into the Hadoop system hdfs
EFS minus LS for list we still show nothing it's still empty so what I can
simply do is I can go up hdfs DFS minus put and then we're going to put my new
file and this is just going to move it from the Linux system because I'm in this file folder into the Hadoop file
system and now if we go in and we type in our list for a new file system you will see in here that I now have just
the one file on there which is my new file and very similar to Linux we can do
cat and the cat command simply evokes reading the file so hdfs DFS minus cat
and I had to look it up remember Cloudera the the format is going to be a
user then of course the path location and the file name and in here when we
did the list here's our list so you can see it lists our file here and we realize that this is under a user Cloud
Dara and so I can now go user Cloudera my new file and the minus cat and we'll
be able to read the file in here and you can see right here this is the file that was in the Linux system is now copied
into the Cloudera system and it's one three four five that what I entered in there and if we go back to the Linux and
do list you'll still see it in here my new file and we can also do something like this in our hdfs minus MV I will do
my new file and we're going to change it to my new new file and if we do that
underneath our Hadoop file system the minus MV will rename it so if I go back
here to our Hadoop file system LS you'll now see instead of my new file it has my new new file coming up and there it is
my new new files we've renamed it we can also go in here and delete this so I can
now come in here so in our hdfs DFS we can also do a remove and this will
remove the file and so if we come in here we run this we'll see that when I come back and do the list the file is
gone and now we just have another empty folder with our Hadoop file system and just like any file system we can take
this and we can go ahead and make directory create a new directory so MK for make directory we'll call this my
dur so we're going to make a directory reminder it'll take it just a second and of course if we do the list command
you'll see that we now have the directory in there give it just a second there it comes myder and just like we
did before I can go in here and we're going to put the file and if you remember correctly from our files in the
setup I called it my new file so this is coming from the Linux system and we're
going to put that into my dir that's the Target in my Hadoop setup and so if I
hit enter on there I can now do the Hadoop list and that's not going to show the files remember I put it in a
subfolder so if I do the quadrupt just this will show my directory and I can do list and then I can can do my der for my
directory and you'll see underneath the my directory in the Hadoop file system it now has my new file put in there and
with any good operating system we need a minus help so just like you can type in help in your Linux you can now come in
here and type in hdfs help and it shows you a lot of the commands in there underneath the Hadoop file system most
of them should be very similar to the Linux on here and we can also do something like this a Hadoop version and
the Hadoop version shows up that we're in Hadoop 2.60 CDH is it where Cloudera 5 and compiled
by Jenkins and it has a date and all the different information on our Hadoop file system so this is some basics in the
terminal window let me go ahead and close this out because if you're going to play with this you should really come in here let me just maximize the
Cloudera that opens up in a browser window and so once we're in here again this is a browser window which you could
access might look like any access for a Hadoop file system one of the fun things to do when you're first starting is to
go under Hue you'll see it up here at the top has Cloudera Hue Hadoop near hbase your Impala your spark these are
standard installs now and Hue is basically an overview of the file system
and so come up here and you can see where you can do queries as far as if you have a hbase or a hive The Hive
database let me go over here to the top where it says file browser and if we go under file browser now this is a Hadoop
file system we're looking at and once we open up the file browser you can now see there's my directory which we created
and if I click on my directory there's my new file which is in here and if I click on my new file it actually opens
it up and you can see from our Hadoop file system this is in the Hadoop file system the file that we created so we
covered the terminal window you can see here's a terminal window up here it might be if you were in a web browser
it'll look a little different because it actually opens up as a web browser terminal window and we've looked a little bit at Hue which is one of the
most basic components of Hadoop one of the original components for going through at your data and your databases
of course now they're up to the Hue four it's gone through a number of changes and you can see there's a lot of
different choices in here for other different tools in the Hadoop file system and I'll go ahead and just close
out of this and one of the cool things with the virtual uh box I can either save the machine State send the shutdown
signal or power off the machine I'll go and just power off the machine completely now suppose you have a
library that has a collection of huge number of books on each floor and you want to count the total number of books
present on each floor what would be your approach you would say I will do it myself but then don't you think that
will take a lot of time and that's obviously not an efficient way of counting the number of books in this
huge collection on every floor by yourself now there could be a different approach or an alternative to that you
could think of asking three of your friends or three of your colleagues and you could then say if each friend could
count the books on every floor then obviously that would make your work
faster and easier to count the books on every floor now this is what we mean by
parallel processing So when you say parallel processing in technical terms you're talking about using multiple
machines and each machine would be contributing its RAM and CPU cores for
processing and your data would be processed on multiple machines at the same time now this type of process
involves parallel processing in our case or in our library example where you
would have person one who would be taking care of books on floor 1 and Counting them person two on floor 2 then
you have someone on floor 3 and someone on floor four so every individual would
be counting the books on every floor in parallel so that reduces the time
consumed for this activity and then there should be some mechanism where all these Counts from every floor can be
aggregated so what is each person doing here each person is mapping the data of
a particular floor or you can say each person is doing a kind of activity or
basically a task on every floor and the task is counting the books on every
floor now then you could have some aggregation mechanism that could
basically reduce or summarize this total count and in terms of map reduce we
would say that's the work of reducer so when you talk about Hadoop map reduce it processes data on different node
machines now this is the whole concept of Hadoop framework right that you not only have your data stored across
machines but you would also want to process the data locally so instead of
transferring the data from one machine to other machine or bringing all the data together into some central
processing unit and then processing it you would rather have the data processed
on the machines wherever that is stored so we know in case of Hadoop cluster we would have our data stored on multiple
data nodes on their multiple disks and that is the data which needs to be processed but the requirement is that we
want to process this data as fast as possible and that could be achieved by using parallel processing now in case of
mapreduce we basically have the first phase which is your mapping phase so in
case of mapreduce programming model you basically have two phases one is mapping and one is reducing now who takes care
of things in mapping phase it is a mapper class and this mapper class has
the function which is provided by the developer which takes care of these individual map tasks which will work on
multiple nodes in parallel your reducer class belongs to the reducing phase so a
reducing phase basically uses a reducer class which provides a function that will Aggregate and reduce the output of
different data nodes to generate the final output now that's how your map reduce Works using mapping and then
obviously reducing now you could have some other kind of jobs which are map only jobs wherein there is no reducing
required but we are not talking about those we are talking about our requirement where we would want to
process the data using mapping and reducing especially when data is huge
when data is stored across multiple machines and you would want to process the data in parallel so when you talk
about mapreduce you could say it's a programming model you could say internally it's a processing engine of
Hadoop that allows you to process and compute huge volumes of data and when we
say huge volumes of data we can talk about terabytes we can talk about petabytes Excel bytes and that amount of
data which needs to be processed on a huge cluster we could also use mapreduce programming model and run a mapreduce
algorithm in a local mode but what does that mean if you would go for a local mode it basically means it would do all
the mapping and reducing on the same node using the processing capacity that
is RAM and CPU cores on the same machine which is not really efficient in fact we
would want to have a reduce work on multiple nodes which would obviously have mapping phase
followed by a reducing phase and intermittently there would be data generated there would be different other
phases which help this whole processing so when you talk about Hadoop map reduce you are mainly talking about two main
components or two main phases that is mapping and reducing mapping taking care of map tasks reducing taking care of
reduced tasks so you would have your data which would be stored on multiple
machines now when we talk about data data could be in different formats we could or the developer could specify
what is the format which needs to be used to understand the data which is coming in that data then goes through
the mapping internally there would be some shuffling sorting and then reducing
which gives you your final output so the way we Access Data from sdfs or the way
our data is getting stored on sdfs we have our input data which would have one or multiple files in one or multiple
directories and your final output is also stored on sdfs to be accessed to be
looked into and to see if the processing was done correctly so this is how it looks so you have the input data which
would then be worked upon by multiple map tasks now how many map tasks that
basically depends on the file that depends on the input format so normally we know that in a Hadoop cluster you
would have a file which is broken down into blocks depending on its size so the default block size is 128 MB which can
then still be customized based on your average size of data which is getting stored on the cluster so if I have
really huge files which are getting stored on the cluster I would certainly set a higher block size so that my every
file does not have huge number of blocks creating a load on name nodes Ram
because that's tracking the number of elements in your cluster or number of objects in your cluster so depending on
your file size your file would be split into multiple chunks and for every Chunk
we would have a map task running now what is this map task doing that is specified within the mapper class so
within the mapper class you have the mapper function which basically says what each of these map tasks has to do
on each of the chunks which has to be processed this data intermittently is written to hdfs where it is sorted and
Shop fault and then you have internal phases such as partitioner which which decides how many reduce Stars would be
used or what data goes to which user you could also have a combiner phase which is like a mini reducer doing the same
reduce operation before it reaches so you have your reducing phase which is
taken care by a radio turnally the reducer function provided by developers which would have reduced
task running on the data which comes as an output from map tasks finally your
output is then generated which is stored on hdfs now in case of Hadoop it accepts
data in different formats your data could be in compressed format your data could be in bar k your data could be in
Avro text CSV PSV binary format and all of these formats are supported however
remember if you are talking about data being compressed then you have to also look into what kind of splitability the
compression mechanism supports otherwise when mapreduce processing happens it would take the complete file as one
chunk to be processed so sdfs accepts input data in different formats this data is stored in sdfs and that is
basically our input which is then passing through the mapping phase now what is mapping phase doing as I said it
reads record by record depending on the input format it reads the data so we have multiple map tasks running on
multiple chunks once this data is being read this is broken down into individual elements and when I say individual
element I could say this is my list of key value pairs so your records based on
some kind of delimiter or without delimiter are broken down into individual elements and thus your math
creates key value pairs now these key value pairs are not my final output these key value pairs are basically a
list of elements which will then be subjected to further processing so you
would have internally shuffling and sorting of data so that all the relevant
key value pairs are brought together which basically benefits the processing and then you have your reducing which
Aggregates the key value pairs into set of smaller tuples or tuples as you
finally your output is getting stored in the designated directory as a list of
aggregated key value pairs which gives you your output so when we talk about mapreduce one of the key factors here is
the parallel processing which it can offer so we know that we our data is
getting stored across multiple data nodes and you would have huge volume of data which is split and randomly
distributed across data nodes and this is the data which needs to be processed and the best way would be parallel
processing so you could have your data getting stored on multiple data nodes or multiple slave nodes in each slave node
would have again one or multiple disks to process this data basically we have
to go for parallel processing approach we have to use the map the map now let's look at the mapreduce
workflow to understand how it works so basically you have your input data stored on sdfs now this is the data
which needs to be processed it is stored in input files and the processing which you want can be done on one single file
or it can be done on a directory which has multiple files you could also later have multiple outputs merged which we
achieve by using something called as chaining of mappers so here you have your data getting stored on sdfs now
input format is basically something to define the input specification and how
the input files will be split so there are various input formats now we can search for that so we can go to Google
and we can basically search for Hadoop map reduce Yahoo tutorial this is one of
the good links and if I look into this link I can search for different input formats and output formats so let's
search for input format so when we talk about input format you basically have
something to Define how input files are split so input files are split up and
read based on what input format is specified so this is a class that provides following functionality it
selects the files or other objects that should be used for input it defines the input split that break a file into tasks
provides a factory for record reader objects that read the file so there are different formats if you look in the
table here and you can see that the text input format is the default format which reads lines of a text file and each line
is considered as a record here the key is the byte offset of the line and the value is the line content it says you
can have key value input format which parses lines into key value pairs everything up to the first tab character
is the key and the remainder is the line you could also have sequence file input format which basically works on binary
format so you have input format and in the same way you can also also search for output format which takes care of
how the data is handled after the processing is done so the key value pairs provided to this output collector
are then returned to Output files the way they are written is governed by output format so it functions pretty
much like input format as described in earlier right so we could set what is
the output format to be followed and again you have text output sequence file output format null output format and so
on so these are different classes which take care of how your data is handled when it is being read for processing or
how is the data being written when the processing is done so based on the input format the file is broken down into
splits and this logically represents the data to be processed by individual map
tasks or you could say individual mapper functions so you could have one or multiple splits which need to need to be
possessed depending on the file size depending on what properties have been set now once this is done you have your
input splits which are subjected to mapping phase internally you have a record reader which communicates with
the input split and converts the data into key value pairs suitable to be read
by mapper and what is mapper doing it is basically working on these key value pairs the map task giving you an
intermittent output which would then be forwarded for further processing now
once that is done and we have these key value pairs which is being worked upon my map your map tasks as a part of your
mapper function are generating your key value pairs which are your intermediate outputs to be processed further now you
could have as I said a combiner phase or internally a mini reducer phase Now
combiner does not have its own class so combiner basically uses the same class
as the reducer class provided by the developer and its main work is to do the
reducing or its main work is to do some kind of mini aggregation on the key value pairs which were generated by map
so once the data is coming in from the combiner then we have internally a
partitioner phase which decides how outputs from combiners are sent to the
reducers or you could also say that even if I did not have a combiner partitioner
would decide based on the keys and values based on the type of keys how
many reducers would be required or how many reduced tasks would be required to
work on your output which was generated by map task now once partitioner has
decided that then your data would be then sorted and shuffled which is then
fed into the reducer so when you talk about your reducer it would basically have one or multiple reduced tasks now
that depends on what or what partitioner decided or determined for your data to
be processed it can also depend on the configuration properties which have been
set to decide how many radio Stars should be used now internally all this data is obviously going through sorting
and shuffling so that you're reducing your aggregation becomes an easier task once we have this done we basically have
the reducer which is the code for the reducer is provided by the developer and
all the intermediate data has then to be aggregated to give you a final output
which would then be stored on sdfs and who does this you have an internal record writer which writes these output
key value pairs from reducer to the output files now this is how your map reduce Works wherein the final output
data can be not only stored but then read or accessed from hdfs or even used
as an input for further mapreduce kind of processing so this is how it overall
looks so you basically have your data stored on sdfs based on input format you
have the splits then you have record reader which gives your data to the mapping phase which is then taken care
by your mapper function and mapper function basically means one or multiple map tasks working on your chunks of data
you could have a combiner phase which is all functional which is not mandatory then you have a partitional phase which
decides on how many reduced tasks or how many reducers would be used to work on your data internally there is sorting
and shuffling of data happening and then basically based on your output format
your record reader will write the output to sdfs directory now internally you
could also remember that data is being processed locally so you would have the
output of each task which is being worked upon stored locally however we do
not access the data directly from data nodes we access it from sdfs so our output is stored on sdfs so that is your
map reduce workflow when you talk about mapreduce architecture now this is how it would look so you would have
basically a edge node or a client program or an API which intends to
process some data so it submits the job to the job tracker or you can say resource manager in case of Hadoop yarn
framework right now before this step we can also say that an interaction with
name node would have already happened which would have given information of data nodes which have the relevant data
stored then your master processor so in Hadoop version one we had job tracker
and then the slaves were called task trackers in Hadoop version 2 instead of job tracker you have resource manager
answer of task trackers you have node managers so basically your resource manager has to assign the job to the
task trackers or node managers so your node managers as we discussed in yarn
are basically taking care of processing which happens on every node so internally there is all of this work
Happening by resource manager node managers and application Master then you can refer to the yarn based tutorial to
understand more about that so here your processing Master is basically breaking
down the application into tasks what it does internally is once your application
is submitted you application to be run on yarn processing framework is handled
by resource manager now forget about the yarn part as of now I mean who does the
negotiating of resources who allocates them how does the processing happen on the nodes right so that's all to do with
how yarn handles the processing request so you have your data which is stored in sdfs broken down into one or multiple
splits depending on the input format which has been specified by the developer your input splits are to be
worked upon by your one or multiple map tasks which will be running within the
container on the nodes basically you have the resources which are being utilized so for each map task you would
have some amount of ram which will be utilized and then further the same data
which has to go through reducing phase that is your reduced task will also be utilizing some RAM and CPU cores now
internally you have these functions which take care of deciding on number of reducers doing a mini reduce and
basically reading and processing the data from multiple data nodes now this is how your mapreduce programming model
makes parallel processing work or processes your data which is stored
across multiple machines finally you have your output which is getting stored on stfs
foreign
so let's have a quick demo on mapreduce and see how it works on a Hadoop cluster
now we have discussed briefly about mapreduce which contains mainly two phases that is your mapping phase and
your reducing phase and mapping phase is taken care by your mapper function and your reducing phase is taken care by
your reducer function now in between we also have sorting and shuffling and then
you have other phases which is partitioner and combiner and we will discuss about all those in detail in
later sessions but let's have a quick demo on how we can run a mapreduce which
is already existing as a package jar file within your Apache Hadoop cluster
or even in your Cloudera cluster now we can build our own mapreduce programs we
can package them as jar transfer them to the cluster and then run it on a Hadoop cluster on yarn or we could be using
already provided default program so let's see where they are now these are
my two machines which I have brought up and basically this would have my Apache Hadoop cluster running now we can just
do a Simple Start hyphen all dot SH now I know that this script is deprecated
and it says instead you start DFS and start yarn but then it will still take
care of static of my cluster on these two nodes where I would have one single
name node two data nodes one secondary name node one resource manager and two
node managers now if you have any doubt in how this cluster came up you can always look at the previous sessions
where we had a walk through in setting up a cluster on Apache and then you
could also have your cluster running using Less Than 3 GB of your total machine RAM and you could have a Apache
cluster running on your machine now once this cluster comes up we will also have a look at the web UI which is available
for name node and results manager now based on the settings what we have given our uis will show us details of our
cluster but remember the UI is only to browse now here my cluster has come up I
can just do a JPS to look at Java related processes and that will show me what are the processes which are running
on C1 which is your data node resource manager node manager and name node and
on my M1 machine which is my second machine which I have configured here I
can always do a JPS and that shows me the processes running which also means
that my cluster is up with two data nodes with two node managers and here I
can have a look at my web UI so I can just do a refresh and the same thing
with this one just do a refresh so I had already opened the web pages so you can always access the web UI using your name
node's host name and 570 Port it tells me what is my cluster ID what is my blog
post it gives you information of what is the space usage how many live nodes you have
and you can even browse your file system so I have put in a lot of data here I can click on browse the file system and
this basically shows me multiple directories and these directories have
one or multiple files which we will use for our mapreduce example now if you see here these are my directories which have
some sample files although these files are very small like 8.7 kilobytes if you look into this directory if you look
into this I have just pulled in some of my Hadoop blogs and I have put it on my sdfs these are a little bigger files and
then we also have some other data which we can see here and this is data which
I've downloaded from web now we can either run a mapreduce on a single file or in a directory which contains
multiple files let's look at that before looking a demo on map reduce also remember map reduce will create a output
directory and we need to have that directory created plus we need to have the permissions to run the mapreduce job
so by default since I'm running it using admin ID I should not have any problem but then if you intend to run mapreduce
with a different user then obviously you will have to ask the admin or you will have to give the user permission to read
and write from sdfs so this is the directory which I've created which will contain my output once the map reduce
job finishes and this is my cluster file system if you look on this UI this shows
me about my yarn which is available for taking care of any processing it as of
now shows that I have total of 8 memory and I have eight week course now that
can be depending on what configuration we have set or how many nodes are available we can look at nodes which are
available and that shows me I have two node managers running each has 8 GB
memory and 8 V cores now that's not true actually but then we have not set the
configurations for node managers and that's why it takes the default properties that is 8GB RAM and 8B cores
now this is my yarn UI we can also look at scheduler which basically shows me the different queues if they have been
configured where you will have to run the jobs we'll discuss about all these in later in detail now let's go back to
our terminal and let's see where we can find some sample applications which we can run on the cluster so once I go to
the terminal I can well submit the mapreduce job from any terminal now here
I know that my Hadoop related directories here and within Hadoop you
have various directories we have discussed that in binaries you have the commands which you can run in s bin you
basically have the startup scripts and here you also notice there is a share directory in the end if you look in the
Shell directory you would find Hadoop and within Hadoop you have various sub directories in which we will look for
mapreduce now this map reduce directory has some sample jar files which we can
use to run a mapreduce on the cluster similarly if you are working on a cloud
error cluster you would have to go into opt Cloudera parcel CDH slash lib and in
that you would have directories for hdfs mapreduce or sdfs yarn where you can
still find the same jars it is basically a package which contains your multiple applications now how do we run a map
reduce we can just type in Hadoop and hit enter and that shows me that I have an option called jar which can be used
to run a jar file now at any point of time if you would want to see what are the different classes which are
available in a particular jar you could always do a jar minus xvf
for example I could say jar x v f and I could say user local Hadoop share Hadoop
map reduce and then list down your jar file so I'll say Hadoop mapreduce
examples and if I do this this should basically unpack it to show me what
classes are available within this particular jar and it has done this it has created a meta file and it has
created a org directory we can see that by doing a LS and here if you look in LS
org since I ran the command from your phone directory I can look into org patchy Hadoop examples which shows me
the classes which I have and those classes contain which mapper or reducer
classes so it might not be just mapper and reducer but you can always have a look so for example I am targeting to
use word count program which does a word count on files and gives me a list of
words and how many times they occur in a particular file or in set of files and
this shows me that what are the classes which belong to word count so we have a in some reducer so this is my reducer
class I have tokenizer mapper that is my mapper class right and basically this is
what is used these classes are used if you run a word count now there are many other programs which are part of this
jar file and we can expand and see that so I can say Hadoop jar and give your
path so I'll say Hadoop jar user local Hadoop share Hadoop map reduce Hadoop
mapreduce examples and if I hit on enter that will show me what are the inbuilt
classes which are already available now these are certain things which we can use now there are other jar files also
for example I can look at Hadoop and here we can look at the jar files which
we have in this particular path so this is one Hadoop mapreduce examples which
you can use you can always look in other jar files like you can look for Hadoop
mapreduce client job client and then you can look at a test one so that is also
an interesting one so you can always look into Hadoop map reduce client job
client and then you have something ending with this so if I would have tried this one using my Hadoop jar
command so in my previous example when we did this it was showing me all the classes which are available and that
already has a word count now there are other good programs which you can try like terrajen to generate dummy data
Terra saw to check your sorting performance and so on and tell a validate to validate the results
similarly we can also do a Hadoop jar as I said on Hadoop mapreduce I think that
was client and then we have job client and then test chart now this has a lot
of other classes which can be used or programs which can be used for doing a stress testing or checking your cluster
status and so on one of them interesting one is there's the fsio but let's not get into all the details in first
instance let's see how we can run a map reduce now if I would want to run a map reduce I need to give Hadoop jar and
then my jar file and if I hit on enter it would say it needs the input and output it needs which class you want to
run so for example I would say word count and again if I hit on enter it tells me that you need to give me some
input and output to process and obviously this processing will be happening on cluster that is our yarn
processing framework unless and until you would want to run this job in a local mode so there is a possibility
that you can run the job in in local mode but let's first try how it runs on
the cluster so how do we do that now here I can do a hdfs LS slash command to
see what I have on my sdfs now through my UI I was already showing you that we
have set of files and directories which we can use to process now we can take up
one single file so for example if I pick up new data and M I can look into the
files here what we have and we can basically run a map reduce on a single
file or multiple files so let's take this file whatever that contains and I would like to do a word count so that I
get a list of words and their occurrence in this file so let me just copy this now I also need my output to be written
and that will be written here so here if I want to run a map reduce I can say
Hadoop which we can pull out from history so Hadoop jar word count now I need to give my input so that will be
new data and then I will give this file which we just copied now I am going to
run the word count only on a single file and I will basically have my output which will be stored in this directory
the directory which I have created already Mr output so let's do this
output and this is fair enough now you can give many other properties you can
specify how many map jobs you want to run how many reduced jobs you want to run do you want your output to be
compressed do you want your output to be merged or many other properties can be
defined when you are specifying word count and then you can pass in an argument to pass properties from the
command line which will affect your output now once I go ahead and submit this this is basically running a simple
inbuilt mapreduce job on our Hadoop cluster now obviously internally it will
be looking for name node now we have some issue here and it says the output already exists what does that mean so it
basically means that Hadoop will create an output for you you just need to give a name but then you don't need to create
it so let's give let's append the output with number one and then let's go ahead
and run this so I've submitted this command now this can also be done in background if you would want to run
multiple jobs on your cluster at the same time so it takes total input paths to process is one so that is there is
only one split on which your job has to work now it will internally try to contact your resource manager and
basically this is done so here we can have a look and we can see some counters
here now what I also see is for some property which it is missing it has run
the job but it has run in a local mode it has run in a local mode so although we have submitted so this might be
related to my yarn settings and we can check that so if I do a refresh when I
have run my application it has completed it would have created an output but the only thing is it did not interact with
your yarn it did not interact with your resource manager we can check those properties and here if we look into the
job it basically tells me that it went for mapping and reducing it would have created an output it worked on my file
but then it ran in a local mode it ran in a local mode so mapreduce remember is
a programming model right now if you run it on yarn you get the facilities of running it on a cluster where yarn takes
care of resource management if you don't run it on yarn and run it on a local mode it will use you
use RAM and CPU cores for processing but then we can quickly look at the output and then we can also try running this on
yarn so if I look into my hdfs and if I look into my output Mr output that's the
directory which was not used actually let's look into the other directory which is ending with one and that should
show me the output created by this map reduce although it ran in a local mode
it fetched an input file from your sdfs and it would have created output in sdfs
now that's my part file which is created and if you look at part minus r minus
these zeros if you would have more than one reducer running then you would have multiple such files created we can look
into this what does this file contain which should have my word count and here
I can say cat which basically shows me what is the output created by my map
reduce let's have a look into this so the file which we gave for processing has been broken down and now we have the
list of words which occur in the file plus a count of those words so if there is some word which is in is more then it
shows me the count so this is a list of my words and the count for that so this
is how we run a sample mapreduce job I will also show you how we can run it on
yeah now let's run mapreduce on yarn and initially when we tried running a map
reduce it not hit yarn but it ran in a local mode and that was because there
was a property which had to be changed in mapreduce hyphen site file so
basically if you look into this file the error was that I had given a property which says
mapred.framework.name and that was not the right property name and it was ignored and that's why it ran in a local
mode so I changed the property to mapreduce dot framework.name restarted my cluster and everything should be fine
now and that mapred hyphen site file has also been copied across the nodes now to
run a mapreduce on a Hadoop cluster so that it uses yarn and yarn takes care of
resource allocation on one or multiple machines so I'm just changing the output here and now I will submit this job
which should first connect to the resource manager and if it connects to the resource manager that means our job
will be run using yarn on the Clusters rather than in a local mode so now now
we have to wait for this application to internally connect to resource manager
and once it starts there we can always go back to the web UI and check if our
application has reached yarn so it shows me that there is one input part to be processed that's my job ID that's my
application ID which you can even monitor status from the command line now here the job has been submitted so let's
go back here and just do a Refresh on my yarn UI which should show me the new
application which is submitted it tells me that it is in accepted State application master has already started
and if you click on this link it will also give you more details of how many map and reduced tasks would run so as of
now it says the application Master is running it would be using the node which is m one we can always look into the
logs we can see that there is a one task attempt which is being made and now if I
go back to my terminal I will see that it is waiting to get some resources from the cluster and once it gets the
resources it will first start with the mapping phase where the mapper function runs it does the map tasks one or
multiple depending on the splits so right now we have one file and one split so we will have just one map task
running and once the mapping phase completes then it will get into reducing which will finally give me my output so
we can be toggling through these sessions so here I can just do a refresh to see what is happening with my
application is it proceeding is it still waiting for resource manager to allocate
some resources now just couple of minutes back I tested this application on yarn and we can see that my first
application completed successfully and here we will have to give some time so that yarn can allocate the resources now
if the resources were used by some other application they will have to be freed up now internally Yan takes care of all
that which we will learn more detail in yarn or you might have already followed the yarn based session now here we will
have to just give it some more time and let's see if my application proceeds with the resources what Yan can allocate
to it sometimes you can also see a slowness in what web UI shows up and
that can be related to the amount of memory you have allocated to your notes now for Apache we can have less amount
of memory and we can still run the cluster and as I said the memory which shows up here 16 GB and 16 cores is not
the true one those are the default settings right but then my yarn should be able to facilitate running of this
application let's just give it couple of seconds and then let's look into the output here again I had to make some
changes in the settings because our application was not getting enough resources and then basically I restarted
my cluster now let's submit the application again to the cluster which
first should contact the resource manager and then basically the map and reduce process should start so here I
have submitted an application it is connecting to the resource manager and then basically it will start internally
an app master that is application Master it is looking for the number of splits which is one it's getting the
application ID and it basically then starts running the job it also gives you
a tracking URL to look at the output and now we should go back and look at our
yarn UI if our application shows up here and we will have to give it a couple of
seconds when it can get the final status change to running and that's where my
application will be getting resources now if you closely notice here I have allocated specific amount of memory that
is 1.5 GB for node manager on every node and I have basically given two cores
each which my machines also have and my yarn should be utilizing these resources
rather than going for default now the application has started moving and we can see the progress bar here which
basically will show what is happening and if we go back to the terminal it will show that first it went in deciding
map and reduce it goes for map once the mapping phase completes then the reducing phase will come into existence
and here my job has completed so now it has basically used we can always look at
how many map and reduce SAS were run it shows me that there was one map and one reduced task now with the number of map
tasks depends on the number of splits and we had just one file which is less than 128 MB so that was one split to be
processed and reduced task is internally decided by the reducer or depending on
what kind of property has been set in Hadoop config files now it also tells me how many input records were read which
basically means these were the number of lines in the file it tells me output records which gives me the number of
total words in the file now there might be duplicates and that which is processed by internal combiner further
processing or forwarding that information to reducer and basically reducer works on 335 records gives us a
list of words and their account now if I do a refresh here this would obviously show my application is completed it says
succeeded you can always click on the application to look for more information it tells me where it ran now we do not
have a history server running as of now otherwise we can always access more information so this leads to history
server where all your applications are stored but I can click on this attempt tasks and this will basically show me
the history URL or you can always look into the logs so this is how you can submit a sample application which is
inbuilt which is available in the jar on your Hadoop cluster and that will utilize your cluster to run now you
could always as I said when you are running a particular job remember to change the output directory and if you
would not want it to be processing a single individual file you could also
point it to a directory that basically means it will have multiple files and depending on the file sizes there would
be multiple splits and according to that multiple map tasks will be selected so if I click on this this would submit my
second application to the cluster which should first connect to resource manager then resource manager has to start an
application Master now here we are targeting 10 splits now you have to sometimes give couple of seconds in your
machines so that the resources which were used are internally already freed up so that your cluster can pick it up
and then your yarn can take care of resources so right now my application is
an undefined status but then as soon as my yarn provides it the resources we
will have the application running on our yarn cluster so it has already started if you see it is going further then it
would launch 10 map tasks and it would the number of reduced tasks would be decided on either the way your data is
or based on the properties which have been set at your cluster level let's just do a quick refresh here on my yarn
UI to show me the progress also take care that when you are submitting your application you need to have the output
directory mentioned however to not create it Hadoop will create that for you now this is how you run a map reduce
without specifying properties but then you can specify more properties you can look into what are the things which can
be changed for your mapper and reducer or basically having a combiner class which can do a mini reducing and all
those things can be done so we will learn about that in the later sessions now we will compare Hadoop version one
that is with mapreduce version one we will understand and learn about the limitations of Hadoop version 1 what is
the need of yarn what is yarn what kind of workloads can be running on yarn what
are yarn components what is yarn architecture and finally we will see a demo on yarn so Hadoop version 1 or
mapreduce version one well that's outdated now and nobody is using Hadoop
version one but it would be good to understand what was in Hadoop version 1 and what were the limitations of Hadoop
version 1 which brought in the thought for the future processing layer that is
yarn now when we talk about Hadoop we already know that Hadoop is a framework
and Hadoop has two layers one is your storage layer that is your hdfs Hadoop
distributed file system which allows for distribute grid storage and processing which allows fault tolerance by inbuilt
replication and which basically allows you to store huge amount of data across multiple commodity machines when we talk
about processing we know that map reduce is the oldest and the most mature
processing programming model which basically takes care of your data processing on your distributed file
system so in Hadoop version 1 mapreduce performed both data processing and
resource management and that's how it was problematic in mapreduce we had
basically when we talk about the processing layer we had the master which was called job tracker and then you had
the slaves which were the task records so your job tracker was taking care of
allocating resources it was performing scheduling and even monitoring the jobs it basically was taking care of
assigning map and reduced tasks to the jobs running on task trackers and tasks
crackers which were co-located with data nodes who are responsible for processing
the jobs so task trackers were the slaves for the processing layer which reported their progress to the job
tracker so this is what was happening in Hadoop version one now when we talk about Hadoop version 1 we would have say
client machines or an API or an application which basically submits the job to the master that is job tracker
now obviously we cannot forget that there would already be an involvement from name node which basically tells
which are the machines or which are the data nodes where the data is already stored now once the job submission
happens to the job tracker job tracker being the master demon for taking care
of your processing request and also resource management job scheduling would
then be interacting with your multiple task trackers which would be running on
multiple machines so each machine would have a task tracker running and that
task tracker which is a processing slave would be co-located with the data nodes now we know that in case of Hadoop you
have the concept of moving the processing to wherever the data is stored rather than moving the data to
the processing layer so we would have task trackers which would be running on multiple machines and these Stars
trackers would be responsible for handling the tasks what are these tasks these are the application which is
broken down into smaller tasks which would work on the data which is
respectively stored on that particular node now these were your slave demons right so your job tracker was not only
tracking the resources so your task trackers were sending heartbeats they
were sending in packets and information to the job tracker which would then be
knowing how many resources and when we talk about resources we are talking about the CPU course we are talking
about the ram which would be available on every node so task trackers would be
sending in their resource information to job tracker and your job tracker would be already aware of what amount of
resources are available on a particular node how loaded a particular node is
what kind of work could be given to the task tracker so job tracker was taking care of resource management and it was
also breaking the application into tasks and doing the job scheduling part assign
different tasks to these slave domains that is your task trackers so job
tracker was eventually overburdened right because it was managing jobs it
was tracking the resources from multiple task trackers and basically it was taking care of job scheduling so job
tracker would be overburdened and in a case if job tracker would fail then it
would affect the overall processing so if the master is skilled if the master demand dies then the processing cannot
proceed now this was one of the limitations of Hadoop person one 1 so when you talk about scalability that is
the capability to scale due to a single job tracker scalability would be hitting
a bottleneck you cannot have a cluster size of more than 4000 nodes and cannot
run more than 40 000 concurrent tasks now that's just a number we could always look into the individual resources which
each machine was having and then we can come up with an appropriate number however with a single job tracker there
was no horizontal scalability for the processing layer because we had single
processing Master now when we talk about availability job tracker as I mentioned would be a single point of failure now
any failure kills all the queued and running jobs and jobs would have to be resubmitted now why would we want that
in a distributed platform in a cluster which has hundreds and thousands of machines we would want a processing
layer which can handle huge amount of processing which could be more scalable which could be be more available and
could handle different kind of workloads when it comes to resource utilization now if you would have a predefined
number of map and reduced slots for each task tracker you would have issues which would relate to resource utilization and
that again is putting a burden on the master which is tracking these resources which has to assign jobs which can run
on multiple machines in parallel so limitations in running non mapreduce
applications now that was one more limitation of Hadoop version 1 and mapreduce that the only kind of
processing you could do is mapreduce and mapreduce programming model although it
is good it is oldest it has matured over a period of time but then it is very
rigid you will have to go for mapping and reducing approach and that was the
only kind of processing which could be done in Hadoop version one so when it comes to doing a real-time analysis or
doing ad hoc query or doing a graph based processing or massive parallel processing there were limitations
because that could not be done in Hadoop person 1 which was having mapreduce
version 1 as the processing component now that brings us to the need for yarn
so yarn it stands for yet another resource negotiator so as I mentioned
before yarn in Hadoop version one well you could have applications which could
be written in different programming languages but then the only kind of processing which was possible was
mapreduce we had the storage layer we had the processing but then kind of limit processing which could be done now
this was one thing which brought in a thought that why shouldn't we have a processing layer which can handle
different kind of workloads as I mentioned might be graph processing might be real-time processing might be
massive parallel processing or any other kind of processing which would be a requirement of an organization now
designed to run mapreduce jobs only and having issues in scalability resource
utilization job tracking Etc that led to the need of something what we call as
yarn now from Hadoop version 2 onwards we have the two main layers have changed
a little bit you have the storage layer which is intact that is your sdfs and then you have the processing layer which
is called ya yet another resource negotiator now we will understand how yarn works but then yarn is taking care
of your processing layer it does support mapreduce So mapreduce processing can
still be done but then now you can have a support to other processing Frameworks
yarn can be used to solve the issues which Hadoop version 1 was posing
something like Resource Management something like different kind of workload processing something like
scalability resource utilization all that is now taken care by yarn now when
we talk about yarn we can have now a cluster size of more than 10 000 nodes
and can run more than 100 000 concurrent tasks that's just to take care of your
scalability when you talk about compatibility applications which were developed for Hadoop version 1 which
were primarily mapreduce kind of processing can run on yarn without any
disruption or availability issues when you talk about resource utilization there is a mechanism which takes care of
dynamic allocation of cluster resources and this basically improves the resource
utilization when we talk about multi tenancy so basically now the cluster can
handle different kind of workloads so you can use open source and proprietary
data access engines you can perform real-time analysis you can be doing graph processing you can be doing ad hoc
querying and this can be supported for multiple workloads which can run in
parallel so this is what yarn offers so what is Yan as I mentioned yarn stands
for yet another resource negotiator so it is the cluster Resource Management layer for your Apache Hadoop ecosystem
which takes care of scheduling jobs and assigning resources now just imagine
when you would want to run a particular application you would basically be telling the cluster that I would want
resources to run my applications that application might be a mapreduce application that might be a hive query
which is triggering a mapreduce that might be a pick script which is triggering a mapreduce that could be
Hive with days as in execution Engine That Could Be a spark application that
could be a graph processing application in any of these cases you would still
you as in in sense client or basically an API or the application would be
requesting for resources yarn would take care of that so yarn would provide the
desired resources now when we talk about resources we are mainly talking about the network related resources we are
talking about the CPU cores or as in terms of yarn we say virtual CPU course
we will talk about Ram that is in GB or MB or in terabytes which would be
offered from multiple machines and Yan would take care of this so with yarn you could basically handle different
workloads now these are some of the workloads which are showing up here you have the traditional mapreduce which is
mainly batch oriented you could have an interactive execution engine something
as days you would have H base which is a column oriented or a four dimensional
database and that would be not only storing data on sdfs but would also need some kind of processing you could have
streaming functionalities which would be from storm or Kafka or spark you could
have graph processing you could have in-memory processing such as spark and its components and you could have many
others so these are different Frameworks which could now run and which can run on
top of yarn so how does yarn do that now when we talk about yarn this is how a
overall yarn architecture looks so at one end you have the client now client
could be basically your Edge node where you have some applications which are running it could be an API which would
want to interact with your cluster it could be a user triggered application which wants to run some jobs which are
doing some processing so this client would submit a job request now what is
resource manager doing this Source manager is the master of your processing layer in Hadoop version 1 we basically
had job tracker and then we had task trackers which were running on
individual nodes so your task trackers were sending your heartbeats to the job tracker your task trackers were sending
it their resource information and job tracker was the one which was tracking the resources and it was doing the job
scheduling and that's how as I mentioned earlier job tracker was overburdened so
job tracker is now replaced by your resource manager which is the master for
your processing layer your task trackers are replaced by node managers which
would be then running on every node and we have a temporary demon which you see here in blue and that's your app master
so this is what we mentioned when we say yet another resource negotiator so App
Master would be existing in a Hadoop version now when we talk about your
resource manager resource manager is the master for processing layer so it would
already be receiving heartbeats and you can say resource information from multiple node managers which would be
running on one or multiple machines and these node managers are not only updating their status but they are also
giving an information of the amount of resources they have now when we talk about resources we should understand
that if I'm talking about this node manager then this has been allocated
some amount of RAM for processing and some amount of CPU cores and that is
just a portion of what the complete node has so if my node has say imagine my
node has around 100 GB RAM and I have saved 60 cores all of that cannot be
allocated to node manager so node manager is just one of the components of of Hadoop ecosystem it is the slave of
the processing layer so we could say keeping in all the aspects such as different Services which are running
might be Cloud era or hot and works related Services running system processes running on a particular node
some portion of this would be assigned to node manager for processing so we
could say for example say 60 GB Ram per node and say 40 CPU cores so this is
what is allocated for the node manager on every machine similarly we would have
here similarly we would have here so node manager is constantly giving an update to resource manager about the
resources what it has probably there might be some other applications running and node manager is already occupied so
it gives an update now we also have a concept of containers which is basically we will we will talk about which is
about these resources being broken down into smaller parts so resource manager
is keeping a track of the resources which every node manager has and it is also responsible for taking care of the
job request how do these things happen now as we see here resource manager at a
higher level you can always say this is the processing Master which does everything but in reality it is not the
resource manager which is doing it but it has internally different services or
components which are helping it to do what it is supposed to do now let's look further now as I mentioned your resource
manager has these services or components which basically helps it to do the
things it is basically a an architecture where multiple components are working
together to achieve what yarn allows so resource manager has mainly two
components that is your scheduler an applications manager and these are at
high level four main components here so we talk about resource manager which is the processing Master you have node
managers which are the processing slaves which are running on every nodes you have the concept of container and you
have the concept of application Master how do all these things work now let's look at yarn components so resource
manager basically has two main components you can say which assist resource manager in doing what it is
capable of so you have scheduler and applications manager now there is when
you talk about resources there is always a requirement for the applications which
need to run on cluster of resources so your application which has to run which was submitted by client needs resources
and these resources are coming in from multiple machines where ever the
relevant data is stored and a node manager is running so we always know that node manager is co-located with
data nodes now what does the scheduler do so we have different kind of schedulers here we have basically a
capacity scheduler we have a fair scheduler or we could have a fee for
scheduler so there are different schedulers which take care of resource allocation so your scheduler is
responsible for allocating resources to various running applications now imagine a particular environment where you have
different teams or different departments which are working on the same cluster so
we would call the cluster as a multi-tenant cluster and on the multi-terrent cluster you would have
different applications which would want to run simultaneously accessing the resources of the cluster how is that
managed so there has to be some component which has a concept of pooling
or queuing so that different departments or different users can get dedicated
resources or can share resources on the cluster so scheduler is responsible for
allocating resources to various running applications now it does not perform monitoring or tracking of the status of
applications that's not the part of scheduler it does not offer any guarantee about restarting the failed
tasks due to Hardware or network or any other failures scheduler is mainly
responsible for allocating resources now as I mentioned you could have different kind of schedulers you could have a fifo
scheduler which was mainly in older version of Hadoop which stands for first in first out you could have a fair
scheduler which basically means multiple applications could be running in the
cluster and they would have a fair share of the resources you could have a capacity scheduler which would basically
have dedicated or fixed amount of resources across the cluster now whichever scheduler is being used
scheduler is mainly responsible for allocating resources then it's your
applications manager now this is responsible for accepting job submissions now as I said at higher
level we could always say resource manager stay doing everything it is allocating the resources it is
negotiating the resources it is also taking care of listening to the clients and taking care of job submissions but
who is doing it in real it is these components so you have applications manager which is responsible for
accepting job submissions it negotiates the first container for executing the
application specific application Master it provides the service for restarting
the application Master now how does this work how do these things happen in
coordination now as I said your node manager is the slave process which would
be running on every machine slave is track talking the resources what it has
it is tracking the processes it is taking care of running the jobs and basically it is tracking each container
resource utilization so let's understand what is this container so normally when you talk about a application request
which comes from a client so let's say this is my client which is requesting or
which is coming up with an application which needs to run on the cluster now this application could be anything it
first contacts your master that's your resource manager which is the master for
your processing layer now as I mentioned and as we already know that your name node which is the master of your cluster
has the metadata in its Ram which is aware of the data being split into
blocks the blocks when stored on multiple machines and other information so obviously there was a interaction
with the master which has given this information of the relevant nodes where
the data exists now for the processing need your client basically the application which needs to run on the
cluster so your resource manager which basically has the scheduler which takes care of allocating resources and
resource manager has mainly these two components which are helping it to do its work now for a particular
application which might be needing data from multiple machines now we know that
we would have multiple machines where we would have node manager running we would have a data node running and data nodes
are responsible for storing the data on disk so your resource manager has to negotiate the resources now when I say
negotiating the resources it could basically ask each of these node managers for some amount of resources
for example it would be saying can I have one GB of RAM and one CPU core from
you because there is some data residing on your machine and that needs to be processed as part of my application can
I again have 1GB and one CPU core from you and this is again because some
relevant data is stored and this request which resource manager makes of holding
the resources of total resources which the node manager has your resource
manager is negotiating or is asking for resources from the processing slave so
this request of holding resources can be considered as a container so resource
manager now we know it is not actually the resource manager but it is the application manager which is negotiating
the resources so it negotiates the resources which are called containers so
this request of holding resource can be considered as a container so basically a
container can be of different sizes we will talk about that so resource manager negotiates the resources with node
manager now node manager which is already giving an update of the resources it has what amount of
resources it holds how much busy it is can basically approve or deny this
request so node manager would basically approve in saying yes I could hold these
resources I could give you this container of this particular size now once the container has been approved or
allocated or you can say granted by your node manager resource manager now knows
that resources to process the application are available and guaranteed
by the node manager so resource manager starts a temporary demon called App
Master so this is a piece of code which would also be running in one of the
containers it would be running in one of the containers which would then take care of execution of tasks in other
containers so your application Master is per application so if I would have 10
different applications coming in from the client then we would have 10 app Masters one app Master being responsible
for per application now what does this app Master do it basically is a piece of code which is responsible for execution
of the application so your app Master would run in one of the containers and then it would use the other containers
which node manager is guaranteed that it will give when the request application request comes to it and using these
containers the App Master will run the processing tasks Within These designated
resources so it is mainly the responsibility of application Master to get the execution done and then
communicate related to the Masters so resource manager is tracking the resources it is negotiating the
resources and once the resources have been negotiated it basically gives the
control to application Master application Master is then running within one of the containers on one of
the nodes and using the other containers to take care of execution this is how it
looks so basically container as I said is a collection of resources like CPU
memory your disk which would be used or which already has the data and network
so your node manager is basically looking into the request from application master and it basically is
granting this request or basically is allocating these containers now again we
could have different sizing of the containers let's take an example here so as I mentioned from the total resources
which are available for a particular node some portion of resources are are
allocated to the node manager so let's imagine this is my node where node
manager has a processing slave is running so from the total resources
which the node has some portion of RAM and CPU cores is basically allocated to
the node manager so I could say out of total 100 GB Ram we can say around 60
cores which the particular node has so this is my Ram which the node has and
these are the CPU cores which the node has some portion of it right so we can
say might be 70 or 60 percent of the total resources so we could say around
60 GB RAM and then we could say around 40 V cores have been allocated to node
manager so there are these settings which are given in the yarn hyphen site
file now apart from this allocation that is 60 GB RAM and 40v cores we also have
some properties which say what will be the container sizes so for example we
could have a small container setting which could say my every container could
have 2GB RAM and say one virtual CPU core so this is my smallest container So
based on the total resources you could calculate how many such small containers could be running so if I say 2GB Ram
then I could have around 30 containers but then I'm talking about one virtual
CPU core so totally I could have around 30 small containers which could be
running in parallel on a particular node and as of that calculation you would say 10 CPU cores are not being utilized you
could have a bigger container size which could say I would go for two CPU cores
and 3gb RAM so 3gb RAM and 2 CPU cores
so that would give me around 20 containers of bigger size so this is the
container sizing which is again defined in the yarn hyphen site file so what we know is on a particular node which has
this kind of allocation either we could have 30 small containers running or we could have 20 big containers running and
same would apply to multiple nodes so node manager based on the request from
application Master can allocate these containers now remember it is within
this one of these containers you would have an application Master running and other containers could be used for your
processing requirement application Master which is per application it is
the one which uses these resources it basically manages or uses these
resources for individual application so remember if we have 10 applications running on yarn then it would be 10
application Masters one responsible for each application your application Master is the one which also interacts with the
scheduler to basically know how much amount of resources could be allocated for one application and your application
Master is the one which uses these resources but it can never negotiate for
more resources to node Manager application Masters cannot do that application master has to always go back
to resource manager if it needs more resources so it is always the resource
manager and internally resource manager component that is application manager which negotiates the resources at any
point of time due to some node failures or due to any other requirements if
application Master needs more resources on one or multiple nodes it will always
be contacting the resource manager internally the applications manager for
more now this is how it looks so your client submits the job request to resource
manager now we know that resource manager internally has scheduler an applications manager node managers which
are running on multiple machines are the ones which are tracking their resources giving this information to the source
manager so that resource manager or I would say its component applications
manager could request resources from multiple node managers when I say
request resources it is these containers so your resource manager basically will
request for the resources on one or multiple nodes node manager is the one
which approves these containers and once the container has been approved your
resource manager triggers a piece of code that is application Master which
obviously needs some resources so it would run in one of the containers and will use other containers to do the
execution so your client submits an application to resource manager resource manager allocates a container or I would
say this is at a high level right resource manager is negotiating the resources and internally who is
negotiating the resources it is your applications manager who is granting this request it is node manager and
that's how we can say resource manager allocates a container application Master basically contacts the related node
manager because it needs to use the containers node manager is the one which launches the container or basically
gives those resources within which an application can run an application Master itself will then accommodate
itself in one of the containers and then use other containers for the processing and it is within these containers the
actual execute happens now that could be a map task that could be a reduced task that could be a spark executor taking
care of smart tasks and many other processing
so before we look into the demo on how yarn works I would suggest looking into
one of the blogs from cloud era so you can just look for yarn untangling and
this is really a good blog which basically talks about the overall functionality which I explained just now
so as we mentioned here so you basically have the master process you have the
worker process which basically takes care of your processing your resource
manager being the master and node manager being the slave this also talks about the resources which each node
manager has it talks about the yarn configuration file where you give all these properties it basically shows you
node manager which reports the amount of resources it has to resource manager now
remember if worker node shows 18 to 8 CPU cores and 128 GB RAM and if your
node manager says 64 week course and RAM 128gb then that's not the total capacity
of your node it is some portion of your node which is allocated to node manager
now once your node manager reports that your resource manager is requesting for containers based on the application what
is the container it is basically a logical name given to a combination of vcore and RAM it is within this
container where you would have basically the process running so once your application starts and once node manager
is guaranteed these containers your application or your resource manager has basically already started an application
Master within the container and what does that application Master do it uses the other containers where the tasks
would run so this is a very good blog which you can refer to and this also talks about mapreduce if you have
already followed the map reduce tutorials in past then you would know about the different kind of tasks that
is map and reduce and these map and reduce tasks could be running within the container in one or multiple as I said
it could be map task it could be reduced task it could be a spark based task which would be running within the
container now once the task finishes basically that resources can be freed up
so the container is released and the resources are given back to yarn so that
it can take care of further processing if you'll further look in this blog you can also look into the part two of it
where you talk mainly about configuration settings you can always look into this which talks about why and
how much resources are allocated to the node manager it basically talks about your operating system overhead it talks
about other services it talks about Cloudera or hortonworks related Services running and other processes which might
be running and based on that some portion of RAM and CPU cores would be
allocated to node manager so that's how it would be done in the yarn hyphen site file and this basically shows you what
is the total amount of memory and CPU cores which is allocated to node manager then within every machine where you have
a node manager running on every machine in the yarn hyphen site file you would have such properties which would say
what is the minimum container size what is the maximum container size in terms
of ram what is the minimum for CPU cores what is the maximum for CPU cores and
what is the incremental size in where RAM and CPU cores can increment so these
are some of the properties which Define how containers are allocated for your application request so have a look at
this and this could be a good information which talks about different properties now you can look further
which talks about scheduling if you look in this particular blog which also talks about scheduling where it talks about
scheduling in yarn which talks about Fair scheduler or you basically having
different cues in which allocations can be done you also have different ways in which queues can be managed and
different schedulers can be used so you can always look at this series of blog you can also be checking for yarn
schedulers and then search for uh definitive guide and that could give you
some information on how it looks when you look for Hadoop definitive guide so
if you look into this book which talks about the different resources as I
mentioned so you could have a fee for scheduler that is first in first out which basically means if a long running
application is submitted to the cluster all other small running applications will have to wait there is no other way
but that would not be a preferred option if you look in V4 scheduler if you look for capacity scheduler which basically
means that you could have different queues created and those queues would have resources allocated so then you
could have a production queue where production jobs are running in a particular queue which has fixed amount
of resources allocated you could have a development queue where development jobs are running and both of them are running
in parallel you could then also look into Fair scheduler which basically means again multiple applications could
be running on the cluster however they would have a fair share so when I say fair share in brief what it means is if
I had given 50 percent of resources to a queue for production and 50 percent of
resources for a queue of development and if both of them are running in parallel then they would have access to 50
percent of cluster resources however if one of the queue is unutilized then
second queue can utilize all cluster resources so look into the fair scheduling part it also shows you about
how allocations can be given and you can learn more about schedulers and how
queues can be used for managing multiple applications now we will spend some time in looking into few ways or few quick
ways in interacting with yarn in the form of a demo to understand and learn
on how yarn works we can look into a particular cluster now here we have a
the designated cluster which can be used you could be using the similar kind of
commands on your Apache based cluster or a Cloudera quick start VM if you already
have or if you have a cloud era or a hot and works cluster running there are different ways in which we can interact
with yarn and we can look at the information one is basically looking into the admin console so if I would
look into Cloudera manager which is basically an admin console for a cloudera's distribution of Hadoop
similarly you could have a hortonworks cluster then access to the admin console so if you have even read access for your
cluster and if you have the admin console then you can search for yarn as
a service which is running you can click on yarn as a service and that gives you
different tabs so you have the instances which tells basically what are the
different roles for your yarn service running so we have here multiple node
managers now some of them show in stop status but that's nothing to worry so we have three and six node managers we have
resource manager which is one but then that can also be in a high availability
where you can have active and standby you also have a job history server which would show you the applications once
they have completed now you can look at the yarn configurations and as I was explaining you can always look for the
properties which are related to the allocation so you can here search for course and that should show you the
properties which talk about the allocations so here if we see we can be
looking for yarn App mapreduce application Masters resource CPU course
what is the CPU course allocated for mapreduce map task reduce task you can
be looking at yarn node manager resource CPU course which basically says every
node manager on every node would be along it with 6 CPU cores and the
container sizing is with minimum allocation of one CPU core and the maximum could be two CPU cores similarly
you could also be searching for memory allocation and here you could then
scroll down to see what kind of memory allocation has been done for the node manager so if we look further it should
give me information of node manager which basically says here that the
container minimum allocation is 2GB the maximum is 3gb and we can look at node
manager which has been given 25 GB per node so it's a combination of this
memory and CPU cores which is the total amount of resources which have been
allocated to every node manager now we can always look into applications tab
that would show us different applications which are submitted on yarn for example right now we see there is a
spark application running which is basically a user who is using spark
shell which has triggered a application on spark and that is running on yarn you can look at different applications
workload information you can always do a search based on the number of days how
many applications have run and so on you can always go to the web UI and you can
be searching for the resource manager web UI and if you have access to that it will give you overall information of
your cluster so this basically says that here we have 100 GB memory allocated so
that could be say 25 GB per node and if we have four node managers running and
we have 24 cores which is six cores per node if we look further here into nodes
I could get more information so this tells me that I have four node managers running and node managers basically have
25 GB memory allocated per node and 6 cores out of which some portion is being
utilized we can always look at the scheduler here which can give us information what kind of scheduler has
been allocated so we basically see that there is just a root q and within root
you have default queue and you have basically users queue based on different
users we can always scroll here and that can give us information if it is a fair
share so here we see that my root dot default has 50 of resources and the
other queue also has 50 percent of resources which also gives me an idea that a fair scheduler is being used we
can always confirm that if we are using a fair scheduler or a capacity scheduler
which takes care of allocation so search for scheduler and that should give you some understanding of what kind of
scheduler is being used and what are the allocations given for that particular scheduler so here we have Fair scheduler
it shows me you have under root you have the root Q which has been given hundred
percent capacity and then you have within that default which also takes hundred percent so this is how you can
understand about yarn by looking into the yarn web UI you can be looking into the configurations you can look at
applications you can always look at different actions now since we do not have admin access the only information
we have is to download the client configuration we can always look at the history server which can give us
information of all the applications which have successfully completed now this is from your yarn UI what I can
also do is I can be going into Hue which is the web interface and your web
interface also basically allows you to look into the jobs so you can click on
Hue web UI and if you have access to that it should show up or you should
have a way to get to your Hue which is a graphical user interface mainly comes with your Cloud era you can also
configure that with a Apache Hotel Works has a different way of giving you the web UI access you can click and get into
Hue and that is also one way where you can look at yarn you can look at the jobs which are running if there are some
issues with it and these these are your web interfaces so either you look from
yarn web UI or here in Hue you have something called as job browser which
can also give you information of your different applications which might have run so here I can just remove this one
which should basically give me a list of all the different kind of jobs or workflows which were run so either it
was a spark based application or it was a map reduce or it was coming from hive
so here I have list of all the applications and it says this was a map reduce this was a spark something was
scaled something was successful and this was basically a probably a hive query
which triggered a mapping reduce job you can click on the application and that tells you how many tasks were run for it
so there was a map task which ran for it you can get into the metadata information which you can obviously you
can also look from the yarn UI to look into your applications which can give you a detailed information of if it was
a map reduce how many map and reduce tasks were run what were the different counters if it was a spark application
it can let you follow through spark history server or job history server so you can always use the web UI to look
into the jobs you can be finding in a lot of useful information here you can
also be looking at how many resources were used and what happened to the job was it successful did it fail and what
was the job status now apart from web UI which always you might not have access
to so in a particular cluster in a production cluster there might be restrictions and the organization might
not have access given to all all the users to graphical user interface like you or might be you would not have
access to the cloud era manager or admin console because probably organization is
managing multiple clusters using this admin console so the one way which you would have access is your web console or
basically your Edge node or client machine from where you can connect to the cluster and then you can be working
so let's log in here and now here we can give different commands so this is the
command line from where you can have access to different details you can
always check by just typing in mapred which gives you different options where you can look at the mapreduce related
jobs you can look at different queues if there are queues configured you can look at the history server or you can also be
doing some admin stuff provided you have access so for example if I just say mapred and queue here this basically
gives me an option says what would you want to do would you want to list all the cues do you want information on a
particular queue so let's try a list and that should give you different cues
which were being used now here we know that per user a queue dynamically gets
created which is under root dot users and that gives me what is the status of the queue what is the capacity has there
been any kind of maximum capacity or capping done so we get to see a huge
list of cues which dynamically get configured in this environment and then you also look at your root dot default I
could have also picked up one particular queue and I could have said show me the jobs so I could do that now here we can
also give a yarn command so let me just clear the screen and I will say yarn and
that shows me different options so apart from your web interface something like
web UI apart from your Yarns web UI you could also be looking for an information
using yarn commands here so these are some list of commands which we can check now you can just type in yarn and
version if you would want to see the version which basically gives you information of what is the Hadoop
version being used and what is the vendor specific distribution version so here we see we are working on cloudera's
distribution 5.14 which is internally using Hadoop 2.6 now similarly you can
be doing a yarn application list so if you give this that could be an
exhaustive list of all the applications which are running or applications which have completed so here we don't see any
applications because right now probably there are no applications which are running it also shows you you could be
pulling out different status such as submitted accepted or running now you
could also say I would want to see the services that I've finished running so I could say yarn application list and app
States as say so here we could be using our Command so
I could say yarn application list and then I would want to see the app states
which gives me the applications which have finished and we would want to list
all the applications which finished now there that might be applications which succeeded right and there is a huge list
of application which is coming in from the history server which is basically showing you the huge list of
applications which have completed so this is one way and then you could also be searching for one particular
application if you would want to search a particular application if you have the application ID you could always be doing
a grip that's a simple way I could say basically let's pick up this one and if
I would want to search for this if I would want more details on this I could obviously do that by calling in my
previous command and you could do a grip if that's what you want to do and if you
would want to search is there an application which is in the list of my applications that shows my application I
could pull out more information about my application so I could look at the log files for a particular application by
giving the application ID so I could say yarn logs now that's an option and every
time anytime you have a doubt just hit enter it will always give you options what you need to give with a particular
command so I can say yarn log application ID now we copied an
application ID and we could just give it here we could give other options like app owner or if you would want to get
into the Container details or if you would want to check on a particular node now here I'm giving yarn logs and then
I'm pointing it to an application ID and it says the log aggregation has not
completed might be this was might be this was an application which was triggered based on a particular
interactive cell or based on a particular query so there is no log existing for this particular application
you can always look at the status of an application you can kill an application
so here you can be saying yarn yarn application and then what would you want
to do with an application hit and enter it shows you the different options so we just tried app States you could always
look at the last one which says status and then for my status I could be giving
my application ID so that tells me what is the status of this application it connects to the resource manager it
tells me what's the application ID what kind of application it was who ran it which was the queue where the job was
running what was the start and end time what is the progress the status of it if it is finished or if it has succeeded
and then it basically gives me also an information of where the application master was running it gives me the
information where you can find this job details in history server if you are
interested in looking into it also gives you a aggregate resource allocation which tells how much GB memory and how
many core seconds it used so this is basically looking out at the application details now I could kill an application
if the application was already running I could always do a yarn application minus
skill and then I could be giving my application now I could try killing this however it would say the application is
already finished if I had an application running and if my application was
already given an application ID by The Source manager I Could Just Kill it I can also say yarn node list which would
give me a list of the node managers now this is what we were looking from the yarn web UI and we were pulling out the
information so we can get this and kind of information from your command line always remember and always try to be
well accustomed with the command line so you can do various things from the command line and then obviously you have
the web uis which can help you with a graphical interface easily able to access things now you could be also
starting the resource manager which we would not be doing here because we are already running in a cluster so you
could give a yarn resource manager you could get the logs of resource manager
if you would want by giving yarn demin so we can try that so you can say yarn
and then demon so it says it does not find the demon so so you can give
something like this get level and here I will have to give the node and the IP
address where you want to check the logs of resource manager so you could be giving this for which we will have to
then get into Cloudera manager to look into the nodes and the IP address you
could be giving a command something like this which basically gives you the level
of the log which you have and I got this resource manager address from the web UI
now I can be giving in this command to look into the demon log and it basically
says you would want to look at the resource manager related log and you
have the log 4J which is being used for logging the kind of level which has been
set as info which can again be changed in the way you're logging the information now you can try any other
commands also from yarn for example looking at the yarn RM admin so you can
always do a yarn RM admin and this basically gives you a lot of other informations like refreshing the cues or
refreshing the nodes or basically looking at the admin ACLS or getting
groups so you could always get group names for a particular user now we could
search for a particular user such as yarn or hdfs itself so I could just say
here I would want get groups and then I could be searching for say username hdfs
so that tells me sdfs belongs to a Hadoop group similarly you could search
for say mapred or you could search for yarn so these are service related users
which automatically get created and you can pull out information related to these you can always do a refresh nodes
kind of command and that is mainly done internally this can be useful when you
are doing commissioning decommissioning but then in case of cloud hortonworks kind of cluster you would
not be manually giving this command because if you are doing a commissioning decommissioning from an admin console
and if you are an administrator then you could just restart the services which are affected and that will take care of
this but if you were working in an Apache cluster and if you were doing commissioning decommissioning then you
would be using in two commands refresh notes and basically that's for
refreshing the nodes which should not be used for processing and similarly you
could have a command refresh notes which comes with stfs so these are different options which you can use with your yarn
on the command line you could also be using curl commands to get more information about your cluster by giving
curl minus X and then basically pointing out to your Source manager web UI
address now here I would like to print out the cluster related metrics and I
could just simply do this which basically gives me a high level information of how many applications
were submitted how many are pending what is the reserved resources what is the
available amount of memory or CPU cores and all the information similarly you can be using the same curl commands to
get more information like scheduler information so you would just replace the metrics with scheduler and you could
get the information of the different queues now that's a huge list we can cancel this and that would give me the
list of all the queues which are allocated and what are the resources allocated for each queue you could also
get cluster information on application IDs and Status running of applications
running in yarn so you would have to replace the last bit of it and you would
say I would want to look at the applications and that gives me a huge list of applications then you can do a
grip and you can be filtering out specific application related information similarly you can be looking at the
notes so you can always be looking at node specific information which gives
you how many nodes you have but this could be mainly used when you have an application which wants to or a web
application which wants to use a curl command and would want to get information about your cluster from an
HTTP interface now when it comes to application we can basically try running
a simple or a sample mapreduce job which could then be triggered on yarn and it
would use the resources now I can look at my application here and I can be
looking into my specific directory which is this one which should have a lot of
files and directories which we have here now I could pick up one of these and I
could be using a simple example to some processing let's take up this file so
there is a file and I could run a simple word count or I could be running a hive query which triggers a mapreduce job I
could even run a spark application which would then show that the application is running on the cluster so for example if
I would say spark to Shell now I know that this is an interactive way of
working with spark but this internally triggers a spark submit and this runs an
application so here when you do a spark 2 Shell by default it will contact yarn
so it gets an application ID it is running on yarn with the master being yarn and now I have access to the
interactive way of working with spark now if I go and look into applications I
should be able to see my application which has been started here and it shows up here so this is my application
3827 which has been started on yarn and as of now we can also look into the yarn
UI and that shows me the application which has been started which basically has one running container which has one
CPU core allocated 2GB RAM and it's in progress although we are not doing anything there so we can always look at
our applications from the yarn UI or as I mentioned from your applications tab within yarn Services which gives us the
information and you can even click on this application to follow and see more information but you should be given
access to that now this is just a simple application which I triggered using spark shell similarly I can basically be
running a map reduce now to run a map reduce I can say Hadoop jar and that
basically needs a class so we can look for the default path which is opt Cloud
era Parcels CDH lib Hadoop map reduce
Hadoop map reduce examples and then we can look at this particular our file and
if I hit on enter it shows me the different classes which are part of this jar and here I would like to use word
count so I could just give this I could say word count now remember I could run
the job in a particular queue by giving in an argument here so I could say minus
d mapred dot job dot Q dot name and then
I can point my job to a particular queue I can even give different arguments in saying I would want my mapreduce output
to be compressed or I want it to be stored in a particular directory and so on so here I have the word count and
then basically what I can be doing is I can be pointing into a particular input path and then I can have my output which
can be getting stored here again a directory which we need to choose and I
will say output new and I can submit my job now once I have submitted my job it
connects to resource manager it basically gets a job ID it gets an application ID it shows you from where
you can track your application you can always go to the yarn UI and you can be
looking at your application and the resources it is using so my application was not a big one and it has already
completed it triggered one map task it launched one reduce task it was working
on around 12 466 records where you have then the output of map which is these
many number of output records which was then taken by a combiner and finally by a reducer which basically gives you the
output so this is my yarn application which has completed now I could be looking into the yarn UI and if my job
has completed you might not see your application here so as of now it shows up here the word count which I ran it
also shows me my previous Park shell job it shows me my application is completed and if you would want further
information on this you can click and go to the history server if you have been
given access to it or directly go to the history server web UI where your application shows up it shows how many
map and reduce tasks it was running you can click on this particular application which basically gives you information of
your map and reduce tasks you can look at different counters for your application right you can always look at
map specific tasks you can always look into one particular task what it did on
which node it was running or you can be looking at the complete application log so you can always click on the logs and
here you have click here for full log which gives you the information and you can always look for your application
which can give you information of App Master being launched or you could have
search for the word container so you could see a job which needs one or
multiple containers and then you could say container is being requested then you could see container is being
allocated then you can see what is the container size and then basically your
task moves from initializing to running in the container and finally you can
even search for release which will tell you that the container was released so
you can always look into the log for more information so this is how you can interact with yarn this is how you can
interact with your command line to look for more information or using your yarn
web UI or you can also be looking into your Hue for more information welcome to
scoop tutorial one of the many features of the Hadoop ecosystem for the Hadoop file system what's in it for you today
we're going to cover the need for scoop what is scoop scoop features scoop
architecture scoop import scoop export scoop processing and then finally we'll
have a little Hands-On demo on Scoops you can see what it looks like so where does the need for scoop come in and our
big data Hadoop file system processing huge volumes of data requires loading data from diverse sources into Hadoop
cluster you can see here we have our data processing and this process of loading data from the heterogeneous
sources comes with a set of challenges so what are the challenges maintaining data consistency ensuring efficient
utilization of resources especially when you're talking about big data we can certainly use up the resources when
importing terabytes and petabytes of data over the course of time loading bulk data to Hadoop was not possible it
was one of the big challenges that came up when they first had the Hadoop file system going and loading data using script was very slow in other words
you'd write a script in whatever language you're in and then it would very slowly load each piece and parse it in so the solution scoop scooped helped
in overcoming all the challenges to traditional approach and could lead bulk data from rdbms to Hadoop very easily so
think your Enterprise server you want to take the from MySQL or your SQL and you
want to bring that data into your dupe Warehouse your data filing system and that's where scoob comes in so what
exactly is scoop scoop is a tool used to transfer bulk of data between Hadoop and
external data stores such as relational databases and MySQL server or the
Microsoft SQL server or MySQL server so scoop equals SQL plus Hadoop and you can
see here we have our rdbms all the data we have stored on there and then your scoop is the middle ground and brings
the import into the Hadoop file system it also is one of the features that goes out and grabs the data from Hadoop and
exports it back out into an rdbms let's take a look at scoop features scoop
features has parallel Import and Export it has import results of SQL query
connectors for all major rdbms databases Kerberos security integration provides
full and incremental load so we look at parallel Import and Export scoop uses yarn yet another resource negotiator
framework to import an export data this provides fault Tolerance on a top of parallelism scoop allows us to import
the result return from an SQL carry into the Hadoop file system or the hdfs and
you can see here where the import results of SQL query come in school provides connectors for multiple relational database management system
rdbms's databases such as MySQL and Microsoft SQL server and it has
connectors for all major rdbms databases scoop supports Kerberos computer network
Authentication Protocol that allows nodes communicating over a non-secure network to prove their identity to one
another in a secure manner scoop can load the whole table or parts of the table by a single command hence it
supports full and incremental load let's dig a little deeper into the scoop architecture we have our client in this
case a hooded wizard behind his laptop you never know who's going to be accessing the Hadoop cluster and the
client comes in and sends their command which goes into scoop the client submits the import export command to import or
export data data from different databases is fetched by scoop and so we have our Enterprise data warehouse
document based systems you have connect connector for your data warehouse a connector for document based systems
which reaches out to those two entities and we have our connector for the rdbms so connectors help in working with a
range of popular databases multiple mappers perform map tasks to load the data onto hdfs the Hadoop file system
and you can see here we have the map task if you remember from Hadoop Hadoop is based on mapreduce because we're not
reducing the data we're just mapping it over it only accesses the mappers and it opens up multiple mappers to do parallel
processing and you can see here the hdfs hbase hive is where the target is for this particular one similarly multiple
map tests will export the data from hdfs onto rdbms using scoop export command so
just like you can import it you can now export it using the multiple routines scoop import so here we have our dbms
data store and we have the folders on there so maybe it's your company's database maybe it's an archive at Google
with all the searches going on whatever it is usually you think with scoop you think SQL you think MySQL server or
Microsoft SQL Server that kind of setup so it gathers the metadata and you see the scoop import so introspect database
to gather metadata primary key information and then it submits so you can see submits map only job remember we
talked about mapreduce it only needs the map side of it because we're not reducing the data we're just mapping it
over scoop device the input data set into splits and uses individual map tests to push the splits into hdfs so
right into the Hadoop file system and you can see down on the right is kind of a small depiction of a Hadoop cluster
and then you have scoop export so we're going to go the other direction and with the other direction you have your Hadoop
file system storage which is your Hadoop cluster you have your scoop job and each one of those clusters then gets a map
mapper comes out to each one of the computers it has data on it so the first step is you've got to gather the
metadata so step one you gather the metadata step two submits map only job introspect database to gather metadata
primary key information scoop divides the input data set into splits and uses individual map tests to push the splits
to rdbms scoop will export Hadoop files back to rdms tables you can think of
this in a number of different manners one of them would be if you're restoring a backup from the Hadoop file system
into your Enterprise machines there's certainly many others as far as exploring data and data science so as we
dig a little deeper into scoop input we have our connect our jdbc and our URL so
specify the jdbc connect string connecting manager we specify The Connection Manager class to use you can
see here driver with the class name manually specify the jdbc driver class to use Hadoop mapreduce home directory
override Hadoop mapped home username set authentication username and of course
help print uses instructions and with the export you'll see that we can specify the jdbc connect string specify
The Connection Manager class to use manually specify jdbc driver class to
use you do have to let it know to override the Hadoop map reduce home and that's true on both of these and set
authentication username and finally you can print out all your help setup so you can see the format for scoop is pretty
straightforward both Import and Export so let's uh continue on our path and look at scoop processing and what the
computer goes through for that and we talk about school processing first scoop runs in the Hadoop cluster it Imports
data from the rdbms the nosql database to the Hadoop file system so remember it
might not be importing the data from a rdbms it might actually be coming from a nosql and there's many out there it uses
mappers to slice the incoming data into multiple form formats and load the data into hdfs it exports data back into an
rdbms while making sure that the schema of the data in the database is maintained so now that we've looked at
the basic commands in our scoop in the scoop processing or at least the basics as far as theory is concerned let's just
jump in and take a look at a demo on scoop [Music]
for this demo I'm going to use our clouderic quick start if you've been watching our other demos we've done
you'll see that we've been using that pretty consistently certainly this will work in any of your your Horton sandbox
which is also a single node testing machine Cloudera is one of um there's a Docker version instead of virtual box
and you can also set up your own Hadoop cluster plan a little extra time if you're not an admin it's actually a
pretty significant Endeavor for an admin if you've been admitting Linux machines for a very long time and you know a lot
of the commands I find for most admins it takes them about two to four hours the first time they go in and create a
virtual machine and set up their own Hadoop in this case though I mean you're just learning and getting set up best to
start with Cloudera Cloudera also includes an install version of MySQL that way you don't have to worry install
the the SQL version for importing data from and two once you're in the Cloudera
quick start you'll see it opens an AI Centos Linux interface and it has a
desktop setup on there this is really nice for learning so you're not just looking at command lines and from in
here it should open up by default to Hue if not you can click on Hue here's a kind of a fun little web-based interface
under Hue I can go under query I can pick an editor and we'll go right down to scoop so now I'm just going to load
the scoop editor inner Hue now I'm going to switch over and do this all in command line I just want to show that
you can actually do this in a hue through the web-based interface the reason I like to do the command line is
specifically on my computer it runs much quicker or if I do the command line here and I run it it tends to have an extra
lag or an added layer in it so for this we're going to go ahead and open our command line the second reason I do this
is we're going to need to go ahead and edit our MySQL so we have something to
scoop in other words I don't have anything going in there and of course we zoom in zoom in this and increase the
size of our screen so for the this demo are Hands-On I'm going to use Oracle
virtualbox manager and the Cloudera quick start if you're not familiar with this we do have another tutorial we put
out and you can send a note in the YouTube video below and let our team know and they'll send you a link or come
visit www.simpylearn.com now this creates a Linux box on my Windows computer so
we're going to be in Linux and it'll be the Cloudera version with scoop it will also be using MySQL MySQL server once
inside the Cloudera virtual box we'll go into the Hue editor now we're going to do everything in terminal window but I
just want you to be aware that under the Hue editor you can go under query editor and you'll see as we come down here
here's our scoop on this so you can run your Scoop from in here now before we do this we have to do a little exploration
in my SQL and MySQL server that way we know what data is coming in so let me go ahead and open up a terminal window in
Cloudera you have a terminal window at the top here that you can just click on and open it up and let me just go ahead and zoom here we'll go View and zoom in
now to get into my SQL Server you typically type in MySQL and this part
will depend on your setup now the Cloudera quick start comes up that the username is root and the password is
Cloudera kind of a strange Quirk is that you can put a space between the minus U and the root but not between the minus p
and the Cloudera usually you'd put in a minus capital P and then it prompts you for your password on here for this demo
I don't worry too much about you knowing the password on that so we'll just go right into MySQL server since this is
the standard password for this quick start and you can see we're now into MySQL and we're going to do just a
couple of quick commands in here there's show databases and you follow by the semicolon that's standard in most of
these shell commands so it knows it's the end of your shell command and you'll see in here in the quick start Cloudera
quick start the MySQL comes with a standard set of databases these are just some of these have to do like with the
uzi which is the uzi part of Hadoop where others of these like customers and employees and stuff like that those are
just for demo purposes they come as a standard setup in there so that people going in for the first time have a
database to play with which is really good for us so we don't have to recreate those databases and you will see in the
list here we have a retail underscore DB and then we can simply do uh use retail
underscore DB this will set that as a default in MySQL and then we want to go
ahead and show the tables and if we show the tables you can see under the database the retail DB database we have
categories customers departments order items orders products so there's a number of tables in here and we're going
to go ahead and just use a standard SQL command and if you did our Hive language
you'll note remember it's the same for hql also on this we're just going to select star everything from departments
so there's our departments table and we're going to list everything on the Departments table and you'll see we have
a six line in here and it has a department ID and a department name two for Fitness three for Footwear so on and
so forth now at this point I can just go ahead and exit but it's kind of nice to have this data up here so we can look at
it and flip back and forth between the screens so I'm going to open up another terminal window and we'll go ahead and
zoom in on this also and it isn't too important for this particular setup but
it's always kind of fun to know what your setup you're working with what is your host name and so we'll go ahead and
just type that in this is a Linux command and it's uh hostname minus F and
you see we're on quick start Cloudera no surprise there now this next command is
going to be a little bit longer because we're going to be doing our first scoop command and I want to do two of them
we're going to list databases and list tables it's going to take just a moment to get through this because there's a
bunch of stuff going on here so we have scoop we have list databases we have connect and under the connect command we
need to let it know how we're connecting we're going to use the jdbc this is a very standard one jdbc MySQL so you'll
see that if you're doing an SQL database that's how you start it off with and then the next part this is where you
have to go look it up it's however it was created so if your admin created a MySQL server with a certain setup that's
what you have to go by and you'll see that usually they list this as localhost so you'll see something like localhost
sometimes there's a lot of different formats but the most common is either localhost or the actual connection so in
this case we want to go ahead and do quick start 3306 and so quick start is the name of
the localhost database and how it's hosted on here and when you set up the quick start for for Hadoop under
Cloudera it's Port 3306 is where that's coming in so that's where all that's coming from and so there's our path for
that and then we have to put in our password we typically type password if you look it up password on the cloud era
quick start is Cloudera and we have to also let it know the username and again if you're doing this you'd probably put
in a minus Capital you can actually just do it for a prompt Cloud for the password so if you leave that out it'll
prompt you but for this doesn't really matter I don't care if you see my password it's the default one for
Cloudera quick start and then the username on here is simply root and then we're going to put our semicolon at the
end and so we have here our full setup then we go ahead and list the databases and you'll see you may get some warnings
on here I haven't run the updates on the quick start I suggest you're not running the updates either if you're doing this
for the first time because it'll do some reformatting on there and it quickly pops up and you can see here's all of
our the tables we went in there and if we go back to on the previous window we
should see that these tables match so here we come in and here we have our databases and you can see back up here
where we had the CM customers employees and so on so the databases match and
then we want to go ahead and list the tables for a specific database so let's
go ahead and do that I'm a very lazy typist so I'll put the up arrow in and you can see here scoop list databases
we're just going to go back and change this from databases to list tables so we want to list the tables in here same
connection so most of the connection is the same except we need to know which tables we're listing an interesting fact
is you can create a table without being under a database so if you left this blank it will show the open tables that
aren't connected directly to a database or under a database but what we want to do is right past this last slash on the 33 306 we want to put that retail
underscore DB because that's the database we're going to be working with and this will go in there and show the
tables listed under that database and here we go we got categories customers departments order items and products if
we flip back here real quick there it is the same thing we had we had categories customers departments order items and so
on and so let's go ahead and run our first import command and again I'm that
lazy typer so we're going to do scoop and instead of list tables we want to go
ahead and import so there's our import command and so once we have our import command in there then we need to tell it
exactly what we're going to import so everything else is the same we're importing from the retail DB so we keep
that and then at the very end we're going to tag on dash dash table that tells it so we can tell it what table we're importing from and we're going to
import departments there we go so this is pretty straightforward because what's nice about this is you can see the
commands are the same I got the same connection might change it for the whatever database I'm in then I come in
here our password and the username are going to be the same that's all under the MySQL server setup and then we let it know what table we're entering in we
run this and this is going to actually go through the mapper process in Hadoop so this is a mapping process it takes
the data and it Maps it up to different parts in the setup in Hadoop on there and then saves that data into the Hadoop
file system and it does take it a moment to zip through which I kind of skipped over for you since it is running a you
know it's designed to run across the cluster not on a single node so when you're running on a single node it's going to run slow even if you dedicate a
couple cores to it I think I put dedicated four cores to this one and so you can see right down here we get to
the end it's now mapped in that information and then we can go in here we can go under we can flip back to our
Hue and under Hue on the top I have there's databases and the second icon over is your Hadoop file system and we
can go in here and look at the Hadoop file system and you'll see it show up underneath our document insert as
departments Cloudera departments and you can see there's always a delay when I'm working in Hue which I don't like and
that's the quick start issue that's not necessarily running out on a server when I'm running it on a server you pretty
much have to run through some kind of server interface I still prefer the terminal window it still runs a lot
quicker but we'll flip back on over here to the command line and we can do the Hadoop type in the Hadoop fs and then
list minus LS and if we run this you'll see underneath our Hadoop file system there is our departments which has been
added in and we can also do Hadoop fs and this is kind of interesting for
those who've gone through the Hadoop file system everything you'll you'll recognize this on here I'm going to list
it the contents of departments and you'll see underneath departments we have part part
m0001002003 and so this is interesting because this is how Hadoop saves these files this is in the file system this is
not in Hive so we didn't directly import this into Hive we put this in the Hadoop file system depending on what you're
doing you would then write the schema for Hive to look at the Hadoop file system certainly visit our Hive tutorial
for more information on hive specific uh so you can see in here are different files that it forms that are part of
departments and we can do something like this we can look at the contents of one of these files FS minus LS or a number
of the files and we'll simply do the full path which is user Cloudera and
then we already know the next one is departments and then after departments we're going to put slash part star so
this is going to say anything that has part in it so we have part Dash m000 and
so on we can go ahead and cat use that cat command or that list command to bring those up and then we can use the
cat command to actually display the contents and that's a Linux command Hadoop Linux command the catinate not to
be confused with catatonic catastrophic there's a lot of cat got your tongue and we see here Fitness Footwear apparel
that should look really familiar because that's what we had in our MySQL server when we went in here we did a select all
on here there it is Fitness Footwear apparel golf outdoors and fan shop and
then of course it's really important slip back on over here to be able to tell it where to put the data so we go
back to our import command so here's our scoop import we have our connect we have the DB underneath our connection our
MySQL server we have our password our username the table going where it's going to I mean the table where it's
coming from and then we can add a Target on here we can put in a Target Dash
directory and you do have to put the full path that's a Hadoop thing it's a good practice to be in and we're going
to add it to Department we'll just do Department one and so here we now add a Target directory in here in user
Cloudera Department one and this will take just a moment before so I'll go ahead and skip over the process since
it's going to run very slowly it's only running on like I said a couple cores and it's also on a single node and now
we can do the Hadoop let's just do the up Arrow file system list we want just
straight list and when we do the Hadoop file system minus LS or list you'll see
that we now have Department one and we can of course do list Department one and you can see we have the files inside
Department one and they mirrored what we saw before with the same files in there and the part mm0 and so on if we were to
look at him it'd be the same thing we did before with the cat so except instead of departments we'd be
Department one there we go one thing that's going to come up with the same data we had before now one of the
important things when you're importing data and it's always a question to ask is do you fill through the data before
it comes in do we want to filter this data as it comes in so we're not storing everything in our file system you would
think Hadoop Big Data put it all in there I know from experience that putting it all in there can turn a
couple hundred terabytes into a petabyte very rapidly and suddenly you're having
to really add on to that data store and you're storing duplicate data sometimes so you really need to be able to filter
your data out and so let's go ahead and use our up Arrow to go to our last import since it's still a lot the same
stuff so we have all of our commands on our import we have the target we're going to change this to Department two
so we're going to create a new directory for this one and then after departments there's another command that we didn't
really slide in here and that's our mapping and I'll show you what this looks like in a minute we're going to put M3 in there that doesn't have
nothing to do with the filtering I'll show you that in a second though what that's for and we just want to put in where so where and what is the where in
this case we want to know where Department ID and if you want to know
where that came from we can flip back on over here we have Department underscore IDs this is where that's coming from
that's just the name of the column on here so we come in here to Department ID is greater than four simple logic there
you can see where you'd use that for maybe creating buckets for ages you know age from 10 to 15 20 to 30. you might be
looking for I mean there's all kinds of reasons why you could use the where command on here in filter information out maybe you're doing word counting and
you want to know words that are used less than a hundred times you want to get rid of the and is and and all the
stuff that's used over and over again so we'll go ahead and put the where and then Department ID is greater than four
we'll go ahead and hit enter on here and this will create our department to set up on this and I'll go ahead and skip
over some of the runtime again it runs really slow on a single node a real quick page through our commands
here we go our list and we should see underneath the list the department two on here now and there it is Department
two and then I can go ahead and do list Department two you'll see the contents in here and you'll see that there is
only three maps and it could be that the data created three Maps but remember I set it up to only use three mappers so
there's zero one and two and we can go ahead and do a cat on there remember this is Department two so we want to
look at all the contents of these three different files and there it is it's greater than four so we have golf is
five outdoor six uh fan shop is seven so we've effectively filtered out our data
and just storing the data we want on our file system so if you're going to store data on here the next stage is to export
the data remember a lot of times you have MySQL server and we're continually dumping that data into our long-term
storage and access a Hadoop file system but what happens when you need to pull that data out and restore a database or
uh maybe you have um you just merged with a new company a favorite topic
merging companies emerging databases that's listed under Nightmare and how many different names for company can you
have so you can see where being able to export is also equally important and let's go ahead and do that and I'm going
to flip back over to my SQL Server here and we'll need to go ahead and create our database we're going to export into
now I'm not going to go too much in detail on this command we're simply creating a table and the table is going
to have it's pretty much the same table we already have in here from departments but in this case we're going to create a
table called Dept so it's the same setup but it's it's just gonna we're just
giving a different name a different schema and so we've done that and we'll go ahead and do a select star from Dept
there we go and it's empty that's what we expect a new database a new data table and it's empty in there so now we
need to go ahead and Export our data that we just filtered out into there so let's flip back on over here to our
scoop setup which is just our Linux terminal window and let's go back up to one of our commands here's scoop Import
in this case instead of import we're going to take the scoop and we're going to export so we're going to just change
that export and the connection is going to remain the same so same connect same
database we're also we're still doing the retail DB we have the same password so none of that changes the big change
here is going to be the table instead of departments remember we changed it and gave it a new name and so we want to
change it here also Dept so Department we're not going to worry about the
mapper count and the where was part of our import there we go and then finally
it needs to know where to export from so instead of Target directory we have an
export directory that's where it's coming from still user Cloudera and we'll keep it as Department two just so
you can see how that data is coming back with the that we've filtered in and let's go ahead and run this and I'll
take it just a moment to go through in steps and again because it's slow I'm just going to go ahead and skip this so you don't have to sit through it and
once we've wrapped up our export we'll flip back on over here to mySQL use the
up arrow and this time we're going to select star from department and we can see that there it is it exported the
golf outdoors and fan shop and you can imagine also that you might have to use the where command in your export also so
there's a lot of mixing the command line for scoop is pretty straightforward you're changing the different variables
in there whether you're creating a table listing a table listing databases very powerful tool for bringing your data
into the Hadoop file system and exporting it so now that we've wrapped up our demo on scoop and gone through a
lot of basic commands let's dive in with a brief history of Hive so the history
of Hive begins with Facebook Facebook begin using Hadoop as a solution to handle the growing big data and we're
not talking about a data that fits on one or two or even five computers where are talking due to the fifth sign if
you've looked at any of our other Hadoop tutorials you'll know we're talking about very big data and data pools and
Facebook certainly has a lot of data it tracks as we know the Hadoop uses map
reduce for processing data mapreduce required users to write long codes and
so you have these really extensive Java codes very complicated for the average person to use not all users reversed in
Java and other coding languages this proved to be a disadvantage for them users were comfortable with writing
queries in SQL SQL has been around for a long time the standard SQL query
language Hive was developed with a vision to incorporate the concepts of tables columns just like SQL so why Hive
well the problem was for processing and analyzing data users found it difficult to code as not all of them were well
versed with the coding languages if you're processing if you're analyzing and so the solution was required a
language similar to SQL which was well known to all the users and thus the The Hive or hql language evolved what is
Hive Hive is a data warehouse system which is used for querying and analyzing large data sets stored in the hdfs or
the Hadoop file system Hive uses a query language that we call Hive ql or hql
which is similar to SQL so if we take our user the user sends out their hive
queries and then that is converted into a mapreduce tasks and then accesses the
Hadoop mapreduce system let's take a look at the architecture of Hive architecture of Hive we have the hive
client so that could be the programmer or maybe it's a manager who knows enough SQL to do a basic query to look up the
data they need the hive client supports different types of client applications in different languages prefer for
performing queries and so we have our Thrift application in the hive Thrift client Thrift is a software framework
Hive server is based on Thrift so it can serve the request from all programming language that support threat and then we
have our jdbc application and the hive jdbc driver jdbc Java database
connectivity jdbc application is connected through the jdbc driver and then you have the odbc application or
the hive odbc driver the odbc or open database connectivity the odbc
application is connected through the odbc driver with the growing development of all of our different scripting
languages python C plus plus spark Java you can find just about any connection
in any of the main scripting languages and so we have our Hive Services as we look at deeper into the architecture
Hive supports various services so you have your Hive server basically your Thrift application or your hive Thrift
client or your jdbc or your hive jdbc driver your odbc application or your
hive odbc driver they all Connect into The Hive server and you have your hive web interface you also have your CLI now
the hive web interface is a GUI is provided to execute Hive queries and
we'll actually be using that later on today so you can see kind of what that looks like and get a feel for what that
means commands are executed directly in CLI and then the CLI is a direct
terminal window and I'll also show you that too so you can see how those two different interfaces work these then
push the code into the hive driver Hive driver is responsible for all the queries submitted so everything goes
through that driver let's take a closer look at the hive driver The Hive driver now performs three steps internally one
is a compiler Hive driver passes query to compiler where it is checked and analyzed then the optimizer kicks in and
the optimized logical plan in the form of a graph of mapreduce and hdfs tasks is obtained and then finally in the
executor in the final step the tasks are executed when we look at the architecture we also have to note the
metastore metastore is a repository for five metadata stores metadata for Hive
tables and you can think of this as your schema and where is it located and it's stored on the Apache Derby DB processing
and resource management is all handled by the map reduce V1 you'll see mapreduce V2 the yarn and the tests
these are all different ways of managing these resources depending on what version of Hadoop you're in Hive uses
mapreduce framework to process queries and then we have our distributed storage which is the hdfs and if you looked at
our Hadoop tutorials you'll know that these are on commodity machines and are
linearly scalable that means they're very affordable a lot of time when you're talking about Big Data you're talking about a tenth of the price of
storing it on Enterprise computers and then we look at the data flow And Hive so in our data flow in Hive we have our
Hive in the Hadoop system and underneath the user interface or the UI we have our driver our compiler our execution engine
and our metastore that all goes into the map reduce and the Hadoop file system so we execute a query and see it coming in
here it goes into the driver step one step two we get a plan what are we going to do refers to the query execution then
we go to the metadata it's like well what kind of metadata are we actually looking at where is this data located what is the schema on it uh then this
that comes back with the metadata into the compiler then the compiler takes all that information and the send plan
returns it to the driver the driver then sends the execute plan to the execution
engine once it's in the execution engine the execution in and acts as a bridge between Hive and Hadoop to process the
query and that's going into your map reduce and your Hadoop file system or your hdfs and then we come back with the
metadata operations it goes back into the meta store to update or let it know
what's going on which also goes to the between it's a communication between the execution engine and The Meta store
execution engine Communications is bi-directionally with the metastore to perform operations like create drop
tables metastore stores information about tables and columns so again we're
talking about the schema of your database and once we have that we have a bi-directional send results
communication back into the driver and then we have the fetch results which goes back to the client so let's take a
little bit look at the hive data modeling Hive data modeling so you have your high data modeling you have your
tables you have your partitions and you have buckets the tables in however created the same weight as done in rdbms
so when you're looking at your traditional SQL server or MySQL server where you might have Enterprise
equipment and a lot of people pulling and moving stuff off of there the tables are going to look very similar and this
makes it very easy to take that information and let's say you need to keep current information but you need to
store all of your years of transactions back into the Hadoop Hive so you match
those those all kind of look the same the tables are the same your databases look very similar and you can easily
import them but you can easily store them into the hive system partitions here tables are organized into
partitions for grouping same type of data based on partition key this can
become very important for speeding up the process of doing queries so if you're looking at dates as far as like
your employment dates of employees if that's what you're tracking you might add a partition there because that might be one of the key things that you're
always looking up as far as employees are concerned and finally we have buckets data present in partitions can
be further divided into buckets for efficient querying again there's that efficiency at this level a lot of times
you're taught you're working with the programmer and the admin of your Hadoop file system to maximize the efficiency
of that file system so it's usually a two-person job and we're talking about Hive data modeling you want to make sure
that they work together and you're maximizing your resources Hive data types so we're talking about Hive data
types we have our primitive data types and our complex data types A lot of this
should look familiar because it mirrors a lot of stuff in SQL in our primitive data types we have the numerical data
types string data type date time data type and miscellaneous data type and
these should be very they're kind of self-explanatory but just in case numerical data is your floats your
integers your short integers all of that numerical data comes in as a number a string of course is character vectors
and numbers and then you have your date time step and then we have kind of a general way of pulling your own created
data types in there that's your miscellaneous data type and we have complex data types so you can store
arrays you can store maps you can store structures and even units in there as we
dig into Hive data types and we have the primitive data types and the complex data types so we look at primitive data
types and we're looking at numeric data types data types like an integer a float a decimal those are all stored as
numbers in the hive data system a string data type data types like characters and strings you store the name of the person
you're working with you know John Doe the city State Tennessee maybe it's Boulder
Colorado USA or maybe it's hyperbad India that's all going to be string
stored as a string character and of course we have our date time data type data types like timestamp date interval
those are very common as far as tracking sales anything like that so you just think if you can type a stamp of time on
it or maybe you're dealing with the race and you want to know the interval how long did the person take to complete whatever task it was all that is date
time data type and then we talk miscellaneous data type these are like Boolean in binary and when you get into
Boolean and binary you can actually almost create anything in there but your yes nose zero one now let's take a look
at complex data types a little closer we have arrays so your syntax is of data
type and it's an array and you can just think of an array as a collection of same entities one two three four if
they're all numbers and you have Maps this is a collection of key value pairs so understanding is so Central to Hadoop
so we store Maps you have a key which is a set you can only have one key per mapped value and so you in Hadoop of
course you collect the same keys and you can add them all up or do something with all the contents of the same key but
this is our map as a primitive type data type in our collection of key value Pairs and then collection of complex
data with comment so we can have a structure we have like column name data type comment column comment so you can
get very complicated structures in here with your collection of data and your commented setup and then we have units
and this is a collection of heterogeneous data types so the Syntax for this is Union type data type data
type and so on so it's all going to be the same a little bit different than the arrays where you can actually mix and match different modes of Hive Hive
operates in two modes depending on the number and size of data nodes we have our local mode and our map reduce mode
when we talk about the local mode it is used when Hadoop is having one data node and the data is small processing will be
very fast in a smaller data sets which are present in local machine and this might be that you have a local file of
stuff you're uploading into the hive and you need to do some processes in there you can go ahead and run those High
processes and queries on it usually you don't see much in the way of a single node Hadoop system if you're going to do
that you might as well just use like an SQL database or even a Java sqlite or
something python ssqlite so you don't really see a lot of single node Hadoop databases but you do see the local mode
in Hive where you're working with a small amount of data that's going to be integrated into the larger database and
then we have the map reduce mode this is used when Hadoop is having multiple data nodes and the data is spread across
various data nodes processing large data sets can be more efficient using this mode and this you can think of instead
of it being one two three or even five computers we're usually talking with the Hadoop file system we're looking at 10
compute fifteen a hundred where this data is spread across all those different Hadoop
nodes difference between Hive and rdbms remember rdbms stands for the relational
database management system let's take a look at the difference between Hive and the rdbms with Hive Hive enforces schema
on read and it's very important that whatever is coming in that's when hive's looking at it and making sure that it
fits the model the rdbms enforce actually writes the data into the
database so it's read the data and then once it starts to write it that's where it's going to give you the error or tell you something's incorrect about your
scheme Hive data size is in petabytes that is hard to imagine you know we're
looking at your personal computer on your desk maybe you have 10 terabytes if it's a high-end computer we're talking
petabytes so that's hundreds of computers grouped together when an rdbms data size is in terabytes very rarely do
you see an rdbms system that's spread over more than five computers and there's a lot of reasons for that with
the rdbms it actually has a high-end amount of rights to the hard drive there's a lot more going on there your
writing and pulling stuff so you really don't want to get too big with an rdbms or you're going to run into a lot of problems with Hive you can take it as
big as you want Hive is based on the notion of write once and read many times this is so important and they call it
worm which is right W10 read R many times m they refer to it as worm and
that's true of any of you a lot of your Hadoop set up it's it's altered a little bit but in general we're looking at
archiving data that you want to do data analysis on we're looking at pulling all that stuff off your rdbms from years and
years and years of business or whatever your company does or scientific research and putting that into a huge data pool
so that you can now do queries on it and get that information out of it with the rdbms it's based on the notion of read
and write many times so you're continually updating this database you're continually bringing up new stuff
new sales the account changes because they have a different licensing now whatever software you're selling all
that kind of stuff where the data is continually fluctuating and then Hive resembles a traditional database by
supporting SQL but it is not a database it is a data warehouse this is very
important it goes with all the other stuff we've talked about that we're not looking at a database but a data
warehouse to store the data and still have fast and easy access to it for doing queries you can think of Twitter
and Facebook they have so many posts that are archived back historically those posts aren't going to change they
made the post they're posted they're there and they're in their database but they have to store it in a warehouse in case they want to pull it back up with
the rdbms it's a type of database management system which is based on the relational model of data and then with
Hive easily scalable at a low cost again we're talking maybe a thousand dollars per terabyte the rdbms is not scalable
at a low cost when you first start on the lower end you're talking about ten thousand per terabyte of data including
all the backup on the models and and all the added Necessities to support it as you scale it up you have to scale those
computers and Hardware up so you might start off with a basic server and then you upgrade to a sun computer to run it
and you spend you know tens of thousands of dollars for that Hardware upgrade with Hive you just put another computer
into your Hadoop file system so let's look at some of the features of hive uh when we're looking at the features of
Hive we're talking about the use of SQL like language called Hive ql a lot of
times you'll see that as hql which is easier than long codes this is nice if you're working with your shareholders
you come to them and you say Hey you can do a basic SQL query on here and pull up the information you need this way you
don't have to take all have your programmers jump in every time they want to look up something in the database they actually now can easily do that if
they're not skilled in programming and script writing tables are used which are similar to The rdbms hence easier to
understand and one of the things I like about this is when I'm bringing tables in from a MySQL server or SQL Server
there's almost a direct reflection between the two so when you're looking at one which is a data which is continually changing and then you're
going into the archive database it's not this huge jump where you have to learn a whole new language you mirror that same
schema into the hdfs into the hive making it very easy to go between the two and then using Hive ql multiple user
simultaneously query data so again you have multiple clients in there and they send in their query that's also true
with the rdbms which kind of queues them up because it's running so fast you don't notice the lag time well you get
that also with the hql as you add more computers in the query can go very quickly depending on how many computers
and how much resources each machine has to pull the information and Hive supports a variety of data types so with
Hive it's designed to be on the Hadoop system which you can put almost anything into the Hadoop file system so with all
that let's take a look at a demo on hive ql or hql before I dive into the
Hands-On demo let's take a look at the website hive.apache.org that's the main website
since Apache it's an Apache open source software this is the main software or
the main site for the build and if you go in here you'll see that they're slowly migrating Hive into beehive and
so if you see beehive versus Hive note The Beehive as the new release is coming out that's all it is it reflects a lot
of the same functionality of Hive it's the same thing and then we like to pull up some kind of documentation on
commands and for this I'm actually going to go to hortonworks Hive cheat sheet
and that's because Horton works and Cloudera two of the most common used builds for Hadoop and for which include
Hive and all the different Tools in there and so hortonworks has a pretty good PDF you can download cheat sheet on
there I believe Cloudera does too but we'll go ahead just look at the Horton one because it's the one that comes up really good and you can see when we look
at the query language it Compares MySQL server to Hive ql or hql and you can see
the basic select we select from columns from table where conditions exist you
know most basic command on there and they have different things you can do with it just like you do with your SQL
and if you scroll down you'll see data types so here's your integer your flow your binary double string timestamp and
all the different data types you can use some different semantics different Keys features functions for running a hive
query command line setup and of course the hive shell setup in here so you can
see right here if we Loop through it it has a lot of your basic stuff in it is we're basically looking at SQL across a
Horton database we're going to go ahead and run our Hadoop cluster Hive demo and
I'm going to go ahead and use the Cloudera quick start this is in the virtual box so again we have an oracle
virtual box which is open source and then we have our Cloudera quick start
which is the Hadoop setup on a single node now obviously Hadoop And Hive are designed to run across a cluster of
computers so we talk about a single node is for Education testing that kind of thing and if you have a chance you can
always go back and look at our demo we had on setting up a Hadoop system in a
single cluster to set a note Down Below in the YouTube 2 video and our team will get in contact with you and send you
that link if you don't already have it or you can contact us at the www.simplylearn.com now in here it's
always important to note that you do need on your computer if you're running on Windows because I'm on a Windows
machine you're going to need it probably about 12 gigabytes to actually run this it used to be goodbye with a lot less
but as things have evolved they take up more and more resources and you need the professional version if you have the
home version I was able to get that to run but boy did it take a lot of extra work to get the home version to let me
use the virtual setup on there and we'll simply click on the Cloudera quick start and I'm going to start that up and this
is starting up our Linux so we have our Windows 10 which is a computer I'm on and then I have the virtual box which is
going to have a Linux operating system in it and we'll skip ahead so you don't have to watch the whole install something interesting to know about the
Cloudera is that it's running on Linux Santos and for whatever reason I've always had to click on it and hit the
escape button for it to spin up and then you'll see the Dos come in here now that
our Cloudera is spun up on our virtual machine with the Linux on we can see here we have our it uses the Thunderbird
browser on here by default and automatically opens up a number of different tabs for us and a quick note
because I mentioned like the restrictions on getting set up on your own computer if you have a home edition computer and you're worried about
setting it up on there you can also go in there and spin up a one month free service on Amazon web service to play
with this so there's other options you're not stuck with just doing it on the quick start menu you can spin this
up in many other ways now the first thing we want to note is that we've come in here into Cloudera and I'm going to
access this in two ways the first one is we're going to use Hue and I'm going to open up Hue and it'll take it a moment
to load from the setup on here and Hue is nice if I go in and use Hue as an
editor into Hive or into the Hadoop setup usually I'm doing it as a from an
admin side because it has a lot more information a lot of visuals less to do with you know actually diving in there
and just executing code and you can also write this code into files and scripts and there's other things you can other
ways you can upload it into Hive but today we're going to look at the command lines and we'll upload it into Hue and
then we'll go into and actually do our work in a terminal window Under The Hive shell now in the Hue browser window if
you go under query and click on the pull down menu and then you go under editor and you'll see Hive there we go there's
our Hive setup I go and click on hive and this will open up our query down here and now it has a nice little B that
shows our Hive going and we can go something very simple down here like show databases and we follow it with the
semicolon and that's the standard in Hive is you always add our punctuation
at the end there and I'll go ahead and run this and the query will show up underneath and you'll see down here since this is a new quick start I just
put on here you'll see it has the default down here for the databases that's the database name I haven't
actually created any databases on here and then there's a lot of other like assistant function tables your databases
up here there's all kinds of things you can research you can look at through Hue as far as a bigger picture the downside
of this is it always seems to lag for me whenever I'm doing this I always seem to run slow so if you're in Cloudera you
can open up a terminal window they actually have an icon at the top you can also go under applications and under
applications system tools and terminal either one will work it's just a regular terminal window and this terminal window
is now running underneath our Linux so this is a Linux terminal window or on our virtual machine which is resting on
our regular Windows 10 machine and we'll go ahead and zoom this in so you can see the text better on your own video and I
simply just clicked on view and zoom in and then all we have to do is type in Hive and this will open up the shell on
here and it takes it just a moment to load when starting up Hive I also want to note that depending on your rights on
the computer you're on in your action you might have to do pseudohive and put in your password and username most
computers are usually set up with the hive login again it just depends on how you're accessing the Linux system and
the hive shell once we're in here we can go ahead and do a simple hql command show databases and if we do that we'll
see here that we don't have any databases so we can go ahead and create a database and we'll just call it office
for today for this moment now if I do show we'll just do the up Arrow up arrow
is a hotkey that works in both Linux and in Hive so I can go back and paste through all the commands I've typed in
and we can see now that I have my there's of course the default database and then there's the office database so
now we've created a database it was pretty quick and easy and we go ahead and drop the database we can do drop
Database Office now this will work on this database because it's empty if your
database was not empty you would have to do cache skate and that drops all the tables in the database and the database
itself now if we do show database and we'll go ahead and recreate our database because we're going to use the office
database for the rest of this Hands-On demo a really handy command to Now set
with the SQL or hql is to use office and what that does is that sets office as
the default database so instead of having to reference the database every time we work with a table we now
automatically assumes that's the database being used whatever tables we're working on the difference is you
put the database name period table and I'll show you in just a minute what that looks like and how that's different if
we're going to have a table and a database we should probably load some data into it so let me go ahead and switch gears here and open up a terminal
window you can just open another terminal window and it'll open up right on top of the one that you have Hive shell running in and when we're in this
terminal window first we're going to go ahead and just do a list which is of course a Linux command you can see all
the files I have in here this is the default load we can change directory to documents we can list in documents and
we're actually going to be looking at employee.csv a Linux command is the cat
you can use this actually to combine documents there's all kinds of things that cat does but if we want to just display the contents of our
employee.csv file we can simply do cat employee CSV and when we're looking at
this we want to know a couple things one there's a line at the top okay so the
very first thing we notice is that we have a header line the next thing we notice is that the data is comma
separated and in this particular case you'll see a space here generally with
these you got to be real careful with spaces there's all kinds of things you got to watch out for because it can cause issues these spaces won't because
these are all strings that the space is connected to if this was a space next to the integer you would get a null value
that comes into the database without doing something extra in there now with most of Hadoop that's important to know
that that you're writing the data once reading it many times and that's true of almost all your Hadoop things coming in
so you really want to process the data before it gets into the database and for
those who of you have studied data transformation that's the adult where you extract transfer form and then load
the data so you really want to extract and transform before putting it into the hive then you load it into the hive with
the transform data and of course we also want to note the schema we have an integer string string integer integer so
we kept it pretty simple in here as far as the way the data is set up the last thing that you're going to want to look up is the source since we're doing local
uploads we want to know what the path is we have the whole path in this case it's home slash Cloudera slash documents and
these are just text documents we're working with right now we're not doing anything fancy so we can do a simple git
edit employee.csv and you'll see it comes up here it's just a text document so I can
easily remove these added spaces there we go and then we go ahead and just save it and so now it has a new setup in
there we've edited it the G edit is usually one of the default that loads into Linux so any text editor will do
back to the hive shell so let's go ahead and create a table employee and what I
want you to note here is I did not put the semicolon on the end here a semicolon tells it to execute that line
so this is kind of nice if you're you can actually just paste it in if you have it written on another sheet and you can see right here where I have create
table employee and it goes into the next line on there so I can do all of my commands at once now just so I don't
have any typo errors I went ahead and just pasted the next three lines in and the next one is our schema if you
remember correctly from the other side we had the different values in here which was ID name Department year of
joining and salary and the ID is an integer name is a string department string you're joining energy salary an
integer and they're in Brackets we put close brackets around them and you could do this all as one line and then we have
row format delimited Fields terminated by comma and this is important because the default is tabs so if I do it now it
won't find any terminated Fields so you'll get a bunch of null values loaded into your table and then finally our
table properties we want to skip the header line count equals one now this is a lot of work for uploading a single
file it's kind of goofy when you're uploading a single file that you have to put all this in here but keep in mind
Hive and Hadoop is designed for writing many files into the database you write
them all in there and then you can they're saved it's an archive it's a data warehouse and then you're able to do all your queries on them so a lot of
times we're not looking at just the one file coming up we're loading hundreds of files you have your reports coming off
of your main database all those reports are being loaded and you have your log files you have I mean all this different
data is being dumped into Hadoop and in this case Hive on top of Hadoop and so we need to let it know hey how do I
handle these files coming in and then we have the semicolon at the end which lets us know to go ahead and run this line
and so we'll go ahead and run that and now if we do a show tables you can see there's our employee on there we can
also describe if we do describe employee you can see that we have our ID integer
name string department string year of joining integer and salary integer and
then finally let's just do a select star from employee very basic SQL and hql
command selecting data and it's going to come up and we haven't put anything in it so as we expect there's no data in it
so if we flip back to our Linux terminal window you can see where we did the cat
employee.csv and you can see all the data we expect to come into it and we also did our PWD and right here you see
the path you need that full path when you are loading data you know you can do
a browse and if I did it right now with just the employee.csv is the name it will work work but that is a really bad
habit in general when you're loading data because it's you don't know what else is going on on the computer you
want to do the full path almost in all your data loads so let's go ahead and flip back over here to our Hive shell
we're working in and the command for this is load data so that says hey we're loading data that's a hive command hql
and we want local data so you got to put down local and path so now it needs to know where the path is now to make this
more legible I'm just going to go ahead and hit enter then we'll just paste the full path in there which I have stored
over on the side like a good prepared demo and you'll see here we have home Cloudera documents employee.csv so it's
a whole path for this text document in here and we go ahead and hit enter in there and then we have to let it know
where the data is going so now we have a source and we need a destination and it's going to go into the table and
we'll just call it employee we'll just match the table in there and because I wanted to execute we put the semicolon
on the end it goes ahead and executes all all three lines now if we go back if
you remember we did the select star from employee just using the up Arrow to page through my different commands I've
already typed in you can see right here we have as we expect we have Rose Sam Mike and Nick and we have all their
information showing in our four rows and then let's go ahead and do uh select and
count we'll just look at a couple of these different select options you can do we're going to count everything from
employee now this is kind of interesting because the first one just pops up with the basic select because it doesn't need
to go through the full map reduce phase but when you start doing a count it does
go through the full map reduce setup in the hive in Hadoop and because I'm doing
this demo on a single node Cloudera virtual box on top of a Windows 10. all
the benefits of running it on a cluster are gone and instead is now going through all those added layers so it
takes longer to run you know like I said when you do a single node as I said earlier it doesn't do any good as an
actual distribution because you're only running it on one computer and then you've added all these different layers to run it and we see it comes up with
four and that's what we expect we have four rows we expect four at the end and if you remember from our cheat sheet
which we brought up here from Horton's it's a pretty good one there's all these different commands we can do we'll look at one more command where we do the uh
what they call sub queries right down here because that's really common to do a lot of sub queries and so we'll do
select star or all different columns from employee now if we weren't using
the office database it would look like this from Office dot employee and either
one will work on this particular one because we have office set as a default on there so from office employee and
then the command where creates a subset and in this case we want to know where the salary is greater than 25
000. there we go and of course we end with our semi colon if we run this query you can see it pops up and there's our
salaries the people top earners we have Rose and it and Mike and HR kudos to
them of course they're fictitional I don't actually we don't actually have a rose and a mic in those positions or maybe we do so finally we want to go
ahead and do is we're done with this table now remember you're dealing with the data warehouse so you usually don't
do a lot of dropping of tables and databases but we're going to go ahead and drop this table here before we drop
it one more quick note is we can change it so what we're going to do is we're going to alter table office employee and
we want to go ahead and rename it there's some other commands you can do in here but rename is pretty common and we're going to rename it to and it's
going to stay in office and turns out one of our shareholders really doesn't
like the word employee he wants employees plural it's a big deal to him so let's go ahead and change that name
for the table it's that easy because it's just changing the meta data on there and now if we do show table tables
you'll see we now have employees not employee and then at this point maybe we're doing some house cleaning because
this is all practice so we're going to go ahead and drop the table and we'll drop table employees because we changed
the name in there so if we did employee just give us an error and now if we do show tables you'll see how the tables
are gone now the next thing we want to go and take a look at and we're going to walk back through the loading of data uh just real quick because we're going to
load two tables in here and let me just float back to our terminal window so we
can see what those tables are that we're loading and so up here we have customer we have a customer file and we have an
order file we want to go ahead and put the customers and the orders into here so those are the two we're doing and of
course it's always nice to see what you're working with so let's do our cat customer dot CSV we could always do G
edit but we don't really need to edit these we just want to take a look at the data in customer and important in here
is again we have a header so we have to skip a line comma separated nothing odd
with the data we have our schema which is integer string integer string integer
so you know you'd want to take that note that down or flip back and forth when you're doing it and then let's go ahead and do cat
order.csv and we can see we have oid which I'm guessing is the order ID we
have a date up something new we've done integers and strings but we haven't done date when you're importing new and you
never worked with the date date's always one of the more trickier fields to port in and that's true of just about any
scripting language I've worked with all of them have their own idea of how date's supposed to be formatted what the
default is this particular format or its year and it has all four digits Dash
month two digits Dash day is the standard import for the hive so if
you'll have to look up and see what the different formats are if you're going to do a different format in there coming in or you're not able to pre-process the
data but this would be a pre-processing of the data thing coming in if you remember correctly from our ad which is
E just in case you weren't able to hear me last time e t l which stands for
extract transform then load so you want to make sure you're transforming this data before it gets into here and so
we're going to go ahead and bring both this data in here and really we're doing this so we can show you the basic join
there is if you remember from our setup merge join all kinds of different things you can do but joining different data
sets is so common so it's really important to know how to do this we need to go ahead and bring in these two data sets and you can see where I just
created a table customer here's our schema the integer name age address
salary here's our delimited by commas and our table properties where we skip a line well let's go ahead and load the
data first and then we'll do that with our order and let's go ahead and put that in here and I've got it split into
three lines you can see it easily we've got load data local in path so we know we're loading data we know it's local
and we have the path here's the complete path for uh oops this is supposed to be order CSV grab the wrong one of course
it's going to give me errors because you can't recreate the same table on there and here we go create table here's our
integer date customer the basic setup that we had coming in here for our
schema row format commas table properties skip header line and then finally let's load the data into our
order table load data local in path home Cloudera documents ordered at CSV into
table order now if we did everything right we should be able to do select star from customer and you can see we
have all seven customers and then we can do select star from order and we have
four orders so this is just like a quick frame we have a lot of times when you have your customer databases in business
you have thousands of customers from years and years and some of them you know they move they close their business
they change names all kinds of things happen so we want to do is we want to go ahead and find just the information
connected to these orders and who's connected to them and so let's go ahead and do it's a select because we're going
to display information so select and this is kind of interesting we're going to do c dot ID and I'm going to Define c
as customer as a customer table in just a minute then we're going to do c dot name and again we're going to define the
c c dot age so this means from the customer we want to know their ID their
name their age and then you know I'd also like to know the order amount so let's do o for DOT amount and then this
is where we need to go ahead and Define what we're doing and I'll go and capitalize from customer so we're going
to take the customer table in here and we're going to name it C that's where the C comes from so that's the customer
table C and we want to join order as o that's where our o comes from so the O
DOT amount is what we're joining in there and then we want to do this on we got to tell it how to connect the two
tables C dot ID equals o Dot customer underscore ID so now we know how they're
drawn and I remember we have seven customers in here we have four orders and as a processes we should get a
return of four different names joined together and they're joined based on of
course the orders on there and once we're done we now have the order number the person who made the order their age
and the amount of the order which came from the order table uh so you have your different information and you can see
how the join works here very common use of tables and hql and SQL and let's do
one more thing with our database and then I'll show you a couple other Hive commands and let's go ahead and do a
drop and we're going to drop Database Office and if you're looking at this and
you remember from earlier this will give me an error and let's just see what that looks like it says failed execute
exception one or more tables exist so if you remember from before you can't just
drop a database unless you tell it to Cascade that lets it no I don't care I
mean tables are in it let's get rid of it and in Hadoop since it's an art it's a warehouse a data warehouse you usually
don't do a lot of dropping maybe at the beginning when you're developing the schemas and you realize you messed up you might drop some stuff but down the
road you're really just adding commodity machines to take up so you can store more stuff on it so you usually don't do
a lot of database dropping and some other fun commands to know is you can do select round 2.3 as round value you can
do a round off in Hive we can do as floor value which is going to give us a
2 so it turns it into an integer versus a float it goes down you know basically truncates it but it goes down and we can
also do ceiling which is going to round it up so we're looking for the next integer above there's a few commands we
didn't show in here because we're on a single node as as an admin to help speciate the process you usually add in
partitions for the data and buckets you can't do that on a single node because the when you add a partition the
partitions it separate nodes but beyond that you can see that it's very straightforward we have SQL coming in
and all your basic queries that are in SQL are very similar to hql let's get
started with pig why Pig what is pig map reduce versus Hive versus pig hopefully
you've had a chance to do our Hive tutorial and our map reduce tutorial if you haven't send a note over to Simply
learn and we'll follow up with a link to you we'll look at Pig architecture working a pig pig latin data model Pig
execution modes a use case Twitter and features a pick and then we'll tag on a
short demo so you can see Pig In Action so why pig as we all know Hadoop uses
mapreduce to analyze and process big data processing Big Data consumed more time so before we had the Hadoop system
they'd have to spend a lot of money on a huge set of computers and Enterprise machines so we introduced the Hadoop map
reduce and so afterwards processing Big Data was faster using up reduce then
what is the problem with map reduce prior to 2006 all mapreduce programs
were written in Java non-programmers found it difficult to write lengthy Java codes they faced issues in incorporating
map sort reduce to fundamentals of mapreduce while creating a program you can see here map phase Shuffle and sort
reduce phase eventually it became a difficult task to maintain and optimize a code due to which the processing time
increased you can imagine a manager trying to go in there and needed in a simple query to find out data and he has
to go talk to the programmers anytime he wants anything so that was a big problem not everybody wants to have a on-call
programmer for every manager on their team Yahoo faced problems to process and analyze large data sets using Java as a
cause or complex and lengthy there was a necessity to develop an easier way to analyze large data sets without using
time-consuming complex Java modes and codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it was developed with a vision to analyze and process large data sets without using
complex Java codes Pig was developed especially for non-programmers pig used
simple steps to analyze data sets which was time efficient so what exactly is pick pig is a scripting platform that
runs on Hadoop clusters designed to process and analyze large data sets and so you have your pig which uses SQL like
queries they're definitely not SQL but some of them resemble SQL queries and then we use that to analyze our data Pig
operates on various types of data like structured semi-structured and unstructured data let's take a closer
look at mapreduce versus Hive versus pig so we start with a compiled language
your map reduce and we have Hive which is your SQL like query and then we have
pig which is a scripting language it has some similarities to SQL but it has a lot of its own stuff remember SQL like
query which is what Hive is based off looks for structured data and so when we get into scripting languages like Pig
now we're dealing more with semi-structure and even unstructured data with a Hadoop map reduce we have a
need to write long complex codes with Hive no need to write complex codes you
could just put it in a simple SQL query or hql Hive ql and in pig no need to
write complex codes as we have pig latin now remember in the map reduce it can produce structured semi-structured and
unstructured data and as I mentioned before Hive can process only structured data think rows and columns where Pig
can process structured semi-structured and unstructured data you can think of structured data as rows and columns
semi-structured as your HTML XML documents if you have on your web pages and unstructured could be anything from
groups of documents and written format Twitter tweets any of those things come
in as very unstructured data and with our Hadoop mapreduce we have a lower level of abstraction with both Hive and
pig we have a higher level abstract so it's much more easy for someone to use without having to dive in deep and write
a very lengthy map reduce code and those map and reduce codes can take 70 80 lines of code when you can do the same
thing in one or two lines with however Pig this is the advantage Pig has over Hive it can process only structured data
in Hive while in pig it can process structured semi-structured and unstructured data some other features to
note that separates the different query languages is we look at map and reduce map reduce supports partitioning
features as does Hive Pig no concept of partitioning in picks it doesn't support your partitioning feature your
partitioning features allow you to partition the data in such a way that it can be queried quicker you're not
mapreduce uses Java and python while Hive uses an SQL like query language known as Hive ql or hql Pig Latin is
used which is a procedural data flow language mapreduce is used by programmers pretty much as
straightforward on Java Hive is used by data analysts pig is used by researchers and programmers certainly there's a lot
of mix between all three programmers have been known to go in and use a hive for quick query and anybody's been able
to use Pig for a quick query or research under map and reduce code performance is really good under Hive code performance
is lesser than map and reduce and pick under Pig Code performance is lesser than mapreduce but better than Hive so
if we're going to look at speed and time the map reduce is going to be the fastest performance on all of those
where Pig will have second and high follows in the back let's look at components of pig pig has two main
components we have pig Latin Pig Latin is the procedural data flow language used in pig to analyze data is easy to
program using Piglet and it is similar to SQL and then we have the runtime engine runtime engine represents the
execution environment created to run Pig Latin programs it is also a compiler that produces mapreduce programs uses
hdfs or your Hadoop file system for storing and retrieving data and as we dig deeper into the pig architecture
we'll see that we have pig latin scripts programmers write a script in pig latin to analyze data using Pig then you have
the grunt shell and it actually says grunt when we start it up and we'll show you that here in a little bit which goes
into the pig server and this is where we have our parser parser checks the syntax of the pig script after checking the
output will be a dag directed acelic graph and then we have an Optimizer
which optimizes after your dag your logical plan is passed to The Logical Optimizer where an optimization takes
place finally the compiler converts the dag into mapreduce jobs and then that is executed on the map reduce under the
execution engine the results are displayed using dump statement and stored in hdfs using store statement and
again we'll show you that kind of end you always want to execute everything once you've created it and so
dump is kind of our execution statement and you can see right here as we were talking about earlier once we get to the
execution engine and it's coded into mapreduce then the map reduce processes it onto the hdfs working a pig pig latin
script is written by the users so you have load data and right Pig script and
pig operations so we look at the working of pig pig latin script is written by
the users there's step one we load data and write Pig script and step two in
this step all the pig operations are performed by parser Optimizer and compiler so we go into the pig
operations and then we get to step three execution of the plan in this stage the results are shown on the screen
otherwise stored in the hdfs as per the code so it might be of a small amount of data you're reducing it to and you want
to put that on the screen or you might be converting a huge amount of data which you want to put back into the
Hadoop file system for other use let's take a look at the pig latin data the data model of pig latin helps pig to
handle various types of data for example we have Adam Rob or 50. Adam represents
any single value of primitive data type in pig latin like integer float string it is stored as a string two bolts so we
go from our atom which are most basic thing so if you look at just Rob or just 50 that's an atom that's our most basic
object we have in pig latin then you have a tuple Tuple represents sequence of fields that can be of any data type
it is the same as a row in rdbms for example a set of data from a single row
and you can see here we have Rob comma five and you can imagine with many of our other examples we've used you might
have the ID number the name where they live their age their date of starting
the job that would all be one row and store it as a tuple and then we create a bag a bag is a collection of tuples it
is the same as a table in rdbms and is represented by brackets and you can see here we have our table with Rob 5 mic 10
and we also have a map a map is a set of key value pairs key is of character
array type and a value can be of any type it is represented by the brackets and so we have name and age where the
key value is Mike and 10. pig latin has a fully nestable data model that means
one data type can be nested within another here's a diagram representation of pig latin data model and in this
particular example we have basically an ID number a name an age and a place and we break this apart we look at this
model from Pig Latin perspective we start with our field and if you remember a field contains basically an atom it is
one particular data type and the atom is stored as a string which it then converts it into either an integer or
number or character string next we have our Tuple and in this case you can see that it represents a row so our Tuple
would be three comma Joe comma 29 comma California and finally we have our bag
which contains three rows in it in this particular example let's take a quick look at Pig execution modes
C Works in two execution modes depending on where the data is reciting and where the pig script is going to run we have
local mode here the pig engine takes input from the Linux file system and the output is stored in the same file system
local mode local mode is useful in analyzing small data sets using Pig and
we have the mapreduce mode here the pig engine directly interacts and executes in hdfs and mapreduce in the mapreduce
mode queries written in pig latin are translated into mapreduce jobs in our run on a Hadoop cluster by default Pig
runs in this mode there are three modes in pig depending on how a pig latin code can be written we have our interactive
mode batch mode and embedded mode the interactive mode means coding and executing the script line by line when
we do our example we'll be in the interactive mode in batch mode all scripts are coded in a file with the
extension dot Pig and the file is directly executed and then there's embedded mode Pig lets its users Define
their own functions udfs in a programming language such as Java so
let's take a look and see how this works in a use case in this case use case Twitter users on Twitter generate about
500 million tweets on a daily basis the Hadoop mapreduce was used to process and
analyze this data analyzing the number of tweets created by a user in the Tweet table was done using mapreduce in Java
programming language and you can see the problem it was difficult to perform mapreduce operations as users were not
well versed with written complex Java codes so Twitter used Apache pig to
overcome these problems and let's see how let's start with the problem statement analyze the user table and
tweet table and find out how many tweets are created by a person and here you can see we have a user table we have Alice
Tim and John with their ID numbers one two three and we have a tweet table in the Tweet table you have your the ID of
the user and then what they tweeted Google was a good whatever it was tennis. spacecraft Olympics politics
whatever they're tweeting about the following operations were perform for analyzing given data first the Twitter
data is loaded into the pig storage using load command and you can see here we have our data coming in and then
that's going into Pig storage and this data is probably on an Enterprise computer so this is actually active
twitters going on and then it goes into Hadoop file system remember the Hadoop file system is a data warehouse for
storing data and so the first step is we want to go ahead and load it into the pig storage into our data storage system
the remaining operations performed are shown Below in join and group operation the tweet and user tables are joined and
grouped using co-group command and you can see here where we add a whole column when we go from user names and tweet to
the ID link directly to the name so Alice was user one Tim was 2 and John 3
and so now they're listed with their actual tweet the next operation is the aggregation the tweets are counted
according to the names the command used is count so it's very straightforward we just want to count how many tweets each
user is doing and finally the result after the count operation is joined with the user table to find out the username
and you can see here where Alice had three Tim two and John 1. Pig reduces the complexity of the operations which
would have been lengthy using mapreduce and joining group operation the tweet and user tables are joined and grouped
using co-group command the next operation is the aggregation the tweets are counted according to the names the
command used as count the result after the count operation is joined with the user table to find out the username and
you can see we're talking about three lines of script versus a mapreduce code of about 80 lines finally we could find
out the number of tweets created by a user in a simple way so let's go quickly over some of the features of pig that we
already went through most of these first ease of programming as Pig Latin is similar to SQL lesser lines of code need
to be written short development time as the code is simpler so we can get our queries out rather quickly instead of
having to have a programmer spend hours handles all kind of data like structured semi-structured and unstructured pig
lets us create user defined functions Pig offers a large set of operators such
as join filter and so on it allows for multiple queries to process unparallel and optimization and compilation is easy
as it is done automatically and internally [Music]
so enough Theory let's dive in and show you a quick demo on some of the commands
you can do and pick today's setup will continue as we have in the last three demos to go ahead and use Cloudera quick
start and we'll be doing this in Virtual box we do have a tutorial in setting that up you can send a note to our
simply learned team and then get that linked to you once your Cloudera quick start has spun up and remember this is
virtualbox we've created a virtual machine and this virtual machine is Centos Linux once it's spun up you'll be
in a full Linux system here and it's you can see we have Thunderbird browser
which opens up to the Hadoop basic system browser and we can go underneath the Hue where it comes up by default if
you click on the pull down menu and go under editor you can see there's our Impala our Hive uh Pig along with a
bunch of other query languages you can use and we're going under Pig and then once you're in pig we can go ahead and
use our command line here and just click that little blue button to start it up and running we will actually be working
in terminal window and so if you're in the Cloudera quick start you can open up the terminal window up top or if you're
in your own setup and you're logged in you can easily use all of your commands here in terminal window and we'll zoom
in that way you get a nice view of what's going on there we go now for our first command we're going to do a Hadoop
command and import some data into the Hadoop system in this case a pig input
let's just take a look at this we have uh Hadoop let's know it's going to be a Hadoop command DFS there's actually Four
variations of DFS so if you have hdfs or you know whatever that's fine all four of them Point used to be different
setups underneath different things and now they all do the same thing and we want to put this file which in this case
is under home Cloudera documents and Sample and we just want to take that and put it into the pig input and let's take
a look at that file if I go under my document browsers and open this up you'll see it's got a simple ID name
profession and age we have one Jack engineer 25. and that was in one of our
earlier things we had in there and so let's go ahead and hit enter and execute this and now we've uploaded that data
and it's gone into our Pig input and then a lot of the Hadoop commands mimic
the Linux commands and so you'll see we have cat as one of our commands or it
has a hyphen before it so we execute that with Hadoop DFS hyphen cat pull it back up pig input because that's what we
called it that's where we put our sample CSV at and we actually cute this you can see from our Hadoop system it's going to
go ahead and pull that up and sure enough it pulls out the data file we just put in there and then we can simply enter the pig Latin or Pig editor mode
by typing in pick and we can see here by our grunt I told you that's how it was
going to tell you we're in pig latin there's our grunt command line so we are now in the pig shell and then we'll go
ahead and put our load command in here and the way this works is I'm going to have office equals load and here's my
load when this case is going to be Pig input we have that in single brackets remember that's where the data is in the
Hadoop file system where we dumped it into there we're going to using Pig storage our data was separated as with a
comma so there's our comma separator and then we have as and in this case we have an ID character array name character
array profession character array and age character array and we're just going to do a model's character arrays just to
keep this simple for this one and then when I hit put this all in here you can see that's our full command line going
in and we have our semicolon at the end so when I hit enter it's now set office up but it hasn't actually done anything
yet it doesn't do anything until we do dump office so there's our Command to
execute whatever we've loaded or whatever setup we have in here and we run that you can see it go through the
different languages and this is going through the map reduce remember we're not doing this locally we're doing this on the Hadoop setup and once we've
finished our dump you can see we have ID name profession age and all the information that we just dumped into our
pick oh we can now do let's say oh let's say we have a request just for we'll
keep it simple in here but just for the name and age and so we can go office we'll call it each as our variable
underscore each and we'll say for each office generate name comma H and for
each means that we're going to do this for each row and if you're thinking map reduce you know that this is a map
function because this may mapping each row and generating name and age on here and of course we want to go ahead and
close it with a semicolon and then once we've created our query or the command line in here let's go ahead and dump
office underscore each and with our semicolon and this will go through our
map reduce setup on here and if we were on a large cluster the same processing
time would happen in fact it's really slow because I have multiple things on this computer and this particular
virtual box is only using a quarter of my processor it's only dedicated to this
and you can see here there it is name and age and it also included the top row since we didn't delete that out of there
or tell it not to and that's fine for this example you need to be aware of those things when you're processing a
significantly large amount of data or any data and we can also do office and we'll call this DSC for descending so
maybe the boss comes to you and says hey can we order office by ID descending and
of course your boss you've taught them how to to your shareholder it sounds a little derogatory we say boss you've
talked to the shareholder and you said and you've taught him a little bit of Pig Latin and they know that they can now create office description and we can
order office by ID description and of course once we do that we have to dump office underscore description so that
it'll actually execute and there goes into our map reduce it'll take just a moment for it to come up because again
I'm running on all the quarter of my processor and you can see we now have our IDs in descending order returned
let's also look at and this is so important with anytime you're dealing with big data let's create office with a
limit and you can of course do any of this instead of with office we could do
this with office descending so you get just the top two IDs on there we're going to limit just to two and of course
to execute that we have to dump office underscore limit and you can just think of dumping your garbage into the pigpen
for the pig to eat there we go dump office limit two and that's going to just limit our office to the top 2. and
for our output we get our first row which had our ID name profession and age and our second row which is Jack who's
an engineer Let's do an uh filter we'll call it office underscore filter you
guessed it equals filter office by profession equals and keep note this is
uh similar to how python does it with the double equal signs for equal for doing a true false statement so for your
logic statement remember to use two equal signs in Pig and we're going to say it equals doctor so we want to find
out how many doctors do we have on our list and we'll go ahead and do our dump we're dumping all our garbage into the
pig pen and we're letting Pig take over and see what it can find out and see who's a doctor on our list and we find
uh employee ID number two Bob is a doctor 30 years old for this next
section we're going to cover something we see it a lot nowadays in data analysis and that's word counting
tokenization that is one of the next big steps as we move forward in our data
analysis where we go from say stock market analysis of highs and lows and all the numbers to what are people
saying about companies on Twitter what are they saying on the web pages and on
Facebook something you need to start counting words and finding out how many words are totaled I mean in the first
part of the document and so on we're going to cover a very basic word count example and in this case I've created a
document called wordrows.txt and you can see here we have simply learned is a company supporting online learning
simply learn helps people attain their certifications simply learn as an online community I love simply learn I love
programming I love data analysis and I went and saved this into my documents folder so we could use it and let me go
ahead and open up a new terminal window for our word count let me go and close the old one so we're going to go in here
and instead of doing this as Pig we're going to do pig minus X local and what
I'm doing is I'm telling the pig to start the pig shell but we're going to be looking at files local to our virtual
box or the Centos machine and let me go ahead and hit enter on there just map this up there we go and it will load Pig
up and it's going to look just the same as the pig we were doing which was defaulted to high to our Hadoop system
to our hdfs this is now defaulted to the local system now we're going to create
lines we're going to load it straight from the file remember last time we took the hdfs and loaded it into there and
then loaded it into Pig since we've gone the local we're just going to run a local script we have lines equals load
home the actual full path home Cloudera documents and I called it wordrows.txt
and as line is a character array so each line and I've actually you can change
this to read each document I certainly have done a lot of document analysis and then you go through and do word counts
and different kind of counts in there so once we go ahead and create our line instead of doing the dump we're going to
go ahead and start entering all of our different setups for each of our steps we want to go through and let's just
take a look at this next one because the load is straightforward we're loading from this particular file since we're
locals loading it directly from here instead of going into the Hadoop file system and it says as and then each line
is read as a character array now we're going to do words equal for each of the
lines generate Flat tokenize Line space as word now there's a lot of ways to do
this this is if you're a programmer you're just splitting the line up by spaces there's actual ways to tokenize
it you gotta look for periods capitalization there's all kinds of other things you play with with this but
for the most basic word count we're just going to separate it by spaces the flattened takes the line and just
creates a it flattens each of the words out so this is we're just going to generate a bunch of words for each line
and then each each of those words is as a word a little confusing in there but if you really think about it we're just
going down each line separating it out and we're generating a list of words one thing to note is the default for
tokenize you can just do tokenized line without the space in there if you do
that it'll automatically tokenize it by space you can do either one and then we're going to do group we're going to group it by words so we're going to
group words by word so when we we split it up each token is a word and it's a
list of words and so we're going to grouped equals group words by word so we're going to group all the same words together and if we're going to group
them then we want to go ahead and count them and so for count we'll go ahead and create a word count variable and here's
our four each so for each grouped grouped is our line where we group all the words in the line that are similar
we're going to generate a group and then we're going to count the words for each grouped so for each line regroup the
words together we're going to generate a group and that's going to count the words we want to know the word count in
each of those and that comes back in our word count and finally we want to take this and we want to go ahead and dump
word count and this is a little bit more what you see when you start looking at run scripts you'll see right here these
these lines right here we have have each of the steps you take to get there so we load our file for each of our lines
we're going to generate and tokenize it into words then we're going to take the words and we're going to group them by
same words for each grouped we're going to generate a group and we're just going to count the words so we're going to
summarize all the words in here and let's go ahead and do our dump word count which executes all this and it
goes through our mapreduce it's actually a local Runner you'll see down here you start seeing where they still have mapreduce but it's a special Runner
we're mapping it that's a part of each row being counted and grouped and then when we do the word count that's the
reducer the reducer creates these keys and you can see I is used three times a
came up once and came up once is to continue on down here to attain online
people company analysis simply learn they took the top rating with four
certification so all these things are encountered in the how many words are used in a data analysis this is probably
the very the beginnings of data analysis where you might look at it and say oh they mentioned love three times so
whatever's going on in this post it's about love and uh what do they love and then you might attach that to the
different objects in here so you can see that pig latin is fairly easy to use
there's nothing really you know it may it takes a little bit to learn the script depending on how good your memory
is as I get older my memory leaks a little bit more so I don't memorize it as much but that was pretty straightforward the script we put in
there and then it goes through the full map reduce localized run comes out and like I said it's very easy to use that's
why people like Pig Latin is because it's intuitive one of the things I like about Pig Latin is when I'm
troubleshooting when we're troubleshooting a lot of times you're working with a small amount of data and
you start doing one line at a time and so I can go lines equal load and there's my loaded text and maybe I'll just dump
lines and then it's going to run it's going to show me how all the lines that I'm working on in the small amount of data and that way I can test that if I
got an error on there that said oh this isn't working maybe I'll be like oh my gosh I'm in mapreduce or I'm in the
basic grunt shell instead of the local path current so maybe it'll generate an error on there and you can see here it
just shows each of the lines going down Hive versus pig on one side we'll have our sharp Stinger on our black and
yellow friend and on the other side our thick hide on our Pig let's start with an introduction to hbase back in the
days data used to be less and was mostly structured we can see we have structured data here we usually had it like in a
database where you had every field was exactly the correct length so if you had a name field at exactly 32 characters I
remember the old access database in Microsoft the files are small if we had you know hundreds of people in one
database that was considered Big Data this data could be easily stored in relational database or rdbms and we talk
about relational database you might think of Oracle you might to go SQL Microsoft SQL MySQL all of these have
evolved even from back then to do a lot more today than they did but they still fall short in a lot of ways and they're
all examples of an rdms or relationship database then internet evolved and huge
volumes of structured and semi-structured data got generated and you can see here with the semi-structured data we have email if
you look at my spam filter you know we're talking about all the HTML Pages XML which is a lot of time is displayed
on our HTML and help desk Pages Json all of this really has just even in the last
each year it almost doubles from the year before how much of this is generated so storing and processing this
data on an rdbms has become a major problem and so the solution is we use
Apache hbase Apache hbase was the solution for this let's take a look at
the history the hbase history and we look at the hbase history we're going to start back in 2 2006 November Google
released a paper on big table and then in 2017 just a few months later
age-based prototype was created as a Hadoop contribution later on in the Year 2007 in October first usable hbase along
with the Hadoop .15 was released and then in January 2008 hbase became the
sub-project of Hadoop and later on that year in October all the way into September the next year hbase was
released to 0.81 version the 0.19 version and 0.20 and finally in May of
2010 hbase became Apache top level project and so you can see in the course of about four years hbase started off as
just an idea on paper and has evolved all the way till 2010 as a solid project
under the Apache and since 2010 it's continued to evolve and grow as a major
source for storing data in semi-structured data so what is hbase
hbase is a column oriented database management system derived from Google's
nosql database bigtable that runs on top of the Hadoop file system or the hdfs
it's an open source project that is horizontally scalable and that's very important to understand that you don't
have to buy a bunch of huge expensive computers you're expanding it by continually adding commodity machines
and so it's a linear cost expansion as opposed to being exponential no SQL database written in Java which permits
faster querying so Java is the back end for the hbase setup and it's well suited
for sparse data sets so can contain missing or n a values and this doesn't
Boggle it down like it would in other database companies using hbase so let's take a look and see who is using this no
SQL database for their servers and for storing their data and we have hortonworks which isn't a surprise
because they're one of the like Cloudera hortonworks they are behind Hadoop and one of the big developments and backing
of it and of course Apache hbase is the open source behind it and we have Capital One as Banks you also see Bank
of America where they're collecting information on people and tracking it so their information might be very sparse
they might have one Bank way back when the collected information as far as the person's family and what their income
for the whole family is and their personal income and maybe another one doesn't collect the family income as you
start seeing where you have data that is very difficult to store where it's missing a bunch of data hubspot's using
it Facebook certainly all of your Facebook Twitter most of your social medias are using it and then of course
there's JPMorgan Chase and Company another bank that uses the hbase as their data warehouse for NOS SQL let's
take a look at an hbase use case so we can dig a little bit more into it to see how it functions telecommunication
company that provides mobile voice and multimedia Services across China the China mobile and China mobile they
generate billions of call detailed records or CDR and so these cdrs and all
these records of these calls and how long they are and different aspects of the call maybe the tower they're broadcasted from all of that is being
recorded so they can track it a traditional database systems were unable to scale up to the vast volumes of data
and provide a cost-effective solution no good so storing in real-time analysis of
billions of call records was a major problem for this company solution Apache H base hbase stores billions of rows of
detailed call records HP performs fast processing of Records using SQL queries
so you can mix your SQL and nosql queries and usually just say nosql
queries because the way the query Works applications of hbase one of them would be in the medical industry hbase is used
for storing genome sequences storing disease history of people of an area and
you can imagine how sparsat is as far as both of those a genome sequence might be only have pieces to it that each person
is unique or is unique to different people and the same thing with disease you really don't need a column for every
possible disease a person could get you just want to know what those diseases those people have had to deal with in
that area e-commerce hbase is used for storing logs about customer search history performs analytics and Target
advertisement for Better Business insights sports hbase stores match details in the history of each match
uses this data for better prediction so when we look at eight space we all want to know what's the difference between hbase versus rdbms that is a relational
database management system hbase versus rdbms so the a space does not have a
fixed schema a schema less defines only column families and we'll show you what
that means later on an rdbms has a fixed schema which describes the structure of the tables and you can think of this as
you have a row and you have columns and each column is a very specific structure how much data can go in there and what
it does with the age base it works well with structured and semi-structured data with the Rd dbms it works only well with
structured data with the age space it can have denormalized data it can contain missing or null values with the
rdbms it can store only normalized data now you can still store a null value in the rdbms but it still takes up the same
space as if you're storing a regular value in many cases and it also for the hbase is built for y tables it can be
scaled horizontally for instance if you were doing a tokenizer of words and word
clusters you might have 1.4 million different words that you're pulling up and combinations of words so with an
rdbms it's built for thin tables that are hard to scale you don't want to store 1.4 million columns in your SQL
it's going to crash and it's going to be very hard to do searches with the age base it only stores that data which is
part of whatever row you're working on let's look at some of the features of the hbase it's scalable data can be
scaled across various nodes as it is stored in the hdfs and I always think about this it's a linear add-on for each
terabyte of data I'm adding on roughly a thousand dollars in commodity Computing with an Enterprise machine we're looking
at about 10 000 at the lower end for each terabyte of data and that includes all your backup and redundancy so it's a
big difference it's like a tenth of the cost to store it across the hbase it has automatic failure support right ahead
log across clusters which provides automatic support against failure consistent read and write hbase provides
consistent read and write of the data it's a Java API for client access provides easy to use Java API for
clients block cache and Bloom filters so the hbase supports block caching and
Bloom filters for high volume query optimization let's dig a little deeper into the hbase storage a space column
oriented storage and I told you we're going to look into this to see how it stores the data and here you can see you
have a row key this is really one of the important references is each row has to have its own key or your row ID and then
you have your column family and in here you can see we have column family one column family two column family three
and you have your column qualifiers so you can have in column family one you can have three columns in there and
there might not be any data in that so when you go into column family one and do a query for every column that contains a certain thing that row might
not have anything in there and not be queried where in column family two maybe you have column one filled out and
column three filled out and so on and so forth and then each cell is connected to the row where the data is actually
stored let's take a look at this at what it looks like when you fill the data in so in here we have a row key with a row
ID and we have our employee ID one two three that's pretty straightforward you probably would even have that on an SQL
server and then you have your column family this is where it starts really separating out your column family might
have personal data and under personal data you would have name City age you
might have a lot more than just that you might have number of children you might have degree all those kinds of different
things that go into personal data and some of them might be missing you might only have the name and the age of an
employee you might only have the name the city and how many children and not the age and so you can see with the
personal data you can now collect a large variety of data and stored in the hbase very easily and then maybe you
have a family of professional data your designation your salary all the stuff that the employee is doing for you in
that company let's dig a little deeper into the hbase architecture and so you can see here what looks to be a
complicated chart it's not as complicated as you think from the Apache a space we have the Zookeeper which is
used for monitoring what's going on and you have your age Master this is the hbase master of science regions and load
balancing and then underneath the region or the hbase master then under the H master or H base Master you have your
reader server serves data for read and write and the region server which is all your different computers you have in
your Hadoop cluster you'll have a region an H log you'll have a store memory store and then you have your different
files for H file that are stored on there and those are separated across the different computers and that's all part
of the hdfs storage system so we look at the Architectural Components or regions
and we're looking at we're drilling down a little bit hbase tables are divided horizontally by a row so you have a key
range into regions so each of those IDs you might have IDs one to twenty twenty
one to 50 or whatever they are regions are assigned to the nodes in the cluster called region servers a region contains
all rows in the table between the region start key and the End Key again 1 to 10
11 to 20 and so forth these servers serve data for read and write and you can see here we have the client and the
get and the git sends it out and it finds out where that startup is between which start keys and in keys and then it
pulls the data from that different region server and so the region sign data definition language operation
create delete are handled by the H master so the hmaster is telling it what are we doing with with this data what's
going out there assigning and reassigning regions for Recovery or load balancing and monitoring all servers so
that's also part of it so you know if your IDs if you have 500 IDs across three servers you're not going to put
400 IDs on server 1 and 100 on the server 2 and leaves Region 3 and Region
4 empty you're going to split that up and that's all handled by the H master and you can see here it monitors region
servers assigns regions to region servers assigns regions to Regions servers and so forth and so forth hbase
has a distributed environment where age Master alone is not sufficient to manage everything hence zookeeper was
introduced it works with hmaster so you have an active h Master which sends a heartbeat signal to zookeeper indicating
that it's active and the Zookeeper also has a heartbeat to the region servers so the region servers send their status to
zookeeper indicating they are ready for read and write operation inactive server acts as a backup if the active age
Master fails it will come to the the rescue active age master and region servers connect with a session to
zookeeper so you see your activate Master selection your region server session they're all looking at the
Zookeeper keeping that pulse an active age Master region server connects with a session to the zoo keeper and you can
see here where we have ephemeral nodes for active sessions via heartbeats to
indicate that the region servers are up and running so let's take a look at hbase read or write going on there's a
special hbase catalog table called The Meta table which holds a location of the regions in the cluster here's what
happens the first time a client reads or writes data to age base the client gets the region server the host the meta
table from zookeeper and you can see right here the client has a request for your region server and goes hey zookeeper can you handle this the
Zookeeper takes a look at it and goes ah metal location is stored in Zookeeper so it looks at its metadata on there and
then the metadata table location is sent back to the client the client will query
The Meta server to get the region server course responding to the row key if it wants to access the client caches this
information along with the meta table location and you can see here the client going back and forth to the region
server with the information and it might be going across multiple region servers depending on what you're querying so we
get the region server for row key from The Meta table that's where that row key comes in and says hey this is where
we're going with this and so once it gets the row key from the corresponding region server we can now put row or get
Row from that region server let's take a look at the hbase meta table special hbase catalog table that maintains a
list of all the region servers in the hbase storage system so you see here we have the meta table we have a row key
and a value table key region region server so the meta table is used to find the region for the given table key and
you can see down here you know our meta table comes in is going to fire out where it's going with the region server and we look a little closer at the write
mechanism in hbase we have right ahead log or wall as you abbreviate it kind of
a way to remember wall is write a head log it's a file used to store new data that is yet to be put on permanent
storage it is used for Recovery in the case of failure so you can see here where the client comes in and it
literally puts the new data coming in into this kind of temporary storage or
the wall on there once it's gone into the wall then the memory store mem store is the right cache that stores a new
data that's not yet been written to disk there is one mem store per column family per region and once we've done that we
have three ack once the data is placed in mimstore the client then receives the
acknowledgment when the minister reaches the threshold it dumps or commits the data into H file and so you can see
right here we've taken our or has gone into the wall the wall then Source it into the different memory stores and
then the memory stores it says Hey we've reached we're ready to dump that into our H files and then it moves it into the age files age files store the Roses
data as stored key value on disk so here we've done a lot of theory let's dive in
and just take a look and see some of these commands look like and what happens in our age base when we're
manipulating a no SQL setup [Music]
so if you're learning a new setup it's always good to start with where is this coming from it's open source by Apache
and you can go to hbase.apache.org and you'll see that it has a lot of information you can
actually download the hbase separate from the Hadoop although most people just install the Hadoop because it's bundled with it and if you go in here
you'll find a reference guide and so you can go through the Apache reference guide and there's a number of things to
look at but we're going to be going through Apache H based shell that's what we're going to be working with and
there's a lot of other interfaces on the setup and you can look up a lot of the different commands on here so we go into
the Apache hbase reference guide we can go down to read hbase shell commands
from a command file you can see here where it gives you different options of formats for putting the data in and
listing the data certainly you can also create files and scripts to do this too but we're going to look at the basics
we're going to go through this on a basic hbase shell and one last thing to look at that is of course if you
continue down the setup you can see here where they have more detail as far as how to create and how to get to your
data on your hbase now I will be working in a virtual box and this is by Oracle
you can download the Oracle virtual box you can put a note in below for the YouTube as we did have a previous
session on setting up virtual setup to run your Hadoop system in there I'm using the Cloudera quick start installed
in here there's Hortons you can also use the Amazon web service there's a number of options for trying this out in this
case we have Cloudera on the Oracle virtual box the virtual box has Linux Centos installed on it and then the
Hadoop it has all the different Hadoop flavors including hbase and I bring this up because my computer is a Windows 10
the operating system of the virtual box is Linux and we're looking at the hbase
data warehouse and so we have three very different entities all running on my
computer and that can be confusing if it's a first time in and working with this setup now you'll notice in our
Cloudera setup they actually have some hbase monitoring so I can go underneath here and click on each base and master
and it'll tell me what's going on with my region servers it'll tell me what's going on with our backup tables right
now I don't have any user tables because we haven't created any and this is only a single node and a single hbase tour so
you're not going to expect anything too extensive in here since this is for practice and education and perhaps
testing out package you're working on it's not for really you can deploy Cloudera of course but when you talk
about a quick start or a single node setup that's what it's really for so we can go through all the different hbase
and you'll see all kinds of different information with zookeeper if you saw it flash by down here what version we're
working in some zookeepers part of the hbase setup where we want to go is we want to open up a terminal window and in
Cloudera it happens to be up at the top and when you click on here you'll see your Cloudera terminal window open and
let me just expand this so we have a nice full screen and then I'm also going to zoom in and we have a nice big
picture and you can see what I'm typing and what's going on and to open up your age base shell simply type hbase shell
to get in and hit enter and you'll see it takes just a moment to load and we'll be in our H based shell for doing hbase
commands once we've gotten into our H base shell you'll see you'll have the hbase prompt information ahead of it we
can do something simple like list this is going to list whatever tables we have it so happens that there's a base table
that comes with hbase now we can go ahead and create and I'm going to type in just create what's nice about this is
it's going to throw me kind of a it's going to say hey there's no just straight create but does come up and
tell me all these different formats we can use for create so we can create our table and one of our families you can
add splits names versions all kinds of things you can do with this let's just start with a very basic one on here and
let's go ahead and create and we'll call it new table now this is to call it new TBL for table new table and then we also
want to do let's do knowledge so let's take a look at this I'm creating a new table and it's going to have a family of
knowledge in it and let me hit enter and it's going to come up it's going to take it a second to go ahead and create it now we have our new table in here so if
I go list you'll now see table and new table so you can now see that we have the new table and of course the default
table that's set up in here and we can do something like uh describe we can describe and then we're going to do new
TBL and when we describe it it's going to come up it's going to say hey name I have knowledge data block encoding none
Bloom filter row or replication scope version all the different information you need new minimum version 0 forever
deleted cells false block size in memory and you can look this stuff up on apache.org to really track it down one
of the things that's important to note is versions so you have your different versions of the data that's stored and
that's always important to understand that we might talk about that a little bit later on and then we have to describe it we can also do a status the
status says I have one active Master going on that's our H base as a whole we can do status summary I should do the
same thing as status so we got the same thing coming up and now that we've created let's go ahead and put something in it so we're going to put new TBL and
then we want Row one you know what before I even do this let's just type in put and you can see when I type in put
it gives us like a lot of different options of how it works and different ways of formatting our data as it goes
in and all of them usually begin with the new table new TBL then we have in
this case we'll call it Row one and then we'll have knowledge if you remember we created knowledge already and we'll do
knowledge Sports and then in knowledge and sports we're going to set that equal
to Cricut so we're going to put underneath this our knowledge setup that we have a thing called Sports in there
and we'll see what this looks like in just a second let's go ahead and put in we'll do a couple of these let's see let's do another Row 1 and this time
instead of sports Let's Do Science you know this person not only you know we have Row one which is both knowledgeable
and cricket and also in chemistry so it's a chemist who plays Cricket in row
one and let's see if we have let's do another row one just to keep it going and we'll do science in this case let's
do physics not only in chemistry but also physicist I have quite a joy in physics myself so here we go we have Row
one there we go and then let's do row two let's see what that looks like when we start putting in row two and in row
two this person is has knowledge in economics this is a master of business
and how or maybe it's Global economics maybe it's just for the business and how it fits in with the country's economics
and we'll call it macroeconomics so I guess it is for the whole country there so we have knowledge economics
macroeconomics and then let's just do one more we'll keep it as row two and this time our Economist is also a museum
position so we'll put music and they happen to have knowledge and they enjoy
oh let's do pop music they're into the current pop music going on so we've loaded our database and you'll see we
have two rows Row one and row two in here and we can do is we can list the contents of our database by simply doing
scan uh scan and then let's just do scan by itself so you can see how that looks
you can always just type in there and it tells you all the different setups you can do with scan and how it works in this case we want to do scan new TBL and
in our scan new TBL we have Row one row one row two row two and you'll see Row
one as a column called knowledge science time step value crickets value physics
so it has the information is when it was created when the timestamp is Row one also has knowledge Sports and a value of
Cricut so we have sports and Science and this is interesting because if you remember up here we also gave it
originally we told it to come in here and have chemistry we had science
chemistry and science physics and we come down here I don't see the chemistry why because we've now replaced chemistry
with physics so the new value is physics on here let me go ahead and clear down a little bit and in this we're going to
ask the question is enabled new table when I hit enter in here you're going to
see it comes out true and then we'll go ahead and disable it let's go ahead and disable new table make sure I have our
quotes around it and now that we've disabled it what happens when we do the scan we do the scan new table and hit
enter you're going to see that we get an error coming up so once it's disabled you can't do anything with it until we
re-enable it now before we enable the table Let's do an alteration on it and
here's our new table and this should look a little familiar because it's very similar to create we'll call this test
info we'll hit enter in there it'll take just a moment for updating and then we want to go ahead and enable it so let's
go ahead and enable our new table so it's back up and running and then we want to describe describe new table and
we come in here you'll now see we have name knowledge and under there we have our data encoding and all the
information under knowledge and then we also have down below test info so now we
have the name test info and all the information concerning the test info on here and we'll simply enable it new
table so now it's enabled oops I already did that I guess we'll enable it twice and so let's start looking at well we
had scan new table and you can see here where it brings up the information like this what if we want to go ahead and get
a row so we'll do R1 and when we do hbase R1 you can see we have knowledge
science and it has a timestamp value physics and we have knowledge Sports and it has a time stamp on it and value
Cricut and then let's see what happens when we put into our new table and in
here we want Row one and if you can guess from earlier because we did something similar we're going to do
knowledge economics and then it's going to be as sort of I think it was what
macroeconomics is now market economics and we'll go back and do our git command
and now see what it looks like and we can see here where we have knowledge economics it has a time stamp value
market economics physics and Cricut and this is because we have economics
science and sports those are the three different columns that we have and then each one has different information in it
and so if you've managed to go through all these commands and look at Basics on here you'll now have the ability to
create a very basic hbase setup no SQL setup based on your columns and your
rows and just for fun we'll go back to the Cloudera where they have the website up for the hbase master status and I'll
go ahead and refresh it and then we can go down here and you'll see user tables table set one and we can click on
details and here's what we just did it goes through so if you're the admin looking at this you can go oh someone
just created new TBL and this is is what they have underneath of it in their new table in there here we will learn on
Apache spark history of spark what is spark Hadoop which is a framework again
West spark components of Apache spark that is spark core spark SQL spark
streaming spark ml lib and Graphics then we will learn on spark architecture
applications of spark spark use cases so let's begin with understanding about
history of Apache spark it all started in 2009 as a project at UC Berkeley amp
Labs by matissah area in 2010 it was open source under a BST license in 2013
spark became an Apache top level project and in 2014 used by data bricks to sort
large scale data sets and it set in New World Record so that's how Apache spark
started and today it is one of the most in demand processing framework or I
would say in memory Computing framework which is used across the Big Data industry so what is Apache spark let's
learn about this Apache spark is a open source in memory Computing framework or
you could say data processing engine which is used to process data in batch and also in real time across various
cluster computers and it has a very simple programming language behind the
scenes that is Scala which is used although if users would want to work on spark they can work with python they can
work with Scala they can work with Java and so on even R for that matter so it
supports all these programming languages and that's one of the reasons that it is called polyglot wherein you have good
set of libraries and support from all the programming languages and developers and data scientists incorporate spark
into their applications or build Spark based applications to process analyze
query and transform data at a very large scale so these are key features of
Apache spark now if you compare Hadoop Wes spark we know that Hadoop is a
framework and it basically has mapreduce which comes with Hadoop for processing
data however processing data using mapreduce in Hadoop is quite slow because it is a batch oriented operation
and it is time consuming if you if you talk about spark spark can process the
same data 100 times faster than mapreduce as it is a in memory Computing
framework well there can always be conflicting ideas saying what if my spark application is not really
efficiently coded and my mapreduce application has been very efficiently coded well then it's a different case
however normally if you talk about code which is efficiently written format
reduce or for spark based processing spark will win the battle by doing almost 100 times faster than mapreduce
so as I mentioned Hadoop performs batch processing and that is one of the paradigms of mapreduce programming model
which involves mapping and reducing and that's quite rigid so it performs batch
processing the intermittent data is written to sdfs and written right back
from sdfs and that makes hadoops mapreduce processing slower in case of
spark it can perform both batch and real-time processing however a lot of
use cases are based on real-time processing take an example of Macy's take an example of retail giant such as
Walmart and there are many use cases who would prefer to do real time processing
or I would say near real-time processing so when we say real time or near real time it is about processing the data as
it comes in or you're talking about streaming kind of data now Hadoop or hadoop's map reduce obviously was
started to be written in Java now you could also write it in Scala or in Python however if you talk about
mapreduce it will have more lines of code since it is written in Java and it will take more times to execute you have
to manage the dependencies you have to do the right declarations you have to create your mapper and reducer and
Driver classes however if you compare spark it has few lines of code as it is
implemented in Scala and Scala is a statically typed dynamically inferred
language it's very very concise and the benefit is it has features from both
functional programming and object oriented language and in case of Scala whatever code is written that is
converted into byte codes and then it runs in the jvm now Hadoop supports
Kerberos authentication there are different kind of authentication mechanisms Kerberos is one of the
well-known ones and it can really get difficult to manage now spark supports
authentication via a shared secret it can also run on yarn leveraging the
capability of capitals so what are spark features which really makes it unique or
in demand processing framework when we talk about spark features one of the key
features is fast processing so spark contains resilient distributed data sets
so rdds are the building blocks for spark and we learn more about rdds later
so spark contains rdds which saves huge time taken in reading and writing
operations so it can be 100 times or you can say 10 to 100 times faster than
Hadoop when we say in memory Computing here I would like to make a note that
there is a difference between caching and in-memory Computing think about it caching is mainly to support read ahead
mechanism where you have your data pre-loaded so that it can benefit it further queries however when we say in
memory Computing we are talking about lazy evaluation we are talking about data being loaded into memory only and
only when a specific kind of action is invoked so data is stored in Ram so here
we can say Ram is not only used for processing but it can also be used for storage and we can again decide whether
we would want that Ram to be used for persistence or just for computing so it
can access the data quickly and accelerate the speed of analytics now spark is quite flexible it supports
multiple languages as I already mentioned and it allows the developers to write applications in Java Scala r or
python it's quite fault tolerance so spark contains these rdds or you could
say execution logic or you could say temporary data sets which initially do
not have any data loaded and the data will be loaded into rdds only when
execution is happening so these can be fault tolerant as these rdds are
distributed across multiple nodes so failure of one worker node in the cluster will really not affect the rdds
because that portion can be recomputed so it ensures loss of data it ensures
that there is no data loss and it is absolutely fault tolerant it is for
better than analytics so sparked has Rich set of SQL queries machine learning
algorithms complex analytics all of this supported by various spark components which we will learn in coming slides
with all these functionalities analytics can be performed better in terms of spark so these are some of the key
features of spark however there are many more features which are related to different components of spark and we
will learn about them so what are these components of spark which I'm talking about Spark core so this is the core
component which basically has rdds which has a core engine which takes care of
your processing now you also have spark SQL so people who would be interested in working on structured data or data which
can be structurized would want to prefer using spark SQL and Spark SQL internally
has components or features like data frames and data sets which can be used
to process your structured data in a much much faster way you have spark
streaming now that's again an important component of spark which allows you to create your spark streaming applications
which not only works on data which is being streamed in or data which is constantly getting generated but you
would also or you could also transform the data you could analyze or process
the data as it comes in in smaller chunks you have Sparks mlib now this is
basically a set of libraries which allows developers or data scientists to build their machine learning algorithms
so that they can do Predictive Analytics or prescriptive descriptive preemptive analytics or they could build their
recommendation systems or bigger smarter machine learning algorithms using these
libraries and then you have Graphics so think about organizations like LinkedIn
or say Twitter where you have data which naturally has a network kind of flow so
data which could be represented in the form of graphs now here when I talk about graphs I'm not talking about pie
charts or bar charts but I'm talking about Network related data that is data
which can be networked together which can have some kind of relationship think about Facebook think about LinkedIn
where you have one person connected to other person or one company connected to other companies so if we have our data
which can be represented in the form of network graphs then spark has a component called Graphics which I allows
you to do graph based processing so these are some of the components of Apache spark spark core spark SQL spark
streaming spark ml lib and Graphics so to learn more about components of spark
let's learn here about spark core now this is the base engine and this is used
for large scale parallel and distributed data processing so when you work with spark at least and the minimum you would
work with is spark core which has rdds as the building blocks of your spark so
it is responsible for your memory management your fault recovery scheduling Distributing and monitoring
jobs on a cluster and interacting with storage systems so here I would like to make a key point that Spar by itself
does not have its own storage it relies on storage now that storage could be
your sdfs that is hadoop's distributed file system it could be a database like
nosql database such as hbase or it could be any other database say rdbms from
where you could connect your spark and then fetch the data extract the data
process it analyze it so let's learn a little bit about your rdds resilient
distributed data sets now spark core which is the base engine or the core
engine is embedded with the building blocks of spark which is nothing but your resilient distributed data set so
as the name says it is resilient so it is existing for a shorter period of time
distributed so it is distributed across nodes and it is a data set where the
data will be loaded or where the data will be existing for processing so it is immutable fault tolerant distributed
collection of objects so that's what your rdd is and there are mainly two operations which can be performed on and
rdd now to take an example of this say I want to process a particular file now
here I could write a simple code in Scala and that would basically mean
something like this so if I say well which is to declare the variable I would
say well X and then I could use what we call as spark context which is basically
the most important entry point of your application so then I could use a method of spark context for example that is
text file and then I could point it to a particular file so this is just a method
of your spark context and Spark context is the entry point of your application
now here I could just give a path in this method so what does this step do it
does not do any evaluation so when I say Val X I'm creating a immutable variable
and to that variable I'm assigning a file now what this step does is is it
actually creates a rdd resilient distributed data set so we can imagine
this as a simple execution logic a EMT data set which is created in memory of
your node so if I would say I have multiple nodes in which my data is split
and stored I'm imagining that your yarn your spark is working with Hadoop so I
have Hadoop which is using say two nodes and this is my distributed file system sdfs which basically means my file is
written to hdfs and it also means that the file related blocks are stored in
the underlying disk of these machines so when I say Val x equals SC dot text file
that is using a method of spark context now there are various other methods like whole text files parallelize and so on
this step will create an rdt so you can imagine this as a logical data set which
is created in memory across these nodes because these nodes have the data
however no data is loaded here so this is the first rdd and I can say first
step in what we call as a tag a dag which will have series of steps which
will get executed at later stage now later I could do further processing on
this I could say well Y and then I could do something on X so I could say x dot
map and I would want to apply a function to every record or every element in this
file and I could give a logic here x dot map now this second step is again
creating an rdd a resilient distributed data set you can say second step in my
dag okay and here you have a external rdd one more rdd created which depends
on the first RTD so my first RTD becomes the base rdd or parent ID D and the
resultant RTD becomes the child rdd then we can go further and we could say well Z and I would say okay now I would want
to do some filter on y so this filter which I am doing here and then I could
give a logic might be I'm searching for a word I am searching for some pattern so I could say well Z equals y dot
filter which again creates one more rdd a resilient distributed data set in
memory and a you can say this is nothing but one more step in the dag so this is
my tag which is a series of steps which will be executed now here when does the
execution happen when the data gets when will the data get loaded into these rdds
so all of this that is using a method using a transformation like map using a
transformation like filter or flat map or anything else these are your
Transformations so the operations such as map filter join Union and many others
will only create rdds which basically means it is only creating execution
Logic No data is evaluated no operation is happening right now only and only
when you invoke an action that is might be you want to print some result might
be you want to take some elements and see that might be you want to do a count so those are actions which will actually
trigger the execution of this dag right from the beginning so if I hear say Z
dot count where I would want to just count the number of words which I am filtering this is an action which is
invoked and this will trigger the execution of dag right from the beginning so this is what happens in a
spark now if I do a z dot count again it
will start the whole execution of dag again right from the beginning so my Z
dot count second time in action is invoked again the data will be loaded in
the first RTD then you will have map then you will have filter and finally you will have result so this is the core
concept of your rdds and this is how RTD works so mainly in spark there are two
kind of operations one is your Transformations and one is your actions
Transformations or using a method of spark context will always and always
create an RTD or you could say a step in the tag actions are something which will
invoke the execution which will invoke the execution from the first rdd till
the last rdd where you can get your result so this is how your rdds work now
when we talk about components of spark let's learn a little bit about spark SQL so spark SQL is a component type
processing framework which is used for structured and semi-structured data processing so usually people might have
their structured data stored in rdbms or in files where data is structured with
particular delimiters and has a pattern and if one wants to process the structured data if one wants to use
spark to do in memory processing and work on the structured data they would
prefer to use spark SQL so you can work on different data formats say CSV Json
you can even work on smarter formats like Avro parquet even your binary files
or sequence files you could have your data coming in from an rdbms which can
then be extracted using a jdbc connection so at the bottom level when you talk about spark SQL it has a data
source API which basically allows you to get the data in whichever format it is now Spark SQL has something called as
data frame API so what are data frames data frames in short you can visualize
or imagine as rows and columns or if your data can be represented in the form
of rows and columns with some column headings so data frame API allows you to
create data frames so like my previous example when you work on a file when you
want to process it you would convert that into an rdd using a method of smart
context or by doing some Transformations so in the similar way when you use data frame so when you want to use spark SQL
you would use Sparks context which is SQL context or
Hive context or spark which allows you to work with data frames so like in my
earlier example we were saying Val x equals SC dot text file now in case of
data frames instead of SC you would be using say spark dot something so spark
context is available for your data frames API to be used in older versions
like spark 1.6 and so on we were using Hive context or SQL context so if you
were working with spark 1.6 you would be saying well x equals SQL context dot
here we would be using spark dot so data frame API basically allows you to create
data frames out of your structured data which also lets spark know that data is
already in a particular structure it follows a format and based on that your Sparks back in DAC scheduler right so
when I say about dag I talk about your sequence of steps so spark is already
aware of what are the different steps involved in your application so your data frame API basically allows you to
create data frames out of your data and data frames when I say I am talking about rows and columns with some
headings and then you have your data frame DSL language or you can use spark
SQL or Hive query language any of these options can be used to work with your data frames so to learn more about data
frames follow in the next sessions when you talk about spark streaming now this
is very interesting for organizations who would want to work on streaming data imagine a store like Macy's where they
would want to have machine learning algorithms now what would these machine learning algorithms do suppose you have
lot of customers walking in the store and they are searching for particular
product or particular item so there could be cameras placed in the store and this is being already done there are
cameras placed in the store which will keep monitoring in which corner of the store there are more customers now once
camera captures this information this information can be streamed in to be processed by algorithms and those
algorithms will see which product or which series of product customers might
be interested in and if this algorithm in real time can process based on the
number of customers based on the available product in the store it can come up with a attractive alternative
price so that which the price can be displayed on the screen and probably
customers would buy the product now this is a real-time processing where the data
comes in algorithms work on it do some computation and give out some result and which can then result in customers
buying a particular product so the whole essence of this machine learning and real-time processing will really hold
good if and when customers are in the store or this could relate to even a
online shopping portal where there might be machine learning algorithms which might be doing real-time processing
based on the clicks which customer is doing based on the clicks based on customer history free based on customer
Behavior algorithms can come up with recommendation of products or better altered price so that the sale happens
now in this case we would be seeing the essence of real-time processing only in
a fixed or in a particular duration of time and this also means that you should have something which can process the
data as it comes in so spark streaming is a lightweight API that allows
developers to perform batch processing and also real-time streaming and
processing of data so it provides secure reliable fast processing of live data streams so what happens here in spark
streaming in brief so you have a input data stream now that data stream could be a file which is constantly getting
appended it could be some kind of metrics it could be some kind of events based on the clicks which customers are
doing or based on the products which they are choosing in a store this input data stream is then pushed in through a
spark streaming application now spark streaming application will broke break this content into smaller streams what
we call as discreticized streams or batches of smaller data on which processing can happen in frames so you
could say process my file every five seconds for the latest data which has come in now there are also some windows
based uh options like when I say windows I mean a window of past three events
window of past three events each event being of five seconds so your batches of
smaller data is processed by Spark engine and this process data can then be
stored or can be used for further processing so that's what spark streaming does when you talk about mlib
it's a low level machine learning library that is simple to use scalable and compatible with various programming
languages now Hadoop also has some libraries like you have Apache mahaut which can be used for machine learning
algorithm items however in terms of spark we are talking about machine learning algorithms which can be built
using ml Libs libraries and then spark can be used for processing so mlib eases
the deployment and development of scalable machine learning algorithms I
mean think about your clustering techniques so think about your classification where you would want to
classify the data where you would want to do supervised or unsupervised learning think about collaborative
filtering and many other data science related techniques or techniques which are required to build your
recommendation engines or machine learning algorithms can be built using Sparks ml lip Graphics is Spark's own
graph computation engine so this is mainly if you are interested in doing a graph based processing think about
Facebook think about LinkedIn where you can have your data which can be stored
and that data has some kind of network connections or you could say it is well
networked I could say x is connected to y y is connected to z z is connected to
a so x y z a all of these are in terms
of graph terminologies we call as vertices or vertex which are basically being connected and the connection
between these are called edges so I could say a is friend to B so A and B
are vertices and friend a relation between them is The Edge now if I have
my data which can be represented in the form of graphs if I would want to do a processing in such way this could be not
only for social media it could be for your network devices it could be a cloud platform it could be about different
applications which are connected in a particular environment so if you have data which can be represented in the
form of graph then Graphics can be used to do ETI L that is extraction
transformation load to do your data analysis and also do interactive graph
computation so Graphics is quite powerful now when you talk about spark your spark can work with your different
clustering Technologies so it can work with Apache mesos that's how Spar came
in where it was initially to prove The credibility of Apache mesos spark can
work with yarn which is usually you will see in different working environments spark can also work as Standalone that
means without Hadoop spark can have its own setup with master and workout processes so usually or you can say
technically spark uses a Master Slave architecture now that consists of a driver program that can run on a master
node it can also run on a client node it depends on how you have configured or
what your application is and then you have multiple executors which can run on
work nodes so your master node has a driver program and this driver program
internally has the spark context so your spark Every Spark application will have
a driver program and that's driver program has a inbuilt or internally used
spark context which is basically your entry point of application for any spark
functionality so your driver or your driver program interacts with your
cluster manager now when I say interacts with cluster manager so you have your spark context which is the entry point
that takes your application request to the cluster manager now as I said your
cluster manager could be say Apache missiles it could be yarn it could be spark Standalone Master itself so your
cluster manager in terms of yarn is your resource manager so your spark
application internally runs as series or setup tasks and processes your driver
program wherever that is run will have a spark context and Spark context will
take care of your application execution how does that do it spark context will
talk to Cluster manager so your cluster manager could be on and in terms of when
I say cluster manager for yarn would be resource manager so at high level we can
say a job is split into multiple tasks and those tasks will be distributed over
the slave nodes or worker nodes so whenever you do some kind of transformation or you use a method of
spark context and rdd is created and this rdd is distributed across multiple
nodes as I explained earlier worker nodes are the slaves that run different tasks so this is how a spark
architecture looks like now we can learn more about spark architecture and its
interaction with yarn so usually what happens when your spark context interacts with the cluster manager so in
terms of yarn I could say resource manager now we already know about yarn so you would have say node managers
running on multiple machines and each machine has some RAM and CPU cores allocated for your node manager on the
same machine you have the data nodes running which obviously are there to have the Hadoop related data so whenever
the application wants to process the data your application via spark contacts contacts the cluster managers that is
resource manager now what does resource manager do resource manager makes a request so resource manager makes
request to the node manager of the machines wherever the relevant data resides asking for containers so your
resource manager is negotiating or asking for containers from node manager saying hey can I have a container of 1GB
RAM and one CPU core can I have a container of 1GB RAM and one CPU core and your node manager based on the kind
of processing it is doing will approve or deny it so node manager would say fine I can give you the container and
once this container is allocated or approved by node manager resource manager will basically start an extra
piece of code called App Master so App Master is responsible for execution of
your applications whether those are spark applications or mapreduce so your application Master which is a piece of
code will run in one of the containers that is it will use the RAM and CPU core and then it will use the other
containers which were allocated by node manager to run the tasks so it is within
this container which can take care of execution so what is a container a combination of RAM and CPU core so it is
within this container we will have a executed process which would run and
this executed process is taking care of your application related tasks so that's
how overall spark Works in integration with yarn now let's learn about this
spark cluster managers as I said spark can work in a standalone mode so that is
without Hadoop so by default application submitted to spark Standalone mode cluster will run in fifo order and each
application will try to use all the available nodes so you could have a spark Standalone cluster which basically
means you could have multiple nodes on one of the nodes you would have the master process running and on the other
nodes you would have the spark worker processes running so here we would not have any distributed file system because
spark is Standalone and it will rely on an external storage to get the data or
probably the file system of the nodes where the data is stored and processing will happen across the nodes where your
worker processes are running you could have spark working with Apache mesos now as I said Apache my source is a open
source project to manage your computer clusters and can also run Hadoop
applications Apache mesos was introduced earlier and Spark came in and has
existence to prove The credibility of Apache missiles you can have spark working with hadoop's yarn this is
something which widely you will see in different working environments so yarn
which takes care of your processing and can take care of different processing Frameworks also supports spark you could
have kubernetes now that is something which is making a lot of news in today's world it is a open source system for
automating deployment scaling and management of containerized applications
so where you could have multiple Docker based images which can be connecting to each other so spark also works with
kubernetes now let's look at some applications of spark so JPMorgan Chase
and company uses Spark to detect fraudulent transactions analyze the
business spends of an individual to suggest offers and identify patterns to
decide how much to invest and where to invest so this this is one of the examples of banking a lot of banking
environments are using spark due to its real-time processing capabilities and in
memory faster processing where they could be working on fraud detection or
credit analysis or pattern identification and many other use cases
Alibaba group that uses also spark to analyze large data sets of data such as
real-time transaction details now that might be based online or in the stores looking at the browsing history in the
form of spark jobs and then provides recommendations to its users so Alibaba
group is using spark in its e-commerce domain you have IQ via now this is a
leading Healthcare company that uses Spark to analyze patients data identify
possible health issue use and diagnose it based on the medical history so there is a lot of work happening in healthcare
industry where real-time processing is finding a lot of importance and real time and faster processing is what is
required so healthcare industry and iqvi is also using spark you have Netflix
which is known and you have Riot games so entertainment and gaming companies like Netflix and ride games use Apache
spark to Showcase relevant advertisements to their users based on the videos that they have watched shared
or liked so these are few domains which find use cases of spark that is banking
e-commerce Healthcare entertainment and then there are many more which are using spark in their day-to-day activities for
real time in memory faster processing now let's discuss about the Spark's use
case and let's talk about conviva which is world's leading video streaming
companies so video streaming is a challenge now if you talk about YouTube which has data you could always read
about it so YouTube has data which is worth watching 10 years so that is huge
amount of data where people are uploading their videos or companies are doing advertisements and this videos are
streamed in or can be watched by users so video streaming is a challenge and especially with increasing demand for
high quality streaming experiences conviva collects data about video
streaming quality to give their customers visibility into the end user
experience they are delivering now how do they do it Apache spark again using
Apache spark convert delivers a better quality of service to its customers by
removing the screen buffering and learning in detail about Network conditions in real time this information
is then stored in the video player to manage live video traffic effect coming
in from 4 billion video feeds every month to ensure maximum retention Now
using Apache spark conveyor has created an auto diagnostics alert it
automatically detects anomalies along the video streaming Pipeline and diagnosis the root cause of the issue
now this really makes it one of the leading video streaming companies based
on auto diagnostic alerts it reduces waiting time before the video starts it
avoids buffering and recovers the video from a technical error and the whole goal is to maximize the viewer
engagement so this is Spark's use case where conviva is using spark in
different ways to stay ahead in video streaming related deliveries if we have
understood and learned about your spark components your spark architecture we
can also see running a spark application now a spark application can run in a
standalone mode that is you could set up your IDE such as Eclipse with a Scala
plugin and then you could have your coded application which is written in
Eclipse to be run in local mode now here I have an example of application which
can be run in a local mode or on a cluster so this application is importing
some packages that is spark context spark conf I have created an object
which is first app it is the main class of your project and other classes can
just be extending app rather than having main here we declare a variable called X
which is pointing to a file in my project directory it looks for a file
which is abc1 dot text and this file basically has some content what are we
doing in the application so we create a variable where we assign the file then we Define name or initializers for
context so remember whenever you work with IDE you don't have spark context or spark available implicitly that has to
be defined so here we create a configuration object we set our application name we set the master as
local if you want to run it in a local mode if you want to run it say on your Windows machine or if you would want to
run on a spark Standalone cluster if I would be running it on yarn then I would remove this property that is set master
now once you have defined your configuration object you can be basically using spark context you can
use method of it which will result in RTD that is what is happening in line 13
Val Y and then finally I can create a variable where I would want to do a flat
map transformation on Y which would result in an internal rdd followed by a map transformation which would again
result in an rdd and finally Reduce by key which is doing an aggregation once
all these steps are done I can decide to display the result on the screen or I can even use save as text file point it
to a particular location and then have my application run now this is my
Eclipse refer to other sessions where I have explained how you can set up Ide on
windows with different environment variables and then you can run your applications on a particular cluster if
I would want to run this application on a cluster then I will have to give a
particular path so here in this case I can say let's do this and I will say my
file is abc1.txt let's save it and I'll also say my output will be getting
stored in a default location as I intend to run it I intend to run this
application on a cluster now in that case the cluster would usually be set up
on Linux machines would have a Hadoop cluster where you can run this application so if I would want to run
this application on a cluster but not locally on the machine I can just delete this part I can keep my application now
if you see my application already compiles and does not show any error message and that is because in my
Project's build path I have added all the spark related jars all the spark
related jars you can get from your spark directory or you can be getting in
manually all the dependencies some people would prefer to use Maven or say SBT for packaging your application as
jar and that can also be done so here my code compiles code is fine it is
pointing to a file and then it is also creating an output which will have word count now what I can do is since I have
my code already written I know that I have SBT installed on this machine
because I would want to package this code AS jar and run it on a cluster so for that we can look in our Command
Prompt and here I can go into workspace is I can go into my Scala project and if
you see here we have your build dot SBT file you have your binaries and you have
your source so you might not have these spark related directories now these exist in my case because I have been
using spark and I've done some previous runs now this is what we have within my
spark apps so to build my package I need a build dot spt file we can see what
does that contain so your build.sbt contains your name version of your
package or Scala version spark version and then repository which spark will
refer to when it wants to have the dependencies for all of your components such as bar core spark SQL ml lab and so
on so this is my build.spd file which exists in the project folder and if you
are intending to use SBT to package your code AS jar and then run it on the
cluster in that case I can even skip adding spark related charge to the build
path so that is only done to make sure that your code compiles now once I have
my code written I have SBT installed I have made the file path changes I can
just go to the command line within my project folder and I can say SBT package
now this will basically resolve all the dependencies based on whatever you have given in the code it will create a jar
file and place it in a particular location and then we can use the same jar file to run on a particular cluster
so SVD package is busy in creating the jar now meanwhile what I can do is I can
open up a lab content and what I can do is I can just say for example simply
learn and here I already have a lab set up so you could have your own spark
Standalone cluster where you you could also run this jar file you could have spark with Hadoop which is what I have
here and I will use that to submit an application on the yarn cluster and
let's go to the web console let me copy my link I'll close this I'll say launch
lab and then I can log in here I can just do paste my password and I'm logged
in so I can just say spark to Shell now that's how it has been configured here to work with your spark version 2. so
this is an interactive way of bringing you your spark shell and running your application but what we are interested
in is running the application as HR file so let's go and see here where we have
our code and let's see if SBT has done the packaging yes it has done and it has created a jar file in this location so
what I can do is I need to get this jar file from this location onto my cluster
so what I can do is I can come in here I can do a FTP so this basically allows me
to push whatever jars I have on my web console so I'll go in here I'll say
connect now I'll search if there is already existing jar file which might
create a conflict so I have something here so I can for now just delete it
it's done I will say upload file and I am interested in getting the jar file so
here we can click on users win 10 I have
my workspace project I can get into Target Scala 2.11 and take my jar file
and say open so this will upload my jar file on the web console or The Terminal
wherein I can connect to my cluster now let's go in here let's quit from spark
shell as we want to run an application on the cluster how do I do it so I again
search if I have my jar file existing it's here so I'll say spark to submit
and then I will point to my jar file and then I can simply say class and I know
my code has a package and a class name so this is package and this is my object
or class name so I can say main dot Scala DOT first app now it will be good
if we check if the file exists which are code points to so I'll just for now
comment it out I will check in my stfs LS in my default user directory this is
where it will search for a file and here we can search if I have a file called
ABC so I don't see anything here so let's do this what I can do is I can
again go to FTP and basically I can do a upload file like what we did earlier and
this time I'll pick up this existing ABC one file which I showed you and upload
it once this is done I can put it on my cluster by just saying hdfs DFS put
let's take the ABC one file and let's put it in my directory so this will be
my input so I'm putting in a file there and now I can do a spark submit to
submit the application on the cluster so if you see here it has basically started
the application it contacts the resource manager it gets an application ID and
now it is doing the processing of my file where I would want to get once this
is done it will be completed and the temporary directory will be deleted so this is how I run a spark application on
a cluster now once this is done I can also go to Sparks history server by
saying this is the path for my spark history server it shows an application which was run today which was doing a
word count you can click on the application it says it ended with save as text file you can click on this and
then it shows you the dag visualization it says we started with text file we did
a flat map we did a map and then there was some shuffling because we wanted to
do Reduce by key so as I said every RDP e by default has two partitions and if
you want to do some aggregate or wider Transformations like Reduce by key sort by key Group by key count by key and so
on in that case similar key related data has to be shuffled and brought into one
partition that's where we see shuffling happening here and it also tells what are the number of tasks which have run
per partition so here we ran a spark application by packaging it as jar using
your SBT so SBD created the jar file and then basically we brought our jar file
onto the cluster and then submitted it using spark submit so this is how a
spark application runs on the cluster so as I said your application has a driver program now your spark applications run
as independent processes and they can run on a cluster across the nodes so we
just saw how we can run s spark application on a cluster now we can
always look at spark applications progress or after it has completed by looking into the spark UI your spark
application as I mentioned has a driver program and when you run your application on the cluster you can
always specify where would you want your driver program to run so in our case when we ran our spark application here
we basically just did a simple spark submit and we gave our jar and the class
name I could also say master and then I could specify if I would want my
application to run in a local mode or I could say yarn but that is default or if
it was a spark stand alone cluster then I could be giving something like this your host name and then your Port so you
could do all of these options by specifying minus minus master I could also specify how many executors I need
how much memory per executor how much cores per executor I need and I could
also say deploy mode as client which basically means my driver will run on
this machine however my execution of application will happen on cluster nodes I can also say deploy mode as cluster
which basically means my driver will run on one of the nodes of the cluster you
could submit your application and then based on whatever arguments you have given your application will be submitted
on the cluster and it will run so you can have your application running so as
I said you have an application which has a driver and it also has either spark
session or spark context which takes your application request to the resource manager now if your application is
completed you can always come back and look into the spark history server or
spark UI if I choose this as my application which I have run I can go to executors and that shows me that there
was one for your driver now that was running on this particular node which is
my client node then I have my executors on my other nodes which are nodes of my
cluster which used one core which ran two tasks for the partitions which they
were working on and there was some shuffling involved as we were doing a world count which uses Reduce by key so
when you run your application your yarn or your cluster manager such as resource
manager negotiates the resources to the slave processes your worker node
basically will have the resources available for any kind of execution now
as I said your resource manager will request for containers your worker nodes
will approve those containers and then within those containers you will have the executor which will run which will
take care of the task to process the data so what is a task it is a unit of work for the data set which has to be
worked upon so your rdds which get created have partitions and for every
partition you have a task which is taken care by the executor so the data is
loaded into the rdd when action is invoked and then your task is worked
upon by the executor so what does it do it basically gets the data into the
partition of rdd and then does the execution as a task on that particular
partition results are then sent back to the driver and you can also have your
output saved on the disk so this is how your application run whenever you run a
particular application you can always go to your web console and scroll or look
into the logs for more information so here our application was run and we can
see right from the beginning here when we talk about spark submit which is done
here it says running spark version 2.2 it basically then will see that there
will be a driver which will be started you can further see that it does some memory calculations it then starts as
spark UI and here we can see that requesting a new application from
cluster with four node managers verify our application has not requested more
than maximum memory capability of the cluster so the container sizing at the
cluster level is 3gb per container and your application should not be requesting for a container bigger than
that it starts a application Master container with specific amount of memory and then finally your execution starts
if we scroll down and look into the logs we will see where my driver runs it will
also see show you how much memory was utilized and it will talk about tax
scheduler which is taking care of execution of your dag that is series of your rdds and finally you will see the
result getting generated or saved so this this is how you run your spark application this is how you can see your
spark applications in history server or spark UI and this is how overall Apache
spark Works utilizing your cluster manager help getting the resources from
the worker processes and then running the executors within those here we will
learn on what is spark streaming spark streaming data sources features of spark
streaming working of spark streaming disk criticized streams caching or
persistence as we call checkpointing in spark streaming and we will then have a
demo on spark streaming so let's learn on what is spark streaming and what it
is capable of so it's an extension of core spark API that basically enables
scalable High throughput fault tolerant stream processing of live data streams
now data can be ingested from different sources like Kafka Flume kinosis or TCP
sockets and then it can be processed using complex algorithms expressed with different kind of functions such as map
reduce join in window and when we have the data processed that data can be pushed out to your file systems
databases and live dashboards so if we look on this image we can clearly see
that we would be working on a input data stream which goes into spark streaming
component of spark which gives us batches of input data or you could say
the streaming data is broken down into smaller patches which would then be worked upon by spark core engine and you
have finally batches of processed data now when we talk about spark streaming and the data sources you could have
streaming data sources coming in from Kafka or Flume or say Twitter API or
also with different formats such as parquet you you could also have static data sources coming in from mongodb
hbase MySQL and postgres so when we talk about spark streaming it receives the
input data streams divides the data into batches which can then be processed by
Spark engine to generate your final stream of results again in batches so your spark streaming actually provides a
high level abstraction called discreticized stream and we will learn about that or what we call as dstream
which represents a continuous stream of data now when we look at your different
data sources from where the data can come in spark streaming would take in this input and then you could have even
your mlib component which could be used that is Sparks component to build
machine learning algorithms wherein you can train your models with live data and you can even use your trained model you
could be also going for structured processing or processing the data which is structurized and that could be done
by using Sparks components that is spark SQL which has data frames and data sets
so you could process your data with data frames interactively query with SQL when
spark streaming is working on the data which is constantly flowing in finally the data which is processed can be
stored in your distributed file system such as sdfs or any other nosql or SQL
based database now when we talk about spark streaming it is good to know some
of the features of spark streaming and here you have some of the features so it
enables fast recovery from failures while it is working on streaming data you have better load balancing and
resource usage and you can also combine the streaming data with static data sets
and perform interactive queries now your spark streaming also supports native
integration with Advanced processing libraries and that is one of the benefits users can have when they are
used using spark streaming let's learn about working of spark streaming so as I
mentioned earlier at one end you have your data streams now those data streams are then caught or perceived by your
receivers which we have to enable in our application your data streams or streaming data which is constantly
getting generated and flowing in is broken down into smaller batches which will then be processed by spark so you
would have your final processed results now if we look at the bigger picture here we will talk about live input data
streams that could be divided into smaller batches and when we say batches these are batches of input data as rdps
so spark streaming performs computation expressed using these streams it
generates rdd Transformations so you would have your spark pad jobs to
execute rdd Transformations which would give you your final processed result now there are various examples and we'll
look at examples later so once your spark streaming works on Breaking the
input into smaller streams it can then process the data finally giving you batches of your result and that is again
on the streaming data now what is these D streams let's understand about these
streams or discretized streams so that's the basic abstraction provided by spark
streaming now it represents a continuous stream of data either the input data stream received from The Source or the
process data stream generated by transforming the input stream now here if we look at the dstream we would say
you would have series of rdds or series of Transformations applied on the data
which is flowing in for a particular time frame now here we say data from time 0 to 1 and that would result in
some of the Transformations which you would perform on the data when it has come in between this time zone or time
frame you would have again data from time one to two and so on so that that's
how your spark streaming works so if we look at your different Transformations now there could be various
Transformations which could be applied on your data so this is something like
your streaming data which comes in you would want to say have a receiver which monitors a particular socket or a
particular port and looks for data coming in we would also Define the time interval and for that time interval the
data is taken so that's a smaller batch or a d stream on which you could have your processing done now within your
application you would have series of steps which are nothing but Transformations which would be performed
on this data within this time frame giving you a result which could be stored which could be seen on your
console or which could just be pushed further for processing and this keeps happening at regular type intervals
whatever you have specified till the spark streaming application continues now when we talk about spark we already
know that there are different kind of Transformations which can be applied so you have map transformation wherein you
have map and then you pass in a function which basically says you would want to
perform a function on every element so when we say map function that would
return a new D stream by passing each element of the source D stream through a
function which is passed similarly it is for flat map where you would be passing in a function you would want to perform
a flat map transformation on the data stream or D stream which comes in so
each input item can be mapped to zero or more output elements you could be doing
a filter wherein you return a d stream by selecting only the records of the source V stream on which the function
returns true so filtering is used when you want to run a transformation where you would want to look at the input data
for a particular time interval as I mentioned earlier and you would want to filter that data as per whatever your
function is applied now you could be doing a union where you could basically be having a union of multiple D streams
so that this would return a new D stream that contains Union of elements in the
source restraint and a other dstream you could be doing a transform function that
contains the union of elements you could be doing a count you could be doing a join so these are some of the
Transformations which can be performed on your D streams now there is also a concept of windowing and that is
basically to process the data for a series of time intervals so when I
mention windowed stream processing I am saying spark streaming would allow you
to apply Transformations over a sliding window of data now this operation is
called as windowed computation let's see how it looks like so if you have your
original D stream which is being which is basically your data coming in now
that would be looked upon for specific time intervals such as time 1 time 2 and
then you could be doing a windowed computation which could basically mean that I could have a window which is a
series of these time intervals on which you would want to perform series of your
rdd Transformations so you have a window dstream at time one then at time two and
time three so that could be one window wherein you would want to get an output so here we see window at time three now
you could also have a another sliding window which would take your time series and we have time 3 Time 4 and time 5
where you could be performing the series of Transformations so this is usually helpful where you would not only want to
process the data at particular time interval but you would want a Consolidated processing for series of
intervals and that's what we mean by windowed computation now before we understand about caching and persistence
we can talk a little bit more on windowing so if one would want to understand the window stream processing
or as we say spark streaming's feature of windowed computations we need to
think it as applying Transformations over a sliding window of data now as we
see here every time the window slides over a source D stream the source rdds
that fall within the window are combined and operated upon to produce the rdds of
windowed stream now in this specific case we can say the operation is applied over last three time units of data and
slides by two time units this shows that any window operation needs to specify two parameters one is the window length
which is basically the duration of window so for example we can say 3 as we see in the figure here and then you have
a sliding interval which is basically the interval at which window operation is performed so these two parameters
must be multiples of the batch interval of source D stream so that's what we do
when we talk about window now there are various other Transformations which can be applied for window based operations
which can be done on your D streams here to talk a little bit more on these
streams which is as I said it's a basic abstraction provided by spark streaming
it represents a just remember it has a continuous stream of data now either the
input data stream received from Source or the process data stream which is generated by transforming the input
Stream So a d stream is represented by you could say a continuous series of
rtds which is Sparks abstraction of a immutable distributed data set so any
operation applied on a dstream it is basically translating to operations on
the underlying rdds now for example we can say converting a stream of lines to
word the flat map operation is applied on each rdd in the line lines now D
stream to generate the rdds of your words T Stream So when we talk about your discretized streams understand that
you would have a series of Transformations which would be applied for this time interval whatever has been
specified now when you talk about your streaming or smart streaming architecture as I mentioned here that is
your receivers now that plays a very important role here so your input D
streams or data streams are representing the stream of input data that is received from your streaming sources now
we could have different kind of data and we could be doing different kind of Transformations so your receiver is
basically an object which receives the data from a source and stores it in Sparks memory for processing and that's
the main role of your receiver now spark streaming provides two categories of
building streaming sources so you have basic sources that is sources directly available in streaming context which is
a class we learn about it such as your file systems or socket connections so those could be your basic sources from
where the data is coming in you could have advanced sources like your Kafka Flume kindnesses Etc and they would be
available through extra utility classes so your receiver is going to be looking
into the data which is constantly getting generated and then basically forwarding it for processing by your
spark streaming you know when we talk about your spark streaming one more important aspect is basically
understanding the caching and persistence now as we know from the
spark core engine or as you should know that rdds are say your logical steps or
rdds are created when you perform some Transformations and these
Transformations or these computations or these rdds can be cached so that it can
improve the performance of your application so the computed rtds or the rdds which are result of some performing
some Transformations they can be cached so that they can be reutilized down your
application for your further processing so when we talk about your caching in persistence these streams also allows
developers to purchase the stream data in memory so similar to your concept of
rdds these streams can allow you to purchase the particular streams data in
memory now that is by doing or using your persist method on a dstream which
will automatically persist every rdd of that dstream in memory it could be every
rdd or it could be specifically chosen rdds now this is really useful if the
data in dstream has to be or would be computed multiple times say in your application so for example if we say
window based operations like Reduce by window or reduced by key and window wherein you have group of operations
being done or you you could have state based operations like update state by key now in any of these cases your D
streams generated by say window based operations are automatically persisted
in memory without the developer calling for the persist method now for input streams that receive data over networks
such as Kafka Flume sockets Etc the default persistence level is set to
replicate the data to two nodes for fault tolerance now one thing which we should remember is unlike your rdds the
default persistence level of T streams keeps the data data serialized in memory
and that we can discuss again further about your serializations or deserialization now one important aspect
which takes care of your fault recovery is checkpointing mechanism in spark
streaming now when I say checkpointing a streaming application as in real scenario we would want must operate 24
bar 7 and if the stream application is constantly running then there has to be
a mechanism which can make your streaming application resilient to
failures which can be unrelated to your application logic so spark streaming needs to do the checkpointing it needs
to checkpoint enough information to a fault tolerant underlying storage system such that it can recover from failures
so your checkpointing is the process to make streaming applications more fault
tolerant or resilient to your failures now this is usually used when you would
want to recover from failure of a node running the driver of streaming application now we know driver is
existing for every application and it is basically one which knows the flow of
your application driver also has the context in case of streaming application we would have say the spark streaming
context which is the entry point of your application now when we talk about your checkpointing to ensure your screen
gaming application is more fault tolerant you have two kinds of checkpointing here so you could have a
metadata checkpointing and you could have data checkpointing now when we talk about metadata what does that include so
metadata includes configuration dstream operations or even incomplete batches so
when we talk about metadata it has configuration so configuration that was used to create the streaming application
you could have t-stream operations that is set of dstream operations that Define the streaming application that is your
series of rdds and then you have incomplete batches or batches whose jobs are queued but not have completed so
this would have to be checkpointed so metadata checkpointing is used for recovering from a node failure running
the streaming application driver now your metadata which has your
configuration incomplete batches and D stream operations need to be saved in an
underlying storage system now when you talk about data checkpointing it is mainly about saving the generated rtds
to reliable storage that is whatever rdds are computed so saving the
information which is saving the computations to a storage like sdfs now
that is used in stateful Transformations combining data across various batches so
when we talk about Transformations whatever rtds are generated that depend on rdds of previous batches if we are
talking about stateful and that can cause the length of dependency chain to
increase or keep increasing with time now to avoid any kind of increase in the
recovery time intermediate rtds can be periodically checkpointed and that could
be done to a reliable storage to basically cut off the growing dependency changes so if we would want to summarize
we would say metadata checkpointing is primarily needed for recovery from driver failures whereas data or RTD
checkpointing is necessary even for basic functioning if stateful Transformations are used and we should
remember we are talking about stateful Transformations here so when we talk about checkpointing now the question is
when would you enable checkpointing or what would you do to enable checkpointing so checkpointing must be
enabled for applications with different kind of requirements so for example if you are using stateful Transformations
where one series of rtds depend on the result of your previous matches so
something like update state by key or reduced by key in window now if these kind of operations are used in your
application then checkpoint directory must be provided to allow for periodic
rdd checkpointing when we say about requirements then recovering from failures of the driver which is taking
care of your application metadata checkpoints should be used when we talk about simple streaming applications
without the stateful Transformations they could be run without enabling checkpointing because you are one batch
of rdds it does not depend on the previous set of rdds which were done in
the previous time frame now recovery from driver failures will also be partial in that case when you talk about
your stateless so some might be received and your unprocessed data might be lost
but then that's pretty much acceptable when you talk over spark streaming applications so here we look at your
checkpointing so we say start now you have a checkpointing now whenever your application is creating a streaming
context it would create it would set a checkpoint path and then you would Define your D stream which is nothing
but series of your rdd Transformations your streaming context starts which is
basically your application entry point into the cluster for processing and at
any point of time if there is any failure you could always recover using the checkpoint which was created in case
of spark streaming one more thing we need to remember is that your checkpointing can be enabled by setting
a direct Tree in a fault tolerant reliable file system such as sdfs wherein the checkpointing information
will be saved so we will have to add some methods within your application
like streaming context with a checkpoint and then pointing to a checkpoint directory and in that way we can have
our stateful Transformations or metadata information stored in the underlying storage whichever we have chosen now
that we have discussed about spark streaming some basics of spark streaming let's also understand about shared
variables or what we call as accumulator and broadcast variables so normally when
you talk about your spark operations such as your map or reduce these are
executed on one of the node of your cluster and what happens is when you
talk about your operations they work on separate copies of all variables used in
the function now in this case variables are copied to each machine and no
updates to the variables on remote machines are propagated back to the driver program now when we talk about
read write shared variables across tasks that would be inefficient so spark
actually provides two limited types of shared variables for common usage patterns and those are your broadcast
variables and your accumulators now when you talk about your accumulators these
are variables that are only added through an associative or commutative
operation so spark natively supports accumulators of numeric types and programmers can add support for your new
types so when we talk about accumulators they can be used to implement counters
such as your map reduce or sums so as a user you can create named or unnamed
accumulators now as we see in the image here a named accumulator in this
instance counter will display in web UI for the stage that modifies that
accumulator spark displays the value for each accumulator Modified by a task in
the tasks table now tracking accumulators in the UI can be useful for understanding the progress of your
running stages but we should remember that as of now this is not supported in
Python might be in future the support for python will also be added now when you talk about your accumulators you can
have say a numeric accumulator created by calling a spark context and its
method such as long accumulator or you could have double accumulator to accumulate values of type long or double
respectively so tasks running on a cluster can then add to it using the add
method now we cannot I mean in this case the value cannot be read but driver program can read the accumulators value
using its Value method we can look at some examples to understand this later now when you talk about your broadcast
variable that is one more type of variables which allows the programmers to keep a read-only variable cast on
each machine rather than shipping a copy of it with tasks now what we know is
sometimes you might be doing very costlier operations like joins where you might be working on multiple rtds they
need to be joined and these rtds could also be pair rdds which could be key
value pairs now whenever you are performing a join there would be kind of two level of shuffling one within a
particular rdd so if you are joining two rdds the first RTD which might have data in the form of key value pairs that rdd
would have to have some shuffling where all the similar keys can be brought into one partition and this would happen on
the second RTD also which has again key value Pairs and then if you are doing a join these rtds will be shipped to the
node or basically the data would be loaded in the memory and then basically your transformation will happen this can
be a costlier affair so what can be done is you could create broadcast variables
so for example if we have two rtds and we want to perform a join operation and if one rdd is known to be smaller then
we can create a broadcast variable of the smaller rdd so that this variable
itself can be shipped to each machine and then this variable can be used for
your join operations with the other rdd which is existing in the memory of those nodes so that saves time and that
improves your performance so spark basically attempts to distribute the broadcast variable using efficient
broadcast algorithms to reduce the communication cost now if for example
you have multiple nodes in a cluster and if you would want to give every node a
copy of large input data set in an efficient manner so spark actions are
executed through a set of stages now we know that and stages are separated by
your Shuffle operations so whenever we talk about narrow dependencies such as map flat map filter you would not have
any shuffling involved but if you go for group by key Reduce by key and such
operations to bring the similar Keys together there would be shuffling involved and also this applies when you
are doing some join operations so in that case broadcast variables could be a really plus where one set of data or rdd
which is already computed that could be broadcasted to other nodes so the data
broadcasted will be cached in serialized form and deserialized before running each task this means that explicitly
creating broadcast variables can be useful when tasks across multiple stages
need the same data or when caching the data in deserialized form is important so we can create broadcast variables
using spark context and its method called broadcast and your broadcast variable can then be shipped to other
nodes which can be used for your other operations now when you talk about smart streaming it is used in various use
cases I mean you could talk about speech recognition you could talk about sentiment analysis you could talk about
uh streaming applications which would be performing some kind of analytics on the
data which is coming in it is also used widely in retail chain companies now if
we look at the example here so big retail chain companies would want to build real-time dashboards so that they
can keep a track of their inventory and operations and for this they would need
one streaming data or data which is constantly getting generated as source which needs to be processed on and this
information can then be populating your dashboards to give you a real-time scenario or real-time information of
what is happening so in case of an inventory dashboard you could use then
these interactive dashboards wherein you could draw insights about the business
and that's what retail companies are doing so how many products are being purchased or products that have been
shipped or how many products have been delivered to customers and this kind of information would be good to capture in
real time so when the streaming data is basically being processed so at one end
you have your data which is getting generated that might be based on the sales which are happening that might be
based on the products which are being shipped or that might be based on the acknowledgment that the products have
been received now while this data is getting generated at various sources it
can be subjected to a spark streaming application which will look into the streaming data perform series of
Transformations which you would want to process the data at regular time intervals and then pushing it to your
dashboards or to your storage layer wherein it could then be used to answer such questions so when we talk about
spark streaming it's an ideal choice to process this kind of data in real time and there are various use cases so at
one end if you see you have a input stream which shows the product status that is how many products were purchased
or shipped or delivered now this would be then handled by your spark streaming and also your spark core engine which
would then process the data to give you an output stream which gives you a status such as what was the total count
of products which are purchased products which were shipped and products which were delivered so this was a quick and
brief introduction to spark streaming and how it works now we can also see how spark streaming works or how we create
an application for that what we can do is we can basically set up our Eclipse
to have your Scala based spark applications and for that what you could
do is you could have your Eclipse which can then be having a Scala plugin added
to it now if somebody would want to look at the Scala plugin then you can always go to Scala Dot
ide.org and this is the place where you can scroll towards the bottom you can
click on stable and that basically also shows you a video how Scala plugin can
be added to your Eclipse it also tells from where you can get the Scala latest
release and this can be added to your Eclipse so in my case it has already been added to the eclipse I'm bringing
it up here and then you can have your applications built using your IDE people
would prefer to use IntelliJ and that's also fine so you could also look for videos on setting up IntelliJ with your
Scala plugin now there are two ways one you can build your application run it on
your Windows machine when you could have some kind of utility like netcat which can be used to send in some messages in
a streaming fashion you could have a receiver which looks at a particular socket at a particular port and you
could build a streaming application within your IDE run it on your Windows machine in a local mode which would then
be looking at your Source where the data is getting generated that's one option the second option is you could build
your application package it as jar and then run it on a cluster now that could
be a spark Standalone cluster or spark with yarn and then you could be packaging your application using tools
like SBT and then use spark submit to submit your application now I'll show you both the ways in which you can work
with streaming applications so first is get your eclipse and make sure that scalar plugin is already added now in my
case on the right top corner I see Scala symbol that says I can be using Scala
perspective now here I have some projects so what you can do is you can create a project by saying new Scala
project you can give it a name name so for example I could say my apps 3 and
then you can say finish so that creates a project now within your project what you can do is you can create a package
so for example I could say something like main dot Scala and then say finish
so that creates your package now also one thing to remember is it would be
good to change your compiler instead of 2.12 to 2.11 normally for different
environments you might have spark with different versions and Scala with 2.10 or 2.11 so it would be good to have the
compiler change to the bundle 2.11 so we can select this project I can do a right
click I can go to the build path I can say configure build path and then I can click on Scala compiler where I will use
project settings and I will choose to 2.11 bundle so that would make sure that your applications can compile just say
okay it might say the compiler settings have changed a full rebuild is required for changes to take a effect just hit on
OK so that's the first thing you need to do the second thing is for your code to
compile it would be good to have your jars added to your build path now we
could do that on as I said you could be writing your application which might not compile within your IDE but you could
package it using SBT and then run it as jar so what I'm doing here is on my
machine within my C drive I already have spark distribution which I've downloaded
so basically I have downloaded spark and then untied it so spark 2.4.3 I have
unzipped or untied it I have kept it in my C and this basically has my spark now
within which I have my jars and this has all my spark related charts so
technically speaking I can even use Windows command line and I can start working on spark in an interactive way
or packaging my application and running it in a local mode so you could all also do this so basically have your spark and
then if you see on my desktop I have a Hadoop folder within which I have a bin
folder and within which I have a win utils.exe and this will be basically required when you want to try out your
spark based applications whether that is streaming or data frames to be tested on
your Windows machine so you can always search and download the win utils.exe place it in a Hadoop folder within the
bin folder on your machine so once you get your win utils.exe once you have
your jars what you can do is for your project you can just say right click you
can go to the build path and you can say configure build path and here you had
add external jars so we can select all of these jars here which I would want to
add to my build path so that my code can compile and I can test it on the Windows
machine itself I can say open and then then just basically do apply and okay so
that has created my project with a package my compiler is changed to 2.11 I have added the external charge the spark
related jars and that is good enough for my code now what we need is a streaming
application which we need to build so let me show you from my existing project how it looks like so within your Source
the same way I have main package and here I have a streaming application so
this streaming application is to test or to work on capturing the data which is
generated at this particular stream on a particular IP and a particular port and I would want to do some series of
Transformations on that so this gives me an example of doing a word count and then I would want to print the results I
could also be saving the output in a particular location so for this application we need basically to import
certain packages that is sales spark streaming spark context and also you
have spark conf so these are the packages which we need to import now here I have created an object I've
called this fifth app and I'm saying extends app and this is because within my project I already have an application
which has been defined as main so if your application has been defined as main one application is already existing
then you can just have your new objects with extending app so we don't need to
define the main method here now here I am saying while conf new spark conf so I
am creating a configuration object I am setting my application name and to test it on Windows we will have to set the
master as local and it is advisable to give it more than one thread because you
would be creating a receiver within your application that would utilize one thread so here I am saying set master
local and then I am saying two threads I am also creating my spark context which
we need to initialize based on the configuration object we just created in the previous step then we need to create
a spark streaming context and this streaming context depends on spark
context and I've given a time interval of 10 seconds so this is the time
interval which I am setting for which I would want to work on the stream of data
which comes in every 10 seconds on a particular socket now here we are
setting up a receiver so I say stream rdd which is basically be spark
streaming context and then you use your socket text stream now there are various
methods of spark streaming context for example if I just go here and if I just do a dot it shows me what are the
different options you have file stream you have q stream you have socket stream receiver stream and so on so I am using
socket text stream and I would want to point it to this machine so I'll say
127.0.0.1 I I will also specify Port 2222 okay and here once I have created
my configuration object my spark context my spark streaming context with the time interval and I have set my receiver to
use the socket text stream method on this particular IP on this particular Port now then I am specifying what I
would want to do on the data which gets generated on this machine at this particular Port so I'm saying well word
counts so I would want to work on the stream RTD that is the D stream I would want to do a flat map on it to split the
data based on space I would want to map every word to word comma 1 and then I
would want to do a Reduce by key now Reduce by key you can pass in specifically the function you would want
to do I could do a count of this I could print the result or I could even save it with an output using the Java method to
create a random string attached to the output once you are done with this here I'm in mentioning spark streaming
context start now this will basically trigger my spark streaming context to start and it will run till we terminate
this application now here to run this on Windows machine you can always look into
your run configuration and what I have done is in my environment I basically
want to use let's say okay now let's look at the configuration what you need
to set so here I have my streaming application and let's look at the Run
configuration which basically shows my application has started and it's running on the environment so if you see here I
have added Hadoop underscore home pointing to this Hadoop directory which has been and when utils X I have also
given spark local IP which is 127.0.0.1 so that is my run
configuration now if you see here my streaming application has started my
streaming application has not yet started probably it is running a different app application so we can come
back and check this so we can go to run as Scala application and let's see what
it does so it tries to connect to 127.0.0.1 so receiver is trying to do
that but it does not find anything on that particular machine on that particular Port so my receiver is not
able to find it is not able to establish a connection now what I can do is I can go to my command line and here I will go
into downloads so I've already downloaded the netcat utility for Windows and here we can basically search
for something which I have on my machine within downloads and that is your netcat
utility so what I would do is I will come back here and I would say NC DOT
exe lvp and then I could say my port which I have specified in my streaming
application and I can just start this netcat utility now here it says listening on 222 it says connection to
127.0.0.1 and then if I look in the background my receiver will now be able
to establish a connection establish a connection with this netcat utility now whatever I type here will be taken for
processing every 10 seconds and we would see a word count while my application is
running so let's test this so I'll say this is a test test is being done and as
soon as I pass in this message we see that there is a word count happening for the stream of data which is coming in we
can say Winters are coming Winters will be cold and once I given these messages
we will see our streaming application which will try to work on the data which
is coming in and process it and show us the result so we can say this is a test
of winters and let's see if it continues to do the processing and shows us down
right so my streaming application is running fine it is looking at The Words which we are passing in and we can say
if it is able to process my data and every 10 seconds the tech stock socket
stream is looking on this machine at this particular Port doing a series of
Transformations and these series of Transformations and then are seen within
your console now in my application if I had used word counts.save as text file
then I could also have my output getting generated at every 10 second interval and that would get saved here I could
also decide to have my output saved and stored on a sdfs or any other storage so
this is a simple streaming application what we saw which is using socket text stream it is looking at this machine on
a particular Port where we are running a netcat utility it does series of your rdd Transformations that is flat map map
and then you're reduced by key and finally we are invoking an action such
as print which basically triggers these rdts which work on my streaming data so
this is one example now what I could also be doing is this is particularly my application now I already have SBT for
windows installed so I can be going into my project space so I could say
workspace I could go into my apps and I'm in my project folder so I could say
SBD package and this will basically then based on my build file which I already
have within my project folder so if you see here this is my build file which
says name the version Scala version spark version and the repository with
the spark to manage the dependencies for all its components like spark core SQL MLM streaming And Hive so you need to
have this build Dot SBT file within your project folder which can be used to
package your application as jar now we have done the packaging and it shows me
that it has created a jar of my applications within this particular folder now once I have this I can
basically then import this package into my cluster where I can run it on a
cluster using spark submit now to run the spark streaming application on a
cluster here I have a two node cluster which will then have my spark Standalone
cluster you could look at the previous videos with wherein I explain how to set up a spark Standalone cluster now here I
can go into the spark directory and then I could just say s bin start all dot sh
which will start my spark master and worker node now I do have Hadoop cluster
also set up on this but right now I am running a spark Standalone cluster which shows me I have a mass certain worker
process on this machine and I have a worker process on this machine so my spark Standalone cluster is running we
can always bring up the UI to see how it looks like so here I can just be giving
HTTP slash slash um1 8080 so that's my
spark Standalone cluster with two worker nodes right now there is no application running we can come back here and we can
check if we have my jar file so I've already placed my jar here now at any
point of time if you have your jar which has been packaged you can always go and
do a jar minus x v f and you can choose your jar to see what is within your jar
which I have already done on this machine so if I look in the main folder I have Scala and within Scala if you see
these are the different classes what we have so my code was packaged as a jar it
has a fifth app class which we would want to run here so what I can do is I I
can just be saying spark submit because now I would want my application to run
on the cluster so I can say class main dot Scala and then my object name so I
will say fifth app now that's my class my jar is this one and we will also say
master which is going to be spark which is running on um1 and it listens on 7077
Port so that would basically start my streaming application but what we would also need is like our windows we would
also need some utility like netcat so basically what I can do is I can here
use a netcat utility to basically connect or send in some messages for our
streaming application so let's say let me search if I have already the netcat
which I might have used here so let's see so we on your window machine you
will have to in install your netcat utility by saying a aptkit netcat so here I can say NC minus L and then I can
say Port which is 2 2 2 so we can see if this is working so this is a test and
let's see if it works so let's start our application streaming application which then needs the receiver to establish a
connection with the particular machine and let's see if that is done so here I
could also be canceling this and give it a particular Port so I can say 192 168
0.18 now the only problem is when we packaged our application when we
packaged our application we should be specifically giving the IP and also
comment out the local so when I package this to run on the cluster I commented out this part and I obviously would not
be giving 127 but the IP where my netcat is running so right now let's test it so
we will say this is a test new test for
our application and I'm sending in some messages so it is already doing the word
count as we expected it to do and if while we are doing it if you come back
here so you would see your application is now running on the cluster so it is my application it is using for Cores
what is the memory per executor and you can basically click on your application now here you can look at the application
detail UI to see what is being done so for our application we have streaming
job running receiver which says this is my application it says it is running you
can basically click at this click on this and it says your dag visualization which will say the streaming job which
is running you can look at the stages if you would want to see if there are multiple stages which we don't have
because we are not doing any wider transformation we can look at storage if we have used some persisting or caching
we can look at the executors which are being used and one of the important things when you talk about your
streaming is the streaming tab which gets activated now that does not show up when you run your patch applications but
in your streaming applications you have the UI shows you the streaming Tab and
that basically says running batches of 10 seconds for 2 minutes 9 seconds already now we see the input rate we see
the receivers we see if there is any delay in scheduling what is the processing time how many batches it has
completed and it basically shows me what is the processing time how many tasks were run and so on so my application is
running fine and unless and until we go ahead and cancel this my application
will continue to run it will keep looking for messages which we keep typing in so for example if I would want
to just copy this and start again where I would keep passing in this is a test
one test one is this where we test
streaming application right and I can be seeing that my messages are being
processed I'm getting a word count so we can say winter winter summer Old Winter
and you can always look at if your streaming application is doing the word count so this is a simple way where you
can run your streaming application either on a Windows machine in a local mode or you could be running on a spark
Standalone cluster or you could also deploy your application to run on a yarn
based cluster now that we have seen a streaming application which does a word count it will be interesting to also see
the window based computation or windowing operation which is supported in spark streaming as I explained and
for that we can look at a different application here which is streaming with
a window option let's look into this what this application does so here I am importing some packages that is
streaming streaming context your spark and Spark context spark configuration I'm also using the spark API Java
function I am then using spark streaming API and I'm also using Storage level
because I also intend to have the persistence or caching done now here we
create an application say streaming app extends app we create our configuration object where we say app name and then we
set master with appropriate number of threads we create the spark streaming context that's we initialize it and then
we have the spark streaming context which depends on SC and we have given a time interval of 10 seconds we then set
up a receiver so we say stream rdd which will use socket extreme like we did
earlier with which will look for this particular machine at this port wherein
I am also using Storage level so what I would want is that the data stream which
would get generated here every 10 seconds should be cached in memory now
we could use different storage levels that is we could use memory only disk only memory and disk you could have
memory and disk with replication Factor so you have different storage levels now here we are then doing some series of
Transformations like what we did earlier but if you see in my previous example we just had reduced by key and here I am
using a transformation which is reduced by key and window now this one basically takes the function as I said the
windowing takes associative or commutative functions so instead of using the shortcut wave we will be
specifying our function so we say it takes a and b and basically it uses this
function now here we have to give the interval that is 30 seconds that's my window time frame and then within that
what will be my each in interval which we would want to look at so these are the two main aspects One is using your
window based transformation and then also specifying your time interval for your windowing which basically means we
would still do a word count but then here we would basically not only be doing a word count for every 10 seconds
for the data which is coming in here but we would also want to have a
Consolidated set of computation done every 30 seconds so that is looking at
the last three time intervals now if you also notice I am also doing a check pointing here wherein I have not
specified a specific directory but what I am saying here is I would want to
checkpoint or I would want to basically save my computations for any kind of
fault recovery now this is a streaming application with window based computation we are using Storage level
for persistence we are also doing the checkpointing which takes care of fault
recovery now to run this application we have already set our environment variables and so on so I can basically
start this application by just doing a run as Scala application now this would
start my streaming application with window based computations now we can
open up our command line where we would want to start a netcat utility so I
could go to downloads let me just minimize this window here and what I can do is I can then be starting my netcat
utility so I could say NC DOT exe minus lvp and on a port 222 as soon as I do
this in background my application receiver will be able to establish connection to this netcat utility
running on this particular Port now that's done so we can see if the word count happens for our 10 second interval
so let's do that so I'll say this is my first test and if I do this it should
give me the word count for the data which I have passed in now I can say this is my second test this is my third
test and if you see here it basically is doing this now if I say this is my first
test again so since we are doing window based operation also we will see a
Consolidated set of Transformations which happen for a window of three
intervals so if you see here now my this is giving me a count but it is not
showing me totally this as 4 or say this because it is only looking at a window
of last two or three time intervals so now it is back because it is looking at
the last one so if I say this is my test test is done test is done so if you do
this it will show you the word count which is for or whatever you have passed
in like test is done is twice test is Thrice but then since windowing based
operation is done you will look at the time interval of the messages which were passed within that particular window now
we can also since my application is running and it is on Windows we can bring up your spark UI so I can go to
http and slash slash I can be looking at my machine and the port so for example I
could do this this shows my streaming application which is running in wherein I can see the window based application
which is running but what would be interesting is to look at the stages if there is any kind of shuffling which is
happening looking at the storage should show me the persistence which we are doing now if you see here it shows me
that we have been persisting in the memory what size it occupies how many partitions were cast and this is because
of the persistence or the Storage level what we have used as memory only now if
we change that to disk we will see our rdds will be cached on disk and then you can look at your streaming tab which
gives you more information about your batches so this is a very simple example obviously you can change what kind of
RTD computations you would want to do here whether you want to do a word count or something else whether you want to
save your processed output now here we are able to do that you can always do a
Refresh on your project and here what we see is we see some entries of a folder
getting created right and we can decide whether we would want our output to be
created we can also see within this what would be containing and since we have set checkpointing in the project folder
it would also be checkpointing temporarily the information for any kind of failure so this is a classic example
of streaming application use using window based computations persisting your computations and also doing the
checkpointing simultaneously in this tutorial here we have some list of questions and an explanation of those so
that you can be well prepared for your Hadoop interviews now let's look at some general Hadoop questions so what are the
different vendor specific distributions of Hadoop now all of you might be aware that Hadoop or Apache Hadoop is the core
distribution of Hadoop and then you have different vendors in the market which have packaged the Apache Hadoop in a
cluster management solution which allows everyone to easily deploy manage monitor
upgrade your clusters so here are some vendor specific distributions we have Cloudera which is the dominant one in
the market we have hortonworks and now you might be aware that clouder and hortonworks have merged so it has become
a bigger entity you have mapper you have Microsoft Azure I IBM's infosphere and
Amazon web services so these are some popularly known vendor-specific distributions if you would want to know
more about the Hadoop distributions you should basically look into Google and
you should check for Hadoop different distributions wiki page so if I type Hadoop different distributions and then
I check for the Wiki page that will take me to the distributions and Commercial support page and this basically says
that the sold products that can be called a release of Apache Hadoop come from apache.org so that's your open
source community and then you have various vendor specific distributions which basically are running in one or
the other way Apache Hadoop but they have packaged it as a solution like a installer so that you can easily set up
clusters on set of machines so have a look at this page and read through about
different distributions of Hadoop coming let's look at our next question so what
are the different Hadoop configuration files now whether you're talking about Apache Hadoop Cloudera hortonworks map r
or no matter which other distribution these config files are the most important and existing in every
distribution of Hadoop so you have Hadoop environment.sh wherein you will have environment variables such as your
Java path what would be your process ID path where will your logs get stored what kind of metrics will be collected
and so on your core hyphen site file has the hdfs path now this has many other
properties like enabling trash or enabling High availability or discussing or mentioning about your zookeeper but
this is one of the most important file you have hdfs hyphen site file now this
file will have other information related to your Hadoop cluster such as your replication Factor where will name node
store its metadata on disk if a data node is running where would data node store its data if a secondary name node
is running where would that store a copy of name nodes metadata and so on your mapred hyphen site file is a file which
will have properties related to your map reduce processing you also have Masters
and slaves now these might be deprecated in a vendor-specific distribution and in
fact you would have a yarn hyphen site file which is based on the yarn
processing framework which was introduced in Hadoop version 2 and this would have all your resource allocation
Source manager and node manager related property properties again if you would want to
look at default properties for any one of these for example let's say hdfs
hyphen site file I could just go to Google and type in one of the properties for example I would say DFS dot name
node.name dot directory and as I know this property belongs to hdfs hyphen
site file and if you search for this it will take you to the first link which says stfs default XML you can click on
this and this will show you all the properties which can be given in your
stfs hyphen site file it also shows you which version you are looking at and you can always change the version here so
for example if I would want to look at 2.6.5 I just need to change the version
and that should show me the properties similarly you can just give a property which belongs to say core hyphen site
file for example I would say FS dot default fs and that's a property which
is in core hyphen sci-fi and somewhere here you would see core minus default
dot XML and this will show you all the properties so similarly you could search for properties which are related to yarn
hyphen site file or mapred hyphen site file so I could say yarn dot resource
manager and I could look at one of these properties which will directly take me to yarn default XML and I can see all
the properties which can be given in yarn and similarly you could say map
reduce dot job dot reduces and I know this property belongs to mapreduce
hyphen site file and this takes you to the default XML so these are important config files and no matter which
distribution of Hadoop you are working on you should be knowing about these config files whether you work as a
Hadoop admin or you work as a Hadoop developer knowing these config properties would be very important and
that would also showcase your internal knowledge about the configs which drive
your Hadoop cluster let's look at the next question so what are the three modes in which Hadoop can run so you can
have Hadoop running in a standalone mode now that's your default mode it would basically use a local file system and a
single Java process so when you say Standalone mode it is as you downloading
Hadoop related package on one single machine but you would not have any process running that would just be to
test Hadoop functionalities you could have a pseudo distributed mode which basically means it's a single node
Hadoop deployment now Hadoop as a framework has many many services so it
has a lot of services and those Services would be running irrespective of your
distribution each service would then have multiple processes so your pseudo
distributed mode is a mode of cluster where you would have all the important
processes belonging to one or multiple Services running on a single node if you
would want to work on a pseudo distributed mode and using a Cloudera you can always go to Google and search
for cloudera's quick start VM you can download it by just saying Cloud era
quick start VM and you can search for this and that will allow you to download a quick start VM follow the instructions
and you can have a single node Cloudera cluster running on your virtual machines for more information you can refer to
the YouTube tutorial where I have explained about how to set up a quick start VM coming back you could have
finally a production setup or a fully distributed mode which basically means that your Hadoop framework and its
components would be spread across multiple machines so you would have multiple services such as hdfs yarn
Flume scope Kafka hbase Hive Impala and
for these Services there would be one or multiple processes distributed across
multiple nodes so this is normally what is used in production environment so you
could say Standalone would be good for testing pseudo distributed could be good for testing and development and fully
distributed would be mainly for your production setup now what are the differences between regular file system
and hdfs So when you say regular file system you could be talking about a Linux file
system or you could be talking about a Windows based operating system so in regular file system we would have data
maintained in a single system so the single system is where you have all your
files and directories so it is having low fault tolerance right so if the
machine crashes your data recovery would be very difficult unless and until you have a backup of that data that also
affects your processing so if the machine crashes or if the machine fails then your processing would be blocked
now the biggest challenge with regular file system is the seek time the time
taken to read the data so you might have one single machine with huge amount of
disks and huge amount of ram but then the time taken to read that data when
all the data is stored in one machine would be very high and that would be with least fault tolerance if you talk
about sdfs your data is distributed so sdfs stands for Hadoop distributed file
system so here your data is distributed and maintained on multiple systems so it
is never one single machine it is also supporting reliability so whatever is
stored in hdfs say a file being stored depending on its size is split into
blocks and those blocks will be spread across multiple nodes not only that
every block which is stored on a node will have its replicas stored on other
nodes replication Factor depends but this makes sdfs more reliable in cases
of your slave nodes or data nodes crashing you will rarely have data loss
because of Auto replication feature now time taken to read the data is
comparatively more as you might have situations where your data is
distributed across the nodes and even if you are doing a parallel read your data
read might take more time because it needs coordination for multiple machines however if you are working with huge
data which is getting stored it will still be beneficial in comparison to reading from a single machine so you
should always think about its reliability through Auto replication feature its fault tolerance because of
your data getting stored across multiple machines and its capability to scale so
when you talk about sdfs we are talking about horizontal scalability or scaling out when you talk about regular file
system you are talking about vertical scalability which is scaling up now let's look at some specific sdfs
questions what is this why is sdfs Fault tolerant now as I just explained in
previous slides your sdfs is Fault tolerant as it replicates data on
different data nodes so you have a master node and you have multiple slave nodes or data nodes where actually the
data is getting stored now we also have a default block size of 128 MB that's
the minimum since Hadoop version 2. so any file which is up to 128 MB would be
using one logical block and if the file size is bigger than 128 MB then it will
be split into blocks and those blocks will be stored across multiple machines
now since these blocks are stored across multiple machines it makes it more fault
tolerant because even if your machines fail you would still have a copy of your
block existing on some other machine now there are two aspects here one we talk
about the first rule of replication which basically means you will never have two identical blocks sitting on the
same machine and the second rule of replication is in terms of rack awareness so if your machines are placed
in racks as we see in the right image you will never have all the replicas
placed on the same rack even if they are on different machines so it has to be fault tolerant and it has to maintain
redundancy so at least one replica will be placed on some other node on some
other rack that's how as DFS is Fault tolerant now here let's understand the
architecture of sdfs now as I mentioned earlier you would in a Hadoop cluster
the main service is your hdfs so for your sdfs service you would have a name
node which is your master process running on one of the machines and you would have data nodes which are your
slave machines getting stored across marketing or the processes running
across multiple machines each one of these processes has an important role to play when you talk about sdfs whatever
data is written to hdfs that data is split into blocks depending on its size
and the blocks are randomly distributed across nodes with auto replication feature these blocks are also Auto
replicated across multiple machines with the first condition that no two
identical blocks will sit on the same machine now as soon as the cluster comes up you
which are part of the cluster and based on config files would start sending their heartbeat to the name node and
this would be every three seconds what does name node do with that name node will store this information in its Ram
so name node starts building a metadata in its RAM and that metadata has
information of what are the data nodes which are available in the beginning now when a data writing activity starts and
the blocks are distributed across data nodes data nodes every 10 seconds will
also send a block report to name node so name node is again adding up this
information in its Ram or the metadata in Ram which earlier had only data node
information now name node will also have information about what are the files the
files are split in which blocks the blocks are stored on which machines and what are the file permissions now while
name node is maintaining this metadata in Ram name node is also maintaining
metadata in disk so that is what we see in the red box which basically has information of whatever information was
written to hdfs so to summarize your name node has metadata in Ram and metadata in disk
your data nodes are the machines where your blocks or data is actually getting stored and then there is a auto
replication feature which is always existing unless and until you have disabled it and your read and write
activity is a parallel activity however replication is in sequential activity
now this is what I mentioned about when you talk about name node which is the
master process hosting metadata in disk and RAM so when we talk about disk it
basically has a edit log which is your transaction log and your FS image which
is your file system image right from the time the cluster was started this
metadata in disk was existing and this gets appended every time read write or
any other operations happen on stfs metadata m is dynamically built every
time the cluster comes up which basically means that if your cluster is coming up name node in the initial few
seconds or few minutes would be in a safe mode which basically means it is
busy registering the information from data nodes so name node is one of the most critical processes if name node is
down and if all other processes are running you will not be able to access the cluster name nodes metadata in disk
is very important for name node to come up and maintain the cluster name nodes metadata in Ram is basically for all or
satisfying all your client requests now when we look at data nodes as I
mentioned data nodes hold the actual data blocks and they are sending these block reports every 10 seconds so the
metadata in name nodes are Ram is constantly getting updated and metadata in disk is also constantly getting
updated based on any kind of write activity happening on the cluster now data node which is storing the block
will also help in any kind of read activity whenever a client requests so
whenever a client or an application or an API would want to read the data it would first talk to name node name node
would look into its metadata online and confirm to the client which machines
could be reached to get to get that's where your client would try to read the data from sdfs which is
actually getting the data from data nodes and that's how your read write requests are satisfied now what are the
two types of metadata in name node server holds as I mentioned earlier metadata in disk very important to
remember edit log NFS image metadata in Ram which is information about your data
nodes files files being split into blocks blocks residing on data nodes and
file permissions so I will share a very good link on this and you can always look for more detailed information about
your metadata so you can search for sdfs metadata directories explained now this
is from hot and works however it talks about the metadata in disk which name node manages and details about this so
have a look at this link if you are more interested in learning about metadata on disk coming back let's look at the next
question what is the difference between Federation and high availability now these are the features which were
introduced in Hadoop version 2. both of these features are about horizontal
scalability of name node prior to version 2 the only possibility was that
you could have one single Master which basically means means that your cluster
could become unavailable if name node would crash so Hadoop version 2 introduced two new features Federation
and high availability however High availability is a popular one so when you talk about Federation it basically
means any number of name nodes so there is no limitation to the number of name
nodes your name nodes are in a Federated cluster which basically means name nodes
still belong to the same cluster but they are not coordinating with each other so whenever a write request comes
in one of the name node picks up that request and it guides that request for the blocks to be written on data nodes
but for this your name node does not have to coordinate with other name node to find out if the block ID which was
being assigned was the same one as assigned by other name node so all of
them belong to a Federated cluster they are linked via a cluster ID so whenever
an application or an API is trying to talk to Cluster it is always going via an cluster ID and one of the name node
would pick up the read activity or write activity or processing activity so all
the name nodes are sharing a pool of metadata in which each name node will
have its own dedicated pool and we can remember that by a term called namespace
or name service so this also provides High fault tolerance suppose your one
name node goes down it will not affect or make your cluster unavailable you
will still have your cluster reachable because there are other name nodes running and they are available now when
it comes to heartbeats all your data nodes are sending their heartbeats to all the name nodes and all the name
nodes are aware of all the data nodes when you talk about high availability this is where you would only have two
name nodes so you would have an active and you would have a standby now normally in any environment you would
see a high availability setup with zookeeper so zookeeper is a centralized
coordination service so when you talk about your active and stand by name notes election of a name node to be to
be made as active and taking care of a automatic failover is done by your
zookeeper High availability can be set up without zookeeper but that would mean
that a admins intervention would be required to make a name node as active from standby or also to take care of
failover now at any point of time in high availability a active name node
would be taking care of storing the edits about whatever updates are
happening on sdfs and it is also writing these edits to a shared location standby
name node is the one which is constantly looking for these latest updates and
applying to its metadata which is actually a copy of whatever your active
name node has so in this way your standby name node is always in sync with
the active name node and if for any reason active name node fails your standby name node will take over and
become the active remember zookeeper plays a very important role here it's a centralized coordination service one
more thing to remember here is that in your high availability secondary name
node will not be allowed so you would have a active name node and then you will have a standby name node which will
be configured on a separate machine and both of these will be having access to a shared location now that shared location
could be NFS or it could be a quorum of General nodes so for more information refer to the tutorial where I have
explained about sdfs high availability and Federation now let's look at some logical question here so if you have a
input file of 3 MB which version 28 MB how many input splits
would be created by sdfs what would be the size of each input split so for this you need to remember
that by default the minimum block size is 128 MB now that's customizable if
your environment has more number of larger files written on an average then
obviously you have to go for a bigger block size if your environment has a lot
of files being written but these files are of smaller size you could be okay with 128 MB remember in Hadoop every
entity that is your directory on sdfs file on sdfs and a file having multiple
blocks each of these are considered as objects and for each object Loops name nodes Ram 150 bytes is
utilized so if your block size is very small then you would have more number of blocks which would directly affect the
name nodes of M if you keep a block size very high that will reduce the number of
blocks but remember that might affect in processing because processing also
depends on split split more number of splits more the parallel processing so setting of block
size has to be done with consideration about your parallelism requirement and
your name nodes Ram which is available now coming to the question if you have a file of 350 MB that would be split into
three blocks and here two blocks would have 128 MB data and the third block
although the block size would still be 128 it would have only 94 MB of data so
this would be the split of this particular file now let's understand about rack awareness how does rack
awareness work or why do we even have racks so organizations always would want to place their nodes or machines in a
systematic way there can be different approaches you could have a rack which would have machines running on the
master processes and the intention would be that this particular rack could have higher bandwidth more cooling dedicated
power supply top of rack switch and so on the second approach could be that you
could have one master process running on one machine of every rack and then you
could have other slave processes running now when you talk about your rack awareness one thing to understand is
that if your machines are placed within racks and we are aware that Hadoop
follows Auto replication the rule of replication in a rack aware cluster
would be that you would never have all the replicas placed on the same rack so
if we look at this if we have block a in blue color you will never have all the three blue boxes in the same rack even
if they are on different nodes because that makes us that makes it less fault tolerant so you would have at least one
copy of block which would be stored on a different rack on a different note now
let's look at this so basically here we are talking about replicas being placed
in such a way now somebody could ask a question can I have my block and its replicas spread across three racks and
yes you can do that but then in order to make it more redundant you are increasing your bandwidth requirement so
the better approach would be two blocks on the same rack on different machines
one copy on a different track now let's proceed how can you restart name node
and all demons in Hadoop so if you were working on an Apache Hadoop cluster then
you could be doing a start and stop using Hadoop demon scripts so there are
these Hadoop demon scripts which would be used to start and stop your Hadoop
and this is when you talk about your Apache Hadoop so let's look at one
particular file which I would like to show you more information here and this talks about your different clusters so
let's look into this and so let's look at the start and stop and here I have a
file let's look at this one and this gives you highlights so if you talk about Apache Hadoop this is how the
setup would be done so you would have it download the Hadoop tar file you would have to untie it edit the config files
you would have to do formatting and then start your cluster and here I have said using scripts so this is in case of
Apache Hadoop you could be using a start all script that internally triggers
start DFS and start yarn and these scripts start DFS internally would run
Hadoop them in multiple times based on your configs to start your different processes then your start yarn would run
yarn demand script to start your processing related process so this is how it happens in Apache
Hadoop now in case of cloud era or hortonworks which is basically a vendor specific distribution you would have say
multiple Services which would have one or multiple demons running across the machines let's take an example here that
you would have machine one machine two and machine 3 with your processes spread across however in case of cloud era and
hot remarks these are cluster Management Solutions so you would never be involved in running a script individually to
start and stop your processes in fact in case of Cloudera you would have a Cloudera SCM server running on one of
the machines and then Cloudera SCM agents running on every machine if you talk about hortonworks you would have
ambari server and ambari agent running so your agents which are running on every machine are responsible to monitor
the processes send also their heartbeat to the master that is your server and
your server is the one or a service which basically will give instructions
to the agents so in case of vendor specific distribution your start and stop of processes is automatically taken
care by these underlying services and these Services internally are still
running these commands however only in Apache Hadoop you have to manually follow these to start and stop coming
back we can look into some command related questions so which command will
help you find the status of blocks and file system health so you can always go for a file system check command now that
can show you the files for a particular sdfs path it can show you the blocks and
it can also give you information on status such as under replicated blocks
over replicated blocks misreplicated blocks default replication and so on so
your fsck file system check utility does not repair if there is any problem with
the blocks but it can give you information of blocks related to the files on which machines they are stored
if they are replicated as per the replication factor or if there is any
problem with any particular replica now what would happen if you store too many small files in a cluster and this
relates to the block information which I give some time back so remember Hadoop
is coded in Java so here every directory every file and file related block is
considered as an object and for every object we within your Hadoop cluster
name nodes Ram gets utilized so more number of blocks you have more would be
usage of name node slam and if you're storing too many small files it would not affect your disk it would directly
affect your name node's Ram that's why in production clusters admin guys or
infrastructure specialist will take care that everyone who is writing data to hdfs follows a quota system so that you
could be controlled in the amount of data you write plus the count of data
and individual rights on hdfs now how do you copy data from local system onto
sdfs so you can use a put command or a copy from local and then given your
local path which is your source and then your destination which is your sdfs path
remember you can always do a copy from local using a minus F option that's a
flag option and that also helps you in writing the same file or a new file to
hdfs so with your minus F you have a chance of overwriting or rewriting the
data which is existing on sdfs so copy from local or minus put both of them do
the same thing and you can also pass an argument when you are copying to control your replication or other aspects of
your file now when do you use DFS admin refresh notes or RM admin refresh notes
so as the command says this is basically to do with refreshing the node
information so your refresh notes is mainly used when say a commissioning or
decommissioning of nodes is done so when in node is added into the cluster or
when a node is removed from the cluster you are actually informing Hadoop master
that this particular node would not be used for storage and would not be used
for processing now in that case you would be once you are done with the process of commissioning or
decommissioning you would be giving these commands that is refresh notes and
other I might win refresh notes so internally when you talk about commissioning decommissioning there are
include and exclude files which are updated and these include and exclude files will have entry of machines which
are being added to the cluster or machines which are being removed from the cluster and while this is being done
the cluster is still running so you do not have to restart your master process however you can just use this refresh
commands to take care of your commenting decommissioning activities now is there
any way to change replication of files on sdfs after they are already written and the answer is of course yes so if
you would want to set a replication Factor at a cluster level and if you
have admin access then you could edit your sdfs hyphen site file or you could
say Hadoop hyphen site file and that would take care of replication Factor being set at a cluster level however if
you would want to change the replication after the data has been written you could always use a set rep command so
set rep command is basically to change the replication after the data is written you could also write the data
with a different replication and for that you could use a minus D DFS dot
replication and give your application Factor when you are writing data to the class
so in Hadoop you can let your data be replicated as per the property set in
the config file you could write the data with a different replication you could
change the replication after the data is written so all these options are available now who takes care of
replication consistency in a Hadoop cluster and what do you mean by under over replicated blocks now as I
mentioned your fsck command can give you information of over or under replicated blocks now in a cluster it is always and
always name node which takes care of replication so for example if you have set up a
replication of three and since we know the first rule of replication which
basically means that you cannot have two replicas residing on the same node it
would mean that if your application is 3 we would need at least three data nodes
available now say for example you had a cluster with three nodes and replication
was set to three at one point of time one of your name node crashed and if that happens your blocks would be under
replicated that means there was an application Factor set but now your
blocks are not replicated or there are not enough replicas as per the replication Factor set this is not a
problem your master process or name node will wait for some time before it will
start the replication of data again so if a data road is not responding or if a
disk has crashed and if name node does not get information of a replica name
node will wait for some time and then it will start re-replication of those
missing blocks from the available nodes however while name node is doing it the
blocks are in under replicated situation now when you talk about over replicated this is a situation where name node
realizes that there are extra copies of block now this might be the case that
you had three nodes running with the replication of three one of the node went down you Network failure or some
other issue within few minutes name node re-replicated the data and then the failed node is back with
set of blocks blocks again name node is smart enough this is a over replication situation and
it will delete set of blocks from one of the nodes it might be the node which has
been recently added it might be your old node which has joined your cluster again or any node that depends on the load on
a particular now we discussed about Hadoop we discussed about sdfs now we will discuss
about mapreduce which is the programming model and you can say processing framework what is distributed cache in
mapreduce now we know that when we talk about mapreduce the data which has to be
processed might be existing on multiple nodes so when you would have your mapreduce program running it would
basically from the underlying disks now this could be a costly operation if every time the
data has to be read from disk so distributed cache is a mechanism wherein
data set or data which is coming from the disk can be cached and available for
all worker nodes now how will this benefit so when a map reduce is running instead of every time reading the data
from disk it would pick up the data from distributed cache and this this will
benefit your map reduce so distributed cash can be set in your
job conf where you can specify that a file should be picked up from
distributed cache now let's understand about these roles so what is a record reader what is a combiner what is a
partitioner and what kind of roles do they play in a map reduce processing
Paradigm or mapreduce operation so record reader communicates with the
input split and it basically converts the data into key value Pairs and these
key value pairs are the ones which will be worked upon by the mapper your
combiner is an optional face it's like mini reduce so combiner does not have
its own class it relies on the reducer class basically your combiner would
receive the data from your map tasks which would have completed works on it
based on whatever reducer class mentions and then passes its output to the
reduced surface partitioner is basically a phase which decides how many reduced
tasks would be used aggregate or summarize your data so partitioner is a
phase which would decide based on the number of keys based on the number of map tasks your partitioner would decide
if one or multiple reduced tasks take care of process so either it could be
partitioner which decides on how many reduce tasks would run or it could be based on the properties which we have
set within the cluster which will take care of the number of reduced tasks which would be used always remember your
partitioner decides how outputs from combiner are sent to reducer and to how
many reducers it controls the partitioning of keys of your intermediate map outputs so map phase
whatever output it generates is an intermediate output and that has to be
taken by your partitioner or by a combiner and then partitioner to be sent
to one or multiple reduce tasks this is one of the common questions which you might face why is mapreduce slower in
processing so we know mapreduce goes for parallel processing we know we can have multiple map tasks running on multiple
nodes at the same time we also know that multiple reduced tasks could be running now why does then mapreduce become a
slower approach first of all your map reduce is a batch oriented operation now
mapreduce is very rigid and it strictly uses mapping and reducing phases so no
matter what kind of processing you would want to do you would have to still provide the mapper function and the
reducer function to work on data not only this whenever your map phase completes the output of your map phase
which is an intermittent output would be written to hdfs and thereafter
underlying discs and this data would then be shuffled and sorted and picked
up for reducing phase so every time your data being written to hdfs and retrieved
from sdfs makes mapreduce a slower approach the question is for a map
release job is it possible to change the number of mappers to be created Now by
default you cannot change the number of map tasks because number of map tasks
depends on the input splits however there are different ways in which you
can either set a property to have more number of map tasks which can be used or
you can customize your code or make it use a different format which can then
control the number of map tasks by default number of map tasks are equal
to the number of splits of file you are processing so if you have a 1GB of file that is
split into eight blocks of 128 MB there would be eight map tasks running on the
cluster these map tasks are basically running your mapper function if you have
hard coded properties in your mapred hyphen site file to specify more number
of map tasks then you could control the number of map tasks let's also talk
about some data types so when you prepare for Hadoop when you want to get into Big Data field you you should start
learning about different data now there are different data formats such as Avro parquet you have a sequence
file or binary format and these are different formats which are used now when you talk about your data types in
Hadoop these are implementation of your writable and writable comparable interfaces so for every data type in
Java you have a equivalent in Hadoop so end in Java would be in writable in
Hadoop float would be float writable long would be long writable double writable Boolean writable array writable
map writable and object writable so these are your different data types that could be used
within your mapreduce program and these are implementation of writable and
writable comparable interfaces what is speculative execution now imagine you
have a cluster which has huge number of nodes and your data is spread across
multiple slave machines or multiple nodes now at a point of time due to a disk degrade on network issues or
machine heating up or more load being on a particular node there can be a
situation where your data node will execute task in a slower manner now in
this case if speculative execution is turned on there would be a shadow task
or a another similar task running on some other node
for the same processing so whichever task finishes first will be accepted and
the other task would be killed so speculative execution could be could be good if you are working in an intensive
workload kind of environment where if a particular node is slower you could
benefit from a unoccupied or a node which has less load to take care of your
processing going further this is how we can understand so node a which might be
having a slower task you would have a scheduler which is maintaining or having
knowledge of what are the resources available so if speculative execution has a property is turned on then the
task which was running slow a copy of that task or you can say shadow task
would run on some other node and whichever task completes first will be considered this is what happens in your
speculative execution now how is identity mapper different from chain
mapper now this is where we are getting deeper into mapreduce Concepts so when you talk about mapper identity mapper is
the default mapper which is chosen when no mapper is specified in mapreduce
driver class so for every mapreduce program you would have a map class which
is taking care of your mapping phase which basically has a mapper function and which would run one or multiple map
tasks right your programming your program would also have a reduced class
which would be running a reducer function which takes care of reduced tasks running on multiple nodes now if a
mapper is not specified within your driver class so driver class is
something which has all information about your flow what's your map class what is your reduce class what's your
input format what's your output format what are the job configurations and so on so identity mapper is the default
mapper which is chosen when no mapper class is mentioned in your driver
it basically implements an identity function which directly writes all its key pairs into output and it was defined
in old map reduce API in this particular package but when you talk about chaining
mappers or chain mapper this is basically a class to run multiple
mappers in a single map task or basically you could say multiple math
tasks would run as a part of your processing the output of first mapper would become as an input to Second
mapper and so on and this can be defined in the under mentioned class or package
what are the major configuration parameters required in a mapreduce program obviously we need to have the
input location we need to have the output location so input location is where the files will be picked up from
and this would preferably on sdfs directory output location is the path
where your job output would be written by your mapreduce program you also need
to specify input and output formats if you don't specify the defaults are considered then we need to also have the
classes which have your map and reduce functions and if you intend to run the
code on a cluster you need to package your class in a jar file export it to
your cluster and then this jar file would have your mapper reducer and Driver classes so these are important
configuration parameters which you need to consider for a mapreduce program now what is the difference or what do you
mean by Maps height join and reduce side join map side join is basically when the
join is performed at the mapping level or at the mapping phase or is performed by the mapper so each input data which
is being worked upon has to be divided into same number of partitions
input to each map is in the form of a structured partition and is in sorted
order so Maps I join you can understand it in a simpler way that if you compare
it with rdbms Concepts where you had two tables which were being joined it will
always be advisable to give your bigger table as the left side table or the
first table for your join condition and it would be your smaller table on the
left side and your bigger table on the right side which basically means the smaller table could be loaded in memory
and could be used for joining so map side join is a similar kind of mechanism
where input data is divided into same number of parties when you talk about reduced side join
here the join is performed by the reducer so it is easier to implement
than map side join as all the sorting and shuffling will send the values or
send all the values having identical keys to the same reducer so you don't need to have your data set in a
structured form so look into your map site join or reduce side join and other joints just to understand how mapreduce
Works however I would suggest not to focus more on this because mapreduce is
still being used for processing but the amount of mapreduce based processing has
decreased overall or across the industry now what is the role of output committer
class in a mapreduce job so output committer as the name says describes the
commit of task output for a mapreduce job so we would have this as mentioned
our Apache Hadoop map reduce output committer you could have a class which extends output committer class so
mapreduce relies on this map reduce relies on the output committer of the job to set up the job initialization
cleaning up the job after the job completion that means all the resources which were being used by a particular
job setting up the task temporary output checking whether a task needs a commit
committing the task output and discarding the task so this is a very important class and can be used within
your mapreduce job what is the process of spilling in mapreduce what does that mean so spilling is basically a process
of copying the data from memory buffer to disk when obviously the buffer usage
reaches a certain threshold so if there is not enough memory in your buffer in
your memory then the content which is stored in buffer or memory has to be
flushed out so by default a background thread starts spelling the content from
memory to disk after 80 percent of buffer size is filled now when is the buffer being used so when your map
reduce processing is happening the data from data is being read from the disk loaded into the buffer and then some
processing happens same thing also happens when you are writing data to the cluster so you can imagine for a hundred
megabyte size buffer the spilling will start after the content of buffer reaches 80 megabytes this is
customizable how can you set the mappers and reducers for a mapreduce job so these are the properties so number of
mappers and reducers as I mentioned earlier can be customized so by default
your number of map tasks depends on the split and number of reduced tasks depends on the partitioning phase which
decides number of reduced tasks which would be used depending on the keys however we can set
these properties either in the config files or provide them on the command
line or also make them part of our code and this can control the number of map
tasks or reduce tasks which would be run for a particular job let's look at one
more interesting question what happens when a node running a map task fails before sending the output to the reducer
so there was a node which was running a map task and we know that there could be one or multiple map tasks or multiple
nodes and all the map tasks have to be completed before the further stages that
such as combiner or reducer come into existence so in a case if a node crashes
where a map task was assigned to it the whole task will have to be run again on
some other node so in Hadoop version 2 yarn framework has a temporary demon
called application master your application Master is taking care of execution of your application and if
a particular task on a particular node failed due to unavailability of node it
is the role of application Master to have this task scheduled on some other
node now can we write the output of mapreduce in different formats of course we can so Hadoop supports various input
and output formats so you can write the output of mapreduce in different formats so you could have the default format
that is text output format wherein records are written as line of text you
could have sequence file which is basically to write sequence files or your binary format files where your
output files need to be fed into another mapreduce jobs you could go for a map
file output format to write output as map files you could go for a sequence file as a binary output format so that's
again a variant of your sequence file input format it basically writes keys
and values to a sequence file so when we talk about binary format we are talking about a non-human readable format DB
output format now this is basically used when you want to write data to say
relational databases or say nosql databases such as hbase so this format
also sends the reduce output to a SQL table now let's learn a little bit about
yarn yarn which stands for yet another resource negotiator it's the processing
framework so what benefits did Yan bring in Hadoop version 2 and how did it solve the issues of mapreduce version 1. so
map reduce version 1 had major issues when it comes to scalability or availability because sorry in Hadoop
version 1 you had only one master process for processing layer and that is
your job tracker so your job tracker was listening to all the task trackers which
were running on multiple machines so your job tracker was responsible for
resource tracking and job scheduling in yarn you still have a processing Master
but that's called resource manager instead of job tracker and now with Hadoop version 2 you could even have
resource manager running in high availability mode you have node managers which would be running on multiple
machines and then you have a temporary demon called application master so in
case of Hadoop version 2 your resource manager or Master is only handling the
client connections and taking care of tracking the resources the jobs scheduling or basically taking care of
execution across multiple nodes is controlled by application Master till
the application comes so in yarn you can have different kind of resource allocations that could be
done and there is a concept of container so container is basically a combination of RAM and CPU cores yarn can run
different kind of workloads so it is not just map reduce kind of workload which
can be run on Hadoop version 2 but you would have graph processing massive parallel processing you could have a
real-time processing and huge processing applications could run on a cluster
based on yarn so when we talk about scalability in case of your Hadoop
version 2 you can have a cluster size of more than 10 000 nodes and can run more
than 100 000 concurrent tasks and this is because for every application which
is launched you have this temporary demon called application master so if I would have 10 applications
running I would have 10 app Masters running taking care of execution of these applications across multiple nodes
compatibility so Hadoop version 2 is fully compatible with whatever was developed as per Hadoop version 1 and
all your processing needs would be taken care by yarn so Dynamic allocation of cluster
resources taking care of different workloads allocating resources across
multiple machines and using them for execution all that is taken care by yarn
multi-tenancy which basically means you could have multiple users or multiple
teams you could have open source and propriety data access engines and all of these
could be basically hosted using the same cluster now how does yarn allocate
resources to an application with Alpha help with architecture so basically you have a client or an application or an
API which talks to resource manager resource manager is as I mentioned
managing the resource allocation in the Clusters when you talk about resource
manager you have its internal two components one is your scheduler and one is your applications manager so when we
say resource manager being the master is tracking the resources The Source manager is the one which is negotiating
the resources with slave it is not actually resource manager who is doing it but these internal components so you
have a scheduler which allocates resources to various running applications so scheduler is not
bothered about tracking your resources or basically tracking your applications so we can have different kind of
schedulers such as feed ful which is which is first in first out you could have a fair scheduler or you could have
a capacity scheduler and these schedulers basically control how
resources are allocated to multiple applications when they are running in
parallel so there is a queue mechanism so scheduler will schedule resources
based on requirements of application but it is not monitoring or tracking the
status of application your applications manager is the one which is accepting the job submissions
it is monitoring and restarting the application Masters so it's application manager which is basically then
launching a application Master which is responsible for an application so this
is how it looks so whenever a job submission happens we already know that resource manager is aware of the
resources which are available with every node manager so on every node which has
fixed amount of RAM and CPU cores some portion of resources that is your RAM
and CPU cores are allocated to node manager now resource manager is already
aware of how much resources are available across nodes so whenever a client request comes in resource manager
will make a request to node manager it will basically request node manager to
hold some resources for processing node manager would basically approve or
disapprove this request holding the sources and these resources
that is a combination of RAM and Co course are nothing but containers we can
allocate containers of different sizes within yarn hyphen site file so your
node manager based on a request from resource manager guarantees the container which would be available for
processing that's when your resource manager starts a temporary demon called
application Master to take care of your execution so your app Master which was
launched by resource manager or we can say internally applications manager will
run in one of the containers because application Master is also a piece of code so it will run in one of the
containers and then other containers will be utilized for execution
this is how yarn is basically taking care of your allocation your application
Master is managing resource needs it is the one which is interacting with scheduler and if a particular node
crashes it is the responsibility of App Master to go back to the master which is
resource manager and negotiate for more resources so your app Master will never
ever negotiate Resources with node manager directly it will always talk to
resource manager and the source manager is the one which negotiates the resources container as I said is a
collection of resources like your RAM CPU Network bandwidth and your container
is located based on the availability of resources on a particular node so which
of the following has occupied the place of a job tracker of mapreduce so it is
your resource manager so resource manager is the name of the master process in Ado version 2. now if you
would have to write yarn commands to check the status of an application so we
could just say yarn application minus status and then the application ID and you could kill it also from the command
line remember your yarn has a UI and you can even look at your applications from
the UI you can even kill your applications from the UI however knowing the command line commands would be very
useful useful can we have more than one resource manager in a yarn-based cluster yes we can that is what Hadoop version 2
allows I'll have you can have a high availability yarn cluster where you have
a active and standby and the coordination is taking care by your
zookeeper at a particular time there can only be one active resource manager and
if active resource manager fails your standby resource manager comes and
becomes active however zookeeper is playing a very important role remember zookeeper is the one which is
coordinating the server State and it is doing the election of active to standby
failover what are the different schedulers available in yarn so you have a fee for scheduler that is first in
first out and this is not a desirable option because in this case a longer
running application might block all other small running applications Your
Capacity scheduler is basically a scheduler where dedicated queues are created and they have fixed amount of
resources so you can have multiple applications accessing the cluster at
the same time and they would be using their own cues and the resources
allocated to them if you talk about Fair scheduler you don't need to have a fixed
amount of sources you can just have a percentage and you could decide what
kind of fairness is to be followed which basically means that if you were allocated 20 gigabytes of memory however
the cluster has 100 gigabytes and the other team was assigned 80 gigabytes of
memory then you have 20 access to the cluster another team has 80 percent however if the other team does not come
up or does not use the cluster in a fair scheduler you can go up to maximum of
100 percent of your cluster to find out more information about your schedulers
you could either look in Hadoop definitive guide or what you could do is you could just go to Google and you
could type for example yarn scheduler let's search for yarn scheduler and then
you can look in Hadoop definitive guide and so this is your Hadoop definitive
guide and it beautifully explains about your different schedulers how do multiple applications run and that could
be in your fifo kind of scheduling it could be in capacity scheduler or it
could be in a fair scheduling so have a look at this link it's a very good link you can also search for yarn untangling
and this is a Blog of four or this is a series of four blocks where it's
beautifully explained about your yarn how it works how the resource allocation
happens what is a container and what runs within the container so you can scroll down you can be reading through
this and you can then also search for part two of it which talks about allocation and so on so coming back
we basically have these schedulers what happens if a resource manager fails
while executing an application in a high availability cluster so in a high
availability cluster we know that we would have two resource managers one being active one being standby and
zookeeper which is keeping a track of the server States so if a RM fails in
case of high availability the standby will be elected as active and then
basically your resource manager or the standby would become the active one and
this one would instruct the application Master to a bot in the beginning then
your resource manager recovers its running state so there is something called as RM State Store where all the
applications which are running their status is stored so resource manager recovers its running state by looking at
your state store by taking advantage of container statuses and then continues to
care of your processing now in a cluster of 10 data nodes each having 16 GB and
10 cores what would be total processing capacity of the cluster take a minute to
think 10 data nodes 6 16 GB Ram per node 10 cores so if you mention the answer as
160 GB RAM and 100 cores then you went wrong now think of a cluster which has
10 data nodes each having 16 GB RAM and 10 cores remember on every node in a
Hadoop cluster you would have one or multiple processes running those processes would need RAM the machine
itself which has a Linux file system would have its own processes so that would also be having some RAM usage
which basically means that if you talk about 10 data nodes you should deduct at
least 20 to 30 percent towards the overheads towards the cloud database Services towards the other processes
which are running and in that case I could say that you could have 11 or 12 GB available on every machine for
processing and say six or seven cores multiply that by 10 and that's your
processing capacity capacity remember the same thing applies to the disk usage also so if somebody asks you in a 10
data node cluster where each machine has 20 terabytes of disks what is my total
storage capacity DFS so the answer would not be 200 you
have to consider the overheads and this is basically which gives you your
processing capacity now let's look at one more question so what happens if requested memory or CPU cores Beyond or
goes beyond the size of container now as I said you can have your configurations which can say that in a particular data
node which has 100 GB Ram I could allocate say 50 GB for the processing
like out of 100 cores I could say 50 cores for processing so if you have 100
GB RAM and 100 cores you could ideally allocate 100 for processing but that's
not ideally possible so if you have 100 GB Ram you would over 50 GB and if you
have 100 cores you would go for 50 . now within this RAM and CPU course
have the concept of containers right so container is a combination of RAM and CPU cores so you could have a minimum
size container and maximum size content now at any point of time if your
application starts demanding more memory or more CPU cores and this cannot fit
into a container location your application will fail your application will fail because you requested for a
memory or a combination of memory and CPU cores which is is more than the
maximum container so look into this yarn tangling website which I mentioned and look for the second blog in those series
which explains about these now here we will discuss on hive Peg
hbase and these components of two which are being used in the industry for
various use cases let's look at some questions here and let's look how you should prepare for them so first of all
we will learn on hive which is a data warehousing package so the question is what are the different components of a
hive architecture now when we talk about Hive we already know that Hive is a data warehousing package which basically
allows you to work on structured data or data which can be structurized so normally people are well versed with
querying or basically processing the data using SQL queries a lot of people
come from database backgrounds and they would find it comfortable if they know structured query language Hive is one of
the data warehousing package which resides within a Hadoop ecosystem it
uses hadoop's distributed file system to store the data and it uses rdbm mess
usually to store the metadata although metadata can be stored locally also so
what are the different components of a hive architecture so it has a user
interface so user interface calls the execute interface to the driver this
creates a session to the query and then it sends the query to the compiler to generate an execution plan for it
usually whenever Hive is set up it would have its metadata stored in an rdbms now
to establish the connection between rdbms and Hadoop we need odbc or jdbc
connector jar file and that connector jar file has a driver class now this
driver class is mandatory to create a connection between Hive and Hadoop so
user interface creates this interface using the driver now we have metastore
metastore stores the metadata information so any object which you create such as database table indexes
their metadata is stored in metastore and usually this meta store is stored in
an rdbms so that multiple users can connect to Hive so your meta store stores the metadata information and
sends that to the compiler for execution of a query what does the compiler do it
generates the execution plan it has a tag now tag stands for direct cyclic
graph so it has a tag of stages where each stage is either a metadata
operation a map or reduced job or an operation on sdfs and finally we have
execution engine that acts as a bridge between Hive and Hadoop to process the query so execution engine communicates
bi-directionally with metastore to perform operations like create or drop tables so these are four important
components of Hive architecture now what is the difference between external table
and manage stable and Hive so we have various kinds of table in Hive such as
external table manage table partition table the major difference between your
managed and external table is in respect to what happens to the data if the table
is dropped usually whenever we create a table in Hive it creates a manage table
or we could also call that as an internal table now this manages the data and moves it into warehouse directory by
default whether you create a manage stable or external table usually the
data can reside in hive's default Warehouse directory or it could be
residing in a location chosen however when we talk about manage table if one
drops a manage table not only the metadata information is deleted but the
tables data is also deleted from sdfs if we talk about external table it is
created with an external keyword explicitly and if an external table is
dropped nothing happens to the data which resides in sdfs so that's the main
difference between your managed and external table what might be the use case if somebody asks you there might be
a migration kind of activity or you are interested in creating a lot of tables
using your queries so in that case you could dump all the data on sdfs and then
you could create a table by pointing to a particular directory or multiple directories now you could then do some
testing of your tables and would decide that you might not need all the tables so in that case it would be advisable to
create external tables so that even if the table is later dropped the data on
sdfs will be intact unlike your manage table where dropping of table will
delete the data from sdfs Also let's learn a little bit on partition so what
is partition And Hive and why is partitioning required in high life if somebody asks you that now normally in
world of rdbms partition is the process to group similar type of data together
and that is usually done on basis of a column or what we call as partitioning key now each table usually has one
column in context of rdbms which could be used to partition the data and why do we do that so that we can avoid scanning
the complete table for a query and restrict the scan to set of data or to a
particular partition in Hive we can have any number of partition keys so
partitioning provides granularity in Hive table it reduces the query latency by scanning only relevant partition data
instead of whole data set we can partition at various levels now if I
compare rdbms with Hive in case of rdbms you could have one column which could be
used for partitioning and then then you could be squaring the specific partition
so in case of rdbms your partition column is usually a part of the table
definition so for example if I have an employee table I might have employee ID employee name employee age and employee
salary has four columns and I would decide to partition the table based on salary column now why would I partition
it because I feel that employee table is growing very fast it is or it will have
huge amount of data and later when we query the table we don't want to scan
the complete table so I could split my data into multiple partition based on a
salary column giving some ranges in Hive it is a little different in Hive you can
do partitioning and there is a concept of static and dynamic partitioning but in Hive the partition column is not part
of table definition so you might have an employee table with employee ID name a H
and that that's it that would be the table definition but you could then have partitioning done based on salary column
which will then create a specific folder on sdfs in that case when we query the
data we can see the partition column also showing up so we can partition the
transaction data for a bank for example based on month like Chan Feb Etc and any
operation regarding a particular month will then allow us to query that particular folder that is where
partitioning is useful now why does Hive not store metadata information in a CFS
if somebody asks you so we know that hives data is stored in sdfs which is
Hadoop distributed file system however the metadata is either stored locally
and that mode of high would be called as embedded mode or you could have hives
metadata stored in rdbms so that multiple clients can initiate data connection now this metadata which is
very important for Hive would not be stored in sdfs so we already know that
sdfs read and write operations are time consuming it is a distributed file system and it can accommodate huge
amount of data so Hive stores metadata information in meta store using rdbms
instead of sdfs so this allows to achieve low latency and faster data
access now if somebody asks what are the components used in Hive query processor
so usually we have the main components are your parser your execution engine
logical plan generation Optimizer and type checking so whenever a query is
submitted it will go through a parser and parser would check the syntax it would check for objects which are being
queried and other things to see if the query is fine now internally you have a semantic analyzer which will also look
at the query you have an execution engine which basically will work on the
execution part that is the best generated execution plan which could be used to get the results for the query
you could also have user defined functions which a user would want to use
and these are normally created in Java or Java programming language and then
basically these user defined functions are added to the class path now you would have a logical plan generation
with which basically looks at your query and then generates a logical plan or the
best execution path which would be required to get to the results internally there is a physical plan
generated which is then looked in by Optimizer to get the best path to get to
the data and that might also be checking your different operators which you are using within your query finally we would
also have type checking so these are important components in Hive so somebody might ask you if you are querying your
data using Hive what are the different components involved or if you could explain what are the different
components which work when a query is submitted so these are the components
now let's look a scenario based question Suppose there are a lot of small CSV
files which are present in a is DFS directory and you want to create a single Hive table from these files so
data in these files have Fields like registration number name email address
so if this is what needs to be done what will be your approach to solve it where
will you create a single Hive table for lots of small files without degrading
the performance of the system so there can be different approaches now we know that there are a lot of small CSV files
which are present in a directory so we know that when we create a table in Hive
we can use a location parameter so I could say create table give a table name
give the column and their data types I could specify the delimiters and finally I could say location and then point it
to a directory on sdfs and this directory might be the directory which has lot of CSV files so in this case I
will avoid loading the data in the table because table being Point table pointing
to the directory will directly pick up the data from one or multiple files and
we also know that Hive does schema check on read so does not do a schema check on
write so in case there were one or two files which did not follow the schema of
the table it would not prevent data loading data would anyways be loaded only when you query the data it might
show you null values if data which was loaded does not follow the schema of the
table this is one approach what is the other approach so let's look at that you can think about sequence file format
which is basically a smart format or a binary format and you can group these
small files together to form a sequence file now this could be one other smarter
approach so we could create a temporary table so we could say create table give a table name give the column names and
their data types we could specify the delimiters as it shows here that is row format and Fields terminated by and
finally we can store that as text file then we can load data into this table by
giving a local file system path and then we can create a table that will store
data in sequence file format so my point one is storing the data in this text
file 0.3 would be storing the data in sequence file format so we say create
table give the specifications we say row format delimited fields are terminated
by comma stored as sequence file then we can move the data from test table into
test sequence file table so I could just say insert overwrite my new table as
select star from other table remember in Hive you cannot do insert update delete
however if the table is existing you can do a insert overwrite from an existing
table into a new table so this could be one approach where we could have lot of
CSV files or smaller files club together as one big sequence file and then store
it in the table now if somebody asks you write a query to in insert a new column
that is integer data type into a hive table and the requirement might be that
you would want to insert this table at a position before an existing column now that's possible by doing an alter table
giving your table name and then specifying change column giving your new column with the data type before an
existing column this is a simple way where you can insert a new column into a hive table what are the key differences
between Hive and pick now some of you might have heard High Visa data warehousing package and Peg is more of a
scripting language both of them are used for data analysis or Trend detection
hypothesis testing data transformation and many other use cases so if we
compare Hive and pick Hive uses a declarative language called Hive ql that
is Hive querying language similar to SQL and it is for reporting or for data
analysis even for data transformation or for your data extraction big uses a high
level procedural language called Pig Latin for programming both of them remember use mapreduce processing
framework so when we run a query in Hive to process the data or when we create
and submit a big script both of them trigger a mapreduce job unless and until we have set them to Local mode Hive
operates on the server side of the cluster and basically works on structured data or data which can be
structuralized pig usually works or operates on the client side of the cluster and allows both structured
unstructured or even I could say semi-structured data Hive does not support Avro file format by default
however that can be done by using the write serializer deserializer so we can
have Hive table related data stored in Avro format in sequence file format in
parquet format or even as a text file format however when we are working on smarter formats like Avro or sequence
file or parquet we might have to use specific serializers deserializers for
Avro this is the package which allows us to use Avro format Pig supports Agro
format by default Hive was developed by Facebook and it supports partitioning and Peg was developed by Yahoo and it
does not support partitioning so these are high level differences there are lots and lots of differences remember
Hive is more of a data warehousing package and Peg is more of a scripting
language or a strictly procedural flow following scripting language which
allows us to process the data now let's get more and let's get more deeper and
learn about big which is as I mentioned a scripting language which can be used
for your data processing it also uses map reduce although we can even have big
run in a local mode let's learn about pig in the next section now let's learn
on some questions about Pig which is a scripting language and it is extensively
used for data processing and data analysis so the question is how is Apache Pig different from mapreduce now
we all know that mapreduce is a programming model it is it's quite rigid
when it comes to processing the data because you have to do the mapping and
reducing you have to write huge code usually mapreduce is written in Java but
now it can also be written in Python it can be written in Scala and other programming languages so if we compare
Pig with mapreduce pig obviously is very concise it has less lines of code when
compared to mapreduce now we also know that big script internally will trigger a mapreduce job however user need not
know about mapreduce programming model they can simply write simple scripts in Pig and that will automatically be
converted into mapreduce however mapreduce has more lines of code Peak is high level language which can easily
perform join operations or other data processing operations map reduce is a
low level language which cannot perform job join operations easily so we can do
join using mapreduce however it's not really easy in comparison to Pig now as
I said on execution every Pig operator is converted internally into a mapreduce
job so every big script which is run which would be converted into mapreduce
job now map reduce overall is a batch oriented processing so it takes more
time to compile it takes more time to execute either when you run a mapreduce
job or when it is triggered by Pink script Pig works with all versions of Hadoop and when we talk about mapreduce
program which is written in one Hadoop version may not work with other versions it might work or it might not it depends
on what are the dependencies what is the compiler you're using what programming language you have used and what version
of Hadoop you are working on so these are the main differences between Apache Pig and mapreduce what are the different
ways of executing pick script so you could create a script file store it in
dot pick or dot text and then you could execute it using the pick command you
could be bringing up the grunt shell that is Pig's shell now that usually
starts with mapreduce mode but then we can also bring it up in a local mode and we can also run pick embedded as an
embedded script in other programming language so these are the different ways of executing your pick script now what
are the major components of pig execution environment this is this is a very common question interviewers would
always want to know different components of Hive different component currents of pig even different components which are
involved in Hadoop ecosystem so when we want to learn about major components of big execution environment here are some
so you have pick scripts now that is written in pig latin using built-in operators and user-defined functions and
submitted to the execution environment that's what happens when you would want to process the data using pick now there
is a parser which does type checking and checks the syntax of the script the
output of parser is a tag direct a cyclic graph so look in Wikipedia for
tag so dag is basically a sequence of steps which run in One Direction then
you have an Optimizer now this Optimizer performs optimization using merge
transform split Etc it aims to reduce the amount of data in the pipeline
that's the whole purpose of Optimizer you have a internal compiler so big
compiler converts the optimized code into a mapreduce job and here user need
not know the mapreduce programming model or how it works or how it is written they all need to know about running the
pick script which would be internally converted into a mapreduce job and finally we have an execution engine so
mapreduce jobs are submitted to the execution engine to generate the desired results so these are major components of
pick execution environment now let's learn about different complex data types
in big big supports various data types the main ones are Tuple bag and map what
is Tuple or Tuple as you might have heard a tuple is an ordered set of fields which can contain different data
types for each field so in Array you would have multiple elements but that would be of same types list can also
have different types your Tuple is a collection which has different fields
and each field can be of different type now we could have an example is 1 comma 3 or 1 comma 3 comma a string or a float
element and all of that form a tuple bag is a set of tuples so that's represented
by curly braces so you could also imagine this like a dictionary which has various different correction elements
what is a map map is a set of key value pairs used to represent data so when you
work in Big Data field you need to know about different data types which are supported by Peg which are supported by
Hive which are supported in other components of Ado so pupil pack map
array array buffer you can think about list you can think about dictionaries
you can think about map which is key value pair so these are your different complex data types other than the
primitive data types such as integer character string Boolean float and so on
now what are the various diagnostic operators available in Apache pick so these are some of the operators or
options which you can give in a pick script you can do a thumb now dump operator runs the pig latin scripts and
displays the result on the screen so either I could do a dumb and see the output on the screen or I can even do a
dump into and I could store my output in a particular file so we can load the
data using load operator in Pig and then Pig also has different internal storage
like Json loader or pick storage which can be used if you are working on specific kind of data and then you could
do a dump either before processing or after processing and dump would produce
the result the result could be stored in a file or seen on the screen you also
have a describe operator now that is used to view the schema of a relation so you can load the data and then you can
view the schema of relation using describe operator explain as we might
already know displays the physical logical and mapreduce execution plans so normally in rdbms when we use X-Plane we
would like to see what happens behind the scenes when a particular script or a query runs so we could load the data
using load operator as in any other case and if we would want to display The Logical physical and mapreduce execution
plans we could use explain operator there is also an illustrate operator now
that gives the step-by-step execution of sequence of statements so sometimes when we would want to analyze our script to
see how good or bad they are or would that really serve our purpose we could use illustrate and again you can test
that by loading the data using load operator and you could just use a illustrate operator to have a look at
the step-by-step execution of the sequence of statements which you would want to execute so these are different
diagnostic operators available in Apache pick now if somebody asks State the
usage of group order by and distinct keywords in big script so as I said big
is a scripting language so you could use various operators so group basically
collects various records with the same key and groups the data in one or more
relations here is an example you could do a group data so that is basically a variable or you can give some other name
and you can say group relation Name by H now say I have a file where I have field
various fields and one of the field is a relational name so I could group that by
a different field order by is used to display the contents of relation in a sorted order whether ascending or
descending so I could create a variable called relation 2 and then I could say order relation name 1 by ascending or
descending order distinct basically removes the duplicate records and it is implemented only on entire records not
on individual records so if you would like want to find out the distinct values and relation name field I could
use distinct what are the relational operators in pig so you have various relational operators which help data
scientists or data analysts or developers who are analyzing the data such as go Group which joins two or more
tables and then performs group operation on the join table result you have cross
it is used to compute the cross product that is a Cartesian product of two or
more relations for each is basically to do some iteration so if it will iterate
through tuples of a relation generating a data transformation so for example if I say variable a equals and then I load
a file in a and then I could create a variable called B where I could say for each a I would want to do something say
group join is to join two or more tables in a relation limit is to limit the
number of output tuples or output results split is to split the relation into two or more relations Union is to
get a combination that it will merge the contents of two or more relations and order is to get a sorted result so these
are some relational operators which are extensively used in pig for analysis
what is the use of having filters in Apache pick now say for example I have
some data which has three Fields here product quantity and this is my phone
sales data so filter operator could be used to select the required values from
a relation based on a condition it also allows you to remove unwanted records
from data file so for example filter the products where quantity is greater than
thousand so I see that I have one row wherein or multiple rows where the
quantity is greater than thousands such as fifteen hundred Seventeen hundred twelve hundred so I could create a
variable called a I would load my file using pick storage as I explained
earlier big storage is an internal parameter which can be used to specify
the delimiters now here my delimiter is comma so I could say using pick storage as and then I could specify the data
type for each field so here being integer product being character array and quantity being integer then B I
could say filter a whatever we have in a by quantity greater than thousand so
it's very concise it's very simple and it allows us to extract and process data in a simpler way now Suppose there is a
file called test dot txt having 150 records in sdfs so this is a file which
is stored on sdfs and it has 150 records where we can consider every record being one line and if somebody asks you to
write a pick command to retrieve the first 10 records of the file first we
will have to load the data so I could create a variable called test underscore data and I would say load my file using
pick storage specifying the delimiter as comma as and then I could specify my Fields what whatever Fields our file
have and then I would want to get only 10 records for which I could use the limit operator so I could say limit on
test data and give me 10 records this is very simple and we can extract 10 records from 150 records which are
stored in the file on sdfs now we have learned on Pig we have learned some
questions on hive you could always look more in books like programming in Hive
or programming in pick and look for some more examples and try out these examples on a existing Hadoop setup now let's
learn on hbase which is a nosql database now edgebase is a four dimensional
database in comparison to your rdbms which usually are two dimensional so
rdbms have rows and columns but hbase has four coordinates it has row key
which is always unique column family which can be any number column qualifiers which can again be any number
per column family and then you have a version so these four coordinates make H
base a four dimensional key value store or a column family store which is unique
for storing huge amount of data and extracting data from hbase there is a
very good link which I would suggest everyone can look at if you would want to learn more on edgebase and you could
just say hbase mapper and this basically brings up a documentation which is from
mapper but then that's not specific to map R and you can look at this link which will give you a detailed
explanation of HP is how it works what are the Architectural Components and how
data is stored and how it makes edgebase a very powerful nosql database so let's
learn on some of the important or critical questions on hbase which might
be asked by the interviewer in an interview when you are applying for a Big Data admin or a developer position
role so what are the key components of edgebase now as I said this is one of
the favorite questions of interviewers where they would want to understand your knowledge on different components for a
particular service edgebase as I said is a nosql database and that comes as a
part of service with Cloudera or hortonworks and with Apache Hadoop you could also set up Edge base as an
independent package so what are the key components of hbase edgebase has a
region server now edgebase follows the similar kind of topology like Hadoop now
Hadoop has a master process that is name node and slave processes such as data
nodes and secondary name node in the same way edgebase also has a master which is Edge master and the slave
processes are called region servers so these region servers are usually co-located with data nodes however it is
not mandatory that if you have 100 data nodes you would have 100 region servers so it purely depends on admin so what
does this region server contain so region server contains hbase tables that
are divided horizontally into regions or you could say group of rows is called
regions so in edgebase you have two aspects one is group of columns which is
called column family and one is group of rows which is called regions now these
regions or these rows are grouped based on the key values or I would say row
Keys which are always unique when you store your data in edgebase you would
have data in the form of rows and columns so group of rows are called regions or you could say these are
horizontal partitions of the table so a region server manages these regions on
the Node where a data node is running a region server can have up to thousand
regions it runs on every node and decides the size of region so region
server as I said is a slave process which is responsible for managing HPS data on the Node each region server is a
worker node or a worker process co-located with data node which will take care of your read write update
delete request from the clients now when we talk about more components of edgebase as I said you have HP H master
so you would always have a connection coming in from a client or an application what does etch Master do it
assigns regions it monitors the region servers it assigns regions to region
servers for load balancing and it cannot do that without the help of Zookeeper so
if we talk about components of hbase there are three main components you have zookeeper you have etch master and you
have region server region server being the slave process your Edge Master being
the master process which takes care of all your table operations assigning regions to the region servers taking
care of read and write requests which come from client and for all of this Edge Master will take in help of
Zookeeper which is a centralized coordination service so whenever a client wants to read or write or change
the schema or any other metadata operations it will contact H Master Edge
Master internally will contact zookeeper so you could have edgebase setup also in
high availability mode where you could have a active Edge master in a backup Edge Master you would have a zookeeper
Quorum which is the way zookeeper works so zookeeper is a centralized coordination service which will always
run with a quorum of processes so zookeeper would always run with odd
number of processes such as 3 5 and 7 because zookeeper works on the concept
of maturity consensus now zookeeper which is a centralized coordination service is keeping a track of all the
servers which are alive available and also keeps a track of their status for
every server which zookeeper is monitoring zookeeper keeps a session alive with that particular server Edge
Master would always check with zookeeper which region servers are available alive
so that regions can be assigned to the region server at one end you have region
server which are sending their status to the Zookeeper indicating if they are ready for any kind of read or write
operation and at other end Edge Master is querying the Zookeeper to check the status now zookeeper internally manages
a meta table now that meta table will have information of which regions are
residing on which region server and what rookies those regions contain so in case
of a read activity Edge Master will zookeeper to find out the region server
which contains that meta table once etch Master gets the information of meta
table it can look into the meta table to find out the row keys and the corresponding region servers which
contain the regions for those row Keys now if we would want to understand row
key and column families in hbase let's look at this and it would be good if we could look this on an Excel sheet so row
key is always unique it acts as a primary key for any hbase table it
allows a logical grouping of cells and make sure that all cells with the same
row key are co-located on the same server so as I said you have four
coordinates for hbase you have a row key which is always unique you have column
families which is nothing but group of columns and when I say column families one column family can have any number of
columns so when I talk about hbase hbase is four dimensional and in terms of H
base it is also called as a column oriented database which basically means
that every Row in one column could have a different data type now you have a row
key which uniquely identifies the row you have column families which could be one or many depending on how the table
has been defined and a column family can have any number of columns or I could
say for every row within a column family you could have different number of
columns so I could say for my Row one I could just have two columns such as name
and City within the column family for my Row 2 I could have name City age
designation salary for my third row I could have thousand columns and all that
could belong to one column family so this is a horizontally scalable database
so column family consists of group of columns which is defined during table creation and each column family can have
any number of column qualifiers separated by a delimiter now a
combination of row key column family column qualifier such as name City age
and the value within the cell is makes the hbase a unique four dimensional
database for more information if you would want to learn on hbase please refer this link which is hbase mapper
and this gives a complete edgebase architecture that has three components
of name node three components that is name node region servers and zookeeper
how it works how hbase H Master interacts with zookeeper what zookeeper
does in coordination how are the components working together and how does hbase take care of read and write coming
back and continuing why do we need to disable a table so there are different
table operations what you can didn't do in hbase and one of them is disabling a table now if you would want to check the
status of table you could check that by is disabled and giving the table name or
is enabled and the table name so the question is why do we need to disable a table now if we would want to modify a
table or we are doing some kind of Maintenance activity in that case we can disable the table so that we can modify
or changes settings when a table is disabled it cannot be accessed through the scan command now if we have to write
a code to open a connection in hbase now to interact with hbase one could either
use a graphical user interface such as Hue or you could be using the command line hbase shell or you could be using
hbase admin API if you are working with Java or say happy base if you are
working with python where you may want to open a connection with hbase so that
you can work with database programmatically in that case we have to create a configuration object that is
configuration my conf and then create a configuration object and then you can
use different classes like Edge table interface to work on a new table you
could use each column qualifier and many other classes which are available in hbase admin API what does replication
mean in terms of hbase so edgebase as I said Works in a cluster way and when you
talk about cluster you could always set up a replication from one hbase cluster
to other edgebase cluster so this replication feature in hbase provides a mechanism to copy data between clusters
or sync the data between different clusters this feature can be used as a disaster recovery solution that provides
High availability for hbase so if I have hbase cluster 1 where I have one master
and multiple region servers running in a Hadoop cluster I could use the same Hadoop cluster to create a hbase replica
cluster or I could have a totally different edgebase replica cluster where my intention is that if things are
changing in a particular table in cluster one I would want them to be replicated across different cluster so I
could alter the edge base table and set the replication scope to 1. now a replication scope of 0 indicates that
table is not replicated but if we set the replication to 1 we basically will have to set up ahbs cluster where we can
replicate hbase tables data from cluster 1 to Cluster so these are the commands
which can be used to enable replication and then replicate the data of table
across clusters can we Import and Export in hbase of course we can it is possible
to Import and Export tables from one edgebase cluster to other hbase cluster or even within a cluster so we can use
the hbase export utility which comes in this particular package give a table name and then a Target location so that
will export the data of hbase table into a directory on sdfs then I could create
a different table which would follow some kind of same definition as the table which was exported and then I
could use import to import the data from the directory on sdfs to my table if you
would want to learn more on hbase Import and Export you could look at hbase
import operations let's search for the link and this is the link where you
could learn more about hbase import export utilities how you could do a bulk
import bulk export which internally uses mapreduce and then you could do a Import
and Export into edgebase tables moving further what do we mean by compaction in
edgebase now we all know that hbase is a nosql database which can be used to store huge amount of data however
whenever a data is written in hbase it is first written to what we call as
write ahead log and also to mem store which is write cache now once the data
is written involved and your mem store it is offloaded to form an internal
edgebase format file which is called H5 and usually these edge files are very small in nature so we also know that
sdfs is good when we talk about few number of larger files in comparison to
large number of smaller files due to the limitation of name nodes memory compaction is process of merging hbase
files that is the smaller edge files into a single large file this is done to reduce the amount of memory required to
store the files and number of disk seeks needed so we could have lot of H files
which get created when the data is written to hbase and these smaller files can then be compacted through a major or
minor compaction creating one big Edge file which internally would then be written to osdfs and sdfs format of
blocks that is the benefit of compaction there is also a feature called Bloom filter so how does Bloom filter work so
Bloom filter or hbase Bloom filter is a mechanism to test whether a h file
contains a specific row or a row column cell Bloom filter is named after its
creator button covered Bloom it is a data structure which predicts whether a
given element is a member of a set of data it provides an in-memory index
structure that reduces the disk that reads and determines the probability of
finding a row in a particular file this is one of very useful features of hbase
which allows for faster access and avoids disk syncs thus edgebase have any
concept of namespace so namespace is when you have similar elements grouped together so namespace yes hway supports
such names space so namespace is a logical grouping of tables analogous to
a database in rdbms so you can create hbase namespace to the schema of rdbms
database so you could create a namespace by saying create namespace and giving it a name and then you could also list the
tables within a namespace you could create tables within a specific namespace now this is usually done in
production environment where a cluster might be multi-tenant cluster and there might be different users of the same
nosql database in that case admin would create specific namespace and for
specific name space you would have different directories on hdfs and users
of a particular business unit or a team can work on their edgebase objects
within a specific name space this is a question which is again very important
to understand about the rights or reads so how does right ahead log wall hell
when a region server crashes now as I said when a write happens it will happen
into mem store and wall that is your edit log or write ahead log so whenever
a write happens it will happen in two places mem store which is the right
cache and wall which is a edit log only when the data is written in both these places and based on the limitation of
mem store the data will be flushed to create an edgebase format file called H5
these files are then compacted and created into one bigger file which will
then be stored on sdfs and sdfs data as we know is stored in the form of blocks
on the underlying data nodes so if a region server hosting a mem store crashes now where is region server
running that would be co-located with data node so if a data node crashes or if a region server which was hosting the
mem store Write cash crashes data in memory the data that in memory which was
not persisted is lost now how does edgebase recover from this as I said
your data is written into wall and mem store at the same time hbase recovers
against that by writing to wall before the write completes so whenever a write
happens it happens in mem store and wall at the same time HBS cluster keeps a
wall to record changes as they happen and that's why we call it as also an edit log if edgebase goes down or the
node that goes down the data that was not flushed from mem store to Edge file can be recovered by replaying the right
ahead log and that's the benefit of your edit log or write ahead log now if we
would have to write hbase command to list the contents and update the column families of a table I could just do a
scan and that would give me complete data of a table if you are very specific
and if you would want to look at a particular row then you could do a get table name and then give the rocky
however you could do a scan to get the complete data of a particular table you could also do a describe to see what are
the different column families and if you would want to alter the table and add a new column family it is very simple you
can just say alter give the HB stable name and then give you a new column family name which will then be added to
the table what are catalog tables in hbase so as I mentioned your zookeeper
knows the location of this internal catalog table or what we call as The
Meta table now catalog tables in edgebase have two tables one is hbase meta table and one is hyphen root the
catalog table Edge base meta exists as an hbase table and is filtered out of
hbase shells list command so if I give a list command on edgebase it would list
all the tables which h space contains but not the meta table it's an internal table this meta table keeps a list of
all regions in the system and location of hbase method stored in Zookeeper so
if somebody wants to find out or look for particular rows they need to know
the regions which contain that data and those regions are located on region
server to get all this information one has to look into this meta table however we will not be looking into meta table
directly we would just be giving a write or a read operation internally your
hbase master queries the Zookeeper zookeeper has the information of where
the meta table exists and that meta table which is existing on region server contains information of row keys and the
region servers where those rows can be found your root table keeps a track of location of The Meta table what is hot
spotting in hbase and how to avoid hot spotting now this is a common problem and always admin guys or guy guys who
are managing the infrastructure would think about it so one of the main idea is that edgebase would be leveraging the
benefit of sdfs you are all read and write request should be uniformly distributed across all of the regions
and region servers otherwise what's the benefit of having a distributed cluster so you would have your data stored
across region servers in the form of regions which are horizontal partitions of the table and whenever read and write
requests happen they should be uniformly distributed across all the regions in
the region servers now hotspotting occurs when a given region serviced by a
region server receives most or all of read and write request which is basically a unbalanced way of read write
operations now hotspot can be avoided by designing the row key in such a way that
data being written should go to multiple regions across the cluster so you could do techniques such as salting hashing
reversing the key and many other techniques which are employed by users
of hbase we need to just make sure that when the regions are distributed across
region servers they should be spread across region servers so that your read
and write request can be satisfied from different region servers in parallel
rather than all read and write request hitting the same region server overloading the region server which may
also lead to the crashing of a particular page and server so these were some of the important questions of hbase
and then there are many more please refer to the link which I specified in during my discussion and that gives you
a detailed explanation of how edgebase works you can also look into hbase definitive guide by O'Reilly or hbase in
action and these are really good books to understand about hbase internals and how it works now that we have learned on
hive which is a data warehousing package we have learned on Pig which is a
scripting or a scripting language which allows you to do data analysis and we
have learned some questions on a nosql database just note it that there are
more than 225 nosql databases existing in market and if you would want to learn
and know about more nosql databases you can just go to Google and type no SQL
databases org and that will take you to the link which is for nosql databases
and this shows there are more than 225 nosql databases existing in market and
these are for different use cases used by different users and for with
different features so have a look at this link now when you talk about data ingestion so let's look at data
ingestion and this is one good link which I would suggest to have a look at which lists down around 18 different
injection tools so when you talk about different data ingestion tools some are for structured data some are for
streaming data some are for data governance some are for data ingestion and transformation and so on so have a
look at this link which also gives you a comparison of different data ingestion tools so here let's learn about some
questions on scoop which is one of the data ingestion tools mainly used for
structured data or you could say data which is coming in from rdbms or data
which is already structured and you would want to ingest that you would want to store that on sdfs which could then
be used for Hive which could be used for any kind of processing using mapreduce
or Hive or pig or spark or any other processing Frameworks or you would want
to load that data into say hi for regime Stables scoop is mainly for structured
data it is extensively used when organizations are migrating from rdbms
to a big data platform and they would be interested in ingesting the data that is
doing Import and Export of data from rdbms to sdfs or vice versa so let's
learn about some important questions on scope which you may be asked by an interviewer when you apply for a big
data related position how is scoop different from Flume so this is a very common question which is asked scoop
which is mainly for structured data so scoop works with rdbms it also works
with nosql databases to Import and Export data so you can import data into
sdfs you can import data into Data browsing packets such as Hive directly
or also in hbase and you could also export data from Hadoop ecosystem to
your rdbms however when it comes to flow Flume is more of a data ingestion tool
which works with streaming data or unstructured data so data which is constantly getting generated for example
log files or metrics from server or some chat messenger and so on so if you are
interested in working on capturing and storing the streaming data in a storage
layer such as sdfs or edgebase you could be using Flume there could be other tools also like Kafka or storm or chukwa
or some nifi and so on scoop however is mainly for structured data you're
loading data in scope is not event driven so it is not based on event it
basically works on data which is already stored in rdbms in terms of Flume it is
completely event driven that is as the messages or as the events happen as the
data is getting generated you can have that data ingested using flow Zoom scoop
works with structured data sources and you have various scope connectors which are used to fetch data from external
data structures or rdbms so for every rdbms such as MySQL Oracle tb2 Microsoft
SQL Server you have different connectors which are available Flume it works on
fetching streaming data such as tweets or log files or server metrics from your
different sources where the data is getting generated and if you are interested in not only ingesting that
data which is getting generated in a streaming fashion but if you would be interested in processing the data as it
arrives scoop can import data from rdbms onto sdfs and also export it back to
rdbms Flume is then used for streaming data now you could have one to one one
to many or many to one kind of relation so in terms of Flume you have components
such as your Source sync and channel that's the main difference between your scoop and flow what are the different
file formats to import data using scope well there are lots and lots of formats
in which you can import data into scoop when you talk about scoop you can have delimited text file format now that's
the default import format it can be specified explicitly using as text file
argument so when I want to import data from an rdbms I could get that data in
hdfs using different compression schemes or in different formats using the
specific arguments so I could specify an argument which will write string based representation of each record to Output
files with delimiters between individual columns and rows so that is the default
format which is used to import data in using scoop so to learn more about your
scoop and different arguments which are available you can click on scoop dot
apache.org you can look into the documentation and I would suggest choosing one of the versions and looking
into the user guide and here you can search for arguments and look for
specific control arguments which show how you can import data using scoop so
here we have common arguments and then you also have import control arguments
wherein we have different options like getting data as Avro as sequence file as
text file or parquet file these are different formats you can also get data in default compression scheme that is
gzip or you can specify compression codec and then you can specify What
compression mechanism you would want to use when you are importing your data using school when it comes to default
format for Flume we could say sequence file which is a binary format that stores individual records in record
specific data types so these data types are manifested as Java classes and scoop
will automatically generate these data types for you so scoop does that when we talk about your sequence file format in
terms of your scoop you could be extracting storage of all data in binary
representation so as I mentioned you can import data in different formats such as
Avro parquet sequence file that is binary format or machine readable format
and then you could also have data in different compression schemes let me just show you some quick examples here
so if I look in uh the content and here
I could search for a scoop based file where I have listed down some examples
so if I would want to use different compression schemes here are some
examples have a look at these so I'm doing a scoop import I'm also giving an argument so that scope which also
triggers a map reduced job or I would say map only job so when you run a scoop
import it triggers a map only job no reduce happens here and you could
specify this parameter or this argument on the command line mapreduce dot framework.name so that you could run
your map only job in in local mode to save time or that would interact with yarn and run a full-fledged map only job
we can give the connection and then connect to whatever rdbms we are connecting mentioning the database name
give your username and password give the table name give it Target directory or
it would create a directory same as the table name which would work only once and then I could say minus Z to get data
in a compressed format that is gzip or I could be specifying compression codec and then I could specify What
compression codec I would want to use say Snappy B is lz4 default I could also
run a query by giving a scope import and when I am specifying a query I if you
notice I have not given any table name because that would be included in the query I can get my data as a sequence
file format which is a binary format which will create a huge file so we could also have compression enabled and
then I could say the output of my map job should use a compression at record level for my data coming in sequence
file so sequence file or a binary format supports compression at record level or
at Block Level I could get my data in a Avro file where data has embedded schema
within the file or a parquet file also so these are different ways in which you can set up different compression schemes
or you can even get data in different formats and you could be doing a simple
scope import for these looking further what is the importance of eval tool in
scope so there is something called as eval tool so scoop eval tool allows users to execute user defined queries
against respective database servers and preview the result in the console so
either I could be running a straight away query to import the data into mystfs or I could just use scoop eval
connect to my external rdbms specify my username and password and then I could
be giving in a query to see what would be the result of the query which we
intend to import now let's learn about how scoop imports and exports data
between rdbms and sdfs with its architecture so rdbms as we know has
your database structures your tables which all of them are logical and
internally there is always metadata which is stored your scope import connects to an external rdbms and for
this connection it uses an internal connector jar file which has a driver class so that's something which needs to
be set up by admin but they need to make sure that whichever rdbms you intend to connect to they need to have the jdbc
connector for that particular rdbms stored within the scoop lib folder so
scoop import gets the metadata and then for your scoop command it converts that
into a map only job which might have one or multiple map tasks now that depends
on your scoop command you could be specifying that you would want to do a
import only in one task or in multiple tasks these multiple map tasks will then
run on a section of data from rdbms and then store it in sdfs so at high level
we could say scoop will introspect database to get gather the metadata it divides the input data set into splits
and this division of data into splits mainly happens on primary key column of
the table now if somebody might ask what if my table in rdbms does not have a
primary key column then when you are doing a scope import either you will have to import it using one mapper task
by specifying hyphen hyphen m equals one or you would have to say split by
parameter to specify a numeric column from rdbms and that's how you can import
the data let me just show you a quick example on this so I could just look in
again into the scoop command file and here we could be looking at an example
so if you see this one here we are specifying minus minus m equals 1 which
basically means I would want to import the data using one map task now in this
case whether the table has a primary key column or does not have a primary key column will not matter but if I say a
minus minus ms6 where I am specifying multiple map tasks to be imported then
this will look for a primary key column in the table which you are importing now
if the table does not have a primary key column then I could be specifying a
split by and then specify the column so that the data could be split into multiple chunks and multiple map tasks
would take it now if the second scenario is your table does not have a primary
key column and it does not have a numeric column on which you could do a split by in that case and if you would
want to use multiple mappers you could still say split by on a textual column
but you will have to add this property so that it allows splitting the data which is
non-numeric all of these options are given in the scoop apache.org link going
further how scoop imports and exports data between rdbms and sdfs with its
architecture so as I said it submits the map only job to the cluster and then it
basically does a import or export so if we are exporting the data from sdfs in
that case again there would be a map only job it would look at multiple splits of the data which is existing
which your map only job would process through one or one table map task and then export it to rdbms suppose you have
a database test DB in MySQL we if somebody asked you to write a command to connect this database and import tables
to scoop so here is a quick example as I showed you in the command file so you could say scoop import this is what we
would want to do you connect using jdbc now this will only work if the jdbc
connector already exists within your scope lib directory admin has to set up
that so you can connect to your rdbms you can point to the database so here
our database name is test underscore DB I could give username and then either I
could give password on the command line or just say capital P so that I would be prompted for the password and then I
could give the table name which I would want to import I could also be specifying minus minus M and specify how
many map tasks do I want to use for this import as I showed in previous screen
how to export a table back to rdbms now for this we need the data in a directory
on hdfs so for example there is a departments table in retail database
which is already imported into scoop and you need to export this table back to rdbms so this is the content of the
table now create a new Department table in rdbms so I could create a table
specifying the column names whether that supports null or no if that has a
primary key column which is always recommended and then I can do a scope export I can connect to the rdbms
specifying my username and password specify the table into which you want to
export the data and then you give export directory pointing to a directory on
hdfs which contains the data this is how you can export data into table seeing
example on this so I could again look into my file and here I have an example
of import this is where you are importing data directly into Hive and
you have scoop import where you are importing data directly into hbase table
and you can then query your hbase table to look at the data you could also do a
export by running your map only job in a local mode connecting to the rdbms
specifying your username specifying the table where you would want to export and the directory on scfs where you have
kept the relevant data this is a simple example of export looking further what
is the role of jdbc driver in scoop setup so as I said if you would want to
use scoop to connect to an external rdbms we need the jdbc odbc connector
jar file now one or admin could download the jdbc connector jar file and then
place the jar file within the scoop lib directory wherever scoop is installed
and this jdbc connector jar file contains a driver now jdbc driver is a
standard Java API which is used for accessing different databases in rdpms
so this connector jar file is very much required and this connector jar file has
a driver class and this driver class enables the connection between your rdbms and your
Hadoop structure each database vendor is responsible for writing their own implementation that will allow
communication with the corresponding database and we need to download the drivers which allow our scope to connect
to external rdbms so your jdbc driver alone is not enough to connect to scoop
we also need connectors to interact with different database so a connector is a
plugable piece that is used to fetch metadata and allow scope to overcome the
differences in SQL dialects so this is how connection can be established so
normally your admins would when they are setting up scoop and Hadoop they would
download say MySQL jdbc connector and
this is how they would go to the MySQL connectors if you are connecting to
mySQL similar early for your other rdbms you could be say going in here you could
be looking for a previous version depending you could be going for platform independent and then you could
be downloading the connected jar file now if you enter this jar file you would
see a MySQL connector jar and if we look in com dot
mysql.jdbc.com.mysql.dbc.driver so this is the package which is within the connector jar file and this has the
driver class which allows the connection of your scoop with your rdbms so these
things will have to be done by your admin so that you can have your scoop connecting to an external rdbms now how
do you update the columns that are already exported so if I do a export and
I put my data in rdbms can I really update the columns that are already exported yes I can using a update key
parameter so scoop export command Remains the Same the only thing I will
have to specify now is the table name your Fields terminated by if you have a
specific delimiter and then you can say update key and then the column name so
this allows us to update the columns that are already exported in rdbms what
is code gen so scoop commands translate into your mapreduce job or map only job
so code gen is basically a tool in scope that generates data access objects Dao
Java classes that encapsulate and interpret imported records so if I do a
scoop code gen connect to an rdbms using my username and give a table this will
generate a Java code for employee table in in the test database so this code gen
can be useful for us to understand what data we have in this particular table
finally can scope be used to convert data in different formats I think I already answered that right if no which
tools can be used for this purpose so scoop can be used to convert data in
different formats and that depends on the different arguments which you use when you do a import such as avrofile
parquet file binary format with record or Block Level compression so if you are
interested in knowing more on different data formats then I think I can suggest a link for that and we can say Hadoop
for match I think it is Tech Mackie afro parquet
Let's see we can find out the Link tech Maggie yeah this is a very good link
which specifies or talks about different data formats which you should know such as your text file format different
compression schemes how is data organization what are the common formats
what do you have in text file structured binary sequence files with compression
without compression what is record level what is Block Level what is a Avro data
file what is a sequel what is a parquet data file or a columnar format another
format like orc RC and so on so please have a look at this and with that we
have come to the end of this video on Big Data engineer full course I hope it was informative and interesting if you
have any questions related to the topics that we covered in this video please ask away in the comment section below our
team will help you solve your queries thanks for watching stay safe and keep learning
hi there if you like this video subscribe to the simply learning YouTube channel and click here to watch similar
videos turn it up and get certified click here
