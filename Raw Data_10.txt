hello everyone welcome to this data scientist full course video by simplyloan
data scientists are professionals who utilize their skills to analyze visualize and find trends in the data
they uncover solutions to business challenges by creating models and generating insights
in this video we will cover all the important data science concepts skills and careers that will help you
become a successful data scientist but before we get started please make sure to subscribe to the simply loan
channel to learn new technologies and up skill yourself we will start by understanding the
basics of data science with a short animated video and look at data science life cycle and its applications
we will then see the different python libraries for data science such as numpy and pandas next we focus on learning the crucial
algorithms used in data science for solving specific problems next we'll look at linear
regression logistic regression decision trees support vector machines as well as some reinforcement learning
algorithms after that we'll see the data science job roles and responsibilities
the skills that you need to possess and understand how to build a data science engineer resume
finally you'll get an idea about the top data science interview questions that are often asked in any job interview this includes
programming sql statistics machine learning as well as some scenario based questions
so let's get started with understanding data science with a short animated introduction are you one of the many who dreams of
becoming a data scientist keep watching this video if you're passionate about data science because we
will tell you how does it really work under the hood emma is a data scientist let's see how a day in her life goes
while she's working on a data science project well it is very important to understand the business problem first
in our meeting with the clients emma asks relevant questions understands and defines objectives for
the problem that needs to be tackled she is a curious soul who asks a lot of
eyes one of the many traits of a good data scientist now she gears up for data acquisition to
gather and scrape data from multiple sources like web servers logs databases apis and online
repositories oh it seems like finding the right data takes both time and effort after the data is
gathered comes data preparation this step involves data cleaning and data transformation
data cleaning is the most time consuming process as it involves handling many complex scenarios here emma deals
with inconsistent data types misspelled attributes missing values duplicate values
and whatnot then in data transformation she modifies the data based on defined mapping rules in a
project etl tools like talent and informatica are used to perform complex transformations that helps the team to
understand the data structure better then understanding what you actually can do with your data is very crucial
for that emma does exploratory data analysis with the help of eda she defines and
refines the selection of feature variables that will be used in the model development but what if emma skips this step she might
end up choosing the wrong variables which will produce an inaccurate model thus exploratory data analysis becomes
the most important step now she proceeds to the core activity of a data science project
which is data modeling she repetitively applies diverse machine learning techniques like
canon decision tree knife base to the data to identify the model that best fits the
business requirement she trains the models on the training data set and test them to select the best performing model emma
prefers python for modeling the data however it can also be done using r and sas
well the trickiest part is not yet over visualization and communication emma meets the clients
again to communicate the business findings in a simple and effective manner to convince the stakeholders
she uses tools like tableau power bi and clickview that can help her in creating
powerful reports and dashboards and then finally she deploys and maintains the model she
tests the selected model in a pre-production environment before deploying it in the production
environment which is the best practice right after successfully deploying it she uses reports
and dashboards to get real-time analytics further she also monitors and maintains
the project's performance well that's how emma completes the data science project we have seen the daily routine of a data
scientist is a whole lot of fun has a lot of interesting aspects and comes with its own share of challenges
now let's see how data science is changing the world data science techniques along with
genomic data provides a deeper understanding of genetic issues in reaction to particular drugs
and diseases logistic companies like dhl fedex have discovered the best rules to
ship the best suited time to deliver the best mode of transport to choose thus leading to cost efficiency
with data science it is possible to not only predict employee attrition but to also
understand the key variables that influence employee turnover also the airline companies can now easily predict
flight delay and notify the passengers beforehand to enhance their travel experience
well if you're wondering there are various roles offered to a data scientist like data analyst machine learning
engineer deep learning engineer data engineer and of course data scientist
the median base salaries of a data scientist can range from 95 000 to 165 000
so that was about the data science are you ready to be a data scientist if yes then start today the world of
data needs you so what is data science data science is the field of study that deals
with modern scientific techniques statistical methods and algorithms to derive insights from
vast volumes of data companies using data science are very numerous with companies
generating data every minute there is a need to make sense of this data
mu sigma fractal fidelity intel jim pact cubela
amazon trendins connect the dots just about every business out there
nowadays uses some form of data science even if it's indirectly to help govern
their business and help them growth and compete in the business world and we use the data science to find
unseen patterns within the data analyze and draw insights from our data and solve
business problems and make decisions and the data science has a life cycle
we start with our data discovery then we go through data preparation exploratory data analysis data modeling
interpret the results and back to data discovery we talk about data discovery data
discovery involves asking the right questions identifying the business problems
finding the best solution and gathering correct data from relevant sources for analysis
this is in the data science life cycle the most important step
now in general it only involves like a small percentage of the time you spend on this but if you're not asking the right
questions you can't go in the right direction data preparation data preparation deals
with cleaning the data finding the inconsistencies tackling missing values converting it into the
right format and making it ready for analysis when i'm running the data science life cycle this is probably where the most
time is spent data can be so messy
you have to break it down into different parts you have to import the different pieces from different databases
are you dealing with straight numbers off of spreadsheets and cells or are you dealing with text documents
and natural tokenite language tokenization exploratory data analysis analyze and
visualize the data using different charts and graphs to find significant patterns and hidden trends that helps to
understand the data better the building of the charts is very creative probably one of the more enjoyable parts
is be able to come in there and describe the data and show that to the shareholders then
we dive into data modeling use classification regression and clustering algorithms to build machine
learning models that can help predict and forecast future trends this is where we start getting into
predictions what's next where's it going to go and then interpret the results
validate the model results draw the final conclusion and check if the performance of the model is in line
with the requirements the final step is the one that you set up in front of the board members and give them suggestions
as to what's next where we're going to spend our marketing money where are we going to start preparing our firefighters for the next fire
breaks next year what is the weather patterns coming in this is where you interpret the results
and say how accurate they are are they valid is it something we're going to keep are we going to throw it out and have to start over
we look at data science components is the mathematics statistics domain expertise
data engineering data visualization and machine learning when we sum this up
in data science really this point in all of this you want to focus on
first is domain expertise now when you're taking your classes obviously you want to go through
the mathematics the statistics the data engineering the data visualization the machine learning and learning all those
tools but it really in the job field the domain expertise comes in
because if you're really good with business and you go into business with these tools then you're able to ask the right
questions and deal with it when we talk about the tools in data science and where they sit
there is a data analysis data analysis focuses more on statistics and mathematics where
the data science size where the data science side focuses more on the machine learning data
visualization and then there's database management which really focuses on the data
engineering part of it and how to manage that data in the data flow and you see all these components come
together and then they form the data science and we talk about tools in data science
we have sas we have matlab we have sql you have your apache spark your r tab
blue python right now in the programming languages python is probably the most widely used
with r coming in a close second the reason for that is these are both open source and anybody can use them
we look at sas and tabloo those are two packages that are paid for which have a lot of automation behind
them so a lot of the larger companies will pay for those packages because they help work out a lot of the
the back end stuff before you even need to use them and of course we have apache spark you
almost have to know apache spark in the growing data lakes today that's your big data how do
you distribute this if you're going to process it over a large amount of data we look at data science applications
there's internet search very common you enter your google thing and it uses the data science
algorithms to figure out what to return voice assistance analyzing the voice coming in and
connecting it with whatever patterns they need to activate so you can ask it to turn your lights on in the living room
healthcare biggest field growing right now how do we analyze health data from a smart watch
and figure out how to know when a disease is being spread across the world health care how
do we identify cancer versus non-cancer so we know who to prioritize and get him in for surgery
where other people might not need that immediate care and even identifying
problems somebody might have with their health long before they even happen what a neat thing to do instead of waiting until you have a heart attack
to know months ahead of time that you're having heart conditions logistics how do we get from point a to
point b if you ever look at some of the major distributors they pre-ship things based on what they guess
people are going to need this way you get your stuff right in the mailbox the day after you order it
e-commerce huge in there because you have to know what kind of marketing we're going to put out who are we selling to what's our
target audience robotics one of the funner ones when you're coming up into the future is how
do we automate the different tasks we have from self-driving cars
to manufacturing lines to maybe even a self-robotic chef in the
kitchen so we talk about solving problems with data science we have to ask possible questions and
the desired algorithms and these really are uh kind of an overview of some of the terminology
you'll want to know in data science so we talk about that how much or how many we're talking about regression
regression basically means a number if you've ever seen stock charts you see
the guess at what the next sales price is or buy price is that's all on your regression
side is it a or b classification
is that a dog or a cat that my photo took a picture of how is this organized clustering this is
where we take things that look alike and put them together so that we can then make predictions on it
how difference is this just like you have clustering you have the opposite anomaly detection how do we find things
that don't fit in where are the outliers what are things we might need to look at that aren't in our model
and what to do next reinforced learning this is probably the newest market is
how do we set something up whereas the data comes in it learns from the new data as to what the next action is going to take
most of these are combined so when we talk about regression classification clustering
anomaly detection reinforced learning a lot of times when you put your model together you might have
multiple parts of this we might look at the clustering of data and then feed it into a classification
is this a bad loan to make it the bank or is this a good loan to give
and from there we might start looking at when we start clustering these things as good and bad and what kind of
setup is we then might run it through a regression model to say hey what is the amount dollar amount this person
should be allowed to take a loan out and you can see that all of these start coming in
and then a lot of the models reinforce learning we talk about like bank loans and things
like that once a year they have to rerun those algorithms and so that's like a human run
reinforced learning now we're starting to automate that so that as the data comes in real time
the models start updating themselves they start figuring out a way to solve new problems
as they occur and of course in robotics reinforced learning is one of the biggest growing fields
logarithms used in data science linear regression logistic regression when you're looking
at both of these uh think numbers we're talking regression models and so linear regression figures out the
best line through the data maybe it's looking for a curved line logistic regression starts looking at
data on a continuum so that it goes to an exponential you have a
spot where it might be one or the other and then you know what it is as it goes to one side or the other decision tree wonderful tool
if you used to call it a hack now it's becoming mainstream to look at the back end what made the
decision so it's really easy to trace back to how you arrived at the decision decision trees are kind of nice that way
nearest neighbors k nearest neighbors is your clustering k means clustering hierarchical clustering so hierarchical
starts dividing them and builds a hierarchy where the k-means and the k-nearest neighbor
look for things that kind of connect each other how close are they in data format db scan
another form of clustering your data principle component analysis so we start breaking up and looking at
the different components in our data coming in these are all algorithms used in data science and there are more
these are kind of the mainline ones and each one of these has many settings which you can play with
to build a better model to predict your data and where you want it to go so let's now talk about the difference
between business intelligence and data science now business intelligence was one of the
initial phases where people started making or wanted to make some sense out of data so
for some of you who may not be aware there were multiple phases of this it revolution so initially there was all
automations you had automation of your selling process manufacturing process you had your erp systems
your crm systems and and so on uh which are basically enterprise resource planning and
customer relationship management erp and crm and all these enterprise applications
were generating a lot of data so then people started saying that okay we need to understand
or get some information out of this data so that's how business intelligence started so if we take from
a data source perspective let's compare these from with each of these criteria the criteria
are what is the data source what is the method what are the skills and what is the focus now if we compare business intelligence
with data science this is how it looks as far as the data source is concerned business intelligence was primarily
using structured data so you had all your enterprise applications like erp
crm and so on and they were working out of pretty much rdbms relational database
management systems like oracle or mssql mysql and so on so all this data was structured in neat
form form of tables rows and columns and then they were all brought into
a centralized place because remember these were still different applications so they were
working off different databases in silos so if you wanted to get a combined view you needed to create what
is known as a data warehouse and bring all this data together and then look at it in a uniform way so this is
what business intelligence was doing pretty much it was structured data and it had reports and dashboards that
was pretty much of what was there in business intelligence now with the data science
in addition to structured data we also use a lot of unstructured data example
web blogs or comments if we are talking about customer feedback there is a structured part there is an unstructured part where
people write free text data science includes that as well and brings everything and then performs analysis so data
source wise that is the different methods in business intelligence pretty much it is
analytical in the sense that okay you have some data we are just trying to present the truth
and mostly what has happened historical data that's it in case of data science
we go beyond that we go deeper in terms of finding why a certain behavior has
occurred and also go beyond just providing a report there is a
deeper statistical analysis that is done that is what is the scientific part and deeper insights are gathered not
just reporting so that's from a method perspective from a skills perspective business intelligence needs a little bit
of statistics but more of visualization because they primarily consisted of dashboards or
they primarily consist of dashboards and reports whereas in data science the visualization of course is there
but there is a lot more of statistics involved because we are looking at things like correlation we are
looking for example if we perform machine learning we try to do regression try to predict what will be
the sales may be in the future and so on and so forth so it is much more
involved in case of data science compared to business intelligence the skills are
many more compared to business intelligence and last but not least what is the focus
focus of business intelligence is pretty much historical data so the sales have happened based on the
sales till today you try to come up with a report what was my sales
maybe for this whole year or maybe for the last five years and so on and so forth in data science you take historical data
but you also combine that with maybe some other required information and you also try to
predict the future so we try to extrapolate maybe the sales and say okay sales as of now as of today
this is sales is 5 million and if we based on the the historical
information we see that sales increase on a maybe i don't know monthly basis 10 percent
that is what history says so our sales for next month will be this much right so that is the focus of
data science it goes beyond just reporting so what are the prerequisites for data science there are three essential
traits required for to be a data scientist one is curiosity you need to be able to ask
questions the first step in data science is asking questions what is the problem we are trying to solve if you ask the right
question only then you'll get the right answer very often this is a very crucial step
where a lot of data science projects fail because you're you may be asking the wrong question and then obviously when you get the
answer that's not the answer you're looking for so it is very important that you ask the right question needless to
say then the second part or the second trait is common sense so you need to be
creative you need to come up with ways to use the data that you have and try to solve the business problem on
hand in many cases you may not have all the data that you need in many cases the data may be incomplete
so that is where you need to come up with ways what are the best ways to fill these gaps wherever this is missing
and that's where common sense comes into play last but not least after doing all this analysis
if you are unable to communicate the results in the right way the whole exercise will fail so
communication is a key trait for a data scientist maybe technically you may be a genius but then
if you are unable to communicate those results in a proper way once again that will not
help so these are the three main traits curiosity common sense and communication skills in a way you
can say these are the three c's okay so what are the other prerequisites
first one so machine learning machine learning is the backbone of data science
data science involves quite a bit of machine learning in addition to the basic statistics that we do so a data scientist needs to
have a good hang or need to be very good at data science the second part is modeling
so modeling is also a part of machine learning in a way but you need to be good at identifying
what are the algorithms that are more suitable to solve a given problem what models can we use and how do we
train these models and so on and so forth so that is the second component then statistics statistics is
like the core foundation of data science so you need to understand statistics and you need to have a good
hang of statistics in order to be a good data scientist and this will also help in getting good
results programming is to some extent required at least some program or the other would
be required as a part of executing the data science project the most common programming languages
are python and r python specially is becoming a very popular programming language in data
science because of its ease of learning because of the multiple libraries that
it supports for performing data science and machine learning and so on so
python is by far one of the most popular languages in data science if any one of
you is wanting to learn a new language that should be python and then of course you need to
understand databases how databases work and how to handle databases how to get
data out of databases and so on so these are some of the key components of data science now coming to
the tools and skills that are used in data science these are some of the skills
from a language perspective it is python or r and from a skills perspective
in addition to some of the programming languages it would help if you have a good knowledge or good understanding
of statistics and what are the tools that are used in data analysis sas is one of the most popular tools
it's been there for very long time and that's the reason it is very popular and
however this is compared to most of the other tools it is a proprietary software whereas python and r are mostly
open source the other tools are like jupiter jupiter notebooks you have r studio
these are more development environments and development tools so jupyter notebooks is a interactive development
environment similarly rstudio is for performing or writing our code
and performing analytics and performing data analysis and machine learning activities you can perform
in rstudio it has a very nice ui and initially r was not so popular
primarily because it did not have user interface and rstudio is a relatively new edition
and after the advent of rstudio r became extremely popular and there are other
tools like matlab and of course some people do with excel as well as far as data warehousing
is concerned some of the skills that are required are etl so in order to extract data
and transform load etl stands for extract transform load so you have data in the databases like
your erp system or a crm system you need to extract that and then do some transformations and then load it
into your warehouse so that all the data from various sources looks uniform then you need some sql
skills which is basically querying the data writing sql queries hadoop is another
important skill especially if you are handling large amounts of data and also one of the specialities of
hadoop is it can be used for handling unstructured data as well so it can be used for large
amounts of structured and unstructured data then spark is a excellent computing
engine for performing data analysis or machine learning in a distributed mode
so if you have large amount of data the combination of spark and hadoop can be
extremely powerful so you store your data in hadoop hdfs and use spark as your
computation engine it works in a distributed mode similar to hadoop like a cluster so that those are
excellent skills for data warehousing and there are some standard tools that are available like informatica
data stage talent and also aws redshift if you want to do some on the cloud i think aws
redshift is again a good tool data visualization tools for data visualization some of the skills that
would be required are let's say r your r provides some very good visualization capabilities
especially for for developing during development and then you have python libraries
matplotlib and so on which provides very powerful visualization capabilities
and that is from skills perspective whereas tools that can be used are tableau is a
very very popular visualization tool again that's a proprietary tool so it's a little
expensive maybe but excellent capabilities from a visualization perspective then there are tools like cognos which
is an ibm product which provides very good visualization capabilities as well and then coming to the machine learning
part of it the skills required there are python which is more for programming part and then you
will need some mathematical skills like algebra linear algebra especially and then
statistics and maybe a little bit of calculus and so on and the tools that are used for machine
learning are spark mlib and apache mahou and on cloud if you want to do
something you can use microsoft azure ml studio as well so these are by no means an exhaustive
list there are actually many many tools and probably a few more skills also maybe there but
this is this gives a quick overview like a summarizing of summarization of the tools and skills now moving on to
the life of a data scientist what does a data scientist do during the course of his work so
let's see so typically a data scientist is given a problem a business problem that he needs to
solve and in order to do that if you remember from the previous slide he basically asks the question as to
what is the problem that he needs to solve so that is the first thing he has got the problem then the next thing is to gather the data that is
required to solve this problem so he goes about looking for data from anywhere it could be the enterprise very
often the data is not provided in the nice format that he would like to have it or
we would like to have it so first step is to get whatever data that is possible
what is known as raw data in whatever format so it could be enterprise data it could be it is a probably a requirement to go and
get some public data in some cases so all that raw data is collected and then
that is processed and analyzed and in prepared into a format in which it can be used
and then it is fed into the analytic system be it a machine learning algorithm or a statistical model and we get the
output and then he puts these output in a proper format for presenting it to the
stakeholders and communicating those insights or the results to the stakeholders so this is a
very high level view of like a day in the life of a data scientist so
gathering data raw data performing some quick analysis on that and maybe processing or manipulating
this data to bring it into a certain good format so that it can be used for the analysis
feeding this into that analysis system that has been designed be it mathematical models machine
learning models and then get the results the insights and then present it in a nice way so
that the stakeholders can understand how about machine learning algorithms so let's see what are the
various machine learning algorithms that would be required for a data scientist so these are a few of the algorithms
again there's not an exhaustive list we have regression is one of the supervised
learning models or techniques so in case of regression you try to let's say come up with a
continuous number so the difference between regression and let's say a classification is that
in case of classification those are discrete values whereas here we are talking about regression where
you let's say you are trying to predict the temperature which is a continuous value or the share price which is a continuous value
so that is regression so you need to know what is regression how to perform regression
and we need to understand clustering so clustering is one of the unsupervised learning techniques in this
case there is no label data that is available and you get some data and then you want to put
this into some shape so that you can analyze it and you try to make sense out of it let's say you have one example
is you have a list of cricketers and they have not been marked as bowlers and
batsmen or all renders or whatever right so you just have their names and maybe how many runs this
code how many wickets they have taken and so on but there is no readily available
information saying that okay this person is a batsman this person is a bowler and so on so i'm talking about cricket
hopefully most of you are familiar with the game of cricket so how do we find out so then we put this
into a clustering mechanism and then the system will say that okay these are the people
who are all who have all scored good amount of runs so they belong to one cluster these are
all the people who have taken good amount of wickets so they belong to one cluster and maybe here are some
people who have taken good amount of wickets and they have made good amount of runs so they may be
belonging to one group and then we take a look at it and then we label them as okay people who have all together
and those who have you know scored many runs they are we label them as batsmen people have
taken a lot of wickets we label them as bowlers and people who have taken
good amount of wickets and also made some good runs we label them as all rounders but the
system will just say okay this is cluster one cluster two cluster 3. the names we give we human beings have
to give the names now decision tree is used for what is known as classification primarily it can also be used for
regression but by and large it is used for classification and here again it's a
very logical way in which the algorithm goes about classifying
the various inputs one of the biggest advantages of decision tree is that it's very easy to understand and it's
very easy to explain why a certain object has been classified in a certain way
compared to maybe some of the other mechanisms like say support vector machines or logistic
regression and so on so that's the advantage of dictionary but that is also very popular algorithm
then we have support vector machines primarily for classification purpose and then we have naive bayes this is a
statistical probability based classification method so these are a few algorithms
there are a few more that are not listed here but there are some more algorithms as well and by the way there are more detailed
or there are detailed videos about each of these algorithms available you can check
in the playlist so now let's talk about the life cycle of a data science project okay the first
step is the concept study in this step it involves understanding the business problem
asking questions get a good understanding of the business model meet up with all the stakeholders
understand what kind of data is available and all that is a part of the first step so here are
a few examples we want to see what are the various specifications and then what is the end goal what is
the budget is there an example of this kind of a problem that has been maybe solved earlier
so all this is a part of the concept study and another example could be a
very specific one to predict the price of a 1.35 carat diamond and there may be
relevant information inputs that are available and we want to predict the price the next
step in this process is data preparation data gathering and data preparation also
known as data munching or sometimes it is also known as data manipulation so what happens here
is the raw data that is available may not be usable in its current format
for various reasons so that is why in this step a data scientist would explore the data
he will take a look at some sample data maybe pick there are millions of records pick a few
thousand records and see how the data is looking are there any gaps is the structure appropriate to be fed
into the system are there some columns which are probably not adding value may not be required for
the analysis very often these are like names of the customers they will probably not add any
value or much value from an analysis perspective the structure of the data maybe the data is
coming from multiple data sources and the structures may not be matching what are the other problems
there may be gaps in the data so the data all the columns all the cells are not filled if
you're talking about structured data there are several blank records or blank columns
so if you use that data directly you'll get errors or you will get inaccurate results so how do you
either get rid of the data or how do you fill this gaps with something meaningful so all
that is a part of data munching or data manipulation so these are some
additional sub topics within that so data integration is one of them
if there are any conflicts in the data there may be data may be redundant yeah data resident redundancy is another
issue there may be you have let's say data coming from two different systems and both of them have customer table for
example customer information so when you merge them there is a duplication issue so how do we resolve
that so that is one data transformation as i said there will be situations where
data is coming from multiple sources and then when we merge them together they may not be
matching so we need to do some transformations to make sure everything is similar we may have to do
some data reduction if the data size is too big you may have to come up with ways to
reduce it meaningfully without losing information then data cleaning so there will be
either wrong values or you know values or there are missing values so how do you handle all of that
a few examples of very specific stuff so there are missing values how do you
handle missing values or null values here in this particular slide we are seeing three types of
issues one is missing value then you have null value you see the difference between the two right so in
the missing value there is nothing blank null value it says null now the system cannot handle if there are null values
similarly there is improper data so it's supposed to be numeric value but there is a string or a non-numeric value so how do
we clean and prepare the data so that our system can work flawlessly
so there are multiple ways and there is no one common way of doing this it can vary
from project to project it can vary from what exactly is the problem we are trying to solve
it can vary from data scientist to data scientist organization to organization so these are like some
standard practices people come up with and and of course there will be a lot of trial and error
somebody would have tried out something and it worked and will continue to use that mechanism so that's how we need to take care of
data cleaning now what are the various ways of doing you know if values are missing how do you take care of that now if the
data is too large and only a few records have some missing
values then it is okay to just get rid of those entire rows for example so if you have a
million records and out of which 100 records don't have full data so there are some missing values in about
100 cards so it's absolutely fine because it's a small percentage of the data so
you can get rid of the entire records which are missing values but that's not a very common situation
very often you will have multiple or at least a large number of data set
for example out of million records you may have 50 000 records which are like having missing values now that's a
significant amount you cannot get rid of all those records your analysis will be inaccurate so how do you handle such
situations so there are again multiple ways of doing it one is you can probably if a particular values
are missing in a particular column you can probably take the mean value for that particular column
and fill all the missing values with the mean value so that first of all you don't get errors because of missing values and second
you don't get results that are way off because these values are completely different from what is
there so that is one way then a few other could be either taking the median value or
depending on what kind of data we are talking about so something meaningful we will have put in there if we are doing some machine learning
activity then obviously as a part of data preparation you need to split the data into training and test
data set the reason being if you try to test with a data set which the system has already seen as a part of
training then it will tend to give reasonably accurate results because it has already
seen that data and that is not a good measure of the accuracy of the system so typically you take the entire
data set the input data set and split it into two parts and again the ratio can vary
from person to person individual preferences some people like to split it into 50 50 some people like it as 63.33
and 33.3 is basically two third and one third and some people do it as 80 20 80 for training and 24
testing so you split the data perform the training with the 80 percent and then use the remaining 20 for
testing all right so that is one more data preparation activity that needs to be done before you start
analyzing or applying the data or putting the data through the model then the next step is
model planning now this models can be statistical models this could be machine learning model so
you need to decide what kind of models you're going to use again it depends on what is the problem
you're trying to solve if it is a regression problem you need to think of a regression algorithm and come up with
a regression model so it could be linear regression or if you are talking about classification
then you need to pick up an appropriate classification algorithm like logistic regression or decision tree or
svm and then you need to train that particular model so that is the model building or
model planning process and the cleaned up data has to be fed into the model
and apart from cleaning you may also have to in order to determine what kind of model you will
use you have to perform some exploratory data analysis to understand the relationship between
the various variables and see if the data is appropriate and so on right so that is the
additional preparatory step that needs to be done so a little bit of details about exploratory data analysis so what
exactly is exploratory data analysis is basically to as the name suggests you're just exploring you just receive the data and
you're trying to explore and find out what are the data types and what is the is the data clean in in each
of the columns what is the maximum minimum value so for example there are out of the box functionality
available in tools like r so if you just ask for a summary of the table it will tell you
for each column it will give some details as to what is the mean value what is the maximum value and so on and so forth
so this exercise or this exploratory analysis is to get an understanding of your data
and then you can take steps to during this process you find that a lot of missing values you need to take steps to
fix those you will also get an idea about what kind of model to be used and so on and so forth what
are the various techniques used for exploratory data analysis typically these would be visualization
techniques like you use histograms uh you can use box plots you can use scatter plots so
these are very quick ways of identifying the patterns or a few of the trends of the data and so on
and then once your data is ready you've decided on the model what kind of model what kind of
algorithm you're going to use if you're trying to do machine learning you need to
pass your 80 percent the training data or rather you use that training data to train your model
and the training process itself is iterative so the training process you may have to perform multiple times and once the
training is done and you feel it is giving good accuracy then you move on to test so you
take the remaining 20 of the data remember we split the data into training and test
so the test data is now used to check the accuracy or how well our model
is performing and if there are further issues let's say and model is still during testing if the accuracy is not
good then you may want to retrain your model or use a different model so this whole
thing again can be iterative but if the test process is passed or if the model passes the
test then it can go into production and it will be deployed all right so what are the various tools that we use
for model planning r is an excellent tool in a lot of ways whether you're doing regular statistical
analysis or machine learning or any of these activities are in along with our studio provides a very
powerful environment to do data analysis including visualization it has a very good integrated
visualization of plot mechanism which can be used for doing exploratory data analysis and then
later on to do analysis detailed analysis and machine learning and so on and so forth then of course you can
write python programs python offers a rich library for performing data analysis
and machine learning and so on matlab is a very popular tool as well especially during education
so this is a very easy to learn tool so matlab is another uh tool that can be used and then last but not
least sas sas is again very powerful it is a proprietary tool and it has
all the components that are required to perform very good statistical analysis or
perform data science so those are the various tools that would be required for
or that that can be used for model building and so the next step is model building so we
have done the planning part we said okay what is algorithm we are going to use what kind of model we are going to use
now we need to actually train this model or build the model rather so that it can then be deployed so what
are the various uh ways or what are the various types of model building activities so it could be let's say in this
particular example that we have taken you want to find out the price of 1.35 carat diamond so
this is let's say a linear regression problem you have data for various carets
of diamond and you use that information you pass it through a linear regression model or you create
a linear regression model which can then predict your price for
1.35 carat so this is one example of model building and then a little bit
details of how linear regression works so linear regression is basically
coming up with a relation between an independent variable and a dependent
variable so it is pretty much like coming up with equation of a straight line which is the best fit for the given data
so like for example here y is equal to mx plus c so y is the
dependent variable and x is the independent variable we need to determine the values of m and
c for our given data so that is what the training process
of this model does at the end of the training process you have a certain value of m and c and
that is used for predicting the values of any new data that comes all right so the
way it works is we use the training and the test data set to train the model
and then validate whether the model is working fine or not using test data and if it is working fine
then it is taken to the next level which is put in production if not the model has to be retrained if
the accuracy is not good enough then the model is retrained maybe with more data or you come up with a newer
model or algorithm and then repeat that process so it is an iterative process once the training is completed training
and test then this model is deployed and we can use this particular model
to determine what is the price of 1.35 carat diamond remember that was our
problem statement so now that we have the best fit for this given data we have the price of
1.35 carat diamond which is 10 000. so this is one example of how this whole process
works now how do we build the model there are multiple ways you can use python for example and use
libraries like pandas or numpy to build the model and implement it this will be
available as a separate tutorial a separate video in this playlist so stay tuned for that moving
on once we have the results the next step is to communicate these results to the appropriate
stakeholders so which is basically taking this results and preparing
like a presentation or a dashboard and communicating these results to the
concerned people so finishing or getting the results of the analysis is not the last step but you need to as
a data scientist take this results and present it to the team that has given you this problem in the first place
and explain your findings explain the findings of this exercise and recommend maybe
what steps they need to take in order to overcome this problem or solve this problem so
that is the pretty much once that is accepted and the last step is to operationalize so if everything is
fine your data scientists presentations are accepted then they put it into practice
and thereby they will be able to improve or solve the problem that they stated in
step one okay so quick summary of the life cycle you have a concept study which is basically
understanding the problem asking the right questions and trying to see if there is enough data to solve this problem and
then even maybe gather the data then data preparation the raw data needs to be manipulated you
need to do data munching so that you have a data in a certain proper format
to be used by the model or our analytics system and then you need to do the model
planning what kind of a model what algorithm you will use for a given problem and then the model building so the exact
execution of that model it happens in step four and you implement and execute
that model and put the data through the analysis in this step and then you get the results this
results are then communicated packaged and presented and communicated to the stakeholders
and once that is accepted that is operationalized so that is the final step
now in the end let's take a quick look at the demand for data scientists data science is an area of great demand
the demand for data scientists is currently huge and the supply is very low so there
is a huge gap so what are some of the industries with high demand for data scientists
i think gaming is definitely one area where it's a industry which is consumer
facing industry and a lot of people play games and growing industry and it requires a lot of data science
so that is an area where data scientists are in demand then we have healthcare for example data science is used for
diagnosis and several other activities within healthcare predicting for example a
disease so healthcare is definitely finance definitely banks insurance companies all of these
there is a huge demand for data scientists marketing is like a horizontal functionality across all industries
there's a demand for data scientists there then of course in technology area so pretty much all of these areas there is
a lot of demand globally there is a huge demand so this is a very very critical skill that would be required
currently as well as in the future so who is a data science engineer are you a data science engineer
or are you going to be looking for a different field what exactly is a data science engineer well a data science engineer is someone
who has programming experience in python and r expert level knowledge
ability to write proficient codes and they we have python and our i'm going to say
python or r once you become really proficient at one language transferring those skills into another
one is usually fairly easy now r is a little different because it is an analytics platform
so going from python to r if you already know the data analytics and python moving to r is pretty easy uh now r
doesn't have all the other code that you can access with python there's so many things you can do with python that
are not data analytics so it's important to keep in mind that you should probably be pretty well-rounded in python
and really have a solid foundation in r or have a solid foundation in r and be well-rounded in another
programming language where you really get the programming side of it strong sql and big data experience so
you have to have a strong coding skill with hands-on big data experience and of course we're showing sql here
most commonly used whether you're using a microsoft sql server or mysql server
you can also start thinking hadoop and spark in big data access
hadoop file system is not a huge jump if you've already learned your sql and you've already learned your basics and
coding hadoop sits on top of all that and does a wonderful job creating huge clusters of data
but you really need to know your sql because that is so common in most of the large companies now accessing
their data and an ability to visualize models and troubleshoot code of the models this is a kind of an
interesting one because these models are can get very complicated so you gotta be able to break them down
into something that you can put the pieces together and digest each of the pieces so being able to visualize these models
is very important and then being able to drill down and troubleshoot the different models or the pieces in those models
you need to be a versatile problem solver equipped with strong analytical and quantitative skills
a self-starter with a strong sense of personal responsibility and technical orientation this is an interesting one because when
you talk about data science engineer it's such a new field that most companies don't have it well defined
they don't know what they're looking for you might not even know what you're looking for you might have an idea and you're looking for patterns but where do
those patterns lead you so you really need that self-starter side to jump in there and figure out where to go
and be able to communicate that back to the team and there we go we have a strong product intuition data analysis skills
and business presentation skills and this was talking about is you got to bring that back to the team
so if you don't have a strong product in tuition if you don't know what's going on with the company what they need and where you're going
with your analysis it's going to be a dead end and you've got to be able to present that to the shareholders
to your co-workers which leads us to great teammate with excellent interpersonal skills and this is kind of
a strange one because you spend so much time behind your desk so you have to be able to kind of float between you're studying all the data and being
able to explain this to people in simple terms that they can understand no one wants somebody to come up and say
yeah the p score and the f score is this that and the other thing and you might have two people in the room understand you
you have to be able to sit down and explain what that means and why so let's take a look at the data science
skill set we talk about data science engineer skill set we're talking about database knowledge
statistics programming tools data wrangling probably the least favorite and most used machine learning
data visualization and then the touch on big data and so we'll start with database knowledge the most
common databases of course your sql structured query language think rows and columns it's an essential
language for extracting a large amount of data from data sets so knowledge of the sql is mandatory for
data science engineers and you can see there's tools required there's the oracle database
i mentioned mysql server microsoft sql server teradata there are so many different
forms of sql and it'll just keep coming back and coming back so if you don't have a solid
basic understanding of sql go get it very important because it will come up you know it's if you don't know it it's
going to bite you and statistics of course we are you know that's what this is all about is figuring out and predicting things so you need to know
your statistics statistics is a subset of mathematics that deals with collecting analyzing and interpreting data
therefore data scientist needs to know statistics so you need to understand your probabilities and what that means
and what the p score means and the f score and means and mode and median all that information
standard deviation all of those you need to be aware of and then we get into the programming tools and i mentioned this earlier a
little bit but you need to master any one of these specific programming languages programming tools such as r python sas
are essential to perform analytics in data and again you know you can move in there and you
can be an expert in just r you can be an expert in just sas and a little bit you can be an expert in
python because most of these when we're talking about data science and data wrangling
you really kind of get down to the point where you really need to have a full programming language at least the basics
and if you're going to be working in data science python right now is the main one but you
know there's also java c plus plus so you need to know a solid language at least the basics of it so you can
understand how to do basic iterations and things like that i always find that interesting that my
sister who runs a university data analytics center that's the first question she asks for people candidates to come and work for
is how do you iterate through data you'd be amazed at how many people don't understand that very basic concept
in programming so when we look at r r is a free software environment for statistical computing and graphs
supports most machine learning algorithms for data analytics like regression association clustering and etc python
python is an open source general purpose programming language and there's our general purpose i was just talking about you need to know a little bit about
python libraries like numpy and scipy are used in data science scipy yeah numpy and scipy are very
central to python and i'd throw pandas in there also that module those are very central to working with python and data analytics
and then sas sas can mine alter manage and retrieve data from a variety of sources
it can perform statistical analysis on the data so where r is the open source platform
sas is more like the paid for platform and so it has some things more automated because people
are paying for it developing it but it's also if you really start using it as a company you're going to have to pay for
it where rs free store is a free software python is also open source and free to use so we're going to talk a little bit
about data wrangling and i mentioned earlier that is both the probably people's least
favorite aspects of data science and also probably one of the places you spend the most time so we talk about
data wrangling it is a process of transforming raw data into an appropriate format to make it
useful for analytics and it involves cleaning raw data structuring raw data
and enriching raw data and this gets interesting because you'll get stuck on something like
in python you might get stuck on something that is a date time based on an integer 64
numpy set you might not know what all those terms mean because you work in r or something like that but you get the point that it's all one particular
format and then you have to figure out how to switch it so that the computer can see it correctly in whatever format you're using or whatever analytics
platform you're using knowledge of machine learning techniques such as supervised machine learning decision trees linear regression k n etc
is useful for few job roles so this is kind of interesting because sometimes that's the
center of the job role of a data scientist and sometimes you just have to be able to apply
the programming skills to it so you can get the answer and let somebody else fine-tune your machine learning
techniques so it really depends on what it is you're working with and you can see here we have our
nearest neighbor set up where it groups things together that look alike linear regression you're drawing a line
through the data or curve so you can predict what the next value is going to be and then our nice decision tree would
start splitting things up and says you know it's sunny out yes let's go swimming no it's raining we're not going to go
swimming yes it's sunny out we might go swim it's raining we'll stay indoors yes
we'll walk the dog no so you have a decision tree that helps figure out how you get to the end result
so decision trees can be pretty powerful and data visualization data visualization is the study
and creation of visual representation of data data visualization uses algorithms statistical graphs plots
information graphics and other tools to communicate information clearly and effectively this one you really have
to master as a data science this one you really have to master as a data scientist
and there certainly are so many different tools to use to visualize it but when you're communicating a picture
is worth a thousand words when you can put a picture up and and people can look at and go oh i see
what's going on that's worth a lot more than saying yeah yeah this is good people hear that and they just kind of go okay it's good but
why and you might say you know okay these numbers make it look good no people want something they can see
and there certainly is uh tabloo is probably one of the most popular ones out there
caleb view power bi google data studio in python there's um the pi kit
and seaborn there's all kinds of different options for visualization you need to master
probably at least one or two of them so you understand how they work and how they're different and then big data big data is a massive
amount of data which cannot be stored and processed using traditional methods big data has various benefits like access to social
data can enable organizations to tune their business strategies big data can improve customer experience
uh and so we're usually when you say big data we're almost always talking about hadoop and apache
spark it used to be hadoop as your file system so that's how you store all your data going across the nodes and
there certainly are other ways to store it when we talk about hadoop we're usually talking about at least 10 terabytes of data when you're dealing
with a hadoop file structure and then you can use spark where hadoop processes on and off the hard
drive so it's continually reading and writing to the hard drive spark is running in your ram and so when you're doing a very
intensive data process you'll run a spark setup which will spread those processes over multiple
computers and of course now the two uh combine and spark sits on top of hadoop and you get
your big data there's also talent tableau has its own server set up
there's splunk uh cassandra you'll hear the cassandra database pentaho i'm not
even sure how to pronounce that one it's not one i've worked with certainly there's a lot of options for big data the two big players are of
course hadoop and spark and even your sql servers and mysql servers will spread across
five different servers and you can be talking about big data across five servers with those pulling them into say
apache spark to do your high-end processing and then there's non-technical skills probably the most important one in data
science and any of our data analytics is intellectual curiosity
updating knowledge by reading contents and relevant books on trends in data science
this is such there's so much going on in this field and it's exploding now that it's hard to keep track of it all
uh so you really need to have that curiosity in the data science probably even more
so than a lot of other fields but i would say in just today's world you need intellectual curiosity because
things are changing so rapidly in today's world the way it's going with our technology understanding how
the problem solved can impact the business business acumen this is the one where a
lot of people kind of skip over you have to have a little business sense if you don't know how it's going to impact the business
you're going to be in trouble why do they want to even pay you to be there unless they know what's going on and they're getting something in return
so this is your bottom line for your paycheck is what's going on with the business how is this going to impact the business what
are you doing for them and communication skills companies look for someone who can clearly and fluently
translate technical findings to a non-technical team and i've talked about this a bunch and i
i can't even iterate this enough if you are struggling with your communication skills
work on building them they don't have to be you know this isn't rocket science where you have to know
all these complicated terms what you need to do is take the complicated terms you know from data science
and reduce that down to something that anybody can understand so when you come up and you have you
know knn no one knows what k n is you know unless they're in data science nearest neighbor looking for things that
match together you got to explain that to people who do not have the technical skill in the data science
arena and a data science engineer needs to work with everyone in the organization including
customers so it's teamwork and that's always one that throws people for a loop but ultimately you are analyzing your
customers information and so sometimes getting in there and rolling up your sleeves if you're
let's say you're doing data analysis for the sales team and you're backing a sales team you might have to get in there and go
start making some sales and making some phone calls and see what it's like what is that team doing when they call 15 people in cold calls
and then they do a follow-up two weeks later or three weeks later which one works better two or three does it work better to call
and send a fax call and send an email you know when you should you show up in person if it's that kind of business
so working with everybody including the customer is very important with the data and with this kind of setup and
non-technical skills for a data scientist so let's take a look at some of the roles data science plays in the job
market let's drill in there just a little bit here so when you have a data scientist they're going to perform predictive
analysis and identify trend and patterns that can help in better decision making
companies hiring data scientists include apple adobe google microsoft i would say that's even
expanding down to smaller companies you know where you're talking about only 100 employees or something like that
they're starting to look at data scientists because they need them to come in and be a part of that team and the role
is understanding challenges of a system and offer best solutions this should
always be the goal of a data scientist is to be looking at the whole system and
then trying to find those patterns that are going to best enhance the company or the business or the whatever group maybe you're
working with the sales team or the marketing team and you can see languages pretty important to have a these are a number
of different languages and again it depends on your own specialty there's all kinds of different variations but we have r
sas python matlab sql hive pig spark you should have at least a knowledge of
all of these maybe a python programmer never used r download it go through the basics so at least you know what's going on in r
versus python because r is pretty powerful for doing a very quick display and then of course a lot of things you can do in python you have to
be able to do queries on database from time to time create and modify algorithms which can
be used to reduce information from large databases and usually this is one of those things you do at the beginning you set it up
and then you kind of let it go until it breaks and you have to go back and fix it because your query isn't working that certainly
is a very common thing to to have happen especially if you're pulling data off the web and the internet
and then we have some more companies that are hiring data analysis like ibm dhl hp and so we're talking about data
analysis the role is responsible for a variety of tasks such as visualization optimization and processing large amount
of data and so when you talk about a data analyst we're also we're still talking a lot of the same stuff but you'll see we
now thrown in javascript and html in here because that's pretty common with a data analyst versus a data scientist
and you can see that sql is still pretty solid and also cc plus is pretty big so some companies
also use java so i'd put java in the languages also on that for a data analyst and then
you have a data architect ensuring that data engineers have best tools and systems to work with
and so companies hiring a data architect are like visa logitech coca-cola they create blueprints for
data management with the best security measures i cannot highlight
that a data architect really spends a lot of time with security measures that's big how many hacks have we seen
in the last year on large companies have lost data for millions of their customers and how badly that affects them and we're
looking at languages as you see that sql is pretty solid across all of these under a data architect xml that's one
that a lot of the data science jobs don't really you don't see as much but you're going to see more xml with a data architect
because there's a lot of xml files that are that have your security measures embedded in them and of course your hive
pig those are hadoop systems we're doing some basic queries so instead of doing the high end queries
you know how these simplified query systems built into hadoop and spark so if you're using big data
you're going to want to know your spark also and then we have our data engineer the data engineer updates the existing
systems with better version of the current technologies to improve the efficiency of the databases
think admin here a lot of work tracking down what version you're in of the different programs what version
of python are you using what version of r are using what version of c plus so companies hiring data engineers are like
amazon spotify facebook and they develop construct test and maintain architectures such as
databases and long scale processing systems and here we have our languages our sql
our matlab sas spss python java ruby c plus plus pearl hive
pig you pretty much as a data engineer at this level when you talk about admin and updating all these databases
need to know all the different stuff that's being used in the company and you're testing out these structures to make sure they work
and so you really are talking admin level uh kind of setup so if you like spinning up an admin
onto a google cloud service or amazon cloud service now you start to get an idea what we're
talking about because then you can test it out up there and then you can bring it back and update it to the company statition creates new methodologies for
engineers to apply and so when we look at this this is where we're digging into the math of which models to use
what setup is going to work what kind of activation do we have on our neural network these little tweaks are going to make a
big difference and we look for methodologies at a large level what kind of data are we going to be looking at how are we going to pull
it together companies hiring a data engineer or we're talking about stations we're talking about linkedin
pepsico johnson and johnson and of course these companies probably hire a little bit of everything
but they have a lot of statisticians working for them and so we look at the role extract and offer valuable reports from the data
clusters through statistical theories and data organization and the languages across the board we're still seeing sql
r matlab sas spss data that's a new one that i've seen in some of the other roles
python pearl an older version and that shouldn't be a surprise because a lot of these companies probably built a lot of
the original packages on pearl if you go look at johnson and johnson and of course your big data high pig and
spark and then our database administrator i mentioned the other one is an admin they're more an admin level for the
software itself so administrator is another level some of the tasks involved are monitoring
operating and maintaining databases installation configuration defining schemas training users etc
and this is interesting training users because when you build a database and you define all the different
tables that are embedded in that database and how they're all connected the end users got to have that information otherwise you're in trouble
when we have companies hiring data engineers of course tabloo twitter reddit the role ensures that all
the databases are available to all relevant users again a little security in there what role do they have
is allowed in and not and is performing correctly and is being kept safe and you better be ready to spend some
time in security because again that's a very big thing nowadays with the number of people who
hack into these databases and the languages are going to be covered you have your c sharp your java sql ruby on rails
xml again there's that xml because a lot of your security is embedded in xml files um
python all the main players depending on what the company's using data and analytics manager so when you
see manager that means that somebody's going to sit on top and organize the people underneath so they're going to improve the business
process as an intermediary between business and i.t oh you better have some really good
communication skills for this one uh some of the companies hiring a data engineers are coursera
motorola slack and of course these companies are hiring data scientists in almost all of these fields
so the role oversees the data science operations and assigns the duties to the team
according to skills and expertise so that's a big one you better be able to communicate what's going on help your
team members come out and be able to communicate with other people so sometimes your job is
just being able to get people to talk to each other the big thing on that is is the whole communication line going
from one end to the other because you can't communicate all the information that everybody's working on
you have to pretty much come in there and and help people communicate what there's going on and also know what
they're up to without being micromanagement because that will crash a company if you micromanage everybody but if you just
let them do whatever they want and they never talk to each other that will also destroy the company you gotta find that nice middle ground
and you really have to have an overview of everything as a data and analytics manager one of the things i
mean you know it's good to have at least one or two solid bases of programming under your belt for a data analytics
manager but you do need to have a very general understanding of all the different tools being used so that you know what's going
on with your different team members of course sql r sas java python matlab all of those are big business analytics
so this is when you're talking about your bi or your business intelligence and your business analytics very
important to know this is kind of a subdivision of a lot of the other stuff we've talked about a lot of the other roles we've
looked at so you possess specialized knowledge of their business domain and apply that knowledge and analysis
specifically to the operation of the business uh so you might be a specialist in banking
you might be a genetics engineer so you know about genetics you might
just be general business helping the marketing so a lot of companies hiring data engineer oracle
uber dell and the role act as a link between the data engineers and the management executives so it's very
similar to when we go back up here we look at data analytics manager a business and analytics is going to also kind of fill that role in
between but they're more specialized in business and you better know sql because we're talking about
business specifically most of the large companies are storing their data that's up and running right now that
people are accessing to purchase something on the home depot website or the uber they're logging in
you better know your sql it's very central to business analytics so let's go ahead and take a look at the
data science engineer salary trends and you'll see that our source is glass
door that's we're pulling a lot of our data from average base pay is around 117 000
in the u.s the average salary and when we look at india we're talking about
000 a year and those are based on some of them you need to go ahead and dig deeper to find out what education level
versus entry level but those are pretty solid base once you're in the industry and once you created your career in there and we look
at the job titles the most common job title is data scientist business intelligence manager
is the second greatest one that's kind of good to note that's why they put that up there as being its own little
subcategory of data science managers a business intelligence manager because it's such a high level
of jobs and then we looked at some of the other ones data architect business intelligence architect again
your bi your business intelligence gets its own ranking because it's such a high level
you know it's a business people want their businesses to make money so that's who's hiring of course data engineer
business intelligence developer business intelligence consultant business intelligence analyst business analyst and data
analyst so you can look at the data analysts as being the general one that doesn't go specific to business so if you're
looking just for your career as far as getting a job probably should have some kind of business
understanding in the background and again you know your companies want to make money so they're going to hire
someone who knows business if banking business if you specialize in banking or retail if you specialize in
retail and marketing and you can see here on the data science salary trends and the growth and data science
job listings it's continually going up it's um since 2014 to 2012 we've gone from 400
to 600 that's a pretty big increase so you know huge growth in this markets one of the biggest growing markets right now
for jobs and careers python libraries let's bring it together we have data
analytics and we have python so when we're talking data analytics we're talking python libraries for
data analytics and the big five players are numpy pandas matplot library scipy which is
going to be in the background so we're not going to talk too much about the scientific formulas inside pi and psi
kit so numpy supports n-dimensional arrays
provides numerical computing tools useful for linear algebra and fourier transform and you can think
of this as just a grid of numbers and you can even have a grid inside a grid
or data it's not even numbers because you can also put words and characters and just about
anything into that array but you can think of a grid and then you can have a grid inside a
grid and you end up with a nice three-dimensional array if you want to talk three-dimensional array you can think of
images you have your three channels of color four if you have an alpha and then you have your x y coordinates
for the image we're looking at so you can go x y and then what are the three channels to generate that color
and numpy isn't restricted to three dimensions you could imagine watching a movie well now you have your
movie clips and they each have their x number of frames and each of those frames have x number of x y coordinates for the
pictures in each frame and then you have your three dimensions for the colors so numpy is just a great way to work with
in-dimensional arrays now closely with numpy is pandas
useful for handling missing data perform mathematical operations provides functions to manipulate data
pandas is becoming huge because it is basically a data frame
and if you're working with big data and you're working in spark or any of the other major packages out there
you realize that the data frame is very central to a lot of that and you can look at it as a excel
spreadsheet you have your columns you have your rows or indexes and
you can do all kinds of different manipulations of the data within including filling in missing data which
is a big thing when you're dealing with large pools or lakes of data where they
might be collected differently from different locations and matplot library
we did kick over the sci pi which is a lot of mathematical computations which usually runs in the background of
the for of numpy and pandas although you do use them they're useful for a lot of other things in there
but the matplot library that's the final part that's what you want to show people and this is your plotting library in
python several toolkits extend matplot library functionality
there's like a hundred different toolkits to extend matplot library which range from how to properly display
star constellations from astronomy there's a very specific one built just for that all the way to some very generic ones
we'll actually add seaborne in when we do the labs in a minute several toolkits extend matplot library
functionality and it creates interactive visualization so there's all kinds of
cool things you can do as far as just displaying graphs and there's even some that you can create interactive graphs we won't do the
interactive graphs but you'll see you'll get a pretty good grasp of some of the different things you can do in matplot
library let's jump over to the demo which is my favorite roll up our sleeves
get our hands in on what we're doing now there's a lot of options when we're dealing with python
you can use pycharm is a really popular one and you'll see this all over the place
so it's one of the main ones that's out there and there's a lot of other ones i used to use netbeans which is kind of lost favor
don't even have it installed on my new computer but the most popular one right now for data
science now pycharm is really popular for python general development for data science we
usually go to jupiter notebook or anaconda and we're going to jump into anaconda
because that's my favorite one to go to because it has a lot of external tools for us
we're not going to dig into those but we will pop in there so you can see what it looks like so with anaconda we have our jupiter lab
we have our notebook these are identical jupiter lab is an upgrade to the notebooks with multiple tabs that's all
it is and we'll be using the notebook and you can see that pycharm is so popular with
python that we even have highlighted here in anaconda as part of the setup
jupyter notebook can also be a standalone so we're actually going to be running jupyter notebook and then you have your
different environments i have we're going to be under main pi 36 there's a root one
and i usually label it pi three six the reason is is currently as of writing this
tensorflow only works in three six and not in 3 7 or 3 8 for doing neural networks
but you can actually have multiple environments which is nice there they separate the kernel so it helps protect your computer when you're doing
development and this is just a great way to do a display or a demo especially if you're looking for that
job pull up your laptop open it up or if you're doing a meeting get it broadcast up to the big screen so that
the ceo can see what you're looking at and when we launch the notebook it
actually opens up a file browser in whatever web browser you have this happens to be chrome
and then you can just go under new there's a lot of different options depending what you have installed python 3 and this just creates an
untitled version of this and you can see here i'm actually in a simply learn folder for other work i've done for simply learn
and that's where i save all my stuff and i can browse through other folders making it real easy to jump from one project to another
and under here we'll go ahead and change the name of this and we'll go ahead and rename it
data analytics data analytics just so i can remember what i was doing
which is probably about 50 of the folders in here or files in here right now uh so let's go ahead and jump in there
and take a look at some of these different tools that we were looking at and as we go through the demo
let's start with the numpy the least visually exciting and i'm going to zoom in here so you can see what we're
doing and the first thing we want to do is import numpy
and we'll import it as np that is the most common numpy terminology and let's go and
change the view so we also have the line numbers i don't know why we probably won't need them but make it for easy reference
and then we'll create a one dimensional array we'll just call this array one and it equals np dot array and you put
your array information in here in this case we'll spell it out uh you can actually do like a range in
other ways there's lots of ways to generate these arrays but we'll just do one two three so three
integers and if we print our array one
we can go ahead and run this and you can see right here prints one two three
you can see why this is a really nice interface to show other people what you're doing with the jupiter notebook so this is the
basic we've created an array this is a one dimensional array and then the array is one two three one of the nice things about
the jupiter notebook is whatever ran in this first setup is still running
it's still in the kernel so it still has the numpy imported as np and it still has our variable
arr1 for array one equal to np array of one two three so we go to the next cell we can check
the type of the array we're just going to print we say hey what's what what is this set up in here and we
want type and then we want what is the type of array one let's go ahead and run that and it
says class numpy nd array so it's its own class that's all we're doing is checking to see what that class is
and if you look at the array class probably the biggest thing you do i don't know how many times i find
myself doing this because i forget what i'm working on and i forget i'm working with a three-dimensional or four-dimensional
array and i have to reformat somehow so it works with whatever other things they have and so we do the
array shape the array shape is just three because it has three members and it's a one dimensional array that's all that is
and with the numpy array we can easily access stick with the print statement if you
actually put a variable in jupyter notebook and it's the last one in the cell it will be the same as a print statement
so if i do this where array one of two is the same as doing print
array of two that's those are identical statements in our jupiter notebook uh we'll go and stick with the print on
this one and it's three so there's our print space two and we have zero 1 2
2 equals 3 we can easily change that so we have array 1 of place 2
equals 5 and then if we print our array 1 you can see right down
here when it comes out it's 1 2 and 5. and there i left the print statement off because it's the
last variable in the list it'll always print the variable if you just put it in like that
that's a jupiter notebook thing don't do that in pycharm i've forgotten before doing a demo
and we talked about multiple dimensions so we'll do an array two dimensional array and this is again
a numpy array and in the numpy array we need
our first dimension we'll do one two three and our second dimension
three four five and you can see right here that when we hit the uh we'll do this we'll
just do array two and we can run that and there's our
array two one two three three four five we can also do array two
of 1 and then we can do let's do 0 it doesn't
really matter which one actually do 2 there we go and if i run this it'll print out 5
because here we are this is 0 0 1 2 3 is under 0 row
3 4 5 is on our one row and we start with 0 and then the 2 0 1 2 goes to the 5.
and then maybe we forgot what we were working with so we'll go do array two dot shape
and if we do array two of shape we'll go and run that we'll see we have
two rows and each row has three elements a two dimensional array two three if you looked up here when we
did it before it just had three comma nothing when you have a single entity it
always saves it as a tuple with a blank space but you can see right here we have two comma three
and if you remember from up here we just did this array two of oh let's go what is it one comma two
we run that we get the five you can also count backwards this is kind of fun and you'll
see i just kind of switched something on you because you can also do one comma two to get to the same spot
now 2 is the last one 0 1 2. it's the last one in there we can count backwards and do minus 1.
and if we run this we get the same answer whether we count it as
let's go back up here whether we count this as 0 1 2 or we count backwards as minus 1 minus 2
minus 3. and you can see that if i change this minus 1 to a minus 2 and run that
i get 4 which is going backwards minus 1 minus 2. so there's a lot of different ways to reference what we're working on
inside the numpy array it's really a cool tool it's got a lot of things you can do with it
and we talked about the fact that it can also hold things that are not values and we'll call this array s for
strings equals np.array
put our setup in there brackets and let's go china
um india
usa uh mexico it doesn't matter we can make whatever
we want on here and if we print that out and we run this you can see that we get
another numpy of ray china india usa mexico it even gives us our d type of a u6
and a lot of times when you're messing with data we'll call this array r for range just to kind of keep it
uniform np dot a range so this is a command inside numpy to
create a range of numbers and if you're testing data maybe you want maybe you have equal time
increments that are spaced a certain point apart but in this case we're just going to do integers
and we're going to do a set up from 0 20 skipping every other one and we'll
print it out and see what that looks like and you can see here we have 0 2 4 6 8
10 12 14 16 18 like you expected it skips every one and just a quick note
there's no 20 on here uh why well this starts at 0 and counts up to 20.
so if you're used to another language where explicitly says less than or less than equal to 20 like
for x equals 0 x plus plus x is less than 20. that's what this is
it just assumes x is less than 20 on here and if we want to create a very uniform
set you know 0 2 4 6 what happens if i want to create numbers
from 0 to 10 but i need 20 increments in there we can do that with line space so we can
create um an r uh we'll call this l equals i don't think we'll actually use
any of this again so i don't know why i'm creating unique identifiers for it uh but we'll do np
lin space and we're going to do 0 to 10 or 0 to 9
remember it doesn't it goes up to 10 and then we want to let's say we have 20
different um increments in there so we're creating a we have a data set and we know it's over
a certain time period and we need to divide that time period by 20 and it happens to just have 10 pieces in
it and here we go you can see right here we have 20 or has 20 pieces in it but it's
over 10 years we got divided in the middle and you can see it does it goes 0.52
remember the others are 10 on the end so it goes up to 10.
uh and then we can also do random there's np.random if you're doing neural networks uh
usually you start it by seeding it with random numbers and we'll just do np.random and we'll
just call this array we'll stop giving it unique numbers we'll print that one out and run it
and you can see we have random numbers they are zero to one so you'll see that all these
numbers are under one and you can easily alter that by multiplying them out or something like
that if you want to do like 0 to 100 you can also round them up if it's integer 0 to 100 there's all kinds of
things you can do but generates a random float between 0 and 1. and you have a couple options you could reshape
that or you can just generate them in whatever shape you want
and so we can see here we did three and four and so you can see three rows by four
variables same thing as doing a reshape of 12 variables to 3 and 4.
and if you're going to do that you might need an empty data set i have had this come up many times
or i need to start off with 0 and i don't know you know because i'm gonna be adding stuff in there or it might be zero and one
where one is uh if you're removing the background of an image you might want the background is zero
and then you figure out where the image is and you set all those boxes to one and you create a mask so creating masks over images is really
big and doing that with a numpy array of zero and we can also uh
give it a space and we'll just do this all in one shot this time and we'll do the same thing like we
did before zeros and in this case we'll do uh two comma three
and so when we run this
i forgot the asterisks around it i knew it was forgetting something there we go so when we run this uh you can see here
we have our ten zeros in a row and maybe this is a mask for an image
and so it has a two rows of three digits in it so it's a very small image
a little tiny pixel and maybe you're looking to do something
the opposite way instead of creating a mask of zeros and filling in with ones maybe you want to create a mask of
ones and fill them in with zeros and we'll just do
just like we did before with the three comma four and when we run this you'll see it's all ones
and we could even do this even maybe we'll do it this way let's do 10
10 by 10 icon and then you have your three colors so creates quite a large array there for
doing pictures and stuff like that when you add that third dimension in
if we take that off it's a little bit easier to see we'll do 10 again
and you can easily see how we have 10 rows of 10 ones
and you can also do something like create an array and we'll do 0 1 2
and then in this array um we actually print it right out we want a repeat
so you can actually do a repeat of the array and maybe you need this array
let's repeat it three times so there's our repeat of an array repeat three
times and if we run this you'll see we have zero zero zero one one one two two two
and whenever i think of a repeat i don't really think of repeating being the first digit three times the second digit
i really always think of it as zero one two zero one two zero one two
it catches me every time but the actual code for that one is going to be tile and again if we do a
range three and we run this you can see how you can generate one
zero one two zero one two zero one two
and if you're dealing with um an identity matrix um we can do that also if you're
big on you're doing your matrixes and we'll just uh identity i guess we'll
go ahead and spell it out today any tricks
and the command we're looking for is um i e y e and we'll do three and then
we'll just go ahead and print this out
there we go there's our identity matrix and it comes out by a three by three array because there's our matrix
and then it puts the ones down the middle and for doing your different matrix math
and we can manipulate that a little bit too we talk about matrixes
we might not want ones across the middle in which case we now have the diagonal so we can do an np dot
diagonal and we do a diagonal let's put in the diagonal
one two three four five and when we run this again this generates a value and by just
putting that value in there's the same as putting print around it or putting array equals and then print array
and you can see it generates a diagonal one two three four five and there's your your beginning of your matrix array for
working with matrixes and we can actually go in reverse let's
create an array equals remember our random random.random and we'll do a five by
five array oops there we go five by five and just so you can see what that
looks like helps if i don't miss type the numbers
which in this case i just need to take out the brackets and there you go you have your your five by five array
set up in there and we can now because we're working with uh matrixes we might want to do this in reverse and
extract the diagonals which would be the point seven nine the point six seven eight and so on
and we simply type in np.diagonal
and we put our array in there and this will of course print it out because it returns it as a variable
and you can see here here's our diagonal going across from our matrix and we did talk about shape earlier if
you remember you can do print the shape out you can also do the dimensions
so in dimensions very similar to shape it comes out and just has two dimensions we can also look at the size so if we do
size on here we can run that and you can see has a size of 25 two dimensions and of course 5x5
and that was from the shape from earlier that we looked at there's our five by five shape and if
you remember earlier we did random well you can also do random i talked a little bit about manipulating
zero to one and how you can get different answers you can also do straight for the integer part and we'll do minus
10 to 10 4 and so we're going to generate random
integers between minus 10 to 10. we're going to generate four of those once when we run that we have seven
minus three minus six minus three they're all between minus ten and ten and there's four of them
and now we jump into some of the functionality of arrays which is really great because
this is where they come in here's your array and you can add 10 to it and if i run this
there takes my original array from up here with the integers and adds 10 to all of
those values so now we have oh this is the decimal that's right this is a random decimal i had stored an array
but this takes a random decimal the random numbers i had from 0 to 1 and adds 10 to them and we can just as
easily do minus 10.
we could even do times two
and we could do divide by two and it would it'll take that random number we generated and cut in
half so now all these numbers are under 0.5 another way you can change the numbers
to what you need on there and as you dig deeper into numpy we can
also do exponential so as an exponential function which would generate some interesting numbers
off of the random so we're taking them to the power i don't even remember what the original numbers in the
array work because we did the random numbers up there here's our original numbers and if you build an exponential on there
this is where you get e to the x on this and just like you can do e to the x you can also do the log so if you're
doing logarithmic functions that reinforce learning you might be
doing some kind of log setup on there and you can see the logarithmic of these different ray numbers
and if you're working with log base 2 you can do you can just change it in
there in p log 2. you have to look it up because this is not log 1 2 3 4 5.
it is log and log 2 so just a quick note that's not a variable going in that is an actual command
there's a number of them in there and you'll have to go look and see what the documentation is but you can
also do log 10. so here's log value 10. some other really cool functions you can
do with this is your sine so we can take a sine value of all of our different
values in there and if you have sine you of course have cosine we can run that
so here's the cosine of those and if you're doing activations in your numpy array and you're doing a
tangent activation there's your tangent for that and the tangent activation is actually from
neural networks that's one of the ways you can activate it because it forms a nice curve between uh from whether you're
generating one to negative one with some discrepancy in the middle
just jumping a little bit in there into neural networks
and then we get into let me just put the array back out there so we can see it while we're doing this as we're getting
into this you can also sum the values so we have np sum and you can do a summation of all
the values in this array and you'll see that if you added all these together they'd equal 12.519
and so on i don't know what the whole setup is in there but you can see right here the the
summation of this one of the things you can also do is by axes so we could do axes equals zero and if we run
the summation of the axis equals zero and you can think of that in numpy
as the rows so that would be or you can think of that in numpy as being
the columns we're summing these columns going across and you can also change this to one and
now we're summing the rows and so that is the summation of this row
and so forth and so forth going down and maybe you don't need to um
know the summation maybe what you're looking for is the minimum so here's our minimal you're looking for
and this comes up a lot because you have like your errors we want to find the minimal error inside of this array and just like um
the other one we can do axes equals zero and you can see here .0645 is the
smallest number in this first column is .0645 and so on and if you have a minimum well you might
also want to know the max maybe we're looking for the maximum profit and here we go you can see maximum 0.79
is the maximum on this first column and just like we did before you can change this to a one
on axes you can take the axes out of here and just find the max value for the
whole array and the max value in here was 0.8344 so on so on
and since we're talking data analytics uh we want to go ahead and look at the mean
uh pretty much the same as the average this is the mean across the whole thing and just like we did before we could
also do axes equals zero and then you'll see this is the mean of this axes and so on
and we have mean we might want to know the median
and there's our median our most common numbers if we have median we might want to know the standard deviation
or if we have the average a lot of times you do the means and the standard deviation we can run that and there's our standard
deviations along the axes we can also do it across the whole array
if we're going to do standard deviations there's also variance which is your var
and there's our variance across the different levels and so if we looked at that we looked at
variance we looked at standard deviation the median and the means there's more but those are the most common ones used with data analytics
and then going through your data and figuring out what you're going to present to the shareholders
and some other things we can do is we can actually take slices uh you'll hear that terminology and a
slice might be um like we have a five by five array but maybe we don't want the whole
array maybe we want from one on we don't want the zero in there so we got
up to four and maybe on the second part we just want two to row three
and see this notation right here says one to the end and if we run this you can see how that
generates a single row to the end and then row two and three now remember it doesn't include
three that's why we only get the one column so if you wanted two and three you would
need to go ahead and go two to four so it goes up to four we could also do this in reverse
just like we learned earlier we can go minus one whoops and when we go to minus one it's the
same thing because we have zero one two three four this is the same thing as two to four it goes two to the last one
also very common with arrays is you're going to want to sort them so we still have our array up here
that we randomly generated and we might want to
sort it and we'll go and throw an axis back in there
axes equals one if we run this you can see from the axes that it sorts it
uh the point two being the lowest value to the highest value by the row we can also change this of
course to axis zero if you're sorting it by column so maybe your values are based on columns
and then of course you can do the whole array and we can sort that don't usually do that but you know i
guess sometimes you might that might come up and so you can see right here we have a
nice sorted array uh something else let's just go ahead and reprint our array so we can look at
it again starting at too many boxes up there something else you can do with an array
is we can take and transpose it this comes up more than you would think
when you transpose it you'll see that the rows in the column are transposed so
where 0.79.57.064 is the column now we've switched it and
we have 0.79.42 as the index
you can see this really more dramatic if we take a slice and we'll just do a slice of the first
couple and then we'll just do all the other the full rows and if we run this
you can see how it comes up a little bit different and we'll just do the same slice up here so you can see how those two look next to each other there we go there's
our slice run and so you can see the slice comes up and it has a one two three four five columns now we
have one two three four five rows and three columns versus three rows
and the original version when they first started putting this together was a function so the original
version was transpose and this still works you can still see it generates the same value as just a
capital t so many times we flip this data because we'll have an x y value
or we'll have an image or something like that and it's being read one way into the next process and the
next one needs it the opposite so this actually happens a lot you need to know how to transpose the data really
quick and we can go ahead oh let's just take
here's our transpose we'll just stick with the transpose on here and instead of doing it this way we
might need to do something called flattening why would you flatten your data if this
is an array going into a neural network you might want to send
it in as one set of values instead of two rows and you can see here is all the values as a single array
it just flattens it down into one array so we covered our scientific means
transpose median some different variations on here
some of the other things we want to do is what happens if we want to append to our array so let's create a new array
i'm getting tired of looking at the same set of random numbers we generated earlier
so we'll go ahead and create a new array here something a little simpler so it's easier to see what we're doing
and four five six seven eight uh that's good enough i'll just do four five six seven eight
and if we print this array there it is four five six seven eight
and we might wanna append something to the array so we have our array we need to extend it you gotta be very careful
about appending things to your array and there's a number of reasons for that one is run time
because of the way the numpy ray is set up a lot of times you build your data and
then push it into the numpy array instead of continually adding on to the array and then it also usually it
automatically generates a copy for protecting your data so there's a lot of reasons to be careful about
appending this way but you can certainly do it and we can just take our array we're going to create a new array array
1 and if we print array 1 and we append 8 to it you'll see 4 5 6 7 and then there's our
8 appended onto the end and if you want to append something to
an array um you'd probably also want to whoops
array one let's try that again there we go now we have the eight appended on to the end
so you can see four five six seven eight and then we pinned it another eight on there
and if you're going to append something you might want to go ahead and insert instead of appending
it might be you need to keep a certain order and we can do the same thing we do our array
and we're going to pin or insert at the beginning and let's go ahead and
insert uh one two three one two three and we go ahead and print
our array two we run it and you can see one two three a pin is inserted at the beginning
inserts a lot more powerful and that you can put it anywhere in the array we can move it to the one spot and there we go one two three we can do
a minus one just for fun and you'll see it comes up one two three
and we're counting backwards by one imagining do a minus zero and run
this and it turns out that minus zero puts it back at the beginning because that's why it registers a zero just
takes a minus sign off and just like we add numbers on we might
want to delete numbers and so let's do an np dot delete
well let's keep it a little bit make it a little easy here to watch we'll go ahead and create an array three
and we'll do np delete and we're just working with array uh two and we want to do is delete
zero space uh so if you look at this here's our array two our array two starts with 1
and when we delete the space on here and print that out we deleted the 1
right out of there and we can also do something like this where we can do it as a slice
and we can do let's do one comma three and if we run one comma three you'll see
we've deleted the one space and the three space out which deleted our two and four
now keep in mind when you're messing with adding lines and deleting lines
you have to be really careful because there's a time element involved as far as where the date is coming from
and it's really easy to delete the wrong data and corrupt what you're working on or to insert stuff where you don't want
it so there's always a warning when we talk about manipulating numpy arrays
and just like anything else we're doing we'll create an array c which equals we'll just do
our um our numpy array then we just created our number array 3 and we can do copy so you can make a
copy of it maybe you want to protect your original data or maybe you're making a mask and
so you copy the array and then the new array make all these alterations and change it from values to
zero to one to mask over the first one and of course we if we do
array c since it equals a copy of array three it's the same thing one
three five six seven eight and now we're getting into uh combine and split arrays
i end up doing a lot of this and i don't know how many times i end up fiddling with this and having
a mess so but but you do it a lot you know you combine your arrays you split them you might
need one set of data for one thing another set of data for the other so let's go and create two arrays array
1 or a2 and i want you to note in the
terminology we're going to look for is concatenate what that means is we're going to take um we'll call this a ray cat i like a
raycat there we go our array cat our concatenated array
we're taking array one and two and it's very important to really pay attention
to your axes and your counts i can't merge two arrays that have like the if their axes are messed up and i'm
merging on axis zero it's going to give me an error and i'll have to reshape them so you got to make sure that whatever you're
concatenating together works and what that means as you can see here
we have one two three four one two three four and then five six seven eight five six seven eight along
the zero axes these each are four values so it's a two
by four value and if we go ahead and switch this to one you can see how this that flips it a
little bit so now we have one two three four five six seven eight it's interesting that we chose that one
if i did something like this where this is now there we go
and we concatenate it um run this and it gives me an answer okay because i have two by two and i'm using axes one
but if i switch this to axis zero where now it's got 3 and 5 it gives me an error
so you've got to be really careful on that to make sure that your whatever axes you are putting together that they match
so like i said this one oops x is 1. axis 1 has two entities and since we're going on axes 1 or by row
you can see that it lets it merge it right onto the n there and you could imagine this if this was a
xy plot of value or the x value going in and the predicted y value coming out
and then you have another prediction and you want to combine them this works really easy for that
and we'll go back and let's just put this back to where we had it
oops i forgot how many changes i made there we go um just put it oops i messed up in my
concatenation order here [Music]
there we go okay so you can see that we went through
the different concatenation axes is really important when you're doing your concatenation values on here
and we'll switch this back to one just because i like the looks of that better there we go two rows now there are
other commands in here so we can do cat v equals npv
v stack this is nothing more than your concatenation
but instead we don't have to put the axes in there because it's v stands for vertical and
so if we print out cat v and we run this
you can see we get the one two three four one two three four and that would be the same as making this axis zero for
vertical stack and if you're going to have a vertical stack you can also have an
h stack so if we change this to from v stack to oops here we go
h stack and we'll just change this from cat to cat and i run this
it's the same as doing axes zero the process is identical in the background um this is like a legacy setup uh your v
stack and your h stack most people just use concatenate and then put the axes in there because it's
much uh has a lot more clarity and is more more commonly used nowadays
the last section in numpy we're going to cover uh is underst is kind of data exploration
um and they'll make a little bit more sense in just a moment sometimes they call them set operations
but let's say we have an array one two three four five six three whatever it is uh things we generate a
nice little array here and what i want to go ahead and do is find the unique
values in that array so maybe i'm generating what they call a one hot
encoder and so these values then all become i need to know how long my bit
array is going to be so each word how many how many each word is represented by a number and
then i want to know just how many of those words are in there if we're doing word count very popular thing to do
and you can see here when we do unique we have 1 two three four five six those are our unique values
uh some of the things we can do with the unique values is we can also instead of doing just unique we can do uniques
our new unique values and counts of each unique value and this is very similar to what we just
did up here where we we're doing np unique but we're going to add a little bit more into there
and it's just part of the arguments in this and we want to do return
counts equals true so instead of just returning the unique
values we want to know how many of those unique values are in each one
and we'll go ahead and print our uniques and print our counts
when we run that you can see here we have our unique value one two three four five six
just like we had before and then there's two of the first of two ones two twos two threes two fours one five two sixes
and so on and you can go through and actually look at that if you want to count them but a quick way to find out your
distribution of different values so you might want to know how often the word the is used versus the word and
if each word is represented as a unique number
and along the set variables we might want to know let me just put a note up here we're
going to start looking at intersection and we might want to also
know differentiation
and uh neither so when we're whoops neighbor neither
um so what we're looking at now is we want to know hey where do these two arrays intersect and we have 1 2 3 4
five three four five six seven we might want to know what is common between the two arrays
and so when we do that we have np
intersect and it's a 1d array one dimensional array
and then we need to go ahead and put array 1 array 2. and if we run this
we can see they intersect at three four five that's what they have common uh and because we're going to go
ahead and go through these and look at a couple different options let's change this from intersect 1d
and we'll do the same thing we'll go ahead and print this so we might want to know the intersection where they have
commonalities another unique word is union of 1d
so instead of intersect we want to know all the values that are in both of them
so here's our union of 1d when we run that you can see we have one two three four five six seven so it's all the different values in there
and the last one of the last words we have two more to go as we want to know what the set difference is
uh and so that's where the you'll see that if you remember set we talked about that being the what they call these things um so the
set difference of a 1d array when we run that you can
see that one is only in one array and two is only in one array
and if we want to know what's in array 1 but not in array 2 we might want to know what is in array 1 but not 2 and what's
in 2 but not 1 and this would be the set x or 1d on here so we have the
four different options here where we can do an intersection what do they both have in common we can do a union
what are all the unique values in both arrays we can see the difference what's in array one but not
array two so set diff one d and then set x or what is not in one but is in
two and what is in not in two but in one so we dug a lot in numpy because we're
talking there's a lot of different little mathematical things going on in numpy
a lot of this can also be done in pandas although usually the heavy lifting is left for numpy because that's what it's
designed for let's go ahead and open up another python3 setup in here and so we want to explore
what happens when you want to display this this is where it starts getting in my opinion a little fun because
you're actually playing with it and you have something to show people and we'll go ahead and rename this we're going to call this pandas
and pie plot so pandas pie plot just so we can remember for next time
and we want to go ahead and import the necessary libraries we're going to import pandas as pd now
remember this is a data frame so we're talking rows and columns and you'll see how
pandas work so nicely when you're actually showing data to people and then we're going to have numpy in
the background numpy works with pandas so a lot of times you just import them by default
seaborn sits on top of the matplot library so sometimes we use the seaborn because
it kind of extends it's one of the 100 packages that extends the matplot library probably the most commonly used
because it has a lot of built-in functionality almost by default i usually just put cborn in there in case i need it
and of course we have matplot library as pi plot as plt and note we have as
pd as np as sns as plt those are pretty standard so when you're
doing your imports i would probably keep those just so other people can read your code and it makes sense to them that's pretty much a standard nowadays
and then we have the strange line here it says ambersign matplot library inline
that is for jupiter notebook only so if you're running this in a different package you'll have a
pop-up when it goes to display the map plot library you can with the most current version of
jupiter usually leave that out and it will still display it right on the page as we go and we'll see what that looks like
and then we're going to go ahead and just do the seaborn the sns.set and we're going to set the color codes
equals true let them just keep the default one so we don't have to think about it too much
and we of course have to run this the reason we run this is because these values are all
set if we don't run this and i access one of these afterward it will crash the cool thing
about jupiter notebooks is if you forgot to import one of these you forgot to install it because you do have to
install this under your anaconda setup or whatever setup you're in you can flip over to anaconda and run
your install for these and then just come back and run it you don't have to close anything out
and we'll go ahead and paste this one in here real quick we have car equals pd dot read underscore csv
and then we have the actual path this path of course will vary depending
on what you are working with so it's wherever you save the file at
and you can see here i have like my onedrive documents simply learn python data analytic using python
slash car csv it's quite a long file when we open that up what we get is we
get a csv file and we have the make the model the year the engine fuel type engine horsepower
cylinders and so on and this is just a comma separated file so each row is like
a row of data think of it as a spreadsheet and then each one is a column of data on
here and as you can see right here it has the make model so it has columns for a header on here
now your pandas just does an excellent job of automatically pulling a lot of this in
so when you start seeing the pandas on here you realize that you are already like
halfway done with getting your data in i just love pandas for that reason numpy
also has it you can load a csv directly into numpy but we're working with pandas
and this is where it really gets cool is i can come down here and i can print remember our print
statement we can actually get rid of it and we're just going to do car head because it's going to print that out
the head is going to print the top values of that data file we just ran in and so you
can see right here it does a nice printout it's all nice and inline because we're in jupyter notebook
i can scroll back and forth and look at the different data and just like we expected we have our
column and it brought the header right in one thing to note is the index it
automatically created an index 0 1 2 3 4 and so on and we're just looking at the head so we've got 0 1 2 3
4. you can change this you might want to just look at the top two we can run that
there's our top two bmws another thing we can do is instead of head we can do tail
and look at the last three values that are in that data file and uh you can see right here
it numbered them all the way up to 11 913 oh my goodness they put a lot of data in this file
i didn't even look to see how big the file was so you can really easily get through and view the different data in
here when you're talking about big data you almost never just print out car
in fact let's see what happens when we do if we run this and we just run the car
it's huge in fact it's so big that the pandas automatically truncates it and just does head plus tail
so you can see the two so we really don't want to look at the whole thing i'm going to go back to let's stick with
the head displaying our data there we go so there's a head of our data gives us a quick look
to see what's actually in there i can zoom out if we want so you can actually get a better view although we'll keep it
zoomed in so you can see the code i'm working on and then from the data standpoint we
course want to look at data types what's going on with our data
what does it look like now this you know you show your when you're talking to your shareholders
they like to see these nice easy to read charts they look like a spreadsheet so it's a nice way of displaying pieces
of the chart we talk about the data types now we're getting into the
data science side of it what are we working with well we have make model we have an integer 64 for the
year engine fuel type is an object if we go up here you can see that there
most of them are um like you know it's a set manual rear wheel drive uh so they might
be very limited number of types in there uh and so forth and you'll it's either gonna be
a float64 an integer or an object is the way it's going to read it on here
and the next thing you're going to know is like your columns
and since it loaded the columns automatically we have here the make the model the year the engine
the size all the way up to the msrp
and just out of something you'll see come up a lot is whenever you're in pandas and you
type in dot values it converts it from a pandas uh list to a numpy array
and that's true of any of these uh so then you end up in a numpy array so you'll see a little switch in there in the way that the data is actually
stored and that's true of any of these in this case we want car dot columns you have a total
list of your car columns and like any good data scientist
we want to start looking at analytical summary of the data set what's going on with our data so we can start trying to piecemeal it
together so we can do car describe and then we'll do is we'll do
include equals all so a nice panda command is to
describe your data if you're working with r this should start looking familiar
and we come down here and you can see count there's a make the model the year how many of each
one how many unique values of each one the top value of each one what's most common
the frequency the mean clearly on some of these it's an object so really can't
tell you what the average is you know it'd just be the top ones the average i guess
the year what's the average year on there all this stuff comes down here your standard deviation
your minimum value your maximum value what's in the lower quarter 50 percent
mark where's that line at and what's in the upper 75 percent the top 25 going into the max now this next part is
just cool uh this is what we always wanted computers to be back like in the 90s instead of 5 000 lines of code to do
this maybe not 5 000. all right i built my own plot library back in 95 and the amount of
code for doing a simple plot was um i don't know probably about 100 lines of code
this is being done in one line of code we have our car which is our pandas we generated that
it's our data frame and we have dot hist for histogram that is the power of seaborn now it's
still going to generate a numpy graph but seaborn sits on top and then we can do the figure size
this is just um so it fits nicely on the paper on here and we do something simple
like this and you can see here where it comes up and does say matplot library and does subplots and everything
but we're looking at a histogram of all the different pieces in our database and we have our
engine cylinders that's always a good one because you can see like they have some that are they had
a null on there so they came out as zero maybe a couple maybe one of them had a two cylinder engine away back when
four is a common six a little less common and then you see the eight cylinder uh 12 cylinder engines well it's got to
be a speedster or something but you can see right here just breaks it down so now you have
how many cars with how many whatever it is cylinders horsepower uh and so on and it does a
nice job displaying it you can see if you're working with your uh um
you're going into your demo it's really nice just to be able to type that in and boom there it is it can see it all the way
across and we might want to zero in and use like a box plot and this time we'll go
ahead and call the seaborn sns box plot and we're going to
go ahead and do vehicle size in versus engine horsepower
xy plot and the data comes from the car so if we run this we end up with a nice box plot you see
our mid-size compact and large you can see the variation there's our outlier showing up
there on the compact that must be a high-end sports car a large car might have a couple engines
and again we have all these outliers and then your deviation on them powerful and quick way to zero in on one
small piece of data and display it for people who need to have it reduced to something
they can see and look at and understand and that's our seabourn box plot or dot sns.box plot and then if we're going
to back out and we want a quick look at what they call pair plotting
we can run that and you can see with the seaborn it just does all the work for you it takes just a moment for it to pull
the data in and compile it
and once it does it creates a nice grid um in this grid if you look at this one space here which
is you might not be able to see the small number it says engine horsepower this is engine horsepower uh to the year
was built and it's just flipped so everything to the right of the middle diagonal is just
the rotation of what's on the left and as you expect the engine horsepower gets bigger and
bigger and bigger as time goes on so the the year it was built the further up in the year the more likely you are to have a heavy
horsepower engine and you can quickly look at trends
with our pair plot coming up and look how fast that was that was it took a couple
a moment to process but right away i get a nice view of all these different
information which i can look at visually and kind of see how things group and look now if i
was doing a meeting i probably wouldn't show all the data
one of the things i've learned over the years is people myself included love to show all our work you know we are taught in school show all
your work prove what you know the ceo doesn't want to see a huge uh grid of
of graphs i guarantee it so we want to do is we want to go ahead and drop
the stuff that might not be interested in and we're gonna i'm not really a car person a guy in the back is obviously so you
have your engine fuel type we're gonna drop that we're gonna drop market category vehicle style popularity number
of doors vehicle size and we have the axes in here if you remember from numpy
we have to include that axis to make it clear what we're working on that's also true with pandas and then we'll look at just that what it
looks like from the head and you can see that we dropped out those categories and now we have the
make model year and so forth and we took out the engine fuel type market category etc
and this should look familiar to you now when you start working with pandas i just love pandas for this reason look
how easy it is it just displays it as a nice spreadsheet for you you can just look at it and view it very easily
it's also the same kind of view you're going to get if you're working in spark or pi spark which is python for spark
across big data this is the kind of thing that they they come up with this is why pandas is so powerful
and we may look at this and decide we don't like these columns and so you can
go in here and we can actually rename the columns simple command car equals car rename
columns equals engine horsepower equals horsepower this is just your standard
python dictionary so it just maps them out and you know instead of having like a
lengthy effect here we had engine horsepower we just want horsepower we don't need to know what's the engine horsepower
engine cylinders we don't need to know that it's for the engine because there's only one thing we're describing if we're talking about cars and that cylinders
and we'll go ahead and just run this and again here's our car head and you can see how that changed
we have model year in horsepower versus model year engine horsepower engine cylinders and
just cylinders again we want to keep reducing this so
it's more and more readable the more readable you get it the better and of course we can also adjust the
size a little bit so that when it prints out instead of splitting it on two lines we get like a single
line we can do that also that's just your control mouse up or plus sign you use in chrome that's a chrome
command and if you remember from numpy we had shape well pandas works the same way
we can look at the shape of the data so we now have 11 914 rows and 10 columns
so you see some similarities because pandas is built on numpy
and questions that come up just like you did in numpy we might want to know duplicate rows and so we can do car and look at this
switch here we're doing a selection this is a pandas selection with the brackets
but we want to select it based on car dot duplicated so how many duplicates on there so we're
starting to look a little bit different as far as how we access some of the data in here this can be a logical statement and we get the number
of duplicate rows we have 989 rows by 10 columns again
and this is one of those troubleshooting things that we end up doing a lot more than we really feel like we
should we might go ahead and do like a car count just to see how many rows we're
dealing with and then right after that we might want to go ahead and say hey
let's drop duplicates so remember we did all the duplicates on there so car equals car dot drop duplicates and then we can
print the head again we'll just do car head here and you can see the data on there
looks the same as before and just note that we did car equals car
draw duplicates there are commands in here where you can do where it changes the actual value and it
works on some of them and not on others depending on what you're doing but by default it always returns a copy
so when we do this we're reassigning it to car and you can see it's the same header but we
want to go ahead and do count and see how the count changes let's go ahead and run this and you can see here
instead of 11 914 we have 10 925 uh so we've removed
about a hundred cars that were duplicated just slightly under a hundred there
and then as we're prepping our data we might wanna know um car is null so it's going to count
the values of null and then we want to sum that up and when we do that we do the car is no
function dot sum uh we end up with uh hp the horsepower at 69 have null values and 30
have cylinders have no values now if you don't put the sum at the end it's just going to return a
mask with the true false of is it null or is it not by zero and one so you're summing up the
ones underneath each column and this of course then you have to
decide what you're going to do with the null values there's a lot of different
options it might be that you need to put in the average or means maybe you want to put in the median value
there's a lot of different ways to fill it usually when you first start out with the data a lot of them you just drop
your null values and you can see here car.drop in a which is equal to all and then we're
going to go ahead and count it and you can see that we've dropped almost another hundred values so from 10
1925 to 10 827 maybe 75 or so values
so we clean that this is really a big part of cleaning data you need to know how to get rid of your null values or at least count them and what to do with
them and of course if we go back to
counting our null values we should now have no null values there we go and you'll
see there's zero null values i don't know how many times i've been running a model that doesn't take no
values and it crashes and i just sit there and look at it trying to get why did that crash it should have worked
it's because i forgot to remove the null values so we've been jumping around a lot we're
going to go back to finding outliers and let's go ahead and bring that back into our seaborn
and if you remember we did a box plot earlier uh this time we're going to do a box plot just on the price and you can see here
our price value and we have the deviation with the two thinner bars on each side of the main value
and then as we get up here we have all these outliers in fact we have one way out here that's
probably a really expensive high-end car is what we're looking at if you were doing fraud analysis
you would be jumping on all over these outliers why are these deviation from the standard what are these people doing
again this is probably like i said a really high-end expensive car out here that's what we're looking at and we can also look at the box plot
for the horsepower and we'll put that in down here and run that and you can see again
here's our horsepower and it just jumps and there's these really odd huge muscle cars out here that are
outliers and we're going to jump into making this a little bit more
as you start displaying your data or your information to your shareholders we're going to look at plotting a
histogram for the number of cars per brand and the first thing we want to go ahead
and do is we have with our car and go back over here here we go
uh we have our make value counts largest plot and we want to do a kind equals bar uh
fig size 10 5. and right off the bat we jump up here we
see chevrolet it's going against what was it it's uh figure resolution the value counts and we want the largest
value so here's our value counts and compared to what the different cars are chevrolet puts out a lot of
different kinds of cars i didn't realize that they made that many cars or different types
and then for readability let's go ahead and add a title number of cars by make number of cars
and make if you had looked at this the first time you would have been like well what the heck am i looking at well we're looking at the number of cars
by make and then you can see here now we're talking about the type of cars and the different ones are put out lotus
i guess only had a few different kinds of cars over there very high-end cars and then
as doing data analytics and as a data scientist one of the things i am most interested
in is the relationship between the variables so this is always a place
to start we want to know what's going on with our variables and how they connect with each other
so the first thing we're going to do is we're going to go ahead and set a figure size because we want to make sure
it fits our graph we'll just go ahead and set this one plot figure set to figure size 2010.
if you never use the matplot library which is sitting behind seaborne whatever is in the plt this is what's
loaded it's like a canvas you're painting on so the second you load that pi plot as plt anything you do to that
is affecting everything on it and then we want to go ahead uh since we're using seaborn
we'll go ahead and create a variable c for relationships or correspondence and car
dot c-o-r-r that's a correlation in seaborne on top of pandas again
one line and you get the whole correlation on there and because we're working with seaborn
let's put it into a nice heat map if you're not familiar with heat maps that means we're just using color
as part of our setup so we have a nice visual
and we can see here that the seaborne connected to the pandas prints out a nice chart we'll talk a little bit about the
color here in a second it prints out a nice chart this is a chart i look at as a data scientist these are the numbers i want to look at
and we'll just highlight one of them here's cylinders versus horsepower the closer to one the
higher the correlation so 0.788 pretty high correlation between the number of cylinders and how heavy the
horsepower is i'm betting if you looked at the year versus uh
horsepower um we just look at that one here's year and horsepower 0.314 not as so much but if you combine
them you don't actually add them but if you combine them you'll start to see an increase in horsepower per year and cylinders you could probably get a
correlation there and just like 0.78 is a positive correlation you might
notice if we look at cylinders and or let's look at horsepower and
mileage uh so if we go here to horsepower to mileage you get a nice um negative we'll do cylinders that's a
bigger number with cylinders to the miles per gallon it's a minus 0.6 so it's a negative correlation the
closer to minus 1 the more the negative correlation is and then the chart you would actually
show people is a nice heat map this is all our colors and it's just those numbers put into a heat map
the darker the color the higher the correlation you can see straight down the middle obviously the
year correlates directly with the air horsepower with horsepower and so on that's why it's a one the closer to the
one the higher the correlation between the two pieces of data now this is a good introduction pandas
goes way beyond this most the functionality in numpy since panda sits on it is also in pandas and then it even has
additional features in it and we use seaborne pretty extensively sitting on top over our pie plot so keep
in mind that our pie plot has a ton of other features in it that we didn't even touch on in here we couldn't even if you had a
soul course in it there's just so many things hidden in there depending on what your domain
you're working on but you can see here here's our seabourn and here's our matplot library
that's all our graphics that we did and then the seaborn works really nicely with the pandas we
really like that understanding linear regression linear regression
is the statistical model used to predict the relationship between independent and dependent
variables by examining two factors the first important one is which
variables in particular are significant predictors of the outcome variable and the second one that we need to look
at closely is how significant is the regression line to make predictions with the highest possible accuracy if it's inaccurate we
can't use it so it's very important we find out the most accurate line we can get since linear regression is based on
drawing a line through data we're going to jump back and take a look at some euclidean geometry the simplest
form of a simple linear regression equation with one dependent and one independent variable is
represented by y equals m times x plus c and if you look at our model here we
plotted two points on here x1 and y1 x2 and y2
y being the dependent variable remember that from before and x being the independent variable so
y depends on whatever x is m in this case is the slope of the line
where m equals the difference in the y2 minus y1 and x2 minus x1 and finally we have c
which is the coefficient of the line or where it happens to cross the zero axes
let's go back and look at an example we used earlier of linear regression we're going to go back to plotting the
amount of crop yield based on the amount of rainfall and here we have our rainfall remember we cannot change
rainfall and we have our crop yield which is dependent on the rainfall so we have our independent and our
dependent variables we're going to take this and draw a line through it as best we can through the
middle of the data and then we look at that we put the red point on the y axis is the amount of crop yield you can
expect for the amount of rainfall represented by the green dot so if we have an idea what the rainfall is for
this year and what's going on then we can guess how good our crops are going to be and we've created a nice line right
through the middle to give us a nice mathematical formula let's take a look and see what the math looks like behind this
let's look at the intuition behind the regression line now before we dive into the math and the
formulas that go behind this and what's going on behind the scenes i want you to note that when we get into
the case study and we actually apply some python script that this math you're going to see here
is already done automatically for you you don't have to have it memorized it is however
good to have an idea what's going on so if people reference the different terms you'll know what they're talking about
let's consider a sample data set with five rows and find out how to draw the regression
line we're only going to do five rows because if we did like the rainfall with hundreds of points of data
that would be very hard to see what's going on with the mathematics so we'll go ahead and create our own two
sets of data and we have our independent variable x and our dependent variable
y and when x was 1 we got y equals 2 when x was 2 y was 4
and so on and so on if we go ahead and plot this data on a graph we can see how it forms a nice line
through the middle you can see where it's kind of grouped going upwards to the right the next thing we want to know is what
the means is of each of the data coming in the x and the y the means
doesn't mean anything other than the average so we add up all the numbers and divide by the total so 1 plus 2 plus 3 plus 4
plus 5 over 5 equals 3 and the same for y we get 4. if we go ahead and plot the means on the
graph we'll see we get three comma four which draws a nice line down the middle a good estimate
here we're going to dig deeper into the math behind the regression line now remember before i said you don't
have to have all these formulas memorized or fully understand them even though we're going to go into a little more detail of how it works
and if you're not a math wiz and you don't know if you've never seen the sigma character before which looks a little bit like an e
that's opened up that just means summation that's all that is so when you see the sigma character it just means we're adding
everything in that row and for computers this is great because as a programmer you can easily iterate through each of
the xy points and create all the information you need so in the top half you can see where we've broken that down into pieces
and as it goes through the first two points it computes the squared value of x the squared value
of y and x times y and then it takes all of x and adds them up all of y adds them up
all of x squared adds them up and so on and so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all the way up to x times y where the sum equals 66. this
all comes from our formula for calculating a straight line where y equals the slope
times x plus the coefficient c so we go down below and we're going to compute more like the
averages of these and we'll explain exactly what that is in just a minute and where that information comes from is called the
square means error but we'll go into that in detail in a few minutes all you need to do is look at the formula
and see how we've gone about computing it line by line instead of trying to have a huge set of
numbers pushed into it and down here you'll see where the slope m equals and then the top part if you
read through the brackets you have the number of data points times the sum
of x times y which we computed one line at a time there and that's just the 66
and take all that and you subtract it from the sum of x times the sum of y and those have both
been computed so you have 15 times 20. and on the bottom we have the number of lines
times the sum of x squared easily computed as 86 for the sum minus i'll take all that
and subtract the sum of x squared and we end up as we come across with our formula you can plug in all
those numbers which is very easy to do on the computer you don't have to do the math on a piece of paper or calculator and you'll get a
slope of 0.6 and you'll get your c coefficient if you continue to follow through that formula
you'll see it comes out as equal to 2.2 continuing deeper into what's going behind the scenes let's find out the predicted
values of y for corresponding values of x using the linear equation where m equals 0.6 and c equals 2.2
we're going to take these values and we're going to go ahead and plot them we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2 equals 2.8 so on and so on and
here the blue points represent the actual y values and the brown points represent
the predicted y values based on the model we created the distance between the actual and predicted values
is known as residuals or errors the best fit lines should have the least
sum of squares of these errors also known as e-square if we put these into a nice
chart where you can see x and you can see why what we actual values were and you can see why i predict it you can
easily see where we take y minus y predicted and we get an answer what is the difference between those two
and if we square that y minus y prediction squared we can then sum those squared values
that's where we get the 0.64 plus the 0.36 plus 1 all the way down until we have a
summation equals 2.4 so the sum of squared errors for this regression line is 2.4
we check this error for each line and conclude the best fit line having the least e square value in a nice graphical
representation we can see here where we keep moving this line through the data points to make sure
the best fit line has the least square distance between the data points and the regression line now we only
looked at the most commonly used formula for minimizing the distance there are lots of ways to minimize the
distance between the line and the data points like sum of squared errors sum of absolute errors
root mean square error etc what you want to take away from this is whatever formula
is being used you can easily using a computer programming and iterating through the data
calculate the different parts of it that way these complicated formulas you see with the different summations and
absolute values are easily computed one piece at a time up until this point we've only been
looking at two values x and y well in the real world it's very rare that you only have two values when
you're figuring out a solution so let's move on to the next topic multiple linear regression
let's take a brief look at what happens when you have multiple inputs so in multiple linear regression
we have well we'll start with the simple linear regression where we had y equals m plus x plus c and we're trying
to find the value of y now with multiple linear regression we have multiple variables coming in
so instead of having just x we have x1 x2 x3 and instead of having just
one slope each variable has its own slope attached to it as you can see here we have m1 m2 m3 and
we still just have the single coefficient so when you're dealing with multiple linear regression you basically take
your single linear regression and you spread it out so you have y equals m1 times
x1 plus m2 times x2 so on all the way to m to the nth x to the nth and then you
add your coefficient on there implementation of linear regression now we get into my favorite
part let's understand how multiple linear regression works by implementing it in python if you remember before we
were looking at a company and just based on its r d trying to figure out its profit
we're going to start looking at the expenditure of the company we're going to go back to that we're going to predict his profit but
instead of predicting it just on the r d we're going to look at other factors like administration costs
marketing costs and so on and from there we're going to see if we can figure out what the profit of that company is going
to be to start our coding we're going to begin by importing some basic libraries
and we're going to be looking through the data before we do any kind of linear regression we're going to take a look at the data to see what we're playing with then
we'll go ahead and format the data to the format we need to be able to run it in the linear regression model and
then from there we'll go ahead and solve it and just see how valid our solution is so let's start with importing the basic
libraries now i'm going to be doing this in anaconda jupiter notebook a very popular
ide i enjoy it's such a visual to look at and so easy to use just any id for python will work just
fine for this so break out your favorite python ide so here we are in our jupyter notebook
let me go ahead and paste our first piece of code in there and let's walk through what libraries we're importing
first we're going to import numpy as np and then i want you to skip one line and look at import pandas as
pd these are very common tools that you need with most of your linear regression the numpy which stands for number python
is usually denoted as np and you have to almost have that for your sk learn toolbox you always import
that right off the beginning pandas although you don't have to have it for your sklearn libraries it does
such a wonderful job of importing data setting it up into a data frame so we can manipulate it rather easily and it
has a lot of tools also in addition to that so we usually like to use the pandas when we can and i'll show you
what that looks like the other three lines are for us to get a visual of this data and take a look at
it so we're going to import matplotlibrary.pipelot as plt and then seaborn as sns
seborn works with the matplot library so you have to always import matplot library and then seaborn sits on top of
it and we'll take a look at what that looks like you could use any of your own plotting libraries you want there's all
kinds of ways to look at the data these are just very common ones and the seaborne is so easy to use
it just looks beautiful it's a nice representation that you can actually take and show somebody and the final line is the amber scion
map plot library inline that is only because i'm doing an inline ide my interface in the anaconda
jupiter notebook requires i put that in there or you're not going to see the graph when it comes
up let's go ahead and run this it's not going to be that interesting so we're just setting up variables in fact it's not going to do anything
that we can see but it is importing these different libraries and setup the next step is
load the data set and extract independent and dependent variables now here in the slide you'll see
companies equals pd.read csv and it has a long line there with the file at the end 1000
companies.csv you're going to have to change this to fit whatever setup you have and the file itself you can request just
go down to the commentary below this video and put a note in there and simply learn we'll try to get in contact with you and
supply you with that file so you can try this coding yourself so we're going to add this code in here and we're going to see that i have
companies equals pd.reader underscore csv and i've changed this path to match my computer
c colon slash simply learn slash 1000 underscore companies dot and then below there we're going to set
the x equals to companies under the i location and because this is companies as a pd
data set i can use this nice notation that says take every row that's what the colon the
first colon is comma except for the last column that's what the second part is or we have a colon
minus one and we want the values set into there so x is no longer a data set a pandas data set but we can
easily extract the data from our pandas data set with this notation and then y we're going to set equal to
the last row well the question is going to be what are we actually looking at so let's go ahead and take a look at
that and we're going to look at the companies dot head which lists the first five rows of data and i'll open up the file in just a
second so you can see where that's coming from but let's look at the data in here as far as the way the pandas sees it
when i hit run you'll see it breaks it out into a nice setup this is what panda is one of the things pandas is really good
about is it looks just like an excel spreadsheet you have your rows and remember when we're programming we always start with
zero we don't start with one so it shows the first five rows zero one two three four and then it
shows your different columns r and d spend administration marketing spend
state profit it even notes that the top are column names it was never told that
but pandas is able to recognize a lot of things that they're not the same as the data rows why don't we go ahead and open this file
up in a csv so you can actually see the raw data so here i've opened it up as a text editor and you can see at the top we have rnd
spend comma administration comma marketing spin comma state comma profit
carriage return i don't know about you but i'd go crazy trying to read files like this that's why we use the
pandas you could also open this up in an excel and it would separate it since it is a comma separated variable file but we
don't want to look at this one we want to look at something we can read rather easily so let's flip back and take a look at that top part the first five row now as nice
as this format is where i can see the data to me it doesn't mean a whole lot maybe you're an expert in business and
investments and you understand what 165 349 dollars and
twenty cents compared to the administration cost of a hundred and thirty six thousand eight hundred ninety seven dollars and eighty cents so on so
on it helps to create the profit of 192 261 and 83 cents that makes no sense to
me whatsoever no pun intended so let's flip back here and take a look at our next set of code where we're going to graph it so we can
get a better understanding of our data and what it means so at this point we're going to use a single line of code to get a lot of
information so we can see where we're going with this let's go ahead and paste that into our
notebook and see what we got going and so we have the visualization and again we're using sns which is
pandas as you can see we imported the map plot library dot pi plot as plt
which then the seaborn uses and we imported the seaborn as sns and then that final line of code helps
us show this in our inline coding without this it wouldn't display and you could display
it to a file in other means and that's the matte plot library in line with the amber sign at the beginning so here we
come down to the single line of code seaborn is great because it actually recognizes the panda data frame
so i can just take the companies dot core for coordinates and i can put that right into the
seaborn and when we run this we get this beautiful plot and let's just take a look at what this plot means
if you look at this plot on mine the colors are probably a little bit more purplish and blue than the original one
we have the columns and the rows we have r and d spending we have administration we have marketing spending and profit
and if you cross index any two of these since we're interested in profit if you cross index profit with profit
it's going to show up if you look at the scale on the right way up in the dark why because those are
the same data they have an exact correspondence so rnd spending is going to be the same
as rnd spending and the same thing with administration costs so right down the middle you get this dark row or dark um
diagonal row that shows that this is the highest corresponding data that's exactly the same and as it becomes
lighter there's less connections between the data so we can see with profit obviously profit is the same as
profit and next it has a very high correlation with r d spending which we looked at earlier and it has a
slightly less connection to marketing spending and even less to how much money we put into the administration
so now that we have a nice look at the data let's go ahead and dig in and create some actual useful linear
regression models so that we can predict values and have a better profit now that we've taken a look at the visualization
of this data we're going to move on to the next step instead of just having a pretty picture we need to generate some hard data some
hard values so let's see what that looks like we're going to set up our linear regression model
in two steps the first one is we need to prepare some of our data so it fits correctly
and let's go ahead and paste this code into our jupiter notebook and what we're bringing in is we're going to bring in the sklearn
pre-processing where we're going to import the label encoder and the one hot encoder to use the label
encoder we're going to create a variable called label encoder and set it equal to capital l label capital e encoder this creates a
class that we can reuse for transferring the labels back and forth now about now you should ask what
labels are we talking about let's go take a look at the data we processed before and see what i'm talking about here
if you remember when we did the companies dot head and we printed the top five rows of data we have our columns
going across we have column zero which is r and d spending column one which is administration
column two which is marketing spending and column three is state and you'll see under state we have new york california
florida now to do a linear regression model it doesn't know how to process new york it knows how to process a
number so the first thing we're going to do is we're going to change that new york california and florida and we're going to change those to
numbers that's what this line of code does here x equals and then it has the colon comma
3 in brackets the first part the colon comma means that we're going to look at all the different rows so we're going to
keep them all together but the only row we're going to edit is the third row and in there we're going to take the label coder
and we're going to fit and transform the x also the third row so we're going to take that third row we're going to set
it equal to a transformation and that transformation basically tells it that instead of having
a new york it has a 0 or 1 or a 2. and then finally we need to do a one
hot encoder which equals one hot ink or categorical features equals three and
then we take the x and we go ahead and do that equal to one hot encoder fit transform x to array this final
transformation preps our data force so it's completely set the way we need it as just a row of
numbers even though it's not in here let's go ahead and print x and just take a look what this data is
doing you'll see you have an array of arrays and then each array is a row of numbers and if i go ahead and just do row 0
you'll see i have a nice organized row of numbers that the computer now understands we'll go ahead and take this out there
because it doesn't mean a whole lot to us it's just a row of numbers next on setting up our data we have
avoiding dummy variable trap this is very important why because the computer has
automatically transformed our header into the setup and it's automatically transferring all these different variables so when we
did the encoder the encoder created two columns and what we need to do is just have the
one because it has both the variable and the name that's what this piece of code does here let's go ahead and paste this in here
and we have x equals x colon comma one colon all this is doing is removing that one
extra column we put in there when we did our one hot encoder and our label encoding let's go ahead and run that and now we
get to create our linear regression model and let's see what that looks like here and we're going to do that in two steps
the first step is going to be in splitting the data now whenever we create a predictive
model of data we always want to split it up so we have a training set and we have a testing set that's very
important otherwise we'd be very unethical without testing it to see how good our fit is and then we'll go ahead and create our
multiple linear regression model and train it and set it up let's go ahead and paste this next piece of code
in here and i'll go ahead and shrink it down a size or two so it all fits on one line so from the sklearn module selection
we're going to import train test split and you'll see that we've created four completely different variables
we have capital x train capital x test smaller case y train smaller case y
test that is the standard way that they usually reference these when we're doing
different models usually see that a capital x and you see the train and the test and the lowercase y what this
is is x is our data going in that's our rnd span our administration our marketing and then y which we're training is the
answer that's the profit because we want to know the profit of an unknown entity that's what we're going to shoot for in this
tutorial the next part train test split we take x and we take y we've already created
those x has the columns with the data in it and y has a column with profit in it and
then we're gonna set the test size equals point two that basically means twenty percent
so twenty percent of the rows are going to be tested we're gonna put them off to the side so since we're using a thousand lines of
data that means that 200 of those lines we're going to hold off to the side to test for later and then the random state equals zero
we're going to randomize which ones it picks to hold off to the side we'll go ahead and run this
it's not overly exciting so it's setting up our variables but the next step is the next step we actually create our
linear regression model now that we got to the linear regression model we get that next piece of the puzzle
let's go ahead and put that code in there and walk through it so here we go we're going to paste it in there and let's go ahead and
since this is a shorter line of code let's zoom up there so we can get a good look and we have from the sklearn
dot linear underscore model we're going to import linear regression now i don't know if you recall from
earlier when we were doing all the math let's go ahead and flip back there and take a look at that do you remember this
or we had this long formula on the bottom and we were doing all this summarization and then we also looked at setting it up
with the different lines and then we also looked all the way down to multiple linear regression where we're
adding all those formulas together all of that is wrapped up in this one section so what's going on here is i'm
going to create a variable called regressor and the regressor equals the linear regression that's a linear
regression model that has all that math built in so we don't have to have it all memorized or have to compute it
individually and then we do the regressor dot fit in this case we do x train and y train
because we're using the training data x being the data in and y being profit what we're looking at
and this does all that math for us so within one click and one line we've created the whole linear
regression model and we fit the data to the linear regression model and you can see that when i run the regressor
it gives an output linear regression it says copy x equals true fit intercept equals true in jobs equal
one normalize equals false it's just giving you some general information on what's going on with that regressor model
now that we've created our linear regression model let's go ahead and use it and if you remember we kept a bunch
of data aside so we're going to do a y predict variable and we're going to put in the x
test and let's see what that looks like scroll up a little bit paste that in here predicting the test
set results so here we have y predict equals regressor dot predict
x test going in and this gives us y predict now because i'm in jupiter inline i can
just put the variable up there and when i hit the run button it'll print that array out i could have just as easily done print
why predict so if you're in a different ide that's not an inline setup like the jupyter notebook you can do it this way print y
predict and you'll see that for the 200 different test variables we kept off to the side
is going to produce 200 answers this is what it says the profit are for those 200 predictions
but let's don't stop there let's keep going and take a couple look we're going to take just a short
detail here and calculating the coefficients and the intercepts this gives us a quick flash at what's
going on behind the line we're going to take a short detour here and we're going to be calculating the coefficient
and intercepts so you can see what those look like what's really nice about our regressor we created is it
already has a coefficients for us we can simply just print regressor dot coefficient
underscore when i run this you'll see our coefficients here and if we can do the regressor coefficient we can also do the regressor
intercept and let's run that and take a look at that this all came from the multiple regression model
and we'll flip over so you can remember where this is going into where it's coming from you can see the formula down here where
y equals m1 times x1 plus m2 times x2 and so on and so on plus c
the coefficient so these variables fit right into this formula y equals slope 1 times column 1 variable
plus slope 2 times column 2 variable all the way to the m into the n and x to the n plus c the
coefficient or in this case you have minus 8.89 to the power of 2 etc etc times the
first column and the second column and the third column and then our intercept is the minus one zero three zero zero nine point boy
it gets kind of complicated when you look at it this is why we don't do this by hand anymore this is why we have the
computer to make these calculations easy to understand and calculate now i told you that was a
short detour and we're coming towards the end of our script as you remember from the beginning i said if we're going to divide this
information we have to make sure it's a valid model that this model works and understand how good it works
so calculating the r squared value that's what we're going to use to predict how good our prediction is
and let's take a look what that looks like in code and so we're going to use this from sklearn.metrics we're going to
import r2 score that's the r squared value we're looking at the error so in the r2 score we take our y
test versus our y predict y test is the actual values we're testing that was the one that was given to us
that we know are true the y predict of those 200 values is what we think it was true and when we go
ahead and run this we see we get a 0.9352 that's the r2
score now it's not exactly a straight percentage so it's not saying it's 93 percent
correct but you do want that in the upper 90s o and higher shows that this is a very valid prediction based on
the r2 score and if r squared value of 0.91 or 92 as we got on our model
remember it does have a random generation involved this proves the model is a good model which means success yay we
successfully trained our model with certain predictors and estimated the profit of the companies using linear regression
all right what is logistic regression logistic regression is an algorithm for performing binary
classification so let's take an example and see how this works let's say your car has not
been serviced for quite a few years and now you want to find out if it is going to break down
in the future so this is like a classification problem find out whether your car will break
down or not so how are we going to perform this classification so here's how it looks
if we plot the information along the x and y axis x is the number of years since the last
service was performed and why is the probability of your car breaking down
and let's say this information was this data rather was collected from several car users it's
not just your car but several car users so that is our labeled data so the data has been collected and
for for the number of years and when the car broke down and what was the probability
and that has been plotted along x and y axis so this provides
an idea or from this graph we can find out whether your car will break down or not we'll
see how so first of all the probability can go from zero to one as you're aware
probability can be between zero and one and as we can imagine it is intuitive as
well as the number of years are on the lower side maybe one year two years or three years
till after the service the chances of your car breaking down are very limited right so for example
chances of your car breaking down the probability of your car breaking down within two years of your last
service are 0.1 probability similarly 3 years is maybe 0.3 and so on but as the number of
years increases let's say if it was six or seven years there is almost a certainty that your
car is going to break down that is what this graph shows so this is an example of a application of the classification
algorithm and we will see in little details how exactly logistic regression
is applied here one more thing needs to be added here is that the dependent variable's outcome is discrete so
if we are talking about whether the car is going to break down or not so that is a discrete value the y
that we are talking about the dependent variable that we are talking about what we are looking at is whether the
car is going to break down or not yes or no that is what we are talking about so here the outcome is discrete and not
a continuous value so this is how the logistic regression curve looks let me explain a little bit what exactly
how exactly we are going to determine the class at the outcome
rather so for a logistic regression curve a threshold has to be said saying that because this is a
probability calculation remember this is a probability calculation and the probability itself will not be 0 or
1 but based on the probability we need to decide what the outcome should be so there has to be a threshold
like for example 0.5 can be the threshold let's say in this case so any value of the probability below
0.5 is considered to be 0 and any value above 0.5 is considered to be 1. so an output of
let's say 0.8 will mean that the car will break down so that is considered as
an output of 1 and let's say an output of 0.29 is considered as 0 which means that the
car will not break down so that's the way logistic regression works now let's do a quick comparison
between logistic regression and linear regression because they both have the term regression in them so it can
cause confusion so let's try to remove that confusion so what is linear regression linear regression
is a process is once again an algorithm for supervised learning however here
you're going to find a continuous value you're going to determine a continuous value it could be
the price of a real estate property it could be your hike how much height you're going to get or it could be
a stock price these are all continuous values these are not discrete compared to a yes or no kind of a
response that we are looking for in logistic regression so this is one example of a linear regression let's say the hr
team of a company tries to find out what should be the salary hike of an employee so they
collect all the details of their existing employees their ratings and their salary hikes what has been given
and that is the labeled information that is available and the system learns from this it is
trained and it learns from this labeled information so that when a new employee's information is fed
based on the rating it will determine what should be the high so this is a linear regression problem and a linear
regression example now salary is a continuous value you can get five thousand five
thousand five hundred five thousand six hundred it is not discrete like a cat or a dog or an apple or a
banana these are discrete or a yes or no these are discrete values right so this way you're trying to find
continuous values is where we use linear regression so let's say just to extend on the scenario we now
want to find out whether this employee is going to get a promotion or not so we want to find out that is a
discrete problem right a yes or no kind of a problem in this case we actually cannot use linear regression
even though we may have labeled data so this is the label data so based on the employee rating
these are the ratings and then some people got the promotion and this is the ratings for
which people did not get promotion that is a no and this is the rating for which people
got promotion we just plotted the data about whether a person has got an employer's got promotion or not yes
no right so there is nothing in between and what is the employee's rating okay and ratings can be continuous that is
not an issue but the output is discrete in this case whether employee got promotion
yes no okay so if we try to plot that and we try to find a straight line this is
how it would look and as you can see it doesn't look very right because looks like there will be lot of errors
thus root mean square error if you remember for linear regression would be very very high and also the the values
cannot go beyond zero or beyond one so the graph should probably look somewhat like this
clipped at zero and one but still the straight line doesn't look right therefore instead of
using a linear equation we need to come up with something different and therefore the logistic regression
model looks somewhat like this so we calculate the probability and if we plot that probability not in
the form of a straight line but we need to use some other equation we will see very soon what that equation is
then it is a gradual process right so you see here people with some of these ratings are
not getting any promotions and then slowly uh at certain rating
they get promotion so that is a gradual process and this is how the math behind logistic regression
looks so we are trying to find the odds for a particular event happening and this is the formula
for finding the odds so the probability of an event happening divided by the probability of the event
not happening so p if it is the probability of the event happening probability of the person getting a
promotion and divided by the probability of the person not getting a promotion that is one minus p
so this is how you measure the odds now the values of the odds range from 0 to infinity so
when this probability is 0 then the odds will the value of the
odds is equal to 0 and when the probability becomes 1 then the value of the odds is 1 by
0 that will be infinity but the probability itself remains between 0 and 1. now this is how an equation of
a straight line looks so y is equal to beta0 plus beta1 x where beta0 is the y-intercept and beta1 is
the slope of the line if we take the odds equation and take a log of both sides then this
would look somewhat like this and the term logistic is actually derived from the
fact that we are doing this we take a log of px by 1 minus px
this is an extension of the calculation of odds that we have seen right and that is equal to beta0 plus beta1 x
which is the equation of the straight line and now from here if you want to find out the value of px you will
see we can take the exponential on both sides and then if we solve that equation we
will get the equation of px like this px is equal to 1 by
1 plus e to the power of minus beta0 plus beta1 x and recall this is nothing but the
equation of the line which is equal to y y is equal to beta0 plus beta 1 x so
that this is the equation also known as the sigmoid function and this is the
equation of the logistic regression all right and if this is plotted this is how the sigmoid curve
is obtained so let's compare linear and logistic regression how they
are different from each other let's go back so linear regression is solved or used to solve regression problems and
logistic regression is used to solve classification problems so both are called regression but linear
regression is used for solving regression problems where we predict continuous values
whereas logistic regression is used for solving classification problems where we
have had to predict discrete values the response variables in case of linear
regression are continuous in nature whereas here they are categorical or discrete in nature
and linear regression helps to estimate the dependent variable when there is a
change in the independent variable whereas here in case of logistic regression
it helps to calculate the probability or the possibility of a particular event happening and linear regression as the
name suggests is a straight line that's why it's called linear regression whereas logistic regression is a sigmoid
function and the curve is the shape of the curve is s it's an s-shaped curve this is another
example of application of logistic regression in weather prediction whether it's going to rain or not rain
now keep in mind both are used in weather prediction if we want to find the discrete values like whether it's
going to rain or not rain that is a classification problem we use logistic regression but
if we want to determine what is going to be the temperature tomorrow then we use linear regression so just keep in mind
that in weather prediction we actually use both but these are some examples of logistic regression so we want to find out
whether it's going to be rain or not it's going to be sunny or not it's going to snow or not these are
all logistic regression examples a few more examples classification of objects this is a
again another example of logistic regression now here of course one distinction is
that these are multi-class classification so logistic regression is not used in
its original form but it is used in a slightly different form so we say whether it is a dog or not a
dog i hope you understand so instead of saying is it a dog or a cat or an elephant we convert this into saying so because
we need to keep it to binary classification so we say is it a dog
or not a dog is it a cat or not a cat so that's the way logistic regression can be used for classifying
objects otherwise there are other techniques which can be used for performing multi-class classification in
healthcare logistic regression is used to find the survival rate of a patient so they
take multiple parameters like trauma score and age and so on and so
forth and they try to predict the rate of survival all right now finally let's take an
example and see how we can apply logistic regression to predict the number that is shown in the
image so this is actually a live demo i will take you into jupiter notebook
and show the code but before that let me take you through a couple of slides to explain
what we are trying to do so let's say you have an 8x8 image and there the image has a number one two
three four and you need to train your model to predict what this number is so how do we do this so the first thing
is obviously in any machine learning process you train your model so in this case we are using logistic
regression so and then we provide a training set to train the model and then we test how
accurate our model is with the test data which means that like any machine learning
process we split our initial data into two parts training set and test set with the training set we train our model
and then with the test set we test the model then we get good accuracy and then we use it for for
inference right so that is typical methodology of training testing and then deploying
of machine learning models so let's uh take a look at the code and see what we are doing so i will not
go line by line but just take you through some of the blocks so first thing we do is import all the libraries and then we
basically take a look at the images and see what is the total number of images
we can display using matplotlib some of the images are a sample of these images and then we split the data into
training and test as i mentioned earlier and we can do some exploratory analysis
and then we build our model we train our model with the training set and then we
test it with our test set and find out how accurate our model is using the
confusion matrix the heat map and use heat map for visualizing this and i will show you in the code what exactly
is the confusion matrix and how it can be used for finding the accuracy in our example we
got we get an accuracy of about 0.94 which is pretty good or 94 which is pretty good all right so what
is the confusion matrix this is an example of a confusion matrix and this is used for identifying the
accuracy of a classification model or like a logistic regression model
so the most important part in a confusion matrix is that first of all this as you can see
this is a matrix and the size of the matrix depends on how many outputs we are expecting right so
the most important part here is that the model will be most accurate when we have the maximum
numbers in its diagonal like in this case that's why it has almost 93 94 percent because the
diagonal should have the maximum numbers and the others other than
diagnose the cells other than the diagonals should have very few numbers so here that's what is
happening so there is a two here there are there's a one here but most of them
are along the diagonal this what does this mean this means that the number that has been fed is zero
and the number that has been detected is also zero so the predicted value and the actual
value are the same so along the diagonals that is true which means that let's let's take this diagonal right if
the maximum number is here that means that like here in this case it is 34 which
means that 34 of the images that have been fed or rather actually there are two misclassifications in there so
36 images have been fed which have number four and out of which 34 have been predicted
correctly as number four and one has been predicted as number eight and another one has been predicted as number
nine so these are two misclassifications okay so that is the meaning of saying
that the maximum number should be in the diagonal so if you have all of them so for an ideal model which
has let's say 100 accuracy everything will be only in the diagonal there will be no numbers
other than zero in all other cells so that is like a hundred percent accurate model okay so that's uh just of
how to use this matrix how to use this confusion matrix i know the name
is a little funny sounding confusion matrix but actually it is not very confusing
it's very straightforward so you are just plotting what has been predicted and what is the
labeled information or what is the actual data that's also known as the ground truth sometimes okay these are some fancy
terms that are used so predicted label and the actual name that's all it is okay yeah so we are showing a little bit
more information here so 38 have been predicted and here you will see that all of them have been predicted
correctly there have been 38 zeros and the predicted value and the actual value is exactly the same whereas
in this case right it has uh there are i think 37 plus five yeah 42
have been fed the images 42 images are of digit 3 and the accuracy
is only 37 of them have been accurately predicted three of them have been predicted as
number seven and two of them have been predicted as number eight and so on and so forth okay
all right so with that let's go into jupiter notebook and see how the code looks so this
is the code in in jupiter notebook for logistic regression in this
particular demo what we are going to do is train our model to recognize
digits which are the images which have digits from let's say 0 to 5 or 0 to 9 and
and then we will see how well it is trained and whether it is able to predict these numbers correctly or not so let's get
started so the first part is as usual we are importing some libraries that are required and then
the last line in this block is to load the digits so let's go ahead and run this
code then here we will visualize the shape of these digits so we can see
here if we take a look this is how the shape is 1797 by 64. these are like 8 by 8 images
so that's that's what is reflected in this shape now from here onwards we are basically once again
importing some of the libraries that are required like numpy and matplot and we will take a look at some of the
sample images that we have unloaded so this one for example creates a figure
and then we go ahead and take a few sample images to see how they look so let me run this
code and so that it becomes easy to understand so these are about five images sample images that we are
looking at zero one two three four so this is how the image is this is how the data is
okay and uh based on this we will actually train our logistic regression model and
then we will test it and see how well it is able to recognize so the way it
works is the pixel information so as you can see here this is an 8 by 8 pixel kind of a image and
the each pixel whether it is activated or not activated that is the information
available for each pixel now based on the pattern of this activation and non-activation of
the various pixels this will be identified as a zero for example right similarly as you can see so
overall each of these numbers actually has a different pattern of the pixel
activation and that's pretty much that our model needs to learn uh for which a
number what is the pattern of the activation of the pixels right so that is what we are going to train our
model okay so the first thing we need to do is to split our data into training
and test data set right so whenever we perform any training we split the data into
training and test so that the training data set is used to train the system so we pass this
probably multiple times and then we test it with the test data set and the
split is usually in the form of the and there are various ways in which you can split this data
it is up to the individual preferences in our case here we are splitting in the
form of 23 and 77 so when we say test size as 20 0.23
that means 23 percent of that entire data is used for testing and the remaining 77
percent is used for training so there is a readily available function which is
uh called train test split so we don't have to write any special code for the
splitting it will automatically split the data based on the proportion
that we give here which is test size so we just give the test size automatically training size will be determined and we pass the
data that we want to split and the the results will be stored in x underscore train and y underscore train
for the training data set and what is x underscore train these are these are the features right
which is like the independent variable and why underscore train is the label
right so in this case what happens is we have the input value which is or the features value which is in x underscore
train and since this is the labeled data for each of them each of the observations we already have
the label information saying whether this digit is a zero or a one or a two so that this this is what will be used
for comparison to find out whether the the system is able to recognize it
correctly or there is an error for each observation it will compare with this right so this is the label so the same
way x underscore train y underscore train is for the training data set x underscore test
y underscore test is for the test data set okay so let me go ahead and execute this code
as well and then we can go and check quickly what is the how many entries are there
and in each of this so x underscore train the shape is 13 83 by 64
and y underscore train has 1383 because there is nothing like the second part is not
required here and then x underscore test shape we see is 414 so actually there are 414
observations in test and 1383 observations in train so that's basically what these
four lines of code are are same okay then we import the logistic regression library and
which is a part of scikit learn so we we don't have to implement the logistic regression process itself we just call
this the function and let me go ahead and execute that so that we have the logistic regression
library imported now we create an instance of logistic regression right so
logistic regr is a is an instance of logistic regression and then we use that
for training our model so let me first execute this code so these two lines so the first line
basically creates an instance of logistic regression model and then the second line way is where we
are passing our data the training data set right this is our the the predictors and uh this is our
target we are passing this data set to train our model all right so once we do this in this
case the data is not large but by and large the training is what takes usually a lot of time so we spend in
machine learning activities and machine learning projects we spend a lot of time
for the training part of it okay so here the data set is relatively small so it was pretty quick so all right so now our
model has been trained using the training data set and we want to see how accurate this is
so what we'll do is we will test it out in probably faces so let me first try out how well
this is working for one image okay i will just try it out with one image my the first
entry in my test data set and see whether it is correctly predicting or not so
and in order to test it so for training purpose we use the fit method there is a method called fit
which is for training the model and once the training is done if you want to test for a particular value new
input you use the predict method okay so let's run the predict method
and we pass this particular image and we see that the shape is
or the prediction is 4. so let's try a few more let me see for the next 10 seems to be
fine so let me just go ahead and test the entire data set okay that's basically what we will do so
now we want to find out how accurately this has uh performed so we use the
score method to find what is the percentage of accuracy and we see here that it has performed up
to 94 percent to accurate okay so that's on this part now what we can also do
is we can um also see this accuracy using what is known as a confusion
matrix so let us go ahead and try that as well so that we can also
visualize how well this model has uh done so let me execute this piece of code
which will basically import some of the libraries that are required and we we basically create
a confusion matrix and instance of con confusion matrix by running confusion matrix and passing
these values so we have so this confusion underscore matrix method takes two
parameters one is the y underscore test and the other is the prediction so what is the y
underscore test these are the labeled values which we already know for the test data set and predictions
are what the system has predicted for the test data set okay so this is known to us and this is
what the system has the model has generated so we kind of create the confusion matrix
and we will print it and this is how the confusion matrix looks as the name suggests it is a
matrix and the key point out here is that the accuracy of the model is determined
by how many numbers are there in the diagonal the more the numbers
in the diagonal the better the accuracy is okay and first of all the total sum of
all the numbers in this whole matrix is equal to the number of observations
in the test data set that is the first thing right so if you add up all these numbers that will be equal to the
number of observations in the test data set and then out of that the maximum number of them
should be in the diagonal that means the accuracy is pretty good if the the numbers in the diagonal are
less and in all other places there are a lot of numbers uh which means the accuracy is very low
the diagonal indicates a correct prediction that this means that the actual value is same as the predicted value here
again actual values same as the predicted value and so on right so the moment you see a number here that
means the actual value is something and the predicted value is something else right similarly here the actual value is
something and the predicted value is something else so that is basically how we read the
confusion matrix now how do we find the accuracy you can actually add up the
total values in the diagonal so it's like 38 plus 44 plus 43 and so on
and divide that by the total number of test observations that will give you the percentage
accuracy using a confusion matrix now let us visualize this confusion matrix in a slightly
more sophisticated way uh using a heat map so we will create a heat map with some
we'll add some colors as well it's uh it's like a more visually visually more appealing so
that's the whole idea so if we let me run this piece of code and this is how the heat map looks uh and as you can see
here the diagonals again are all the values are here most of the values so which means
reasonably this seems to be reasonably accurate and yeah basically the accuracy score is 94 percent this is calculated
as i mentioned by adding all these numbers divided by the total test value so the total number of
observations in test data set okay so this is the confusion matrix for
logistic regression all right so now that we have seen the confusion
matrix let's take a quick sample and see how well the system has classified and we will take a few
examples of the data so if we see here we we picked up randomly a few of them
so this is uh number four which is the actual value and also the predicted value both are four
this is an image of zero so the predicted value is also zero actual value is of course zero then this
is the image of nine so this has also been predicted correctly nine and actual value is nine and this
is the image of one and again this has been predicted correctly as like the actual value okay so this was a
quick demo of logistic regression how to use logistic regression to identify images need for confusion
matrixes classification models have multiple output categories most error measures will tell us the
total error in our model but we cannot use it to find out individual instances of errors in our
models so you have your input coming in you have your classifier it measures the error and it says oh 53
of these are correct but we don't know which 53 percent are correct is it 53
correct uh on guessing on the spam is it 23 guessing on spam and 27
guessing on what's not spam this is where the confusion matrix comes in so
during the classification we also have to overcome the limitations of accuracy accuracy can be misleading for
classification problems if there is a significant class imbalance
a model might predict the majority class for all cases and have a high accuracy score
and so you can see here we have our email coming in and there's two spams the classifier comes in and goes hey it
only catches one of those spams and it misclassifies one that's not spam so our model predicted eight out of ten
instance and will have an accuracy of 80 percent but is it classifying correctly
a confusion matrix represents a table layout of the different outcomes of prediction and results of a
classification problem and helps visualize its outcomes and so you see here we have our simple
chart predicted and actual the confusion matrix helps us
identify the correct predictions of a model for different individual classes as well as the errors so you'll see here
that the values predicted by our classifier are along the rows this is what we're going to guess it is or our model is guessing what this is
based on its training so we've already trained the model to guess whether it's spam or not spam or
whatever it is you're working on and then the actual values of our data set are along the columns
so this is the actual value that's supposed to be people who can speak english will be
classified as positives so because they have a remember 001. do you speak english yes
no and you could extend this that they might have do you speak french do you speak whatever languages
and so you might have a whole lot of classifiers that you would look at each one of these people who cannot speak english will be
classified as negatives so there'll be a zero so you know zero ones the number of times our actual positive
values are equal to predicted positive values gives us true positive tp
the number of times our actual negative values are equal to predictive negative values gives us true negative t in
the number of times our model wrongly predicts negative values as positives
gives us a false positive fp and you'll see when you're working
with these a lot you know memorizing that is false positive you can easily figure out what that is and pretty soon you're just
looking at the fp or the tp depending on what you're working on and the number of times our model wrongly predicts
positive values as negatives gives us a false negative fp
now i'm going to do a quick step out here let's say you're working in the medical
and we're talking about cancer do you really want
a bunch of false negatives you want zero under false negative
so when we look at this confusion matrix if you have five percent false positives
and five percent false negatives it'd be much better to even have twenty percent false positives because
they go in and test it and zero false negatives the same might be true
if you're working on uh say a car driving is this a safe place for the car to go
well you really don't want any false positives you know yes this is safe right over the cliff
so again when you're working on the project or whatever it is you're working on this chart suddenly has huge value
we were talking about spam email how many important emails say from your banking
overdraft charge coming in that you want to be a a true a false
negative you don't want it to go in the spam folder likewise you want to get as much of the spam out of there but you don't want to
miss anything really important confusion matrix metrics are performance
measures which help us find the accuracy of our classifier there are four main metrics accuracy
precision recall and f1 score the f1 score is the one i usually hear
the most and accuracy is usually what you put on your chart when you're sending in front of the
shareholders how accurate is it people understand accuracy f1 score is a little bit more
on the math side and so you got to be a little careful when you're quoting f1 scores in the
when you're sitting there with all the shareholders because a lot of them will just glaze over so confusion matrix metrics are
performance measures which help us find the accuracy of our classifier there are four main metrics
accuracy the accuracy is used to find the portion of the correctly classified values
it tells us how often our classifier is right it is the sum of all true values divided
by the total values and this makes sense again it's one of
those things i don't want to fall you know what depends on what you're looking for
are you looking for not to miss any spam mails are you looking to drive down the road
and not run anybody over precision is used to calculate the model's ability to classify
positive values correctly it answers the question when the model predicts a positive value
how often is it right it is the true positive divided by the total number of predicted positive
values again this one depends on what project you're working on
whether this is what you're going to be focusing on so recall it is used to calculate the
model's ability to predict positive values how often does the model actually
predict the correct positive values it is the true positives divided by the total number of
actual positive values and then your f1 score it is the harmonic mean of recall and
precision it is useful when you need to take both precision and recall into account
consider the following two confusion matrix derived from two different classifier
to figure out which one performs better we can find the confusion matrix for both of
them and you can see we're back to does it classify whether they can speak english or or
non-speaker they speak some they don't know the english language and so we put these two
confusion matrixes out here we can go ahead and do the math behind that we can look up the accuracy that's a t p n plus t n over the t f
plus t n plus f p plus f n and so we get an accuracy of 0.8125
and we have a precision if you do the precision which is your tp truth positive over tp plus
fp we get 0.891 and if we do the recall we'll end up
with the 0.825 that's your tp over tp plus fn and then of course your f1 score which
is 2 times precision times recall over precision plus recall and we get
the 0.857 and if we do that um with another model let's say we had two different models
and we're trying to see which one we want to use for whatever reason
we might go ahead and compute the same things we have our accuracy our precision and our recall and our f1
score and as we're looking at this we might look at the accuracy because that's really what we're
interested in is how many people are we able to classify as being able to
speak english i really don't want to know if i'm you know i i really don't want to know if they're non-speakers i'd rather miss 10
people speaking english instead of 15. and so you can see from these charts we'd probably go with the
first model because it does a better job guessing who speaks english and has a higher accuracy because in this case
that is what we're looking for so uh with that we'll go ahead and pull up a demo so you can see what this looks
like in the python setup in in the actual coding for this we'll go into anaconda
navigator if you're not familiar with anaconda it's a really good tool to use as far as doing display
and demos and for quick development as a data scientist i just love the
package now if you're going to do something heavier lifting there's some limitations with anaconda and with the setup
and generally you can do just about anything in here with your python and for this we'll go with jupiter
notebook uh jupiter lab is the same as jupiter notebook you'll see they now have integration with pi charm if you work in pi charm
uh certainly there's a lot of other integrations that anaconda has and we've opened up
i simply learned files i work on and create a new file called confusion matrix demo
and the first thing we want to note is the data we're working with here i've opened it up in a wordpad or
notepad or whatever you can see it's got a row of headers
comma separated and then all the data going down below and then i save this in the same file so
i don't have to remember what path i'm working on of course if you have your data separated and you're working with a lot
of data you probably want to put it into a different folder or file depending on what you're doing and the first thing we're going to do is
go ahead and import our tools we're going to use the pandas that's our data frame
if you haven't had a chance to work with the data frame please review panda's data frame going to simply learn
you can pull up the pandas data frame tutorial on there and then we're going
to use uh the scikit framework which is all denoted as sklearn and i can just pull this in you
can see here's the scikit learn.org with the stable version
that you can import into your python and from here we're going to use the
train test split for splitting our data we're going to do some pre-processing
we're going to do use the logistic regression model that's our actual machine learning model
we're using and then what this court this particular setup is about is we're going to do the accuracy score
the confusion matrix and the classifier report so let me go ahead and run that and
bring all that information in and just like we open the file we need
to go ahead and load our data in here so we're going to go ahead and do our pandas read csv
and then just because we're in jupyter notebook we can just put data to read the data in here a lot of times we'll actually let me
just do this i prefer to do the just the head of the date or the top part and you can see we have
age sex i'm not sure what cp stands for test bps cholesterol so a lot of
different measurements if you were in this domain you want to know what all these different
measurements mean i don't want to focus on that too much because when we're talking
about data science a lot of times you have no idea what the data means if you've ever looked up the breast
cancer measurement it's just a bunch of measurements and numbers unless you're a doctor you have no idea
what those measurements mean but if it's your specialty in your domain you better know them so we're going to
go ahead and create y and it's going to we're going to set it equal to the target uh so here's our target value here
and there it's either one or zero so we have a classifier if you're
dealing with one zero true false what do you have you have a classifier
and then our x is going to be everything except for the target
so we're going to go ahead and drop the target axis equals 1. remember that's columns versus the
index or rows axis equals 0 would would give you an error but you would drop like
row 2. and then we'll go ahead and just print that out so you can see what we're looking at and here we have y data x data
and you can see from the x data we have the x head and we can go ahead and just do print
the y head data and run that
so this is all loading the data that we've done so far if there's a confusion in there go back
and rewind the tape and review it and then we need to go ahead and split our data into our
x train x test y train y test and then keep in mind you always
want to split the data before we do the scalar and the reason is is that
you want the scalar on the training data to be set on the training data data or fit to it but not on the test
data think of this as being out in the field you're not you could actually alter your
results so it's always important to do make sure whatever you do to the training data
or whatever fit you're doing is always done on the training not on the test and then we want to go
ahead and scale the data now we are working with a linear regression model and i'll
mention this here in a minute when we get to the actual model uh so some sometimes you don't need to
scale when you're working with linear regression models it's not going to change your result as much as say a neural network where it has a huge
impact but we're going to go ahead and take here's our x train x test y train y test we create our
scalar we go ahead and scale the scale is going to fit the x train and then we're going to go ahead
and take our x train and transform it and then we also need to take our x test
and transform it based on the scale on here so that our x is now between that nice minus one to one and
so this is all uh our pre data setup and hopefully uh all of that looks
fairly familiar to you if you've done a number of our other classes and you're up to the setup on here and then we want to go
ahead and do is create our model and we're going to use the logistic regression model and from the
logistic regression model we're going to go ahead and fit our x train and y train and then we'll run our predicted value
on here and let's go ahead and run that and so now
we are we actually have like our x test and our prediction so if you remember
from our matrix we're looking for the actual versus the prediction
and how those compare and if i take us back up here you're going
to notice that we imported the accuracy score the confusion matrix and the
classification report and there's of course our logistic regression the model we're using for
this and i did mention is going to talk a little bit about scalar and the regression model
the scalar on a lot of your regression models your basic mass standard regression models
and i'd have to look it up for the logistic regression model when you're using a standard regression model you don't need to scale the data
it's already just built in by the way the model works in most cases uh but if you're in a
neural network and you're there's a lot of other different setups then you really want to take this and fit that on there and so
we can go in and do the accuracy and this is if you remember correctly we were looking at the accuracy
with the english speaking uh so this is saying our accuracy as to whether this person
is i believe this is the heart data set
it's going to be accurate about 85 percent of the time as far as whether it's going to predict the person's going
to have a heart condition or the one as it comes up with the zero one on there
which would mean at this point that you have an 85 percent uh being correct on telling someone they're extremely high
risk for a heart attack kind of thing and so we want to go ahead and create our confusion matrix
and let me just do that of course the software does everything
for us so we'll go ahead and run this and you can see right here here's our 25
prediction correct predictions right here and if you remember from our slide i'll
just bring this over says a nice visual we have our true positive false positive so we had 25 which were
true that it said hey this person is going to be high risk at heart and we had four that were still high
risk that it said were false so out of these 25 people or out of
these 29 people and that makes sense because you have 0.85 out of 29 people it was correct on 25 of
them and so uh here's our accuracy score we were just looking at that our accuracy is your true positive and
your true negative over all of them so how true is it there is our accuracy coming up here 0.85 and then we have our
nice matrix generated from that and you can see right here is a similar matrix we had going for the from the
slide and this starts to this should start asking questions at this point so if you're in a board meeting or
you're working with this you really want to start looking at this data here and saying well is this
good enough is this number of people and hopefully you'd have a much larger data set
it might is my confusion matrix showing for the true positive and false positive is that acceptable for
what we're doing uh and of course if you're going to put together whatever data you're putting out you might want to separate the
true negative false positive false negative true positive and you can simply do that by doing the
confusion matrix and then of course the ravel part lets you set that up so you can just split
that right up into a nice tuple and the final thing we want to show you here in
the coding on this part is the confusion matrix metrics
and so we can come in here and just use the matrix equals classification report
the y test and the predict and then we're going to take that classification report and go ahead and print that out and you
can see here it does a nice job giving you your accuracy your micro
average your weighted average you have your precision your recall your f1 score and your
support all in one window so you can start looking at this data and saying oh okay our precision's at .83
0.87 for getting a a positive and 0.83 for the negative side for a zero
and we start talking about whether this is a valid information or not to use
and when we're looking at a heart attack prediction we're only looking at one aspect what's the chances of this
person having a heart attack or not um you might have something where we went back to the languages
maybe you also want to know whether they speak english or hindi or french and you can see right
here that we can now take our confusion matrix and just expand it as big as we need to depending
on how many different classifiers we're working on what is a decision tree let's go through
a very simple example before we dig in deep decision tree is a tree shaped diagram
used to determine a course of action each branch of the tree represents a possible decision or current
or reaction let's start with a simple question how to identify a random vegetable from a shopping bag so we have this
group of vegetables in here and we can start off by asking a simple question is it red and if it's not
then it's going to be the purple fruit to the left probably an eggplant if it's true it's going to be one of the
red fruits is the diameter greater than two if false is going to be a what looks
to be a red chili and if it's true it's going to be a bell pepper from the capsicum family
so it's a capsicum problems that decision tree can solve so let's look at
the two different categories the decision tree can be used on it can be used on the classification the
true false yes no and it can be used on regression where we figure out what the next value is in
a series of numbers or a group of data in classification the classification tree will determine
a set of logical if-then conditions to classify problems for example discriminating
between three types of flowers based on certain features in regression a regression tree is used when the
target variable is numerical or continuous in nature we fit the regression model to the target variable
using each of the independent variables each split is made based on the sum of squared error
before we dig deeper into the mechanics of the decision tree let's take a look at the advantages of
using a decision tree and we'll also take a glimpse at the disadvantages the first thing you'll notice is that
it's simple to understand interpret and visualize it really shines here because you can see exactly what's
going on in a decision tree little effort is required for data preparation so you don't have to do
special scaling there's a lot of things you don't have to worry about when using a decision tree it can handle
both numerical and categorical data as we discovered earlier and non-linear parameters
don't affect its performance so even if the data doesn't fit an easy curved graph you can still use
it to create an effective decision or prediction if we're going to look at the advantages
of a decision tree we also need to understand the disadvantages of a decision tree the first disadvantage is overfitting
overfitting occurs when the algorithm captures noise in the data that means you're solving for one specific
instance instead of a general solution for all the data high variance the model can get unstable due to small
variation in data low bias tree a highly complicated decision tree tends to have a low bias
which makes it difficult for the model to work with new data decision tree important terms before we
dive in further we need to look at some basic terms we need to have some definitions to go with our decision
tree in the different parts we're going to be using we'll start with entropy entropy is a measure of randomness or
unpredictability in the data set for example we have a group of animals in this
picture there's four different kinds of animals and this data set is considered to have a high entropy you
really can't pick out what kind of animal it is based on looking at just the four animals as a big clump of uh entities so as we
start splitting it into subgroups we come up with our second definition which is information
gain information gain it is a measure of decrease in entropy after the data set is split so in this
case based on the color yellow we've split one group of animals on one side as true and those who aren't yellow
as false as we continue down the yellow side we split based on the height true or false equals 10 and on the other
side height is less than 10 true or false and as you see as we split it the entropy continues to be less and
less and less and so our information gain is simply the entropy e1 from the top and how it's changed to
e2 in the bottom and we'll look at the deeper math although you really don't need to know a huge amount of math when
you actually do the programming in python because they'll do it for you but we'll look on the actual math of how they compute
entropy finally we went on the different parts of our tree and they call the leaf node leaf node carries the classification or
the decision so it's the final end at the bottom the decision node has two or more branches
this is where we're breaking the group up into different parts and finally you have the root node the topmost decision node is known as the
root node how does a decision tree work wonder what kind of animals i'll get in the
jungle today maybe you're the hunter with the gun or if you're more into photography you're a photographer with a camera so
let's look at this group of animals and let's try to classify different types of animals based on their features using a decision
tree so the problem statement is to classify the different types of animals based on their features using a decision
tree the data set is looking quite messy and the entropy is high in this case so let's look at a training set or a
training data set and we're looking at color we're looking at height and then we have our different
animals we have our elephants our giraffes our monkeys and our tigers and they're of different colors and
shapes let's see what that looks like and how do we split the data we have to frame the conditions that
split the data in such a way that the information gain is the highest note gain is a measure of decrease in
entropy after splitting so the formula for entropy is the sum that's what this symbol looks like that
looks like kind of like a e funky e of k where i equals 1 to k
k would represent the number of animals the different animals in there where value or p value of i
would be the percentage of that animal times the log base two of the same the percentage of
that animal let's try to calculate the entropy for the current data set and take a look at what that looks like
and don't be afraid of the math you don't really have to memorize this math just be aware that it's there and this
is what's going on in the background and so we have three giraffes two tigers one monkey
two elephants a total of eight animals gathered and if we plug that into the formula we get an entropy that equals three over
eight so we have three giraffes a total of eight times the log usually they use base two on the log so
log base two of three over eight plus in this case it says yellow fence two over eight two elephants over total
of eight times log base two two over eight plus one monkey over total of eight
log base two one over eight and plus two over eight of the tigers log base two
over eight and if we plug that into our computer or calculator i obviously can't do logs in
my head we get an entropy equal to 0.571 the program will actually calculate the
entropy of the data set similarly after every split to calculate the gain now we're not going to go
through each set one at a time to see what those numbers are we just want you to be aware that this is a
formula or the mathematics behind it gain can be calculated by finding the difference of the subsequent entropy
values after a split now we will try to choose a condition that gives us the highest gain we will do that by splitting the
data using each condition and checking that the gain we get out of them the condition that gives us the highest gain will be used to make the
first split can you guess what that first split will be just by looking at this image as a human is probably pretty easy to
split it let's see if you're right if you guessed the color yellow you're correct let's say the condition that
gives us the maximum gain is yellow so we will split the data based on the color yellow if it's true that group of animals goes
to the left if it's false it goes to the right the entropy after the splitting has to
decreased considerably however we still need some splitting of both the branches to attain an entropy value
equal to zero so we decide to split both the nodes using height as the condition since every branch now contains single label type we
can say that entropy in this case has reached the least value and here you see we have the giraffes the tigers the monkey and the
elephants all separated into their own groups this tree can now predict all the classes of animals present in the data set with 100
accuracy that was easy use case loan repayment prediction let's get into
my favorite part and open up some python and see what the programming code in the scripting looks like in here we're going to want
to do a prediction and we start with this individual here who's requesting to find out how good his customers are
going to be whether they're going to repay their loan or not for his bank and from that we want to generate a problem statement
to predict if a customer will repay loan amount or not and then we're going to be using the decision tree algorithm in python
let's see what that looks like and let's dive into the code in our first few steps of implementation
we're going to start by importing the necessary packages that we need from python and we're going to load
up our data and take a look at what the data looks like so the first thing i need is i need something to edit my python and run it
in so let's flip on over and here i'm using the anaconda jupiter notebook now you can use any
python ide you like to run it in but i find the jupyter notebook's really nice for doing things on the fly
and let's go ahead and just paste that code in the beginning and before we start let's talk a little bit about what we're
bringing in and then we're going to do a couple things in here we have to make a couple changes as we go through this first part of the import
the first thing we bring in is numpy as np that's very standard when we're dealing with
mathematics especially with very complicated machine learning tools you almost always see the numpy come in for
your num your number it's called number python it has your mathematics in there in this case we actually could take it out but
generally you'll need it for most of your different things you work with and then we're going to use pandas as pd
that's also a standard the pandas is a data frame setup and you can liken this to
taking your basic data and storing it in a way that looks like an excel spreadsheet so as we come
back to this when you see np or pd those are very standard uses you'll know that that's the pandas and
i'll show you a little bit more when we explore the data in just a minute then we're going to need to split the data so i'm going to bring in our train
test and split and this is coming from the sk learn package cross validation
in just a minute we're going to change that and we'll go over that too and then there's also the sk.tree import
decision tree classifier that's the actual tool we're using remember i told you don't be afraid of the mathematics it's going to be done
for you well the decision tree classifier has all that mathematics in there for you so you don't have to figure it back out
again and then we have sklearn.metrics for accuracy score we need to score our
our setup that's the whole reason we're splitting it between the training and testing data and finally we still need the sklearn
import tree and that's just a basic tree function is needed for the decision tree classifier
and finally we're gonna load our data down here and i'm gonna run this and we're gonna get two things on here
one we're gonna get an error and two we're gonna get a warning let's see what that looks like so the first thing we had is we have an
error why is this error here well it's looking at this it says i need to read a file and when this was
written the person who wrote it this is their path where they stored the file
so let's go ahead and fix that and i'm going to put in here my file
path i'm just going to call it full file name and you'll see it's on my c drive and this very lengthy
setup on here where i stored the data2.csv file don't worry too much about the full path
because on your computer will be different the data.2 csv file was generated
by simplylearn if you want a copy of that you can comment down below and request it here in the youtube
and then if i'm going to give it a name full file name i'm going to go ahead and change it here
to full file name so let's go ahead and run it now and see what happens
and we get a warning when you're coding understanding these
different warnings and these different errors that come up is probably the hardest lesson to learn
so let's just go ahead and take a look at this and use this as a opportunity to understand what's going
on here if you read the warning it says the cross validation is depreciated so it's a warning on it's
being removed and it's going to be moved in favor of the model selection so if we go up here we have sklearn dot
cross validation and if you research this and go to the sklearn site you'll find out that you
can actually just swap it right in there with model selection
and so when i come in here and i run it again that removes a warning what they've done
is they've had two different developers develop it in two different branches and then they decided to keep one of
those and eventually get rid of the other one that's all that is and very easy and quick to fix
before we go any further i went ahead and opened up the data from this file remember the the data
file we just loaded on here the data underscore2.csv let's talk a little bit more about that and see what that looks
like both as a text file because it's a comma separated variable file and in a spreadsheet
this is what it looks like as a basic text file you can see at the top they've created a header and it's got one two three four five
columns and each column has data in it and let me flip this over because we're also going to look at this uh
in an actual spreadsheet so you can see what that looks like and here i've opened it up in the open office calc
which is pretty much the same as excel and zoomed in and you can see we've got our columns
and our rows of data a little easier to read in here we have a result yes yes no we have initial payment last
payment credit score house number if we scroll way down
we'll see that this occupies a thousand and one lines of code or lines of data
with the first one being a column and then 1 000 lines of data
now as a programmer if you're looking at a small amount of data i usually start by pulling it up in different sources so
i can see what i'm working with but in larger data you won't have that option it'll just be
too too large so you need to either bring in a small amount that you can look at it like we're doing right now
or we can start looking at it through the python code so let's go ahead and move on and take the next couple steps to explore the
data using python let's go ahead and see what it looks like in python to print the length and
the shape of the data so let's start by printing the length of the database we can use a simple lin function from
python and when i run this you'll see that it's a thousand long and that's what we
expected there's a thousand lines of data in there if you subtract the column head this is one of the nice
things when we did the balance data from the panda read csv
you'll see that the header is row 0 so it automatically removes a row
and then shows the data separate it does a good job sorting that data out for us
and then we can use a different function and let's take a look at that and again we're going to utilize the
tools in panda and since the balance underscored data was loaded as a panda data frame
we can do a shape on it and let's go ahead and run the shape and see what that looks like
what's nice about this shape is not only does it give me the length of the data we have a thousand lines it also tells me there's five columns so
we were looking at the data we had five columns of data and then let's take one more step to explore the data using python
and now that we've taken a look at the length and the shape let's go ahead and use the pandas
module for head another beautiful thing in the data set that we can utilize so let's put that on our sheet here and
we have print data set and balance data dot head and this is a
pandas print statement of its own so it has its own print feature in there and then we went ahead and gave a label
for our print job here of dataset just a simple print statement and we run that let's just take a closer
look at that let me zoom in here there we go pandas do such a wonderful
job of making this a very clean readable data set so you can look at the
data you can look at the column headers you can have it when you put it as the head it prints
the first five lines of the data and we always start with zero so we have five lines we have zero one two three
four instead of one two three four five that's a standard scripting and programming set as you wanna start with
the zero position and that is what the data head does it pulls the first five rows of data puts in a nice format that you can look
at and view very powerful tool to view the data so instead of having to flip and open up an excel
spreadsheet or open office cal or trying to look at a word doc where it's all scrunched
together and hard to read you can now get a nice open view of what you're working with we're working with a
shape of a thousand long five wide so we have five columns and we do the full data head you can
actually see what this data looks like the initial payment last payment credit scores house number
so let's take this now that we've explored the data and let's start digging into the decision tree so in our next step we're going to
train and build our data tree and to do that we need to first separate the data out
we're going to separate into two groups so that we have something to actually train the data with and
then we have some data on the side to test it to see how good our model is remember with any of the machine learning you always want to have some
kind of test set to weigh it against so you know how good your model is when you distribute it let's go ahead and
break this code down and look at it in pieces so first we have our x and y where do x and y come from
well x is going to be our data and y is going to be the answer or the target you can look at
source and target in this case we're using x and y to denote the data in
and the data that we're actually trying to guess what the answer is going to be and so to separate it we can simply put in
x equals the balance of the data.values the first brackets means that we're going to
select all the lines in the database so it's all the data and the second one says we're only going to
look at columns one through five remember i always start with zero zero is a yes or no
and that's whether the loan went default or not so we want to start with one if we go back up here that's the initial
payment and it goes all the way through the house number well if we want to look at one through
five we can do the same thing for y which is the answers and we're going to set that just equal
to the zero row so it's just the zero row and then it's all rows going in there so now we've
divided this into two different data sets one of them with the
data going in and one with the answers
next we need to split the data and here you'll see that we have it split into
four different parts the first one is your x training your x test your y train
your y test simply put we have x going in where
we're going to train it and we have to know the answer to train it with and then we have x test where we're going to test that
data and we have to know in the end what the y was supposed to be
and that's where this train test split comes in that we loaded earlier in the modules this does it all for us and you can see
they set the test size equal to 0.3 so that's roughly 30 percent will be used in the test and then we use a
random state so it's completely random which rows it takes out of there and then finally we get to actually
build our decision tree and they've called it here clf underscore entropy that's the actual decision tree or
decision tree classifier and in here they've added a couple variables which we'll explore in just a
minute and then finally we need to fit the data to that so we take our clf entropy that we
created and we fit the x train and since we know the answers for x trade or the y train we go ahead and put those in and let's
go ahead and run this and what most of these sklearn modules do is when you set up the variable in this
case we set the clf entropy called decision tree classifier it automatically prints out what's in that
decision tree there's a lot of variables you can play within here and it's quite beyond the scope of this tutorial to go through all of
these and how they work but we're working on entropy that's one of the options we've added that it's completely a random state
of 100 so 100 percent and we have a max depth of 3. now the max depth if you remember above
when we were doing the different graphs of animals means it's only going to go down three layers before it stops
and then we have minimal samples of leaves is five so it's going to have at least five leafs at the end so i'll have
at least three splits i'll have no more than three layers and at least five end leaves with the final result at
the bottom now that we've created our decision tree classifier not only created it but
trained it let's go ahead and apply it and see what that looks like so let's go ahead and make a
prediction and see what that looks like we're going to paste our predict code in here
and before we run it let's just take a quick look at what it's doing here we have a variable y predict that we're
going to do and we're going to use our variable clf entropy that we created and then you'll
see dot predict and it's very common in the sk learn modules that their different tools have to
predict when you're actually running a prediction in this case we're going to put our x test data in here
now if you delivered this for use in actual commercial use and distributed it this would be the new
loans you're putting in here to guess whether the person is going to be uh pay them back or not
in this case so we need to test out the data and just see how good our sample is how good of our tree does at
predicting the loan payments and finally since anaconda jupiter notebook is it works as a command line
for python we can simply put the y predict e in to print it i could just as easily have put the print
and put brackets around y predict en to print it out we'll go ahead and do that it doesn't matter which way you do it
and you'll see right here that runs a prediction this is roughly 300 in here remember it's 30 percent of
a thousand so you should have about 300 answers in here and this tells you which each one of
those lines of our test went in there and this is what our y predict came out
so let's move on to the next step we're going to take this data and try to figure out just how good a model we have so here we go since
sklearn does all the heavy lifting for you and all the math we have a simple line of code to let us
know what the accuracy is and let's go ahead and go through that and see what that means and what that looks like let's go ahead and paste this
in and let me zoom in a little bit there we go so you have a nice full picture and
we'll see here we're just going to do a print accuracy is and then we do the accuracy score
and this was something we imported earlier if you remember at the very beginning let me just scroll up there
real quick so you can see where that's coming from that's coming from here down here from sklearn.metrics
import accuracy score and you could probably run a script make your own script to do this very easily
how accurate is it how many out of 300 did we get right and so we put in our y test that's the
one we ran the predict on and then we put in our y predict e n that's the answers we got
and we're just going to multiply that by a hundred because this is just going to give us an answer as a decimal and we want to see it as a percentage
and let's run that and see what it looks like and if you see here we got an accuracy of 93.66667
so when we look at the number of loans and we look at how good our model fit we can tell people it has about a 93.6
fitting to it so just a quick recap on that we now have accuracy set up on here and
so we have created a model that uses the decision tree algorithm to predict whether a customer will repay the loan
or not the accuracy of the model is about 94.6 percent the bank can now use this model to
decide whether it should approve the loan request from a particular customer or not and so this information is really
powerful we may not be able to as individuals understand all these numbers because they have thousands of numbers that come
in but you can see that this is a smart decision for the bank to use a tool like this to help them to
predict how good their profit's going to be off of the loan balances and how many are going to
default or not how does a random forest work as a whole so to begin our random forest classifier
let's say we already have built three trees and we're gonna start with the first tree that looks like this
just like we did in the example this tree looks at the diameter if it's greater than or equal to three
it's true otherwise it's false so one side goes to the smaller diameter one side goes to larger
diameter and if the color is orange it's going to go to the right true we're using oranges now instead of lemons
and if it's red it's going to go to the left false we build a second tree very similar but split differently instead of
the first one being split by a diameter this one when they created it if you look at that first bowl it has a lot of
red objects so it says is the color red because that's going to bring our entropy down
the fastest and so of course if it's true it goes to the left if it's false it goes to the right and
then it looks at the shape false or true and so on and so on and tree three is a diameter equal to one
and it came up with this because there's a lot of cherries in this bowl so that would be the biggest split on there is is the diameter equal to one
that's going to drop the entropy the quickest and as you can see it splits it into true if it goes false and they've added
another category does it grow in the summer and if it's false it goes off to the left if it's
true it goes off to the right let's go ahead and bring these three trees you can see them all in one image
so this would be three completely different trees categorizing a fruit and let's take a fruit now let's try
this and this fruit if you look at it we've blackened it out you can't see the color on it so it's missing data remember one
of the things we talked about earlier is that a random forest works really good if you're missing data if you're
missing pieces so this fruit has an image but maybe the person had a black and white camera when they took the picture
and we're going to take a look at this and it's going to have they put the color in there so ignore the color down there but the diameter
equals three we find out it grows in the summer equals yes and the shape is a circle and if you go to the right you can look
at what one of the decision trees did this is the third one is the diameter greater than equal to three
is the color orange well it doesn't really know on this one but if you look at the value it'd say true and go to the
right tree two classifies it as cherries is the color equal red is the shape a circle true it is a
circle so this would look at it and say oh that's a cherry and then we go to the other classifier and it says
is the diameter equal one well that's false does it grow in the summer true so it
goes down and looks at as oranges so how does this random forest work the first one says
it's an orange the second one said it was a cherry and the third one says it's an orange
and you can guess if you have two oranges and one says it's a cherry when you add that all together
the majority of the vote says orange so the answer is it's classified as an orange even though
we didn't know the color and we're missing data on it i don't know about you but i'm getting tired of fruit so let's switch
and i did promise you we'd start looking at a case example and get into some python coding today we're going to use the case
the iris flower analysis this is the exciting part as we roll up our sleeves
and actually look at some python coding before we start the python coding we need to go ahead and create a problem
statement wonder what species of iris do these flowers belong to let's try to predict the species of the
flowers using machine learning in python let's see how it can be done so here we begin to go ahead and implement
our python code and you'll find that the first half of our implementation is all about
organizing and exploring the data coming in let's go ahead and take this first step which is loading the
different modules into python and let's go ahead and put that in our favorite editor whatever your favorite editor is
in this case i'm going to be using the anaconda jupiter notebook which is one of my
favorites certainly there's notepad plus plus and eclipse and dozens of others or just
even using the python terminal window any of those will work just fine to go ahead and explore
this python coding so here we go let's go ahead and flip over to our jupyter notebook and i've already opened up a new page
for python 3 code and i'm just going to paste this right in there and let's take a look and see what we're bringing into our python the
first thing we're going to do is from the sklearn.datasets import load iris now this isn't the
actual data this is just the module that allows us to bring in the data the load iris
and the iris is so popular it's been around since 1936 when ronald fisher published a paper on it and they're
measuring the different parts of the flower and based on those measurements predicting what kind of flower it is and
then if we're going to do a random forest classifier we need to go ahead and import a random forest classifier from the sk learn
module so sklearn.ensemble import random fourth classifier and then we want to bring in two more
modules and these are probably the most commonly used modules in python and data science
with any of the other modules that we bring in and one is going to be pandas we're going to import pandas as pd
pd is a common term used for pandas and pandas is basically creates a data format for us where
when you create a pandas data frame it looks like an excel spreadsheet and you'll see that
in a minute when we start digging deeper into the code panda is just wonderful because it plays nice with all the other modules in there
and then we have numpy which is our numbers python and the numbers python allows us to do
different mathematical sets on here we'll see right off the bat we're going to take our np
and we're going to go ahead and seed the randomness with it with 0. so np.random.seed is seating that is 0. this code doesn't
actually show anything we're going to go ahead and run it because i need to make sure i have all those loaded and then let's take a look
at the next module on here the next six slides including this one are all about exploring the data
remember i told you half of this is about looking at the data and getting it all set so let's go ahead and take
this code right here the script and let's get that over into our jupiter notebook and here we go
we've gone ahead and run the imports and i'm going to paste the code down here
and let's take a look and see what's going on the first thing we're doing is we're actually loading the iris data and if you remember up
here we loaded the module that tells it how to get the iris data now we're actually assigning that data
to the variable iris and then we're going to go ahead and use the df to define data frame and that's going to equal pd
and if you remember that's pandas as pd so that's our pandas and panda data
frame and then we're looking at iris data and columns equals iris feature
names and we're going to do the df head and let's run this you can understand what's going on here
the first thing you want to notice is that our df has created what looks like an excel spreadsheet and
in this excel spreadsheet we have set the columns so up on the top you can see the four different columns
and then we have the data iris.data down below it's a little confusing without knowing where this data is coming from
so let's look at the bigger picture and i'm going to go print i'm just going to change this for a moment
and we're going to print all of iris and see what that looks like so when i print all of iris i get this
long list of information and you can scroll through here and see all the different titles on there
what's important to notice is that first off there's a brackets at the beginning so this is a python dictionary
and in a python dictionary you'll have a key or a label and this label pulls up
whatever information comes after it so feature names which we actually used over here under columns
is equal to an array of sepal length steeple width petal length petal width these are the different
names they have for the four different columns and if you scroll down far enough you'll also see data down here oh goodness it
came up right towards the top and data is equal to the different data we're looking at
now there's a lot of other things in here like target we're going to be pulling that up in a minute and there's also the names
the target names which is further down and we'll show you that also in a minute let's go ahead and set that back to the
head and this is one of the neat features of pandas and panda data frames
is when you do df.head or the pandadataframe.head it'll print the first five lines of the
dataset in there along with the headers if you have it in this case we have the column header set to iris features
and in here you'll see that we have zero one two three four in python most arrays always
start at zero so when you look at the first five it's going to be zero one two three four not one two three four five
so now we've got our iris data imported into a data frame let's take a look at the next piece of code in here and so in this section
here of the code we're gonna take a look at the target and let's go ahead and get this
into our notebook this piece of code so we can discuss it a little bit more in detail so here we are in our jupyter
notebook i'm going to put the code in here and before i run it i want to look at a couple things going on so we have
df species and this is interesting because right here you'll see where i have df species
in brackets which is uh the key code for creating another column and here we have iris.target now these
are both in the pandas setup on here so in pandas we can do either one i could have just as easily
done iris and then in brackets target depending on what i'm working on
both are acceptable let's go ahead and run this code and see how this changes and what we've
done is we've added the target from the iris data set as another column on the end
now what species is this is what we're trying to predict so we have our data which tells us the
answer for all these different pieces and then we've added a column with the answer that way when we do
our final setup we'll have the ability to program our our neural network to look for these this different data and know what a
setosa is or a vera color which we'll see in just a minute or virginica those are the
three that are in there and now we're going to add one more column i know we're organizing all this data over and
over again it's kind of fun there's a lot of ways to organize it what's nice about putting everything
onto one data frame is i can then do a print out and it shows me exactly what
i'm looking at and i'll show you that where you where that's different where you can alter that and do it slightly differently
but let's go ahead and put this into our script up to debt now and here we go we're going to put
that down here and we're going to run that and let's talk a little bit about what we're doing
now we're exploring data and one of the challenges is knowing how
good your model is did your model work and to do this we need to split the data and we split it
into two different parts they usually call it the training and the testing and so in here we're going to go ahead
and put that in our database so you can see it clearly and we've set it df remember you can put
brackets this is creating another column is train so we're going to use part of it for training and this equals
np remember that stands for numpy.random.uniform so we're generating a random number
between 0 and 1 and we're going to do it for each of the rows that's where the length df
comes from so each row gets a generated number and if it's less than 0.75 it's true
and if it's greater than 0.75 it's false this means we're going to take 75
of the data roughly because there's a randomness involved and we're going to use that to train it and then the other 25
we're going to hold off to the side and use that to test it later on so let's flip back on over and see what
the next step is so now that we've labeled our database for which is training and which is testing
let's go ahead and sort that into two different variables train and test and let's take this code
and let's bring it into our project and here we go let's paste it on down here and before i
run this let's just take a quick look at what's going on here is we have up above we created remember
there's our def dot head which prints the first five rows and we've added a column is train at the end and so we're going
to take that we're going to create two variables we're going to create two new data frames one's called
train one's called test 75 in train 25 percent and test and then to
sort that out we're going to do that by doing df our main original data frame with the iris
data in it and if df is trained equals true that's going to go in the train and if
df is trained equals false it goes in the test and so when i run this we're going to
print out the number in each one let's see what that looks like and you'll see that it puts 118 in the
training module and it puts 32 in the testing module which lets us know that there was 150
lines of data in here so if you went and looked at the original data you could see that there's 150 lines and that's roughly 75 in one and 25
percent for us to test our model on afterward so let's jump back to our code and see where this goes
in the next two steps we want to do one more thing with our data and that's make it readable to humans i
don't know about you but i hate looking at zeros and ones so let's start with the
features and let's go ahead and take those and make those readable to humans
and let's put that in our code let's see here we go paste it in and
you'll see here we've done a couple very basic things we know that the
columns in our data frame again this is a panda thing the df columns and we know the first four of
them zero one two three that'd be the first four are going to be the features or the titles of those columns
and so when i run this you'll see down here that it creates an index sepa length sepa width pedal length and
pedal width and this should be familiar because if you look up here here's our column titles going across and here's the first
four one thing i want you to notice here is that when you're in a command line
whether it's jupyter notebook or you're running command line in the terminal window
if you just put the name of it it'll print it out this is the same as doing print
features and the short hand is you just put features in here if you're actually writing a code and
saving the script and running it by remote you really need to put the print in there but for this when i run it you'll
see it gives me the same thing but for this we want to go ahead and we'll just leave it as features
because it doesn't really matter and this is one of the fun thing about jupiter notebooks is i'm just building the code as we go
and then we need to go ahead and create the labels for the other part so let's take a look and see what that for our final step in prepping our data
before we actually start running the training and the testing is we're going to go ahead and convert
the species on here into something the computer understands so let's put this code into our script
and see where that takes us all right here we go we've set y equal
to pd.factorize train species of zero
so let's break this down just a little bit we have our pandas right here pd factorize what's
factorized doing i'm going to come back to that in just a second let's look at what train species is and
why we're looking at the group zero on there and let's go up here and here is our species
remember this on that we created this whole column here for species and then it has setosa setosa setosa
setosa and if you scroll down enough you'd also see virginica and veracolor we need to
convert that into something the computer understands zeros and ones so the trained species of zero
because this is in the format of a of an array of arrays so you have to have the zero on the end
and then species is just that column factorize goes in there and looks at the fact
that there's only three of them so when i run this you'll see that y generates an array that's equal to in
this case it's the training set and it's zeros ones and twos representing the three different kinds
of flowers we have so now we have something the computer understands and we have a nice table
that we can read and understand and now finally we get to actually start doing the predicting so here we go
we have two lines of code oh my goodness that was a lot of work to get to two lines of code but there is
a lot in these two lines of code so let's take a look and see what's going on here and put this into
our full script that we're running and let's paste this in here and let's take a look and see what this
is we have we're creating a variable clf and we're going to set this equal to the random
forest classifier and we're passing two variables in here and there's a lot of variables you can play with
as far as these two are concerned they're very standard in jobs all that does is to prioritize it
not something to really worry about usually when you're doing this on your own computer you do in jobs equals two
if you're working in a larger or big data and you need to prioritize it differently this is what that number
does is it changes your priorities and how it's going to run across the system and things like that and then the random state is just how it
starts 0 is fine for here but let's go ahead and run this
we also have clf.fit train features comma y and before we run it let's talk
about this a little bit more clf dot fit so we're fitting
we're training it we are actually creating our random forest classifier right here this
is a code that does everything and we're going to take our training set remember we kept our test off to the side and we're going to take our
training set with the features and then we're going to go ahead and put that in and here's
our target the y so the y is 0 1 and 2 that we just created and the features is
the actual data going in that we put into the training set let's go ahead and run that
and this is kind of an interesting thing because it printed out the random force classifier
and everything around it and so when you're running this in your terminal window on a script like
this this automatically treats us like just like when we were up here and i typed in y and printed out y
instead of print y this does the same thing it treats this as a variable and prints it out
but if you're actually running your code that wouldn't be the case and what is printed out is it shows us
all the different variables we can change and if we go down here you can actually see in jobs equals two
you can see the random state equals zero those are the two that we sent in there you would really have to dig deep to
find out all these different meanings of all these different settings on here some of them are self-explanatory if you kind of
think about it a little bit like max features is auto so all the features that we're putting in there is just going to
automatically take all four of them whatever we send it it'll take some of them might have so many features
because you're processing words there might be like 1.4 million features in there because you're doing legal
documents and that's how many different words are in there at that point you probably want to limit the maximum features that you're going
to process and leaf nodes that's the end nodes remember we have the fruit and we're talking about the leaf nodes like i said
there's a lot in this we're looking at a lot of stuff here so you might have in this case there's probably only think three leaf nodes
maybe four you might have thousands of leaf nodes at which point you do need to put a cap on that and say okay it can only go so
far and then we're going to use all of our resources on processing this and that really is what most of these
are about is limiting the process and making sure we don't uh overwhelm a system and there's some
other settings in here again we're not going to go over all of them warm start equals false warm start is if you're programming it
one piece at a time externally since we're not we're not going to have like we're not going to continually
train this particular learning tree and again like i said there's a lot of things in here that you'll want to look up more detail from the sk
learn and if you're digging in deep and running a major project on here for today though all we need to do is
fit or train our features and our target y so now we have our training model what's
next if we're going to create a model we now need to test it remember we set aside the test feature
test group 25 of the data so let's go ahead and take this code and let's put it into
our script and see what that looks like okay here we go and we're going to run this
and it's going to come out with a bunch of zeros ones and twos which represents the three type of
flowers the setosa the virginica and the versacolor and what we're putting into our predict
is the test features and i always kind of like to know what it is i am looking at
so real quick we're going to do test features and remember features is an
array of sepal length simple width pedal length pedal width
so when we put it in this way it actually loads all these different columns that we loaded into features
so if we did just features let me just do features in here seeing what features looks like this is just playing with the with
pandas data frames you'll see that it's an index so when you put an index in like this
into test features into test it then takes those columns and creates
a panda data frames from those columns and in this case we're going to go ahead
and put those into our predict so we're going to put each one of these lines of data
the 5.0 3.4 1.5 0.2 and we're going to put those in and we're going to predict what our
new forest classifier is going to come up with and this is what it predicts it predicts
0 0 0 1 2 1 1 2 2 2 and and again this is the
flower type setosa virginica and versacolor so now that we've taken our test
features let's explore that let's see exactly what that data means to us
so the first thing we can do with our predix is we can actually generate a different prediction model when i say
different we're going to view it differently it's not that the data itself is different so let's take this next piece
of code and put it into our script so we're pasting it in here and you'll
see that we're doing uh predict and we've added underscore proba for probability
so there's our clf.predict probability so we're running it just like we ran it up
here but this time with this we're gonna get a slightly different result and we're only going to
look at the first 10. so you'll see down here instead of looking at all of them which was
what 27 you'll see right down here that this generates a much larger field on the probability and let's take
a look and see what that looks like and what that means so when we do the
predict underscore prabha for probability it generates three numbers so we had three leaf nodes
at the end and if you remember from all the theory we did this is the predictors the first one is
predicting a 1 for setosa it predicts a 0 for virginica and it predicts a 0
for versa color and so on and so on and so on and let's uh you know what i'm going to change this just a little bit
let's look at 10 to 20 just because we can and we start to get
a little different of data and you'll see right down here it gets to this one this line right here
and this line has 0 0.5 0.5 and so if we're going to vote and we
have two equal votes it's going to go with the first one so it says setosa gets zero votes
virginica gets 0.5 votes versacolor gets 0.5 votes but let's just go with the
virginica since these two are equal and so on and so on down the list you can see how they vary on here
so now we've looked at both how to do a basic predict of the features and we've looked at the predict
probability let's see what's next on here so now we want to go ahead and start
mapping names for the plants we want to attach names so that it makes a little more sense for us
and this we're going to do in these next two steps we're going to start by setting up our predictions and
mapping them to the name so let's see what that looks like and let's go ahead and paste
that code in here and run it and this goes along with the next piece of code so we'll skip through this quickly and
then come back to it a little bit so here's iris dot target names
and uh if you remember correctly this was the the names that we've been talking about this whole time the setosa virginica
versus color and then we're going to go ahead and do the prediction again we've run we could have just set a
variable equal to this instead of re-running it each time but we're going to run it again clf dot predict test features remember
that returns the zeros the ones and the twos and then we're going to set that equal to predictions so this time we're actually
putting it in a variable and when i run this it distributes it it comes out as an array and the
array is setosa satosa satosa satosa setosa we're only looking at the first five we
could actually do let's do the first 25 just so we can see a little bit more on there and you'll see that it starts mapping it
to all the different flower types the versacolor and the virginica in there and let's see how this goes with the
next one so let's take a look at the top part of our species in here and
we'll take this code and put it in our script and let's put that down here and paste
it there we go and we'll go ahead and run it and let's talk about both these sections of code here
and how they go together the first one is our predictions and i went ahead and
did uh predictions through 25 let's just do five and so we have setosa
satoshi satosa satoshi that's what we're predicting from our test model and then we come down
here we look at test species i remember i could have just done test.species.head
and you'll see it says setosa satosa satosa setosa and they match so the first one is what
our forest is doing and the second one is what the actual data is now is we
need to combine these so that we can understand what that means we need to know how good our forest is how good it
is at predicting the features so that's where we come up to the next step which is lots of fun we're going to use a single line of code
to combine our predictions and our actuals so we have a nice chart to look at and let's go ahead and put that in our
script in our jupiter notebook here let's see let's go ahead and paste that in and then i'm gonna
because i'm on the jupiter notebook i can do a control minus so you can see the whole line there
there we go resize it and let's take a look and see what's going on here we're going to create in pandas remember
pd stands for pandas and we're doing a cross tab this function takes two sets of data
and creates a chart out of them so when i run it you'll get a nice chart down here and we have the predicted species
so across the top you'll see the setosa versus color virginica and the actual species setosa versacolor
virginica and so the way to read this chart and let's go ahead and take a look on how to read this chart here
when you read this chart you have setosa where they meet you have versus color where they meet
and you have virginica where they meet and they're meeting where the actual and the predicted agree
so this is the number of accurate predictions so in this case it equals 30. if you had 13 plus 5 plus 12 you get
30. and then we notice here where it says virginica but it was supposed to be versacolor this is inaccurate so now we have two
two inaccurate predictions and 30 accurate predictions so we'll say that the model accuracy is
93 that's just 30 divided by 32 and if we multiply it by 100
we can say that it is 93 percent accurate so we have a 93 accuracy with our model i did want to
add one more quick thing in here on our scripting before we wrap it up so let's flip back
on over to my script in here we're going to take this line of code from up above
i don't know if you remember it but predix equals the iris.target underscore names
so we're going to map it to the names and we're going to run the prediction and we read it on test features
but you know we're not just testing it we want to actually deploy it so at this point i would go ahead and
change this and this is an array of arrays this is really important when you're running these to know that
so you need the double brackets and i could actually create data maybe let's just do two flowers so
maybe i'm processing more data coming in and we'll put two flowers in here and
then i actually want to see what the answer are is so let's go ahead and type in preds and print that out and
when i run this you'll see that i've now predicted two flowers that maybe i measured in my
front yard as versacolor and versacolor not surprising since they put the same
data in for each one this would be the actual end product
going out to be used on data that you don't know the answer for
so that's going to conclude our scripting part of this so in this example last week my son and i
visited a fruit shop dad is that an apple or a strawberry so the question comes up what fruit do they just pick up from the
fruit stand after a couple of seconds you can figure out that it was a strawberry so let's take this model a
step further and let's uh why not build a model which can predict an unknown data and in this
we're going to be looking at some sweet strawberries or crispy apples we want to be able to label those two
and decide what the fruit is and we do that by having data already put in so we already have a bunch of strawberries we
know our strawberries and they're already labeled as such we already have a bunch of apples we know our apples and are labeled as such
then once we train our model that model then can be given the new data and the new data is this image in this
case you can see a question mark on it and it comes through and goes it's a strawberry in this case we're using
the support vector machine model svm is a supervised learning method that looks at data and
sorts it into one of two categories and in this case we're sorting the strawberry into the strawberry site
at this point you should be asking the question how does the prediction work before we dig into an example with
numbers let's apply this to our fruit scenario we have our support vector machine we've taken it and we've taken labeled sample
of data strawberries and apples and we draw on a line down the middle between the two
groups this split now allows us to take new data in this case an apple and a strawberry
and place them in the appropriate group based on which side of the line they fall in and that way we can predict the unknown as colorful and tasty as the
fruit example is let's take a look at another example with some numbers involved and we can take a closer look at how the math works
in this example we're going to be classifying men and women and we're going to start with a set of people with a different height
and a different weight and to make this work we'll have to have a sample data set a female where you have their height and
weight 174 65 174 88 and so on and we'll need a sample data set of the male
they have a height 179 90 180 to 80 and so on let's go ahead and put this on a graph so we have a nice visual
so you can see here we have two groups based on the height versus the weight and on the left side
we're going to have the women on the right side we're going to have the men now if we're going to create a classifier let's add a new data point
and figure out if it's male or female so before we can do that we need to split our data first
we can split our data by choosing any of these lines in this case we draw in two lines
through the data in the middle that separates them in from the women but to predict the gender of a new data point
we should split the data in the best possible way and we say the best possible way
because this line has a maximum space that separates the two classes here you can see there's
a clear split between the two different classes and in this one there's not so much a clear split
this doesn't have the maximum space that separates the two that is why this line best splits the data we don't
want to just do this by eyeballing it and before we go further we need to add some technical terms to this
we can also say that the distance between the points in the line should be as far as possible in technical terms we can say
the distance between the support vector and the hyperplane should be as far as possible
and this is where the support vectors are the extreme points in the data set and if you look at this data set they
have circled two points which seem to be right on the outskirts of the woman and one on the outskirts of the men
and hyperplane has a maximum distance to the support vectors of any class now you'll see the line down the middle
and we call this the hyperplane because when you're dealing with multiple dimensions it's really not just a line but a plane
of intersections and you can see here where the support vectors have been drawn in dashed lines
the math behind this is very simple we take d plus the shortest distance to the closest positive point which would be on
the min side and d minus is the shortest distance to the closest negative point which is on the women's side
the sum of d plus and d minus is called the distance margin or the distance between the two
support vectors that are shown in the dashed lines and then by finding the largest
distance margin we can get the optimal hyperplane once we've created an optimal hyperplane
we can easily see which side the new data fits in and based on the hyperplane we can say the new data point belongs to the male
gender hopefully that's clear how that works on a visual level as a data scientist
you should also be asking what happens if the hyperplane is not optimal if we select a hyperplane having low
margin then there is a high chance of misclassification this particular svm model the one we
discussed so far is also called referred to as the ls vm so far so clear but a question should be
coming up we have our sample data set but instead of looking like this what if it looked like this where we
have two sets of data but one of them occurs in the middle of another set you can see here where we
have the blue and the yellow and then blue again on the other side of our data line in this data set
we can't use a hyperplane so when you see data like this it's necessary to move away from a 1d
view of the data to a two-dimensional view of the data and for the transformation we use what's called a
kernel function the kernel function will take the 1d input and transfer it to a
two-dimensional output as you can see in this picture here the 1d when transferred to a two-dimensional
makes it very easy to draw a line between the two data sets what if we make it even more complicated
how do we perform an svm for this type of data set here you can see we have a two-dimensional data set where the data
is in the middle surrounded by the green data on the outside in this case we're going to
segregate the two classes we have our sample data set and if you draw a line through it's obviously not an optimal hyperplane in
there so to do that we need to transfer the 2d to a 3d array and when you translate it
into a three-dimensional array using the kernel you can see where you can place a hyperplane right through it and easily
split the data before we start looking at a programming example and dive into the script let's look at the advantage of the
support vector machine we'll start with high dimensional input space or sometimes referred to as
the curse of dimensionality we looked at earlier one dimension two dimension three dimension when you get to a
thousand dimensions a lot of problems start occurring with most algorithms that have to be adjusted for the svm automatically does that in high
dimensional space one of the high dimensional space one high dimensional space that we work on
is sparse document vectors this is where we tokenize the words in documents so we can run our machine
learning algorithms over them i've seen ones get as high as 2.4 million different tokens
that's a lot of vectors to look at and finally we have regularization parameter the relation
parameter or lambda is a parameter that helps figure out whether we're going to have a bias or overfitting of the data whether it's
going to be over fitted to a very specific instance or it's going to be biased to a high or low value
with the svm it naturally avoids the overfitting and bias problems that we see in many other algorithms
these three advantages of the support vector machine make it a very powerful tool to add to your repertoire of
machine learning tools now we did promise you a used case study we're actually going to dive into some
python programming and so we're going to go into a problem statement and start off with the zoo so in the zoo example we have family
members going to the zoo when we have the young child going dead is that a group of crocodiles or alligators well that's hard to
differentiate and zoos are a great place to start looking at science and understanding how things work
especially as a young child and so we can see the parents sitting here thinking well what is the difference between a crocodile and an alligator
well one crocodiles are larger in size alligators are smaller in size snout widths the crocodiles have a
narrow snout and alligators have a wider snout and of course in the modern day and age the father is sitting here thinking how
can i turn this into a lesson for my son and he goes let a support vector machine segregate the two groups
i don't know if my dad ever told me that but that would be funny now in this example we're not going to use
actual measurements and data we're just using that for imagery and that's very common in a lot of machine learning
algorithms and setting them up but let's roll up our sleeves and we'll talk about that more in just a moment as we break into our python script so
here we arrive in our actual coding and i'm going to move this into a python editor
in just a moment but let's talk a little bit about what we're going to cover first we're going to cover in the code
the setup how to actually create our svm and you're going to find that there's only two lines of code that actually
create it and the rest of it is done so quick and fast that it's all here in the first page and we'll show you what that looks like
as far as our data because we're going to create some data i talked about creating data just a minute ago and so we'll get into the creating data here
and you'll see this nice correction of our two blobs and we'll go through that in just a second and then the second part is we're going
to take this and we're going to bump it up a notch we're going to show you what it looks like behind the scenes but let's start
with actually creating our setup i like to use the anaconda jupiter notebook because it's very easy to use but you
can use any of your favorite python editors or setups and go in there but let's go ahead and switch over there and see what
that looks like so here we are in the anaconda python notebook or anaconda jupiter notebook
with python we're using python 3. i believe this is 3.5 but it should be work in any of your 3x versions and you'd
have to look at the sklearn and make sure if you're using a 2x version an earlier version let's go and put our code in there
and one of the things i like about the jupyter notebook is i go up to view and i'm going to go ahead and toggle the
line numbers on to make it a little bit easier to talk about and we can even increase the size because this is edited
in this case i'm using google chrome explorer and that's how it opens up for the editor although anyone any like i said any
editor will work now the first step is going to be our imports and we're going to import four different
parts the first two i want you to look at are line one and line two are numpy as
np and matplot library dot pi plot as plt now these are very standardized
imports when you're doing work the first one is the numbers python we need that because part of the
platform we're using uses that for the numpy array and i'll talk about that in a minute so you can understand why we want to use a
numpy array versus the standard python array and normally it's pretty standard setup to use np for numpy the map plot
library is how we're going to view our data so this has you do need the np for the sk learn module but the map plot library
is purely for our use for visualization and so you really don't need that for the svm but we're going to put it there
so you have a nice visual aid and we can show you what it looks like that's really important at the end when you finish everything so you have a nice
display for everybody to look at and then finally we're going to i'm going to jump one ahead to line number four that's the
sklearn.datasets.samplesgenerator import make blobs and i told you that we were gonna make up data and this is a
tool that's in the sk learning to makeup data i personally don't wanna go to the zoo get in trouble for jumping over the
fence and probably get eaten by the crocodiles or alligators as i work on measuring their snouts and width
and length instead we're just going to make up some data and that's what that make blobs is
it's a wonderful tool if you're ready to test your your setup and you're not sure about what data you're going to put in there you
can create this blob and it makes it real easy to use and finally we have our actual svm the
sklearn import svm on line three so that covers all our imports we're going to create remember i use the
make blobs to create data and we're going to create a capital x and a lowercase y equals make blobs in samples equals 40.
so we're going to make 40 lines of data it's going to have two centers with a random state equals 20
so each each each group's going to have 20 different pieces of data in it and the way that looks
is that we'll have under x um an x y plane so i have two numbers under x
and y will be 0 1 that's the two different centers so we have yes or no in this case alligator or
crocodile that's what that represents and then i told you that the actual sk learner the svm
is in two lines of code and we see it right here with clf equals svm dot svc kernel equals linear
and i set sql to one although in this example since we are not uh regularizing the data
because we want to be very clear and easy to see i went ahead you can set it to a thousand a lot of times when you're not doing that but for this thing linear
because it's a very simple linear example we only have the two dimensions and it'll be a nice linear
hyperplane it'll be a nice linear line instead of a full plane so we're not dealing with a huge amount of data
and then all we have to do is do clf dot fit x comma y and that's it clf has been
created and then we're going to go ahead and display it and i'm going to talk about this display here in just a second but
let me go ahead and run this code and this is what we've done is we've created two blobs you'll see the blue on
the side and then kind of an orangish on the other side that's our two sets of data they represent one represents
crocodiles and one represents alligators and then we have our measurements in this case we have like the
width and length of the snout and i did say i was going to come up here and talk just a little bit about our plot and you'll see plt that's what
we imported we're going to do a scatter plot that means we're just putting dots on there and then look at this notation i
have the capital x and then in brackets i have a colon comma 0. that's from numpy if you did that in a
regular array you'll get an error in a python array you have to have that in a numpy array it turns out that our make blobs returns
a numpy array and this notation is great because what it means is the first part is the colon means
we're going to do all the rows that's all the data in our blob we created under capital x and then the second part
has a comma 0. we're only going to take the first value and then if you notice we do the same thing but
we're going to take the second value remember we always start with zero and then one so we have column zero and column one
and you can look at this as our x y plots the first one is the x plot and the second one is the y
plot so the first one is on the bottom 0 2 4 6 8 and 10 and then the second one
x of the one is the 4 5 6 7 8 9 10 going up the left hand side s equals 30 is just the size of the dot
so we can see them instead of real tiny dots and then c map equals plt.cm.paired
and you'll also see the c equals y that's the color we're using two colors zero one and
that's why we get the nice blue and the two different colors for the alligator and the crocodile
now you can see here that we did this the actual fit was done in two lines of code a lot of times will be a third line
where we regularize the data we set it between like minus one and one and we reshape it but for this it's not
necessary and it's also kind of nice because you can actually see what's going on and then if we wanted to we wanted to
actually run a prediction let's take a look and see what that looks like and to predict some new data and we'll show this again as we get
towards the end of digging in deep you can simply assign your new data in this case i am giving it a
width and length 3 4 and a width and length 5 6 and note that i put the data as a set of
brackets and then i have the brackets inside and the reason i do that is because when we're looking at data it's designed
to process a large amount of data coming in we don't want to just process one line at a time and so in this case i'm processing two
lines and then i'm just going to print and you'll see clf dot predict new data so the clf and the dot predict
part is going to give us an answer and let's see what that looks like and you'll see 0 1. so predicted the first one the 3 4
is going to be on the one side and the 5 6 is going to be on the other side so one came out as an alligator and one
came out as a crocodile now that's pretty short explanation for the setup but really we want to dug
in and see what it's going on behind the scenes and let's see what that looks like so
the next step is to dig in deep and find out what's going on behind the scenes and also put that in a nice pretty graph
we're going to spend more work on this than we did actually generating the original model and you'll see here
that we go through a few steps and i'll move this over to our editor in just a second we come in we create our original data
it's exactly identical to the first part and i'll explain why we redid that and show you how not to redo that and
then we're going to go in there and add in those lines we're going to see what those lines look like and how to set those up
and finally we're going to plot all that on here and show it and you'll get a nice graph with the what we saw earlier when we
were going through the theory behind this where it shows the support vectors and the hyperplane
and those are done where you can see the support vectors as the dashed lines and the solid line which is the hyperplane
let's get that into our jupiter notebook before i scroll down to a new line
i want you to notice line 13 has plot show and we're going to talk about that here in just a second but let's scroll
down to a new line down here and i'm going to paste that code in and you'll see that the plot show has moved
down below let's scroll up a little bit and if you look at the top here of our new section one two three and four is the same code
we had before and let's go back up here and take a look at that we're gonna fit the values
on our svm and then we're gonna plot scatter it and then we're gonna do a plot show so you should be asking why are we
redoing the same code well when you do the plot show that blanks out what's in the plot
so once i've done this plot show i have to reload that data now we could do this simply by removing
it up here re-running it and then coming down here and then we wouldn't have to rerun these first four
lines of code now in this it doesn't matter too much and you'll see the plot show was down here and then removed
right there on line five i'll go ahead and just delete that out of there because we don't want to blank out
our screen we want to move on to the next setup so we can go ahead and just skip the first four lines because we did
that before and let's take a look at the ax equals plt.gca
now right now we're actually spending a lot of time just graphing that's all we're doing here okay so this is how we display a nice
graph with our results in our data ax is very standard note used variable when you talk about
plt and it's just setting it to that axis the last axis in the plt it can get very confusing if you're
working with many different layers of data on the same graph and this makes it very easy to reference the ax so this
reference is looking at the plt that we created and we already mapped out our two blobs on and then we want to
know the limits so we want to know how big the graph is we can find out the x limit and the y limit simply with the
get x limit and get y limit commands which is part of our metplot library and then we're going to create a
grid and you'll see down here we have we've set the variable xx equal to np.line space
x limit 0 x limit 1 comma 30 and we've done the same thing for the y space and then we're going to go in here
and we create a mesh grid and this is a numpy command so we're back to our numbers
python let's go through what these numpy commands mean with the line space and the mesh grid
we've taken x x small s x x equals n p line space and we have our x limit zero
and our x limit one and we're going to create 30 points on it and we're going to do the same thing for
the y axis now this has nothing to do with our evaluation it's all we're doing is we're creating a
grid of data and so we're creating a set of points between 0 and the x limit we're creating 30 points
and the same thing with the y and then the mesh grid loops those all together so it forms a nice grid
so if we were going to do this say between the limit 0 and 10 and do 10 points we would have a 0 0 1 1 0 1
0 2 0 3 0 4 to 10 and so on you can just imagine a point at each corner one of those boxes and
the mesh grid combines them all so we take the yy and the xx we created and creates the full grid and we've set that grid into the yy
coordinates and the xx coordinates now remember we're working with numbi and python we like to separate those we
like to have instead of it being x comma 1 you know x comma y and then x 2 comma y 2 and this in the
next set of data it would be a column of x's and a column of y's and that's what we have here is we have
a column of y's and we put it as a capital yy and a column of x's capital xx with all those different points being
listed and finally we get down to the numpy v stack just as we created those
in the mesh grid we're now going to put them all into one array x y array now that we've created the stack
of data points we're going to do something interesting here we're going to create a value z and the z equals the clf
that's our that's our support vector machine we created and we've already trained and we have a
dot decision function we're going to put the x y in there so here we have all this data
we're going to put that x y in there that data and we're going to reshape it and you'll see that we have the x
dot shape in here this literally takes the xx resets it up connected to the y and the
z value lets us know whether it is the left hand side is going to generate
three different values the z value does and it'll tell us whether that data is a support vector to the left
the hyperplane in the middle or the support vector to the right so it generates three different values
for each of those points and those points have been reshaped so they're right on a line on those three different lines
so we've set all of our data up we've labeled it to three different areas and we've reshaped it and we've
just taken 30 points in each direction if you do the math you have 30 times 30 so it's 900 points of data and we
separated between the three lines and reshaped it to fit those three lines we can then go back to our map plot
library we've created the ax and we're going to create a contour and you'll see here we have contour
capital xx capital yy these have been reshaped to fit those lines z is the labels so now we have the three
different points with the labels in there and we can set the colors equals k and i told you we had three different
labels but we have three levels of data the alphas just makes it
kind of see-through so it's only 0.5 of the value in there so when we graph it the data will show up from behind it
wherever the lines go and finally the line styles this is where we set the two support vectors to be dash
dashed lines and then a single one is just a straight line that's what all that setup does and then finally we take our ax dot
scatter we're going to go ahead and plot the support vectors but we've programmed it in there so that they look
nice like the dash dash line and the dashed line on that grid and you can see here when we do the clf
dot support vectors we are looking at column 0 and column 1 and then again we have the s equals 100
so we're going to make them larger and the line width equals 1 face colors equals none let's take a look and see
what that looks like when we show it and you can see we get down to our end result it creates a really nice graph
we have our two support vectors and dashed lines and they have the near data so you can
see those two points or in this case the four points where those lines nicely cleave the data and then you have
your hyperplane down the middle which is as far from the two different points as possible creating the maximum distance
so you can see that we have our nice output for the size of the body and the width of the snout
and we've easily separated the two groups of crocodile and alligator congratulations you've done it we've made it of course
these are pretend data for our crocodiles and alligators but this hands-on example will help you to
encounter any support vector machine projects in the future and you can see how easy they are to set up
and look at in depth so what is k-means clustering k-means clustering is an unsupervised
learning algorithm in this case you don't have labeled data unlike in supervised
learning so you have a set of data and you want to group them and as the
name suggests you want to put them into clusters which means objects that are similar in nature
similar in characteristics need to be put together so that's what k means clustering is all
about the term k is basically is a number so we need to tell the system how many
clusters we need to perform so if k is equal to two there will be two clusters if k is equal to three
three clusters and so on and so forth that's what the k stands for and of course there is a way of finding
out what is the best or optimum value of k for a given data we will look at that
so that is k means clustering so let's take an example k-means clustering
is used in many many scenarios but let's take an example of cricket the game of cricket
let's say you received data of a lot of players from maybe all over the country or all over
the world and this data has information about the runs scored by the people ordered by
the player and the wickets taken by the player and based on this information
we need to cluster this data into two clusters batsman and bowlers so this
is an interesting example let's see how we can perform this so we have
the data which consists of primarily two characteristics which is the runs and
the wickets so the bowlers basically take wickets and the batsmen score runs there will be of course a few
bowlers who can score some runs and similarly there will be some batsmen who will
who would have taken a few wickets but with this information we want to cluster those players into batsmen and
bowlers so how does this work let's say this is how the data is so there are information there is
information on the y-axis about the run scored and on the x-axis about the wickets
taken by the players so if we do a quick plot this is how it would look and when we do the clustering
we need to have the clusters like shown in the third diagram rtms we need to
have a cluster which consists of people who have scored high runs which is basically
the batsman and then we need a cluster with people who have taken a lot of wickets which is typically the
bowlers there may be a certain amount of overlap but we will not talk about it right now so with k-means clustering we will have
here that means k is equal to two and we will have two clusters which is batsman and bowlers so how does this
work the way it works is the first step in k-means clustering is the allocation of two centroids
randomly so two points are assigned as so-called centroids so
in this case we want two clusters which means k is equal to two so two points have
been randomly assigned as centroids keep in mind these points can be
anywhere there are random points they are not initially they are not really the centroids
centroid means it's a central point of a given data set but in this case when it starts off
it's not really the centroid okay so these points though in our presentation here we have shown
them one point closer to these data points and another closer to these data points they can be assigned randomly anywhere
okay so that's the first step the next step is to determine the distance of each of the data points from
each of the randomly assigned centroids so for example we take this point and
find the distance from the centroid and the distance from this centroid this point is taken
and the distance is found from this centroid and the center and so on and so forth so for every point the distance is measured
from both the centroids and then whichever distance is less
that point is assigned to that centroid so for example in this case visually it is very obvious that all
these data points are assigned to this centroid and all these data points are assigned to this centroid and that's what is
represented here in blue color and in this yellow color the next step is to
actually determine the central point or the actual centroid for these two
clusters so we have this one initial cluster this one initial cluster
but as you can see these points are not really the centroid centroid means it should be the
central position of this data set central position of this data set so that is what needs to be determined
as the next step so the central point of the actual centroid is determined
and the original randomly allocated centroid is repositioned to the actual centroid
of these new clusters and this process is actually repeated now what might happen
is some of these points may get reallocated in our example that is not
happening probably but it may so happen that the distance is found between each of these data
points once again with these centroids and if there is if it is required some points may be
reallocated we will see that in a later example but for now we will keep it simple so
this process is continued till the centroid repositioning stops
and that is our final cluster so this is our so after iteration
we come to this position this situation where the centroid doesn't need any more repositioning and that means
our algorithm has converged convergence has occurred and we have the cluster two clusters we
have the clusters with a centroid so this process is repeated the process of calculating the
distance and repositioning the centroid is repeated till the repositioning stops which means that
the algorithm has converged and we have the final cluster with the data
points and the centroids so this is what you're going to learn from this session we will talk about the types of
clustering what is k-means clustering application of k-means clustering k-means clustering is done using
distance measure so we will talk about the common distance measures and then we will talk about how k-means
clustering works and go into the details of k-means clustering algorithm and then we will end with a demo and a
use case for k-means clustering so let's begin first of all what are the types of
clustering there are primarily two categories of clustering hierarchical clustering
and then partitional clustering and each of these categories are further subdivided into
agglomerative and divisive clustering and k-means and fuzzy c means clustering
let's take a quick look at what each of these types of clustering are in hierarchical clustering the
clusters have a tree-like structure and hierarchical clustering is further divided into
agglomerative and divisive agglomerative clustering is a bottom-up
approach we begin with each element as a separate cluster and merge them into successively
larger clusters so for example we have a b c d e f we start by combining b and c form
one cluster d e and e form one more then we combine d e and f one more bigger cluster and then add bc
to that and then finally a to it compared to that divisive clustering or divisive clustering
is a top down approach we begin with the whole set and proceed to divide it into successively smaller clusters so we have
abcdef we first take that as a single cluster and then break it down
into a b c d e and f then we have partitional clustering
split into two subtypes k means clustering and fuzzy c means in k-means clustering
the objects are divided into the number of clusters mentioned by the number k
that's where the k comes from so if we say k is equal to 2 the objects are divided into two clusters c1 and c2
and the way it is done is the features or characteristics are compared and all objects having
similar characteristics are clubbed together so that's how k means clustering is done
we will see it in more detail as we move forward and fuzzy c means is very similar to
k means in the sense that it clubs objects that have similar characteristics together but
while in k means clustering two objects cannot belong to or any object a single object cannot
belong to two different clusters in c means objects can belong to more than one cluster
so that is the primary difference between k-means and fuzzy c means so what are some of the applications of
k-means clustering k-means clustering is used in a variety of examples or variety of business cases in
real life starting from academic performance diagnostic systems search engines and wireless sensor
networks and many more so let us take a little deeper look at each of these examples
academic performance so based on the scores of the students students are categorized into a b
c and so on clustering forms a backbone of search engines when a search is
performed the search results need to be grouped together the search engines very often use
clustering to do this and similarly in case of wireless sensor networks
the clustering algorithm plays the role of finding the cluster heads which collects all the data in its
respective cluster so clustering especially k means clustering uses distance measure so let's take a
look at what is distance measure so while these are the different types of clustering in this video we will focus
on k-means clustering so distance measure tells how similar some objects
are so the similarity is measured using what is known as distance measure and
what are the various types of distance measures there is euclidean distance there is
manhattan distance then we have squared euclidean distance measure and cosine distance measure these are some
of the distance measures supported by k-means clustering let's take a look at each of these
what is euclidean distance measure this is nothing but the distance between two points so we have learnt in high school how to
find the distance between two points this is a little sophisticated formula for that
but we know a simpler one is square root of y2 minus y1 whole square plus x2 minus
x1 whole square so this is an extension of that formula so that is the euclidean distance between two points
what is the squared euclidean distance measure it's nothing but the square of the euclidean distance as
the name suggests so instead of taking the square root we leave the square as it is and then we have
manhattan distance measure in case of manhattan distance it is the sum
of the distances across the x-axis and the y-axis and note that we are
taking the absolute value so that the negative values don't come into play so that is the manhattan distance measure
then we have cosine distance measure in this case we take the angle between the two
vectors formed by joining the points from the origin so that is the cosine distance measure okay so that was
a quick overview about the various distance measures that are supported by k-means now let's go and check how
exactly k-means clustering works okay so this is how
k-means clustering works this is like a flowchart of the whole process there is a starting
point and then we specify the number of clusters that we want now there are a couple of ways of doing
this we can do by trial and error so we specify a certain number maybe k is equal to 3
or 4 or 5 to start with and then as we progress we keep changing until we get the best
clusters or there is a technique called elbow technique whereby we can determine the value of k
what should be the best value of k how many clusters should be formed so once we have the value of k we
specify that and then the system will assign that many
centroids so it picks randomly that to start with randomly that many points that are
considered to be the centroids of these clusters and then it measures the distance of
each of the data points from these centroids and assigns those points to
the corresponding centroid from which the distance is minimum so each data
point will be assigned to the centroid which is closest to it and thereby we have
k number of initial clusters however this is not the final clusters
the next step it does is for the new groups for the clusters that have been formed
it calculates the main position thereby calculates the new centroid
position the position of the centroid moves compared to the randomly allocated one
so it's an iterative process once again the distance of each point is measured from this new centroid point
and if required the data points are reallocated to the new centroids
and the mean position or the new centroid is calculated once again if the centroid moves then the iteration
continues which means the convergence has not happened the clustering has not converged
so as long as there is a movement of the centroid this iteration keeps happening but once
the centroid stops moving which means that the cluster has converged or the clustering process
has converged that will be the end result so now we have the final position of the centroid and
the data points are allocated accordingly to the closest centroid i know it's a little difficult
to understand from this simple flowchart so let's do a little bit of visualization and see if we can explain
it better let's take an example if we have a data set for a grocery shop so let's say we have a data set for a
grocery shop and now we want to find out how many clusters this has to be spread across
so how do we find the optimum number of clusters there is a technique called the elbow
method so when these clusters are formed there is a parameter called within sum of squares and the lower
this value is the better the cluster is that means all these points are very
close to each other so we use this within sum of squares as a measure to find
the optimum number of clusters that can be formed for a given data set
so we create clusters or we let the system create clusters of a variety of numbers maybe
of 10 10 clusters and for each value of k the within ss is
measured and the value of k which has the least amount of within ss or wss
that is taken as the optimum value of k so this is the diagrammatic
representation so we have on the y axis the within sum of squares or wss
and on the x-axis we have the number of clusters so as you can imagine if you have k is
equal to 1 which means all the data points are in a single cluster the within s value
will be very high because they are probably scattered all over the moment you split it into two there
will be a drastic fall in the within ss value that's what is represented here but then as the value
of k increases the decrease the rate of decrease will not be so high it will continue to
decrease but probably the rate of decrease will not be high so that gives us an idea so from here we
get an idea for example the optimum value of k should be either 2 or 3 or at the most
4 but beyond that increasing the number of clusters is not dramatically changing the value
in wss because that pretty much gets stabilized okay now that we have got the value of k
and let's assume that these are our delivery points the next step is basically to assign
two centroids randomly so let's say c1 and c2 are the centroids assigned
randomly now the distance of each location from the centroid is measured and each
point is assigned to the centroid which is closest to it so for example these points are very
obvious that these are closest to c1 whereas this point is far away from c2 so these points will be
assigned which are close to c1 will be assigned to c1 and these points or locations which are
close to c2 will be assigned to c2 and then so this is the how the initial
grouping is done this is part of c1 and this is part of c2 then the next step
is to calculate the actual centroid of this data because remember c1 and c2 are not the centroids
they have been randomly assigned points and only thing that has been done was the data points which are closest to
them have been assigned but now in this step the actual centroid will be calculated
which may be for each of these data sets somewhere in the middle so that's like the main point that will be calculated and the centroid
will actually be positioned or repositioned there same with c2
so the new centroid for this group is c2 in this new position and c1 is in this new position once
again the distance of each of the data points is calculated from these centroids now remember it's
not necessary that the distance still remains the or each of these data points still remain in the same group
by recalculating the distance it may be possible that some points get reallocated like so you see this so this point
earlier was closer to c2 because c2 was here but after recalculating repositioning
it is observed that this is closer to c1 than c2 so this is the new grouping so some
points will be reassigned and again the centroid will be calculated and if the centroid doesn't change so
that is the repetitive process iterative process and if the centroid doesn't change once the centroid stops changing
that means the algorithm has converged and this is our final cluster with this
as the centroid c1 and c2 as the centroids these data points as a part of each cluster
so i hope this helps in understanding the whole process iterative process of k-means clustering
so let's take a look at the k-means clustering algorithm let's say we have
x1 x2 x3 n number of points as our inputs and we want to split this into k
clusters or we want to create k clusters so the first step is to randomly pick k points and call them
centroids they are not real centroids because centroid is supposed to be a center point
but they are just called centroids and we calculate the distance of each and
every input point from each of the centroids so the distance of x1 from
c1 from c2 c3 each of the distances we calculate
and then find out which distance is the lowest and assign x1 to that particular random
centroid repeat that process for x2 calculate its distance
from each of the centroids c1 c2 c3 up to ck and find which is the lowest distance
and assign x2 to that particular centroid same with x3 and so on so that is the first round of assignment
that is done now we have k groups because there are we have assigned the
value of k so there are k centroids and so there are k groups all these inputs
have been split into k groups however remember we picked the centroids randomly so they are not
real centroids so now what we have to do we have to calculate the actual
centroids for each of these groups which is like the mean position which means that the position of the
randomly selected centroids will now change and they will be the main positions of these newly
formed k groups and once that is done we once again repeat this process of
calculating the distance right so this is what we are doing as a part of step four we repeat step two and three so we
again calculate the distance of x1 from the centroid c1 c2
c3 and then c which is the lowest value and assign x1 to that calculate the distance of x2
from c1 c2 c3 or whatever up to ck and find whichever is the lowest
distance and assign x2 to that centroid and so on in this process there may be
some reassignment x1 was probably assigned to cluster c2 and after doing this calculation maybe
now x1 is assigned to c1 so that kind of reallocation may happen so we repeat the steps 2 and 3
till the position of the centroids don't change or stop changing and that's when
we have convergence so let's take a detailed look at each of these steps so we randomly pick
k cluster centers we call them centroids because they are not initially they are not really the
centroids so we let us name them c1 c2 up to ck and then step two
we assign each data point to the closest center so what we do we calculate the
distance of each x value from each c value so the distance between x1
c1 distance between x1 c2 x1 c3 and then we find which is the
lowest value right that's the minimum value we find and assign x1 to that particular centroid then we
go next to x2 find the distance of x2 from c1 x2 from c2
x2 from c3 and so on up to ck and then assign it to the point or to the
centroid which has the lowest value and so on so that is step number two in step number three
we now find the actual centroid for each group so what has happened
as a part of step number two we now have all the points all the data points
grouped into k groups because we we wanted to create k clusters
right so we have k groups each one may be having a certain number of input values they
need not be equally distributed by the way based on the distance we will have
k groups but remember the initial values of the c1 c2 were not really the
centroids of these groups right we assigned them randomly so now
in step 3 we actually calculate the centroid of each group which means
the original point which we thought was the centroid will shift to the new position which is
the actual centroid for each of these groups okay and we again calculate the distance
so we go back to step two which is what we calculate again the distance of
each of these points from the newly positioned centroids and if required we
reassign these points to the new centroids so as i said earlier
there may be a reallocation so we now have a new set or a new group we still have k
groups but the number of items and the actual assignment may be different from what
was in step two here okay so that might change then we perform
step three once again to find the new centroid of this new group so we have again a new set of clusters
new centroids and new assignments we repeat this step two again once again we find and
then it is possible that after iterating through three or four or five times the centroid
will stop moving in the sense that when you calculate the new value of the centroid
that will be same as the original value or there will be very marginal change
so that is when we say convergence has occurred and that is our final
cluster that's the formation of the final cluster all right so let's see a couple of demos
of k-means clustering we will actually see some live demos in python notebook using
pattern notebook but before that let's find out what's the problem that we are trying to solve
the problem statement is let's say walmart wants to open a chain of stores across the state of
florida and it wants to find the optimal store locations now the issue here is
if they open too many stores close to each other obviously they will not make profit but
if they if the stores are too far apart then they will not have enough sales so how do they optimize this now
for an organization like walmart which is an e-commerce giant they already have the addresses
of their customers in their database so they can actually use this information or this
data and use k-means clustering to find the optimal location now before we go into the
python notebook and show you the live code i wanted to take you through very quickly a summary
of the code in the slides and then we will go into the python notebook so in this block
we are basically importing all the required libraries like numpy matplotlib and so on
and we are loading the data that is available in the form of let's say the addresses
for simplicity sake we will just take them as some data points then the next thing we do is quickly do
a scatter plot to see how they are related to each other with respect to each other
so in the scatter plot we see that there are a few distinct groups already being formed so you can
actually get an idea about how the cluster would look and how many clusters what is the optimal
number of clusters and then starts the actual k-means clustering process so we will
assign each of these points to the centroids and then check whether they
are the optimal distance which is the shortest distance and assign each of the
points data points to the centroids and then go through this iterative
process till the whole process converges and finally we get an output like this
so we have four distinct clusters and which is if we can say that this is how
the population is probably distributed across florida state and the centroids
are like the location where the store should be the optimum location where the store
should be so that's the way we determine the best locations for the store and
that's how we can help walmart find the best locations for their stores in florida so now let's take this
into python notebook let's see how this looks when we are running running the code live all right so this
is the code for k-means clustering in jupiter notebook we have a few examples here which we
will demonstrate how k-means clustering is used and even there is a small implementation of
k-means clustering as well okay so let's get started okay so this block
is basically importing the various libraries that are required like matplotlib and numpy and so on and
so forth which would be used as a part of the code then we are going and creating blobs which
are similar to clusters now this is a very neat feature which is available in scikit learn
make blobs is a nice feature which creates clusters of data sets so that's a
wonderful functionality that is readily available for us to create some test data kind of thing okay so that's
exactly what we are doing here we are using make blobs and we can specify how many
clusters we want so centers we are mentioning here so it will go ahead and so we just
mentioned four so it will go ahead and create some test data for us
and this is how it looks as you can see visually also we can figure out that there are four distinct
classes or clusters in this data set and that is what make blobs actually provides
now from here onwards we will basically run the standard k-means functionality
that is readily available so we really don't have to implement k-means itself
the k-means functionality or the function is readily available you just need to feed the data and we'll
create the clusters so this is the code for that we import k-means
and then we create an instance of k-means and we specify the value of k this n
underscore clusters is the value of k remember k means in k means k is basically the number of
clusters that you want to create and it is a integer value so this is where we are
specifying that so we have k is equal to 4 and so that instance is created
we take that instance and as with any other machine learning functionality fit is
what we use the function or the method rather fit is what we use
to train the model here there is no real training kind of thing but that's the call okay
so we are calling fit and what we are doing here we are just passing the data so x has these values
the data that has been created right so that is what we are passing here and
this will go ahead and create the clusters and then we are using
after doing uh fit we run the predict which uh basically assigns for each of
these observations which cluster where it belongs to all right so it will name the clusters
maybe this is cluster 1 this is 2 3 and so on or i will actually start
from 0 cluster 0 1 2 and 3 maybe and then for each of the observations it will
assign based on which cluster it belongs to it will assign a value so that is stored in
y underscore k means when we call predict that is what it does and we can take a quick look at these
uh y underscore k means or with the cluster numbers that have been assigned for each
observation so this is the cluster number assigned for observation 1 maybe this is
for observation 2 observation 3 and so on so we have how many about i think 300
samples right so all the 300 samples there are 300 values here each of them the cluster number is given and the
cluster number goes from 0 to 3. so there are four clusters so the numbers go from 0
1 2 3. so that's what is seen here okay now so this was a quick example of
generating some dummy data and then clustering that okay and this can be applied if you have
proper data you can just load it up into x for example here and then run the cable so this is the central
part of the k-means clustering program example so you basically create an instance and you mention how
many clusters you want by specifying this parameter and underscore clusters and that is also the value of k and then
pass the data to get the values now the next section of this code is the implementation of
a k-means now this is kind of a rough implementation of the k-means algorithm
so we will just walk you through i will walk you through the code uh at each step what it is doing
and then we will see a couple of more examples of how k-means clustering uh can be used
in maybe some real-life examples real-life use cases all right so in this case here what we
are doing is basically implementing k means clustering and there is a function for a library
calculates for a given two pairs of points it will calculate the the distance
between them and see which one is the closest and so on so this is like this is pretty much like what
k means does right so it calculates the distance of each point or each data set from
predefined centroid and then based on whichever is the lowest this particular data point is
assigned to that centroid so that is basically available as a standard function and we will be
using that here so as explained in the slides the first step that is done in case of k-means clustering
is to randomly assign some centroids so as a first step we randomly
allocate a couple of centroids which we call here we are calling as centers
and then we put this in a loop and we take it through an iterative process
for each of the data points we first find out using this function pairwise distance argument for each of
the points we find out which one which center or which randomly selected centroid
is the closest and accordingly we assign the data or the data point
to that particular centroid or cluster and once that is done for all the data
points we calculate the new centroid by finding out the mean position because
the center position right so we calculate the new centroid and then we check
if the new centroid is the coordinates or the position is the same as the previous centroid
the positions we will compare and if it is the same that means the process has converged so
remember we do this process till the centroids or the centroid doesn't move anymore
right so the centroid gets relocated each time this reallocation is done so the moment
it doesn't change anymore the position of the centroid doesn't change anymore we know that convergence has occurred so
till then so you see here this is like an infinite loop while true is an infinite loop it only
breaks when the centers are the same the new center and the old center positions are the
same and once that is uh done we return the centers and the labels now of course
as explained this is not a very sophisticated and advanced implementation very basic implementation
because one of the flaws in this is that sometimes what happens is the centroid the position will
keep moving but in the change will be very minor so in that case also with that is actually convergence
right so for example the change is point zero zero zero one we can consider that as convergence
otherwise what will happen is this will either take forever or it will be never ending so
that's a small flaw here so that is something additional checks may have to be added here but again as
mentioned this is not the most sophisticated implementation uh this is like a kind of a rough
implementation of the k means clustering okay so if we execute this code this is
what we get as the output so this is the definition of this particular function and then we call that find
underscore clusters and we pass our data x and the number of clusters which is 4
and if we run that and plot it this is the output that we get so this is of course each
cluster is represented by a different color so we have a cluster in green color yellow color and so on
and so forth and these big points here these are the centroids this is the final position of the centroids and as you can see
visually also this appears like a kind of a center of all these points here right similarly
this is like the center of all these points here and so on so this is the example or this is an example of an
implementation of k means clustering and next we will move on to see a couple of
examples of how k means clustering is used in maybe some real life scenarios or use
cases in the next example or demo we are going to see how we can use
k-means clustering to perform color compression we will take a couple of images
so there will be two examples and we will try to use k-means clustering to
compress the colors this is a common situation in image processing when you have an image
with millions of colors but then you cannot render it on some devices which may not have
enough memory uh so that is the scenario where something like this can be used
so before again we go into the python notebook let's take a look at quickly the the
code as usual we import the libraries and then we import the
image and then we will flatten it so the reshaping is basically we have
the image information is stored in the form of pixels and if the image is like for example 427
by 640 and it has three colors so that's the overall dimension of the of the initial image
we just reshape it and then feed this to our algorithm
and this will then create clusters of only 16 clusters so this this colors
there are millions of colors and now we need to bring it down to 16 colors so we
use k is equal to 16 and this is how when we visualize this is
how it looks there are these are all about 16 million possible colors the input color space has 16 million
possible colors and we just sub compress it to 16 colors so this is
how it would look when we compress it to 16 colors and this is how the original
image looks and after compression to 16 colors this is how the new image looks as you
can see there is not a lot of information that has been lost
though the image quality is definitely reduced a little bit so this is
an example which we are going to now see in python notebook let's go into the python node
and once again as always we will import some libraries and load this image called
flower dot jpg okay so let we load that and this is how it looks this
is the original image which has i think 16 million colors and this is the shape of this
image which is basically what is the shape is nothing but the overall size right so this is 427 pixel by 640 pixel
and then there are three layers which is this three basically is for rgb which is red green blue
so color image will have that right so that is the shape of this now what we need to do is data let's
take a look at how data is looking so let me just create a new cell and show you what is in data basically
we have captured this information so data is what let me just show you
here all right so let's take a look at china
what are the values in china and if we see here this is how the data is
stored this is nothing but the pixel values okay so this is like a matrix and each
one has about four for this 427 by 640 pixels all right so this is how it looks now
the issue here is these values are large the numbers are a large so we need to normalize them to
between zero and one right so that's why we will basically create one more variable which is data which will
contain the values between zero and one and the way to do that is divide by 255 so we divide china by 255
and we get the new values in data so let's just run this piece of code and this is the shape so
we now have also yeah what we have done is we changed using reshape we converted into the
three-dimensional into a two-dimensional data set and let us also take a look at how
let me just insert probably a cell here and take a look at how data
is looking all right so this is how data is looking and now you see this is the values are between zero and
one right so if you earlier noticed in case of china the values were large numbers now everything is between 0 and 1. this
is one of the things we need to do all right so after that the next thing that we need to do is to
visualize this and we can take random set of maybe 10 000 points
and plot it and check and see how this looks let us just plot this and
so this is how the original the color the pixel distribution is these are two plots one is red against
green and another is red against blue and this is the original distribution of the color so then what we will do is we will use k
means clustering to create just 16 clusters for the various colors
and then apply that to the image now what will happen is since the data is
large because there are millions of colors using regular k-means may be a little time consuming so there is another
version of k-means which is called mini batch gaming so we will use that which is which processes in
the overall concept remains the same but this basically processes it in smaller batches that's
the only thing okay so the results will pretty much be the same so let's go ahead and execute this piece of
code and also visualize this so that we can see that there are these this is how the 16 colors would
look so this is red against green and this is red against blue there is a quite a bit of
similarity between this original color schema and the new one right so it doesn't look
very very completely different or anything like that now we apply this the newly created colors
to the image and we can take a look uh how this is looking now we can compare
both the images so this is our original image and this is our new image so as you can
see there is not a lot of information that has been lost it pretty much looks like the
original image yes we can see that for example here there is a little bit uh it appears a little dullish compared
to this one right because we kind of took off some of the finer details of the color but overall
the high level information has been maintained at the same time the main advantage is that now this can
be this is an image which can be rendered on a device which may not be that very
sophisticated now let's take one more example with a different image in the second example we will take an
image of the summer palace in china and we repeat the same process
this is a high definition color image with millions of colors and also
three dimensional now we will reduce that to 16 colors using k means clustering
and we do the same process like before we reshape it and then we cluster the
colors to 16 and then we render the image once again and we will see that the
color the quality of the image slightly deteriorates as you can see here
this has much finer details in this which are probably missing here but then
that's the compromise because there are some devices which may not be able to handle
this kind of high density images so let's run this chord in python notebook all
right so let's apply the same technique for another picture which is even more intricate and has probably
much complicated color schema so this is the image now once again we can
take a look at the shape which is 427 by 640 by 3 and this is the new data would look
somewhat like this compared to the flower image so we have some new values here
and we will also bring this as you can see the numbers are much big
so we will much bigger so we will now have to scale them down to values between 0 and
1 and that is done by dividing by 255 so let's go ahead and
do that and reshape it okay so we get a two-dimensional
matrix and we will then as a next step we will go ahead and visualize this how
it looks does the 16 colors and this is basically how it would look
16 million colors and now we can create the clusters out of this the 16
k means clusters we will create so this is how the distribution of the pixels would
look with 16 colors and then we go ahead and apply this
and visualize how it is looking for with the new just the 16 color so once
again as you can see this looks much richer in color but at the same time
and this probably doesn't have as we can see it doesn't look as rich as this one but
nevertheless the information is not lost the shape and all that stuff and this can be also rendered on a
slightly a device which is probably not that sophisticated okay so that's pretty much it so we have
seen two examples of how color compression can be done using k-means clustering
and we have also seen in the previous examples of how to implement k-means the code to
roughly how to implement k-means clustering and we use some sample data using blob to
just execute the k-means clustering dimensionality reduction dimensionality reduction refers to the
technique that reduces the number of input variables in a data set
and so you can see on the table on the right shows the orders made at an automobile parts retailer
the retailer sells different automobile parts from different companies and you can see we have company b-packs
iso max and they have the item the tire the axle an order id a price number and a quantity in order
to predict the future cells we find out that using correlation analysis
that we just need three attributes therefore we have reduced the number of attributes
from five to three and clearly we don't really care about the part number i don't think the part number would have
an effect on how many tires are bought and even the store who's buying them
probably does not have an effect on that in this case that's what they've actually done is remove those and we just have the item the tire the
price and the quantity one of the things you should be taking away from this is in the scheme of
things we are in the descriptive phase we're describing the data
and we're pre-processing the data what can we do to clean it up why dimensionality reduction well number
one less dimensions for a given data set means less computation or training time that
can be really important if you're trying a number of different models
and you're re-running them over and over again and even if you have seven gigabytes of data that can start taking days to go through
all those different models so this is huge this is probably the hugest part as far as reducing
our data set redundancy is removed after removing similar entries from the data set
again pre-processing some of our models like a neural network if you put in two of the same data
it might give them a higher weight than they would if it was just once we want to get rid of that redundancy
it also increases the processing time if you have multiple data coming in space required to store
the data is reduced so if we're committing this into a big data
pool we might not send the company that bought it why would we want to store two whole extra columns when we added into that
pool of data makes the data easy for plotting in 2d and 3d plots
this is my favorite part very important you're in your shareholder meeting you want to be able to give them a
really good clear and simplified version you want to reduce it down to something people can
take in it helps to find out the most significant features and skip the rest
which also comes in in post scribing leads to better human interpretation
that kind of goes with number four it makes data easy for plotting you have a better interpretation when we're looking at it
principle component analysis so what is it uh principal component analysis is a
technique for reducing the dimensionality of data sets increasing interpretability but at the
same time minimizing information loss so we take some very complex data set with lots of
variables we run it through the pca we reduce the variables we end up with the reduced
variable setup this is very confusing to look at because
if you look at the end result we have the different colors all lined up so what we're going to take a look at is
let's say we have a picture here let's say you are asked to take a picture of some toddlers and you are deciding which angle would be the best
to take the picture from so if we come up here we look at this we say okay this is you know one angle
we get the back of a lot of heads not many faces so we'll do it from here we might get
the one person up front smiling a lot of the people in the class are missing so we have a huge amount off to
the right of blank space maybe from up here again we have the back of someone's head
and it turns out that the best angle to click the picture from might be this bottom left angle you look
at it you say hey that makes sense it's a good configuration of all the people
in the picture now when we're talking about data it's not you really can't do it by what
you think is going to be the best we need to have some kind of mathematical formula so it's consistent and so it
makes sense in the back end one of the projects i worked on many years ago
has something similar to the iris if you've ever done the iris data set it's probably one of the
most common ones out there where they have the flower and they're measuring the stamen in the
petals and they have width and they have length of the petal instead of putting through the width and
the length of the petal we could just as easily do the width-to-length ratio we can divide the
width by the length and you get a single number where you had two that's the kind of idea that's going on
into this in pre-processing and looking at what we can do to bring the data down the very simplified example on my iris
pedal example when we look at the similarity in pca we find the best picture or
projection of the data points and so we look down at from one angle we've drawn a line down there we can see
these data points based on in this case just two variables now keep in mind we're usually talking about 36
40 variables almost all of your business models usually have about 26 to
27 different variables they're looking at same thing with like a bank loan model we're
talking 26 to 36 different variables they're looking at that are going in so we want to do is we want to find the
best view in this case we're just looking at the xy we look down at it and we have our
second idea pc2 and again we're looking at the x i this x y this time from a different direction
here for our e's we can consider that we get two principal components namely pc1 and
pc2 comparing both the principal components we find
the data points are sufficiently spaced in pc1 so if you look at what we got here we
have pc1 you can see along the line how the data points are spaced versus the spacing in pc2
and that's what they're coming up with what is going to give us the best look for these data points when we combine
them and we're looking at them from just a single angle whereas in pc2 they are less spaced
which makes the observation and further calculations much more difficult therefore we accept the pc one and not
the pc2 as the data points are more spaced now obviously the back end calculations
are a little bit more complicated when we get into the math of how they decide what is more valuable this gives you an
idea though that when we're talking about this we're talking about the perspective which would help in understanding how
pca analysis works we want to go ahead and do is dive into the important
terminologies under uh pca and important terminologies are views
the perspective through which data points are observed and so you'll hear that if someone's
talking about a pca presentation and they're not taking the time to reduce it to something that the average
person shareholders can understand you might hear them refer to it as the different views what view are we taking dimension number of columns in a data
set are called the dimensions of that data set and we talked about you'll hear features dimensions
i was talking about features there's usually when you're running a business you're talking 25 26 27 different
features minimal and then you have the principal component new variables that are constructed as
linear combinations or mixtures of the initial variables principal component is very important
it's a combination if you remember my flower example it would be the width over the length of the pedal
as opposed to putting both width and length in you just put in the ratio instead which is a single number
versus two separate numbers projections the perpendicular distance between the principal component
and the data points and that goes to that line we had earlier it's that right angle
line of where those point all those points fall onto the line important properties
important properties number of principal components is always less than or equal to the number of attributes
that just makes common sense you're not going to do 10 principal properties with only three
features you're trying to reduce them so it's just kind of goofy but it is important to remember that people will throw
weird code out there and just randomly do stuff with instead of really thinking it through principal components are orthogonal
and this is what we're talking about that right angle from the line when we do pc1 we're looking at how
those points fall on to that line same thing with pc2 we want to make sure
that pc1 does not equal pc2 we don't want to have the same two principal points
when we do two points the priority of principal components decreases as their numbers
increase this is important to understand
if you're going to create one principle component everything is
summarized into that one component as we go to two components the priority how much it holds
value decreases as we go down so if you have five different points each one of those points is going to
have less value than just the one point which has everything summarized in it how pca works i said there was more in
the back end we talk about the math this is what we're talking about is how does it actually work so now we have understanding that
you're looking at a perspective now we want to see how that math side works pca performs the following operations in
order to evaluate the principal components for a given data set
first we start with the standardization then we have a covariance matrix
computation and we use that to generate our i gene vectors and i gene values
which is the feature vector and if you remember the i gene vector is like a translation for moving the
data from x equals one to x equals two or whatever we're altering it and the ign value is the final value
that we generate when we talk about standardization the main aim of this step
is to standardize the range of the attributes so that each one of them lie within similar boundaries this
process involves removal of the mean from the variable values and scaling the data
with respect to the standard deviation and you can see here we have z equals
the variable values minus the mean over the standard deviation the covariance matrix
computation covariance matrix is used to express the correlation between any two or more attributes in
multi-dimensional data set the covariance matrix has the entries as
the variance and the covariance of the tribute values the variance is denoted by var and the covariance is denoted by
cov on the right we can see the covariance matrix for two attributes and their values
when we do a hands-on and look at the code we'll do a display of this so you can see what we're talking about and
what that looks like for now you can just notice that this is a matrix that we're generating with the variance
and then the covariance of x to y on the right side we can see the covariance table for more than two
attributes in a multi-dimensional data set this is what i was talking about we
usually are looking at not just one feature two features we're usually looking at
25 30 features going on and so if we do a setup like this we
should see all those different features as the different variables covariance matrix tells us how the two
or more variables are related positive covariance indicate that the value of one variable is directly
proportional to the other variable negative covariance indicate that the value of one variable is
inversely proportional to the other variable that is always important to note whenever we're doing any of these
matrixes that we're going to be looking at that positive and negative whether it's inverted or not and then we have the iogene values and
the i gene vectors eigenvalues and hygiene vectors are the mathematical value
that are extracted from the covariance table they are responsible for the generation
of a new set of variables from the old set of variables which further lead to the construction
of the principal components igen vectors do not change directions after linear transformation
i gene values are the scalars or the magnitude of the i gene vectors and again this is just chain
transforming that data so we're going to change uh the vector b to the b prime as denoted
on the chart and so we have like multiple variables how do we calculate that new variable
and then we have feature vectors feature vectors is simply a matrix that has igen vectors of
the components that we decide to keep as the columns here we decide whether we must keep or
discard the less significant principal components that we have generated in the above steps
this becomes really important as we start looking at the back end of this and
we'll do this in the demo but one of the more important steps to understand
and so we have the pca example consider matrix x within rows or observations and
k columns or variables now for this matrix we would construct a variable space with
as many dimensions as the variable but for our simplicity let's consider this three dimensions for
now now each observation row of the matrix x is placed in the k-dimensional variable
space such that the rows in the data table form a swarm of points in this space now we find the mean of
all the observations and then place it along the data points on the plot
the first principal component is a line that best accounts for the shape of the point swarm
it represents the maximum variance direction in the data each observation may be projected onto
this line in order to get a coordinate value along the pc1 this value is known as a score
usually only one principal component is insufficient to model the systematic variation for a data set
thus a second principal axis is created the second principle component is
oriented such that it reflects the second largest source of variation in the data while being
orthogonal to pc1 pc2 also passes through the average point
let's go ahead and pull this up and just see what that means inside our python scripting
i'm going to use the anaconda navigator and i will be in python 3.6
for this example i believe there's even like a 3.9 out i tend to stay in 3.6 because a lot of
the models i use especially with the neural networks are stable in 3 6.
and then we open up our jupiter i'm in chrome and we go ahead and create a new python
three and for ease of use our team in the back
was nice enough to put this together for me and we'll go and start with the libraries the first thing i like to do
whenever i'm looking at any new setup uh well you know what let's do let's do the libraries first
we're going to do our basic libraries which is matplot library the plt from the matplot library pandas
our data frame pd numpy our numbers array np seaborn for graphing sns that goes with
the plot that actually sits on that plot library so the seaborn sits on there and then we have our amber sign because
we're in jupiter notebook matplot library in line the newer version actually doesn't require that but i put it in there
either anyway just because i'm so used to it and then we want to go ahead and take a
look at the data and in this case we're going to
pull in certainly you can have lots of fun with different data but we're going to use the cancer data set
and one of the reasons the cancer data set is it has like 36 35 different features so it's kind of fun to use that as our
base for this and we'll go ahead and run this and look at our keys
and the first thing we notice in our keys for the cancer data set as we have our data we have our target
our frame target names description feature names and file name
so what we're looking for in all this is let's take a look at the description
let's go in here and pull up the description on here
i'm not going to spend a huge amount of time on the description because this is we don't want to get
into a medical domain we want to focus on our pca setup
what's important is you start looking at what the different attributes are what they mean
if you were in the medical field you'd want to note all these different things whether what they're measuring where it's coming from
you can actually see the actual different measurements they're taking
no missing attributes we page all the way to the bottom and you're going to have your data
in this case our target and if you dig deep enough to the target
let's actually do this let's go ahead and print target names
real quick here i always like to just take a look and see what's on the other end of this
target names run that yeah
so the target name is is it malignant or is it b9 so in other words is this dangerous
growth or is it something we don't have to worry about that's the bottom line with the cancer in this case
and then we can go ahead and load our data and uh you know let me go up a just a notch here for easy of reading
it's hard to get that just right that's all you have to do uh so let's go ahead and look at our data uh our we're going to use our
pandas and we're going to go ahead and do our data frame it's going to equal cancer data
columns equals cancer feature equals feature names so remember up here we already loaded the the
names up of our of the features in there what is going to come out of this let me just see if we can get to that
it's at the top of target names that's just this list of names here in
the setup and we can go ahead and run this code
and i'll print the head and you can see here we have the mean radius the mean texture mean perimeter i don't know about you
this is a wonderful data set if you're playing with it because like many of the data that most of the data that comes in half the time we
don't even know we're looking at uh we're just handed a bunch of stuff as a data scientist going what the heck is
this and so this is a good place to start because this has a number of different features in there we have no idea what
these feature means or where they come from we want to just look at the data and figure that out
and now we actually are getting into the pca side of it as we've noticed before it's difficult to visualize high dimensional data
we can use pca to find the first two principal components and visualize the data this new
two-dimensional space with a single scatter plot before we do this we need to go ahead and scale our
data now i haven't run this to see if you really have to scale the data on this
but as just a general run time i almost do that as the first
step of any modeling even if it's pre-modeling as we're doing here
in neural networks that is so important with pca visualization it's already going to scale it when we
do the means and deviation inside the pca but just in case it's always good to
scale it and then we're going to take our pca with the scikit
learn uses very similar process to other pre-processing functions that come with scikit-learn
we instantiate a pca object find the principal components using the fit method then apply the rotation and
dimensionality reduction by calling transform we can also specify how many components
we want to keep when creating the pca object
and so the code for this oops getting a little bit ahead let me
go and run this code so the code for this is
from sk learn decomposition import pca pca equals pca in components equals 2
and that's really important to note that because we're only going to want to look at two components
i would never go over four components especially if you're going to demo this with somebody else
if you're showing this to the shareholders the whole idea is to reduce it to something people can see
and then the pca fit we're going to is going to take the scaled data that we generated up here
and then you can see we've created our pca model with in components equals 2.
now whenever i use a new tool i like to go in there and actually see what i'm
using so let's go to the scikit webpage for the pca
and you can see in here here's our call statement it describes what all the different setups you have on there probably the
biggest one to look at would be well the biggest one is your components how many components do you want
which you have to put in there pretty much and then you also might look at the svd solver it's on auto right now but
you can override that and do different things with it it does a pretty good job as it is
and if we go down all the way down to
um there we go to our methods if you notice
we have fit we have fit transform nowhere in here is predict because this
is not used for prediction it's used to look at the data again we're in the describe
setup we're fitting the data we're taking a look at it we've already looked at
our minimum maximum we've already looked at what's in each quarter we've done a full description of the data this is part of describing
the data that's the biggest thing i take away when i come zooming in here and of course i have
examples of it down here if you forget and the biggest one of course is the
number of components and then i mean the rest you can play with
the actual solver whether you're doing a full or randomized there's different things it does pretty good on the auto
and now we can transform this data to its first two principal components
and so we have our xpca we're going to set that equal to pca
transform scaled data so there we go there's our first transformation
and let's just go ahead and print the scaled data shape and the xpca data shape
and the reason we want to do this is just to show us uh what's going on here we've taken 30
features i think i said 36 or something like that but it's 30 and we've compressed it down to two
features and we decided we wanted two features and that's where this comes from we still have 569 data sets
i mean data rows data sets we still have 569 rows of data but instead of computing 30 features
we're now only doing our model on two features
so let's go ahead and plot these and take a look and see what's going on
and we're just going to use our plt figure we'll set the figure size on here
here's our scatter plot xpca x underscore pca of of one
these are two different perceptions we're using uh and then you'll see right here c for
color cancer equals target and so remember we have zero we have one and if i remember
correctly 0 was malignant 1 was b9 so everything in the zero column is
going to be one color and the other color is going to be 1 and then we're going to use the plasma map just kind of tell you what color it
is add some labels first principal component second principle component and we'll go ahead and run this and you
can see here instead of having a chart one of those heat maps with 30
different columns in it we can look at this and say hey this one actually did a
pretty good job of separating the data
and a couple of things when i'm looking at this that i notice is first we have a very clear area where it's
clumped together where it's going to be benign and we have a huge area
it's still clumped together more spread out where it's going to be malignant or i think i had that backwards and then
in the middle because we're dealing with something in this particular case cancer we would try to separate i would
be exploring how to separate this middle group out in other words there's an area where
everything overlaps and we're not going to have a clear result on it just because those are the people
you want to go in there and have extra tests or treat it differently versus going in
and saying just cutting into the can into the cancer so the body absorbs it and it dissipates
versus actively going in there removing it testing it going through chemo and
all the different things that's a big difference you know as far as what's going to happen here in that
middle line where the overlap is going to be huge that's domain specific going back to the
data we can see here clearly by using these two components we can easily separate
these two classes so the next step is what does that mean interpreting the components
unfortunately with this great power of dimensionality reduction comes the cost of not being able to
easily understand what these components represent i don't know what principle component one looks work represents or second
principle the components correspond to combinations of original features
the components themselves are stored as an attribute of the filtered pca object and so we talk look at that we
can go ahead and do look at the pca components this is in our model we built we've trained it we can run that and you
can see here's the actual components uh it's the two components have each have their own array
and within the array you can see the what the scores are using and these actually give weight to what
features are doing what so in this numpy matrix array each row represents a
principal component and each column relates back to the original features what's
really neat about this is we can now go in reverse and drop this onto a heat map
and start seeing what this means and so let me go ahead and just put this down up here to get it
down here we'll go ahead and put this in here we're going to use our df comp data
frame and we do our pca components and i want you to notice how easy this
is we're going to set our columns equal to cancer feature names
that just makes it really easy and we're dumping it into a data frame what's neat about a data frame
is when we get to seaborn it will pull that data frame apart and and set it up for us what we want
and so we're just going to do the see the seaborne heat map of our data frame composition and we'll
use the plasma coloring and it creates a nice little color graph
here you can see we have the mean radius and all the different features along the bottom
on the right uh we have a scale so we can see we have the dark colors all the way to the really light colors which are what's
really shining there this is like the primary stuff we want to look at
so this heat map and the color bar basically represent the correlation between the various features
and the principal component itself so you know very powerful map to look at
and then you can go in here and we might notice that the mean radius look how how on the bottom of the map it
is on some of this so you have some interesting correlations here that change the
variations on that and what means what this is more when you get to a post
scribe you can also use this to try to guess as what these things mean and what you want to change to get a better result
why reinforcement learning training a machine learning model requires a lot of data which might not always be available to
us further the data provided might not be reliable learning from a
small subset of actions will not help expand the vast realm of solutions that may work for a particular problem
you can see here we have the robot learning to walk very complicated setup when you're
learning how to walk and you'll start asking questions like if i'm taking one step forward and left what happens if i pick up a 50 pound
object how does that change how a robot would walk these things are very difficult to
program because there's no actual information on it until it's actually tried out learning from a small subset of actions will not help
expand the vast realm of solutions that may work for a particular problem and we'll see here learned how to walk
this is going to slow the growth that technology is capable of machines
need to learn to perform actions by themselves and not just learn off humans and you see the objective
climb a mountain real interesting point here is that as human beings we can go into a
very unknown environment and we can adjust for it and kind of explore and play with
it most of the models the non-reinforcement models in computer machine learning aren't able to
do that very well there's a couple of them that can be used or integrated to see how it goes is what we're talking
about with reinforcement learning so what is reinforcement learning reinforcement learning is a
sub-branch of machine learning that trains a model to return an optimum solution for a problem by taking
a sequence of decisions by itself consider a robot learning to go from one place to another
the robot is given a scenario must arrive at a solution by itself the robot can take different paths to
reach the destination it will know the best path by the time taken on each path and might even come
up with a unique solution all by itself and that's really important is we're looking for
unique solutions we want the best solution but you can't find it unless you try it
so we're looking at uh our different systems our different model we have supervised versus unsupervised versus
reinforcement learning and with the supervised learning that is probably the most controlled environment
we have a lot of different supervised learning models whether it's linear regression neural networks there's all kinds of
things in between decision trees the data provided is labeled data with output values
specified and this is important because we talk about supervised learning you already know the answer for all this
information you already know the picture has a motorcycle in it so your supervised learning you already know that um
the outcome for tomorrow for you know going back a week you're looking at stock you can already have like the graph of
what the next day looks like so you have an answer for it and you have labeled data
which is used you have an external supervision and solves problems by mapping labeled input to no one output
so very controlled unsupervised learning and unsupervised learning is really
interesting because it's now taking part in many other models they start with and you can actually insert an unsupervised
learning model in almost either supervised or reinforcement learning as part of the
system which is really cool data provided is unlabeled data the outputs are not specified machine
makes its own predictions used to solve association with clustering problems
unlabeled data is used no supervision solves problems by understanding patterns and discovering output
so you can look at this and you can think um some of these things go with each other they belong together
so it's looking for what connects in different ways and there's a lot of different algorithms that look at this
when you start getting into those are some really cool images that come up of what unsupervised learning is how it
can pick out say the area of a donut one model will see the area of the donut and the other one
will divide it into three sections based on its location versus what's next to it so there's a lot of stuff that goes in
with unsupervised learning and then we're looking at reinforcement learning probably the biggest
industry in today's market in machine learning or growing market it's very it's very infant stage as far as how it
works and what's going to be capable of the machine learns from its environment using rewards and errors
used to solve reward based problems no predefined data is used no supervision follows
trail and error problem solving approach so again we have a random first you start with a random i try this
it works and this is my reward doesn't work very well maybe or maybe doesn't even get you where you're trying to get it to do and you
get your reward back and then it looks at that and says well let's try something else and it starts to play with these
different things finding the best route so let's take a look at important terms in today's reinforcement
model and this has become pretty standardized over the last few years so these are really good to
know we have the agent agent is the model that is being trained via reinforcement learning
so this is your actual entity that has however you're doing it with using a neural network or
a cue table or whatever combination thereof this is the actual
agent that you're using this is the model and you have your environment uh the
training situation that the model must optimize to is called its environment and you can see here i guess we have a
robot who's trying to get chest full of gems or whatever and that's the output and then you have your
action this is all possible steps that can be taken by the model and it picks one action and you can see
here it's picked three different routes to get to the chest of diamonds and gyms
we have a state the current position condition returned by the model and you could look at this
if you're playing like a video game this is the screen you're looking at so when you go back here the environment
is a whole game board so if you're playing one of those mobius games you might have the whole game board
going on uh but then you have your current position where are you on that game board what's around that
what's around you if you were talking about a robot the environment might be moving around
the yard where it is in the yard and what it can see what input it has in that location
that would be the current position condition returned by the model and then the reward to help the model
move in the right direction it is rewarded points are given to it to appraise some kind of action
so yeah you did good or didn't do as good trying to maximize the reward and have
the best reward possible and then policy policy determines how an
agent will behave at any time it acts as a mapping between action and present state this is part of the
model what is your action that you're you're going to take what's the policy you're using to have an output from your agent one of
the reasons they separate policy as its own entity
is that you usually have a prediction of a different options and then the
policy well how am i going to pick the best based on those predictions i'm going to guess at different options and we'll actually
weigh those options in and find the best option we think will work so it's a little tricky
but the policy thing is actually pretty cool how it works let's go and take a look at a reinforcement learning example
and just in looking at this we're going to take a look consider what a dog that we want to train so the dog would
be like the agent so you have your your puppy or whatever and then your environment is going to be
the whole house or whatever it is where you're training them and then you have an action we want to teach the dog to fetch
so action equals fetching and then we have a little biscuit so we can get the dog perform various actions by offering
incentives such as a dog biscuit as a reward the dog will follow a policy to maximize
this reward and hence will follow every command and might even learn new actions like
begging by itself uh so you have you know so we start off with fetching it goes oh i get a biscuit for that
it tries something else you get a handshake or begging or something like that and it goes oh this is also reward based
and so kind of explores things to find out what will bring is biscuit and that's very much like how
reinforced model goes as it looks for different rewards how do i find can i try different things and find
a reward that works the dog also will want to run around and play in explorer's environment
uh this quality of model is called exploration so there's a little randomness going on
in exploration and explores new parts of the house
climbing on the sofa doesn't get a reward in fact it usually gets kicked off the sofa
so let's talk a little bit about markov's decision process markov's decision process is a
reinforcement learning policy used to map a current state to an action where the agent continuously interacts
with the environment to produce new solutions and receive rewards and you'll see here's all of our
different vocabulary we just went over we have a reward our state our agent our environment interaction
and so even though the environment kind of contains everything that you really
when you're actually writing the program your environment's going to put out a reward in state that goes into the
agent the agent then looks at this state or it looks at the reward usually um
first and it says okay i got rewarded for whatever i just did or it didn't get rewarded and it looks
at the state and then it comes back and if you remember from policy the policy comes in
and then we have a reward the policy is that part that's connected at the bottom and so it looks at that policy and says
hey what's a good action that will probably be similar to what i
did or sometimes they're completely random but what's a good action that's going to bring me a different reward
so taking the time to just understand these different pieces as they go is pretty important in most
of the models today um and so a lot of them actually have templates based on this you can pull in and start
using pretty straightforward as far as once you start seeing how it works
uh you can see your environment sends it says hey this is the agent did this if you're a character in the game this happened
and it shoots out a reward in a state the agent looks at the reward looks at the new state
and then takes a little guess and says i'm going to try this action and then that action goes back into the environment it
affects the environment the environment then changes depending on what the action was and then it has a new state
and a new reward that goes back to the agent so in the diagram shown we need to find the shortest path between node a
and d each path has a reward associated with it and the path with a maximum reward is
what we want to choose the nodes a b c d denote the nodes to travel from node
a to b is an action reward is the cost of each path and policy is each path taken
and you can see here a can go uh to b or a can go to c right off the bat or it
can go right to d and if you explored all three of these you would find that a going to d was a zero reward a going to c
and d would generate a different reward or you could go a c b d there's a lot of
options here and so when we start looking at this diagram you start to realize
that even though today's reinforced learning models do really good at finding an answer they end up
trying almost all the different directions you see and so they take up a lot of work or a
lot of processing time for reinforcement learning they're right now in their infant stage and they're really good at solving
simple problems and we'll take a look at one of those in just a minute in the tic-tac-toe game but you can see here once it's gone
through these and it's explored it's going to find the a c d is the best reward he gets a full
30 points for it so let's go ahead and take a look at a reinforcement learning demo
in this demo we're going to use reinforcement learning to make a tic-tac-toe game you'll be playing this game against
the machine learning model and we'll go ahead and we're doing it in python so let's go ahead and go through
i always not always actually have a lot of python tools let's go through anaconda which will
open up a jupiter notebook seems like a lot of steps but it's worth it to keep all my stuff separate and
it's also has a nice display when you're in the jupiter notebook for doing python
so here's our anaconda navigator i open up the notebook which is going to take me to a webpage and i've gone in here and
created a new python folder in this case i've already done it and enabled it to change the name to tic-tac-toe
and then for this example we're going to go ahead and import a couple things we're going to
import numpy as np we'll go ahead and import pickle lumpy of course is our number array and
then pickle is just a nice way sometimes for storing different information different
states that we're going to go through on here and so we're going to create a class
called state we're going to start with that and there's a lot of lines of code to
this class that we're going to put in here don't let that scare you too much there's not as much here
it looks like there's going to be a lie here but there really is just a lot of setup going on in the in our class state and so we have up here
we're going to initialize it we have our board it's a tic tac toe
board so we're only dealing with nine spots on the board we have player one player two uh
is end we're gonna create a board hash we'll look at that in just a minute we're just going to store some
information in there symbol player equals one so there's a few things going on as far as the
initialization then something simple we're just going to get the hash of the board
you get the information from the board on there which is columns and rows we want to know when a winner occurs uh
so if you get three in a row that's what this whole section here is for uh let me go ahead and scroll up a
little bit and you can get a copy of this code if you send a note over to
simplylearn we'll send you over this particular file and you can play with it yourself and see how it's put together
i don't want to spend a huge amount of time on this because this is just some real general python coding
but you can see here we're just going through all the rows and you add them together and if it equals three three in a row
same thing with columns diagonals so you gotta check the diagonal that's what all
this stuff does here is it just goes through the different areas actually let me go ahead and put
there we go and then it comes down here we do our sum and it says true minus three
just says did somebody win or is it a tie so you gotta add up all the numbers on there anyway just in case
they're all filled up and next we also need to know available positions
these are ones that don't no one's ever used before this way when you try something or the computer tries something
it's not going to give it an illegal move that's what the available positions is doing then we want to update our state and so
you have your position going in we're just sending in the position that you just chose and you'll see there's a
little user interface we put in there we can pick the row and column in there
and again i mean this is a lot of code uh so really it's kind of a thing you'd
want to go through and play with a little bit and just read through it get a copy of it a great way to understand how this works
and here is a given reward so we're going to give a reward result
equals self winner this is one of the hearts of what's going on here is we have a result self.winner
so if there's a winner then we have a result that the result equals one here's our feedback if it doesn't equal
one then it gets a zero so it only gets a reward in this particular case
if it wins and that's important to know because different systems of reinforced
learning do rewarding a lot differently depending on what you're trying to do this is a very simple example
with a three by three board imagine if you're playing a video game
certainly you only have so many actions but your environment is huge you have a lot going on in the
environment and suddenly a reward system like this is going to be just
it's going to have to change a little bit it's going to have to have different rewards and different setup and there's all kinds of advanced ways
to do that as far as weighing you add weights to it and so
they can add the weights up depending on where the reward comes in so it might be that you actually get a reward
in this case you get the reward at the end of the game and i'm spending just a little bit of
time on this because this is an important thing to note but there's different ways to add up those rewards it might have like if you
take a certain path the first reward is going to be weighed a little bit less than the last reward
because the last reward is actually winning the game or scoring or whatever it is so this reward system gets really
complicated in some of the more advanced uh setups in this case though
you can see right here that they give a a 0.1 and a 0.5 reward
just for getting picking the right value and something that's actually valid instead of picking an invalid value
so rewards again that's like key it's huge how do you feed the rewards back in then we have a
board reset that's pretty straightforward it just goes back and resets the board to the beginning because it's going to try out all these
different things while it's learning it's going to do it by trial and error so you have to keep resetting it
and then of course there's the play we want to go ahead and play rounds equals 100 depends on what you
want to do on here you can set this different you can obviously set that to higher level
but this is just going to go through and you'll see in here that we have player 1 and player 2.
this is this is the computer playing itself one of the more powerful ways to learn
to play a game or even learn something that isn't a game is to have two of these models that are basically
trying to beat each other and so they always they keep finding explore new things this one works for this one so this one
tries new things it beats this we've seen this in chess i think with some big one where they had
the two players in chess with reinforcement learning it was one of the ways they trained one of the top computer chess playing
algorithms uh so this is just what this is it's going to choose an action it's going to
try something and the more it try stuff um the more we're going to record the hash we actually have a board
hash where they self get the hash setup on here where it stores all the information
and then once you get to a win one of them wins it gets the reward uh then we go back and reset and try
again and then kind of the fun part we actually get down here is we're going to play with a human so we'll get a chance to
come in here and see what that looks like when you put your own information in and then it just comes in here and does the
same thing it did above it gives it a reward for its things or sees if it wins or ties
looks at available positions all that fun of fun stuff and then finally we want to show the
board so it's going to print the board out each time really as an integration is not that
exciting what's exciting uh in here is one looking at this reward system whoops play one more up
the reward system is really the heart of this how do you reward the different setup and the other one is when it's
playing it's got to take an action and so what it chooses for an action is also
the heart of reinforcement learning how do we choose that action and those are really key to right now
where reinforcement learning is in today's technology
is figuring this out how do we reward it and how do we guess the next best action
so we have our environment and you can see the environment is we're going to be or the
state which is kind of like what's going on we're going to return the state depending on what happens
and we want to go ahead and create our agent in this place our player so each one is let me go and
grab that until we look at a class player
this is where a lot of the magic is really going on is what how is this player figuring out how to maneuver around the board and then the
board of course returns a state that it can look at and reward so we want to take a look at this we have
name self state this is class player when you say class player we're not talking about a human player we're
talking about just the computer players and this is kind of interesting so remember i told
you depending on what you're doing there's going to be a decay gamma
explore rate these are what i'm talking about is how do we train it um as you try different moves
it gets to the end the first move is important but it's not as important as the last one
and so you could say that the last one has the heaviest weight and then as you as you get there the
first one let's see the first move gives you a five reward the second gives you a two reward
and the third one gives you a 10 reward because that's the final ending you got it the 10's gonna count more than the first
step uh and here's our we're going to get the board information coming in
and then choose an action this was the second part that i was talking about that was so important
so once you have your training going on we have to do a little randomness and you can see right here is our np
random uniform so it's picking out a random number take a random action
this is going to just pick which row and which column it is um and so choosing the action this one
you can see we're just doing random states um choice length of positions action position
and then it skips in there and takes a look at the board for p in positions it's actually storing
the different boards each time you go through so it has a record of what it did so it can properly weigh the values
and this simply just depends a hash state what's the last date pinned it to the to our states on here
here's our feedback rewards the reward comes in and it's going to take a look at this
and say is it none what is the reward and here is that formula
remember i was telling you about up here that was important because it has
decay gamma times the reward this is where as it goes through each step
and this is really important this is this is kind of the heart of this of what i was talking about earlier uh
you have step one and this might have a reward of two you
have step two i should probably should have done abc this has a step three
uh step 4 and so on until you get to step in and this might
have a reward of 10. so reward a 10
we're going to add that but we're not adding uh let's say this one right here let's say
this reward here right before 10 was um let's say it's also 10 that just makes the
math easy so we had 10 and 10. we had 10 this is 10 and 10 in whatever
it is but it's time it's 0.9 so instead of putting a full 10 here
we only do 9 that's a 0.9 times 10.
and so this formula as far as the decay times the reward
minus the cell state value it basically adds in it says here's one
or here's two i'm sorry i should have done this abc would have been easier uh so the first move goes in here and it
puts two in here uh then we have our self set up on here
you can see how this gets pretty complicated in the math but this is really the key is how do we train our states and we want the
the final state the win to get the most points if you win you get most points
and the first step gets the least amount of points so you're really training this almost in
reverse you're training you're training it from the last place where you have like it says okay this is now where's need to sum up my rewards
and i want to sum them up going in reverse and i want to find the answer in reverse kind of an interesting uh play on the
mind when you're trying to figure this stuff out and of course we want to go ahead and
reset the board down here save the policy load policy
these are the different things that are going in between the agent and the state to figure out what's going on
let's go ahead and load that up and then finally we want to go ahead and create a human player
and the human player is going to be a little different in that you choose an action row and column
here's your action uh if action is if action in positions
meaning positions that are available you return the action if not it just keeps asking you until
you get the action that actually works and then we're going to go ahead and append to the hash state which we don't
need to worry about because it returns the action up here and feed forward uh again this is
because it's a human um at the end of the game bat propagate and update state values
this part isn't being done because it's not programming uh the model
the model is getting its own rewards so we've gone ahead and loaded this in here so here's all our pieces and the
first thing we want to do is set up p1 player 1 p2
player 2 and then we're going to send our players to our state so now it has p1 p2 and it's going to
play and it's going to play 50 000 rounds now we can probably do a lot less than
this and it's not going to get the full results in fact you know what let's go ahead and just do five just to
play with it because i want to show you something here oops somewhere in there i forgot to load
something there we go i must just forgot to run this
run oops forgot a reference there for the
board rows and columns three by three there is actually in the state it references that we just tack it on on
the end it was supposed to be at the beginning so now i've only set this up with
let's see where we go in here i've only set this up to train
five times and the reason i did that is we're going to
come in and actually play it and then i'm going to change that and we can see how it differs on there
there we go and then you make it through a run and we're going to go ahead and save the policy
so now we have our player 1 and our player 2 policy the way we set it up it has two separate
policies loaded up in there and then we're going to come in here and
we're going to do uh player 1 is going to be the computer experience rate 0 load policy 1
human player human and we're going to go ahead and play this i remember i only went through it just
one round of training in fact minimal training and so it puts an x there and i'm going to go ahead and do row 0
column 1. you can see this is very basic on here and so i
put in my zero and then i'm going to go zero block it zero zero and you can see right here it let
me win uh just like that i was able to win zero
two and whoo human wins so i only trained it five times we're
going to run this again and this time instead of five
let's do five thousand or fifty thousand i think that's what the guys in the back had and this takes a
while to train it this is where reinforcement learning
really falls apart look how simple this game is we're talking about a three by three set of
columns and so for me to train it on this
um i could do a q table which would take which would go much quicker you could build a quick q table with
almost all the different options on there and you would probably get a the same
result much quicker we're just using this as an example so when we look at
reinforcement learning you need to be very careful what you apply it to it sounds like a
good deal until you do like a large neural network where you're doing you set the neural
network to a learning increment of one so every time it goes through it learns and then you
do your actions you pick from the learning setup and you actually try actions on the
learning setup until you get what you think is going to be the best action so you actually feed what you think is
right back through the neural network there's a whole layer there which is really fun to play with
and then it has an output well think of all those processes i mean that is just a huge amount of
work it's going to do let's go ahead and skip ahead here give it a moment it's going to take a
minute or two to go ahead and run now to train it we went ahead and let it
run and it took a while this this took i got a pretty powerful processor and it took about
five minutes plus to run it and we'll go ahead and run our
player setup on here oops brought in the last whoops i brought in the last round so
give me just a moment to re do the policy save there we go i forgot to save the policy back in there
and then go ahead and run our player again so we've saved the policy then we want
to go ahead and load the policy for p1 as the computer and we can see the computer's gone in the bottom right
corner i'm going to go ahead and go 1-1 which is the center
and it's gone right up the top and if you ever played tic-tac-toe you know the computer has me but we'll
go ahead and play it out row zero column two
there it is and then it's gone here and so i'm going to go ahead and go row zero one two
no zero one there we go and column zero that's where i want it oh and it says
okay you your action there we go boom uh so you can see here we've got a didn't catch the win on this it said tie
kind of funny that didn't catch the win on there but if we play this a bunch of times you'll find it's going to win more and
more the more we train it the more the reinforcement happens this lengthy training process
is really the stopper on reinforcement learning as this changes reinforcement learning
will be one of the more powerful packages evolving over the next decade or two
in fact i would even go as far as to say it is the most important machine learning tool and artificial
intelligence tool out there as it learns not only a simple tic tac toe board
but we start learning environments and the environment would be like in language if you're translating a language or
something from one language to the other so much of it is lost if you don't know the context it's in what the
environments it's in and so being able to attach environment and context and all those things together
is going to require reinforcement learning to do
so again if you want to get a copy of the tic tac toe board it's kind of fun to play with uh run it you can test it out you can do
you know test it for different values you can switch from p1 computer
where we loaded the policy 1 to load the policy 2 and just see how it varies there's all kinds of things you can do
on there in this video we will learn the top job roles that you can get after becoming a data science
professional so what really is data science data science is the art of discovering
patterns and trends from vast volumes of data using modern analytical tools and techniques it helps you draw meaningful
insights to make business decisions as you can see on the screen data science uses vast volumes of data which
is fed to a machine that runs several algorithms to process the data and finally derive insights
data science continues to evolve as one of the most promising and in demand career paths for skilled professionals
data science involves knowledge of programming mathematics and algorithms as well as domain expertise data science
is being used in almost every sector of business these days with companies generating vast volumes
of data on a regular basis they want to make sense of this data so now
let's look at the various applications of data science data science is transforming the
healthcare sector and is playing a pivotal role in monitoring a patient's health and deciding the necessary steps to be taken
in order to prevent potential diseases from taking place the primary use of data science in the health industry
is through medical imaging there are various imaging techniques like x-ray mri and ct scan all these techniques
visualize the inner parts of the human body data science is also used for drug discovery and genomics
which is the study of sequencing and analysis of genomes retail sectors use data science to
analyze their customer data and build sophisticated models to understand the customer behavior and find his or her sore points thereby
a customer tends to be easily influenced by the tricks developed by the retailers data science is widely used for
inventory management which refers to the stalking goods in order to use them in the time of crisis
data science proves to be really efficient in deciding the location to set up a new store
merchandising has become an essential part of retail business the merchandising mechanisms go through
the data pick up the insights and form the priority sets for the customers taking into account seasonality
relevancy and trends data science is mostly applied in the marketing domain for profiling
search engine optimization customer management responsiveness and running real-time marketing
campaigns predictive analytics is used in marketing to understand customers behavior
cluster models collaborative filtering and regression analysis are applied to spot the correlation
patterns in the customer's behavior to predict future tendencies in purchasing data science is also used to qualify and
prioritize leads bring the right product to the market and for targeting new customers
data science in automotives isn't just about self-driving cars advancement in technologies can help
keep auto organizations competitive by improving everything from research to design manufacturing to marketing
processes image recognition and anomaly detection can quickly detect and eliminate faulty parts before they
get into the vehicle manufacturing workflow root cause analysis predictive maintenance and supply chain
optimization are done using data science in the automobile sector next we have fraud detection frauds
can happen in many forms and it affects virtually every industry it has become a billion dollar business
and it is increasing every year the pwc global economic crime survey of 2018
found that half that is around 49 percent of the 7200 companies they
surveyed had experienced fraud of some kind fraud regularly happens in the banking and insurance sector
as well as in e-commerce data science techniques and algorithms can help you overcome these frauds
the last application is regarding recommender systems data science helps companies make better
decisions and recommender systems help data scientists succeed in it recommender systems are tools designed
for interacting with large and complex information spaces and privatizing items in these spaces
that are likely to be of interest to the user personalized recommendations are an important part of many online e-commerce
applications like amazon.com netflix and pandora now that we have
looked at the various applications of data science let's see the different job roles you can get into after becoming a data
science professional so the first job role we have is of a data analyst
a data analyst is responsible to collect process and perform analysis on large data sets
they bring technical expertise to ensure the quality and accuracy of that data then process design and present it in
ways to help people businesses and organizations make better decisions they also have to perform queries on the
databases from time to time they should have technical expertise in sql business intelligence and
programming languages such as java and python now coming to the salary a data analyst
in the united states earns around 62 453 dollars per annum while in india you
can expect to earn nearly 5 lakhs 21 000 rupees per annum some of the companies hiring for data
analysts are electronic arts microsoft bloomberg amazon accenture and siemens
the second job role we have is of a business analyst business analysts are people who are
responsible for bridging the gap between it and the business using data analytics to assess processes
determine requirements and deliver data driven recommendations and reports to executives and
stakeholders they create a detailed business analysis outlining problems
opportunities and solutions for a business a business analyst should have good communication and problem solving
skills also they need to know python r bi tools sql and excel
now moving to their salaries a business analyst earns around 68 346 dollars per annum
in the united states while in india they earn nearly 7 lakh rupees per annum
the companies that can help you start your career as a business analyst at dell gen pact philips iqvia and honeywell to
name a few next we have data engineer or data architect
a data engineer or architect develops constructs tests and maintains architectures such
as databases and large-scale processing systems they discover opportunities for data acquisition develop data set processes
for data modeling data mining and production they also recommend ways to improve data
reliability efficiency and data quality they should be well versed with big data analytics
hadoop scala data warehousing data modeling etl and linux operating systems
a data engineer earns around hundred and three thousand dollars in the united states while in india they
earn around eight lakhs eighty five thousand rupees per annum companies hiring for data engineers and
architects are fidelity investments capgemini gen pact accenture
standard charter and hewlett packard enterprise at number 4 we have machine learning
engineer a machine learning engineer uses different algorithms and statistical modeling
to make sense of vast volumes of data they are also expected to perform eb testing
build data pipelines and implement common machine learning algorithms such as regression classification clustering etc they
should know programming probability and statistics sql data visualization and machine learning
coming to their salary a machine learning engineer earns around hundred and fourteen thousand dollars per annum in the united
states while in india they can earn around 8 lakh rupees per annum some of the companies that can help you
excel in your career as a machine learning engineer are apple oracle cloudex lab rapido qualcomm
and amazon and finally we have the most exciting job role in the field of data
science that is of a data scientist a data scientist makes value out of data
they proactively fetch information from various sources and analyze it for better understanding about how the business performs
and builds ai tools that automate certain processes within the company a data scientist understands the
challenges in business and comes up with the best solutions using modern tools and techniques to analyze visualize and build
prediction models to make business decisions they need to have experience querying databases and using statistical computer
languages such as r python sql etc they should have knowledge of advanced
statistical techniques and concepts such as regression properties of distributions and statistical tests
as per current trends a data scientist can earn around 113 000 dollars per annum in the united states
while in india they can earn an average of 10 lakhs 47 000 rupees per annum companies hiring for data
scientists are amazon ibm apple rakuten general electric
philips and google now let me repeat the list once again first we cover data analyst then
business analyst followed by data engineer or architect machine learning engineer and finally
data scientist so with that we have covered the top five job roles in data science
apart from these job roles you can also become a statistician a database administrator an analytics
consultant and business intelligence professional let's go ahead and take a look at building a resume always exciting
putting ourselves out selling ourselves and if you looked at some of our other videos dealing with
resumes you'll see a trend here uh the top part is so different than what resumes were
in the 90s uh in the 2000 to 2010 they've evolved they've really evolved
it used to be 2000 2010 maybe linkedin maybe one other reference now you're
going to see that we want those references this is a sales tactic which now has come into resumes
used to be that if you're a real estate agent every real estate agent i knew i used to deal with real estate software for the real estate industry back in the
90s every real estate agent won their picture on their business card they wanted them
their picture if they could put on their contracts they want people to see the face
so that's really a big change is to make sure that it stands out you stand out here's a picture of somebody so you're
more than just a couple letters and a name but of course you need your contact information should always be at the top uh you have your summary
what are you focusing on be a little careful with your summary because if you have everything in your summary and then
they scroll down to experience and education and skills they're gonna stop the second you repeat
yourself in your resume that usually means to the reader hey this person doesn't have anything more to offer me i'm done
so be a little careful how you word your summary most companies appreciate when you come in
here and you've adjusted this summary to be both about you and how you're going to serve that
company so it's worth researching that company to find out how those connect and
put that in the summary take some time to do that that's actually a pretty big deal and the references are huge also
especially in data science when you're talking about any of the programming or data science data analytics
having a place to go where they can look it up and scroll down and see different things you're doing whether it's
linkedin in this case which is the business profile most commonly used github where you have stuff published
facebook i'm always hesitant because that tends to push more towards uh social media type jobs and
other jobs but certainly there's people who have facebook who do marketing and stuff like
that but these links having these basic links here is important uh people are starting to look for that for
some other uh setup maybe you have a personal website this is a good place to put that so that they now have a multitude of links
that go back to you and highlight who you are and then the next part or next four
parts so for the next four parts we have a combination of experience
education skills certifications and you can see they're organized
if you have you know a lot of people like to see what kind of degree you have they want to know where it came from
and if you just got out of college you're going to put education at the top and then maybe you'll put skills after
that and then your experience at the bottom if you've been in the field for years
you know my degree just to give you my age goes back to the nine early 90s so i usually put education at
the very bottom and then because a lot of the stuff i'm trying to sell myself on right now is my skills i actually put that at the
top and i'll put my education my certifications at the bottom my skills and then my experience is since it's a
huge part of my resume goes next you can organize these in whatever order you want that's going to work best for you
and sell you so remember you're selling yourself this is your image probably don't wear um an old tie-dye
t-shirt with holes in it you know something nice because it is professional and of course your summary
your and then what do you have to offer the company and again when i put out resumes and i haven't
done a resume in a while you go in there and you can take this and reorganize this so if the company's looking for
something specific you might put the experiences specific to that company you might even take experience if you
have like a long job history like i do i've gone into a lot of different things you might leave out those companies or
those experiences that had nothing to do with data science because it just becomes overwhelming resume should only take about
30 seconds to glance over maybe a minute tops because after that point you've lost the
person's interest and if they want to dig deeper they now have links they have your website they have linkedin and they can now take this and they come
back to it and they go okay let's look at this person a little closer so quick overview this is your sell
sheet selling you to the company so always tie it to the company so that
you have that what am i going to give this company what are they going to get from me welcome to data science interview
questions before we dive in and start going through the questions one at a time we're going to start with some of the
logical kind of concept that's enters in a lot of interviews in this one you have two buckets one of
three leaders and the other five leaders you're expected to measure exactly four leaders how will you complete the
task and no you only have the two buckets you don't have a third bucket or anything like that just the two buckets and the object of the question like this
is to see how well you are thinking outside the box in this case you're in a larger box you have two
buckets and also the pattern which you go on and what that means is if you look at the two buckets
and we'll show you their answer in just a second you have a bucket with three liters and a bucket with five liters and the first thought is what happens if
you go from left to right so we have a direction and what happens if you pour the three liters into the five liter bucket well if you
pour the three liters into five liter bucket you have an empty bucket of three liters and what's really important here is i was thinking outside the box you
realize that you have a five-liter bucket that has three liters in it and two empty leaders so you have two
additional leaders you can fill up if we continue that process we can pour from the left to right from the small
bucket to the large bucket you can now measure in two additional liters into the five liter bucket
and three minus two is one and you can keep doing that you can empty the five liter bucket in pour those three liters in that one
liter in and then you can pour three liters in what's cool about these questions is you explore them as you realize there's multiple ways usually to solve
i went from small bucket to big bucket the simply learned team their solution that they pulled out was you fill the
five liter bucket and empty it into the three liter bucket now you're left with two liters in the five liter bucket so that's great we can
empty the three liter bucket so now we're going from large to small remember we went from small to large so you can go both either way but you
have to go one way or the other it turns out and you can empty the three liter bucket and pour the contents of the five liter bucket in it
so the three liter bucket now has two liters and if it has two liters that means it has an empty one liter and by
now you probably have guessed that if you have an empty space you can start using that empty space of one liter as a measuring so we fill the five
liter bucket again and we pour the water in the three liter bucket it already has the two liters and so we're only pouring one liter in
there and five minus one is four so interview questions they break up into all kinds of different patterns
we have logic like this one which is a lot of fun we have questions that come up that are more vocabulary lists the difference
between supervised and unsupervised learning probably one of the fundamental breakdowns
in data science and supervised learning uses known and labeled data as input
supervised learning has a feedback mechanism most commonly used supervised learning algorithms are decision tree
logistic regression support vector machine and you should know that those are probably the most common used right now and there certainly are so many coming
out so that's a very evolving thing and be aware of a lot of the different algorithms that are out there outside of
the deep learning because a lot of these work faster on raw data numbers than they do than a deep neural network
would unsupervised learning uses unlabeled data as input unsupervised learning has no feedback
mechanism most commonly used unsupervised learning algorithms are k-means clustering hierarchical
clustering the appropriate algorithm and there certainly are more i'm going to say k-means
definitely is at the top of the list in the hierarchical clustering those two are used so many times so
really important to understand what those are and how they're used and most important is understand that supervised learning is you have
your data set where you have training data and you have all those different pieces moving around
but you you're able to train it you know the answers and unsupervised we're just grouping things together that look like they go
together how is logistic regression done logistic regression measures the relationship between the
dependent variable our label what we want to predict and the one or more independent variables are features
by estimating probability using its underlying logistic function sigmoid and whenever i draw these charts
i always end up drawing them the right hand side first because you want to know what your output is what is you want out of here and the left hand side what do you have
going in so you have your in and out you can see we have a nice labeled image here to help you remember this
we have our inputs we have our linear model we have our probabilities what are the probabilities
of it being a certain way based on these features coming in the sigmoid function and it's important
to note that the sigmoid function is maybe the most commonly used but it's only one of a number of functions that are out there
and the sigmoid function turns our probabilities into a value between 0 and 1 or very close to 0 and very close to 1
between 0.1 and 0.009 and based on that we generate an answer in this case a 0 or 1.
how is logistic regression done so last time we talked about the sigmoid function generally depending on what your
interview and level of math and what expertise you're going in for the market you'll have to understand that formula
of the probability equals 1 over 1 plus e to the negative y and that's e to the base 2.
so you have your probability function or your sigmoid function which pushes it as you can see we have a nice visual of
that that helps a lot to have that visual on the sigmoid function you definitely should know your y equals m
times x plus c your base euclidean geometry of forming a line in the slope plus the intercept the
y-intercept and then you have your natural log and the natural log is to the e as opposed to a base 2 or
base 10 so your natural log to the e of the probability over 1 minus a probability
equals your m times x plus c or your euclidean line that helps a lot as far as the graphing
and understanding the sigmoid function so we'll just keep pushing on to question number three explain the steps
in making a decision tree and i noticed last time we brought up the decision tree in the forest a lot
of questions came up what is the difference so let's go through that when you make a decision tree you're going to take the entire
data set as input you're going to calculate entropy of the target variable as well as the predictor attributes i
remembered entropy is just how chaotic is it so if you have like you know banana and
grapes and oranges if you're mixing in fruit and that's your data coming in you have all these different objects that are so separate from each other and
the more they become uniform the lower the entropy and we call that information gain so we gain
information on sorting different objects from each other so you have your entropy you have to calculate your information gain of all attributes
and then you choose the attribute with the highest information gain as the root node so if you can separate your group
and each group chaos and each group is lowered whichever split lowers the chaos the most that's where you split it and
that's your root node at that point you repeat the same procedure on every branch till the decision node of each branch
finalized so understanding that setup is pretty important as far as decision trees and you can see
here we have a nice visual of decision tree for example if you want to build a decision tree to decide whether we should accept or decline a job offer
since these are interview questions that's a good one to ask and just as a tip you should be pretty aware of the formula for entropy
and information gain so you need to look those up if you don't remember those and the salary if it's greater than 50
000 no decline the offer yes it's got a good salary the commute is greater than an hour
yes decline the offer no offers incentives yes except the offer no incentives
decline the offer so we use decision tree pretty much for everything if you want and if you have a decision tree then you
also should understand how do you build a random forest model and remember that a random forest is
built up of a number of decision trees so if you split your data up into a lot of different packages
and you do a decision tree in each of those different groups of data the random forest is bringing all those
trees together so how do you build a random forest model randomly select k features from a total of m features
where k is less than m among the k features calculate the node d using the best split point split the
node into daughter nodes using the best split repeat steps two and three steps until leaf nodes are finalized
build force by repeating steps one to four for n number times to create n number of trees so you can see it's got the same
build pattern as the tree but instead you're building a number of different trees little small trees so it
all have an end leaf node random forest has a vote at the end and whoever gets the most votes wins
that's the answer how can you avoid overfitting of your model very important question in any kind of
mathematical scientific data science setup in any of them there are three main methods to avoid
overfitting and you should really understand overfitting overfitting means that your model is only set for a very
small amount of data and ignores the bigger picture keep the model simple take into account fewer variables
thereby removing some of the noise and the training data good advice for any programming at all
use cross-validation techniques such as k-folds cross-validation use regularization techniques such as
lasso that penalize certain model parameters if they're likely to cause overfitting and you should also be well aware that
your cross validation techniques that's like a pre data or your lasso and your regularization techniques are usually
during the process so when you're prepping your data that's when you're gonna do a cross validation such as like splitting your data into three groups
and you train it on two groups and test it on one and then switch which two groups you tested on that kind of thing
so can you solve another one of these i love these things there are nine balls out of which one
ball is heavy in weight and the rest are of the same weight and how many minimum weighings will you
find the heavier ball and when we say weighing think of a scale where you can put objects on one side and the other and
you can see which side is heavier and you want to minimize that you want to split the balls up in such a way that
you're going to do as few measurements as you can you will need to perform two wangs so you can get it down to just two wings
and i always think if there's nine balls and divide them into three groups of three place three balls on each side so you can just randomly
pick six of the balls and three on one side three on the other and if they balance out both sides are equal then you know
the heavy weight isn't in any of those so out of the remaining three balls from step one take two balls and place one ball on each
side a little tricky there because i always want to put all three i want to put two on one side and one on the other but no
just take randomly pick two of those put one on each side if they balance out then the left out ball the one you didn't measure will
be the heavier one otherwise you'll see it in the balance you'll see which one's heavier because it'll take one of the balls down now we go to scenario b
where they did not balance out so now we know which side has a heavier ball in it and it's just very similar to what we
did before if the balls in step one do not balance out then take those three balls that have the heavier side on them and reproduce
step two to find out the heavier ball difference between univariate bivariate
and multivariate analysis and hopefully if you know a little latin you'll kick
in there that you have uni and you have bi and you have multi because the answer is in the words themselves
so the first one this type of data contains only one variable so that's see univariate
purpose of the univariate analysis is to describe the data and find patterns that exist within it so when you only see one
one variable coming in in this case we're using height of students you're limited as far as what you can do
with that data so you can come up and draw different patterns and conclusions from those patterns using the means the
median the mode dispersion range minimum maximum so we're describing the data so all those words
would describe the data and that's about all you can do with data like that there's no correlation there's nothing to go beyond that as far
as guessing or predicting anything so we move into bivariate you know uni means one
by means two bivariate this type of data involves two different variables the analysis of this type of data deals
with causes and relationships and the analysis is done to find out the relationship among the two variables
and this is always a favorite one because everybody loves ice cream in the summer when it's hot and very few people go for ice cream in the winter when it's
really cold so it's easy to see the correlation in the data the temperature and ice cream cells in summer season
and you can see here where the temperature goes from 20 to 35 and as the temperature goes up so does
the sales of ice cream it goes from 2 000 i'm not sure 2 thousand what's i'm guessing it's a very large
chain because if they're selling two thousand ice cream cones and they have a lot of business good for them a little vendor on the corner selling
two thousand ice cream cones a day and thirty one hundred the next day here the relationship is visible from
the table that temperature in cells are directly proportional to each other so the hotter the temperature we can
predict an increase in cells so the word prediction should come up so we have description and prediction when
the data involves three or more variables it is categorized under multivariate it is similar to bivariate
but contains more than one dependent variable in this example another really common one the data for house price prediction
the patterns can be studied by drawing conclusions using mean median and mode dispersion or range minimum maximum etc
and so you can start describing the data that's what all that was and then using that description to guess
what the price is going to be so this is very good if you're in the market and you have already looked at the area and you
already know that a two bedroom zero floor 900 square foot house is usually runs about 40 000
you can guess what the next one that looks similar to it is and i'll just throw in another word in there i don't see very often unless you're
really a hardcore data science we talked about describing the data descriptive we talked about predictive
and there's also postscriptive postscriptive means we're going to change the variables to try to guess what the
outcome is if we change what's going on so that would be the next step but that usually doesn't show up unless you're dealing with some really
hardcore data science groups what are the feature selection methods to select the right variables there are
two main methods for feature selection there's filter methods and wrapper methods and
when you're filtering your before we discuss the two methods real quick the best analogy for uh selecting features is bad
data in bad answer out so when we're limiting or selecting our features it's
all about cleaning up the data coming in so it's cleaner and is more representative of what we're trying to
predict filter method filter methods as they come in we have linear discrimination analysis anova chi squared chi square is probably
the most common one and these are all part of pre-processing we're taking out all the outliers all the things that have a
difference that is very different from the data we're looking at the odd ones and sometimes you take the odd ones out
and then you analyze them separately to see why they're odd but remember your filter methods you want to pull all that weird stuff
out wrapper methods on the other hand are forward selection backward selection
recursive feature elimination and one of the most important things to remember about wrapper methods is they're very
labor-intensive you have to have some pretty high-end computers if you're doing a lot of data analysis with the wrapper method
and um just quickly forward selection means you have all your different features they're
off to the side and we test just one feature at a time we keep adding them in until we get a good fit
backward as we have all the features and we start we run a test on that to see how well it does and then we start removing features to see what works
and recursive which is the most processing hungry algorithm out there goes through and just recursively looks through all the
different features and how they pair together but again we have filter method and wrapper method and it's important to understand that we're
sorting the data out and finding out which features are going to represent the data the best and which ones are not
going to really add any value to our models let's jump number eight in your choice of language
write a program that prints the numbers from one to fifty but for multiples of three print fizz
instead of the number and for the multiples of five print abuzz for numbers which are multiples of both
three and five print fizz buzz and this really is testing your knowledge and iterating over data
very important my sister who runs at the university the data science team is in charge of their department
it's the first question she asks in her interview of anybody who comes in is how do they iterate through data
so if this question comes up a lot and it's very important you have an understanding and there's actually a slight error on
this code which i'll point out in just a second the concept is we have fizz buzz in range and you have range 51
which in this case goes from 0 to 51 and i'm gonna challenge you to see if you can catch the error and i'll tell you at the end
of the code where the error is what that means is that we're going to go through all the numbers 0 1 2 3 4 and
we're going to process through this loop if the remainder of fizzbuzz divided by 3 equals 0 and fizzbuzz
divided by 5 also equals 0 then print fizzbuzz continue and elseif fizzbuzz
divided by three equals zero then print fizz print fizz continue else if is buzz divided by five equals zero
print buzz continue and print fizzbuzz you fit the print the answer in this case fizzbuzz is either gonna be the number
we generated which is 0 or it'll be the fizz buzz fizz or buzz that's a mouthful now if you didn't
catch the error in the code which is always a fun game find the error it has to do with the range
and it's important to remember the range here says range to 51 that's 0 to 51 which is is correct we want to go
to 51 because it stops it gets to 50 and it stops so that's 0 to 50. but if you remember the question
asked from 1 to 50. so the range should be one comma 51 not just 51 which does 0 to 51.
in this particular script in python you can leave out the continue but the continue in the script skips the
next lcf so it doesn't keep processing it going down and in programming a lot of scripts you don't need the continuance this would depend on what
script you chose and there's probably some other ways to do this it's a lot of fun and you can see here from the output we end up with fizz buzz for zero
which shouldn't be there one two fizz four buzz fizz seven eight fizzbuzz eleven fizz and so on sounds
like a drinking game from my college days so long ago many decades ago you are given a data set consisting of
variables having more than thirty percent missing values how will you deal with
them oh the joy of messy data coming in ways to handle missing data values data set is huge we can just simply
remove the rows with missing data values there's the quickest way i.e we use the rest of the data to
predict the values you just go in there and say any row of our data that has a n a in it get rid
of it that doesn't work with smaller data so a smaller data you start running into problems because you lose a lot of data and so we can substitute missing values
with the mean or average of the rest of the data using pandas data frame and python there's different ways
to do this obviously in different languages and even in python there's different ways to do this but in python it's real easy you can do
the df.mean so you get the mean value so if you set mean equal to that then you can do a df.fill in a
with the mean value very easy to do in a python panda script and if you're using python you should
really know pandas and numpy number python and pandas data frames for the given points
how will you calculate the euclidean distance in python so back to our basic algebra from
high school euclidean distance is the line on the triangle and so if we're given the points plot
one equals one comma three plot two equals two comma five we know that from this
we can take the difference of each one of those points square them and then take the square root of everything so the euclidean
distance equals the square root of plot 1 0 minus plot 2 of 0 squared plus plus one of one minus plot two of one
squared mouthful there and you can remember if you have multiple dimensions that go past two dimensions you could
have plot three can simply be the distance from plot one you only need to do one side of that or plot two you can do either way and
square that and take the square root of that another mind bender how to calculate some how to figure out the
solution to something what is the angle between the hour and minute hands of a clock when the time is halfed past
six so you want to kind of imagine that clock where the large hand is pointed down to the 30 and
the other half is gonna be right between the six and the seven because it's half past six there's actually a couple ways to solve
this but let's take a look and see how they did it note a clock is a complete circle having 360 degrees
in one hour the hour hand covers 360 over 12 so it equals 30 degrees for each
hour in one minute the minute hand covers 360 degrees over 60 minutes
or 6 degrees per minute the minute hand has traveled for 30 minutes so it has covered 30 times 6 which
equals 180 degrees so we know that's 180 degrees from the 12. the hour hand is traveled
for six point five hours six and a half six point five so it's covered six point five times thirty which equals 195
degrees the difference between the two will give the angle between the two hands thus the required angle equals 195 minus
180 equals 15 degrees and this is nice the way they solved it because you can now punch in any kind of time within reason the hard
part is on the hours is you have to be able to convert the hours into decimals explain dimensionality
reduction and list its benefits dimension reduction refers to the process of converting a set of data
having vast dimensions into data with lesser dimensions fills to convey similar information concisely it helps
in data compressing and reducing the storage space it reduces computation time as less dimensions lead to less computing it
removes redundant features for example there's no point in storing a value in two different units meters and inches and i certainly
rented a lot with this with text analysis i've been known to run a text analysis over a series of documents
ends up with over 1.4 million different features that's a lot of different words being used
and if you do what they call a buy connect them you connect two words together now you're up to 4.8 million different
features and you start having to figure ways to bring that down what can we get rid of that kind of thing so you can see where
that can get really high in on processing and learning how to reduce the list dimensions is very important how will
you calculate eigenvalues and eigenvectors of a three by three matrix and what they're really looking
for here is when you write it out for the eigen is that you know that you're going to use the lambda that's the most common one obviously you can
use any symbol you want but lambda is usually what they use and that you do it down the middle diagonal and so when you
take that matrix and you take the characteristic equation you end up with the determinant
and that's the minus 2 minus lambda minus four two minus two one minus lambda two four two
five minus lambda and that's what they're looking for and you know that's equal to zero so when you're doing a matrix in the eigen set up with the
eigenvectors that's all going to come out equal to 0 and then you can go ahead and write the whole equation out so we can expand the
determinant as you can see right here the minus 2 minus lambda times it's a mouthful i'll leave it up here
for a second so you can look at it when you break it down into the algebraic functions you end up with
minus lambda cubed plus 4 lambda squared plus 27 lambda minus 90 equals 0. so now we have a nice
algebraic equation built from the eigenvectors and always remember you can hit the
pause button and you can also send a note send a note to simply learn if you have more questions on vectors or on this definitely have
that resource available to you um or post down in below on the youtube video comments and so
when we calculate the eigenvalues and eigenvectors of a three by three matrix as we continue on down the math of this
and to be honest i really don't like working with matrixes like this it's important to understand the math behind it and it's important to know the
code just enough so that you're not lost when someone's explaining it or it comes up when i'm working on different data
science models of course if you're dealing with the high-end math side of it then you better know this
first is by hitting trial so you try in different variables to solve for zero and you can come in here and you'll find
that if we put in the three in there we end up with a zero at the end and substitute the three
hence we end up with lambda minus three as one of the factors and you can do the math going out on
that where we have lambda cubed minus four lambda squared minus 27 lambda plus 90 equals lambda minus 3 times lambda
squared minus lambda minus 30. so eigenvalues based on that one are 3
minus 5 and 6. and then from there we can calculate the eigenvector for lambda equals three and you can see
here where the matrix as we write it out is the minus five minus four two minus two minus two minus two
four two two that's from the beginning put in the x y and z equals zero zero zero and so when we put
in those numbers and we calculate them out we have for x equals one we have the minus five minus four y plus two z
equals zero minus two minus two y plus two z equals zero and subtracting the two equations we just had
we get three plus two y equals zero y equals minus three over two and z equals minus one over two that's going
back to the first equation and similarly we can calculate the eigenvectors for minus five and six how should you maintain your
deployed model oh distribution time my favorite i spent 10 years in software distribution so
first thing and this is true not just of your data science model but of any computer code going out there this
basic setup can work although usually there's a little added steps in there first we're going to monitor it so we have a constant monitoring of
all the model is needed to determine the performance accuracy of the models so yeah we want to just keep an eye on
it we want to make sure they're accurate we want to make sure that whatever they're supposed to predict or i threw in that
bonus word post script where you change something and you want to figure out how your changes are going to affect things we
need to monitor it and make sure it's doing what it's supposed to do evaluation metrics of the current model is calculated to determine
if new algorithm is needed and then we compare it the new models are compared against each other
to determine which model performs the best and then we do a rebuild the best performing model is rebuilt on the
current state of data this is interesting i found this out just recently if you're in weather prediction the really big weather areas have about
seven or eight different models depending on what's going on and so you actually have almost a
little force going on there where they're like which model is going to fit best and this is what we're going to use to predict the weather with
so not only do you don't necessarily get rid of the models but you figure out which models fit data of what's going on or
the current state of data what are recommender systems most commonly used nowadays in marketing
so very big industry understanding recommender systems predicts the rating or preference a user
would give to a product and they they're split into two different areas one is collaborative filtering and a good example of that is
the last.fm recommends tracks that are often played by other users with similar interests so people who if
you're on amazon people who bought this also bought that this got me a few times and then there's contact based filtering
and we're looking at content instead of looking at who else is listening to the music leave the example pandora which uses the
properties of a song to recommend music with similar properties so you have collaborative filtering and content based filtering
how to find rmse and msc in linear regression model hopefully you remember what the
two acronyms mean because that is like half the answer we have the root mean square error and
the mean square error in linear regression model so we're looking for error the rmse and the msc are the two of the
most common measures of accuracy for a linear regression model and you can see here we have the root mean square error
rmse equals and this is the square root of the sum of the predicted minus the
actual squared over the total number so we're just looking for the average mean so we're looking for the
average over the end and the reason you need to know about the difference between rmse versus msc
is when you're doing a lot of these models and you're building your own model why do you need to take the square root of it
it doesn't change the value as far as the way you're using it because you're looking as to see whether the error is
greater or less than so why add that extra computation in so a lot of models use the msc which indicates the mean square
error or the average error and it's the same formula minus the square root at the end or across the whole thing another riddle
to solve if it rains on saturday with a probability of 0.6 and it rains on sunday with a
probability 0.2 what is the probability that it rains this weekend and the trick in probabilities on this
case is we're not we need to know what is the probability of it not raining what's it not what's the chance
of it not raining on saturday and if it doesn't rain on saturday we want to take that and combine that with the chance of it not
raining on sunday the total probability which in this case we're just going to use 1 minus the probability that it will not
rain on saturday so it's 1 minus 0.6 we're going to take that as a union which we simply just multiply them
together of the probability they will not rain on sunday and it's important to recognize the
union here or the and you can see by the formula down here we end up with 0.68 or
68 chance that it will rain on the weekend and there are a couple other ways to solve
this but this is probably the most traditional way of doing that how can you select k for k means so
first you better understand that what k means is and that k is the number of different groupings and
most commonly we use is the elbow method to select k for k means the idea of the elbow method is to run k-means
clustering on the data set where k is the number of clusters within the sum of squares wss is defined
as the sum of the square distance between each member of the cluster and its centroid and you should know all the
terms for your k means on there and with the elbow point and again here's our iteration in our code we talked about that earlier
you iterate starting with um usually you don't start right at one but you just might start with two three or
four and you just see where it comes out and you can see the nice elbow there which is easy to see graphically where the
number of k clusters and the wss value drops and then it just kind of flattens out
and there's no reason to take the k means any further what is the significance of p-value oh good one especially if you're dealing
with r because that's the first thing that pops up p-value typically less than or equal to
0.05 indicates a strong evidence against the null hypothesis
and you should know what a difference why we use null hypothesis instead of the hypothesis so you reject the null
hypothesis very important that term null hypothesis in any scientific
setup and also in data science it doesn't mean that it's true it means that there's a high correlation
that it's true so if your null hypothesis means it's not true your hypothesis is has a high correlation that it's
probably true and if the p-value is typically greater than 0.05 it indicates a weak evidence against the
null hypothesis so you fail to reject the whole null hypothesis and if you reject that then
your actual hypothesis is probably not true the correlation of your data with what
you think it's saying is is probably incorrect and if you're right at the cutoff of 0.05 it's considered to be marginal
could go either way and again you can use that p value on different features to decide whether
you're going to include your features as far as something worth exploring in your data science model how can outlier values be treated oh
good one you can drop outliers only if it is a garbage value so sometimes you end up with like one
outlier that just is probably someone's measurement sway off height of an adult equals abc feet this
cannot be true as height cannot be a string value in this case outliers can be removed if the outliers have extreme values they
can be removed for example if all the data points are clustered between 0 to 10 but one point lies at 100 then we can
remove this point and again sometimes you just look for the outlier so you can see what's going on if there's something unusual there so
maybe the equipment's not calibrated correctly if you cannot drop outliers you can try the following
try a different model data detected as outliers by linear model can be fit by non-linear model
so we be sure you are choosing the right model so if it has like more of a curved look to it instead of a straight line you
might need to use something other than just a straight line linear model try normalizing the data this way the extreme data points are pulled to a
similar range if you can use algorithms which are less affected by outliers example random
forest so there is another solution is you can come up with the random force which a lot of times completely bypasses your outliers how can you say
that a time series data is stationary oh that's an interesting term stationary meaning it's
not moving but it's a time series we can say that a time series is stationary when the variance and mean of the series is constant with time
and this graphic example is very easy to see we have our the variance is constant with time so we
have our first variable y and x and x being the time factor and y being the variable
as you can see goes through the same values all the time it's not changing in the long period of time so
that's stationary and then you can see in the second example the waves get bigger and bigger so that's non-stationary here the
variance is changing with time again we have y which stays constant so that if you look at the bigger picture
it's the same wave over and over again and then of course we have uh where the wave is growing in size going up
it can also go down so it'd also be non-stationary how can you calculate accuracy using confusion matrix
oh great one confusion matrixes are so useful when you're taking that first look at data and also when you're showing the
shareholders and you want to ask them for money how can you calculate accuracy using confusion matrix
so you have your total data that we're looking at is 650 and you have your predicted values and
your actual values and you have your predicted p and your actual p and so we look at this you'll note that if the
predicted p and the actual p are 262 but our predicted p also had 15 that
weren't correct so you can see there's a false positive there of 15 and the same thing with the n you can
see where n predicts n and it has a false negative of 26 out of the total number of n values in there and so
we can do an accuracy on there the true positive plus the true negative is our total observations
so you have a total of 0.93 accuracy or 93 percent and just a quick note on this this is so
important because it's one thing if someone is being diagnosed with uh say cancer you know this is life
death or is my nuclear reactor going to blow up suddenly if the p is the probability of
it blowing up and this can say you have 15 that's a lot less than say the 26 chances of it blowing up you
know so the actual domain of your data is very important so if you're non-positive you don't
really care about the predicted value having non-positive as positive because they're going to do a biopsy on the
cancer or whatever anyway but you're very interested if you have a positive an actual positive value which is looked
at as negative a false negative that's really important in that domain depending on what domain you're in
write the equation and calculate precision and recall rate and so continuing with our confusion
matrix i was just talking about the different domains we have the precision equals 262 over 277 so your precision is
the true positive over the true positive plus false positive and the recall rate is your true positive over the total positive
plus false negative and you can see here we have that 262 over 277 equals a 94 percent and the
recall over here is the 262 over 280 which equals 9.9 or 90 percent and oh
good we're going to take a pause for another brain teaser if a drawer contains 12 red socks
16 blue socks and 20 white socks how many must pull out to be sure of
having a matching pair the last time i went through these kind of brain teaser
things was like 20 years ago and i had six people sitting across the table waiting for my answer that's kind of
mind-numbing when you're in an interview like that hopefully you're not stuck in an interview like that but uh on this
you need to ask yourself how many different colors of socks are there so they've thrown a lot of
extra data in here that you don't need to solve the answer the answer is four an example your first pick is white your
second pick is red third pick is blue so no pairs yet and that means when you get to the fourth pick
there's a hundred percent chance you're going to have a match so the most is going to be four that you ever have to pull out of your drawer if it was four
colors the answer would be five and so on it doesn't matter how many white socks you have or how many red socks or blue socks
different pairs you have it's the different colors a number of different colors people who bought this also bought
recommendations seen on amazon as a result of which algorithm we covered this earlier
recommendation engine is done with collaborative filtering collaborative filtering exploits the behavior of other users in
their purchase history in terms of ratings selection etc it makes predictions on what you might
interest a person based on the preference of many other users and this algorithm features of the items
are not known and we have a nice example here where they took a snapshot of a cells page it says for example suppose x
number of people buy a new phone and then also buy tempered glass with it next time when a person buys a phone
he'll be recommended to buy tempered glass along with it and if you remember the vocabulary words
we covered earlier this is the recommendation this is collaborative the other word was
content based so looking at things with similar content versus collaborative which is similar people
remember you know you're not going to know every vocabulary word but it also doesn't hurt to get your three by five
cards out and make yourself a vocabulary stack of cards buy an app on your phone for it sql
query i remember back in the 90s it was so important to know sql query and only a few people
got it nowadays it's just part of your kit you have to know some basic sql so write a basic sql query to list all
orders with customer information and you can kind of make up your own name for the database and you can pause it here if you want to
write that down on a paper and let's go ahead and look at this we have to list all orders with customer information and so usually you have an
order table and a customer table and you have an order id a customer id order number total amount
and then from your customer table you have id first name last name city county and so if we're going to write in sql with this
we're going to select keyword there for sql selecting order number total amount first name last name city
country so that's the columns we're going to look at we're going to do that from our order where we're going to join it with our
customer and we're going to join it on the order customer id equals the customer id so very basic sql
query that's going to return a table of data for us you are given a data set on cancer detection you built a
classification model and achieved an accuracy of 96 percent whoo 96 why shouldn't you be happy with your
model performance what can you do about it that's an interesting one because this comes up that's one of the standard data sets on
there is for cancer detection cancer detection results in imbalanced data in an imbalanced data set accuracy
should not be based as a measure of performance because it is important to focus on the remaining
four percent which are the people who are wrongly diagnosed we talked a little bit about this earlier you have to know your domain you
know this is the medical cancer domain versus weather domain you know whether channel they get by with 50
wrong in cancer you don't want four percent of the people being wrongly diagnosed wrong diagnosis is of a major concern
because there can be people who have cancer but we're not predicted so in an imbalanced data set
accuracy should not be used as a measurement performance which of the following machine learning algorithm can be used for inputting
missing values of both categorical and continuous variables so we have a couple choices here we have k
means clustering we have linear regression we have the k n n nearest neighbor and decision tree
in which of the following machine learning algorithms can be used for inputting missing values of both categorical and
continuous variables now certainly you can use some pre-processing to do some of that but you should have gone with the
k nearest neighbor because it can compute the nearest neighbor and if it doesn't have the value it just computes
the nearest neighbor based on all the other features where when you're dealing with k-means clustering or linear regression
you need to do that in your pre-processing otherwise it'll crash decision trees also although there's some variants on
that too can you solve another riddle always fun ones given a box of matches and two ropes not necessarily identical
measure a period of 45 minutes and in this particular setup the ropes
are not uniform in nature and the rope takes exactly 60 minutes to completely burn out so
each rope takes up to 60 minutes to burn out and there's actually a couple different solutions to this but let me go ahead
and one of the things is they're not uniform in nature so even though they take 60 minutes anyways let's go ahead and see what they
did to solve it and then we can also look at different options we have two ropes a and b light a from
both ends and b from one end okay when a is finished burning we know that 30 minutes
have elapsed and b has 30 minutes remaining now light the other end of b also so that the remaining part of b
will burn taking 15 minutes to burn this we have gotten 30 plus 15 equals 45
minutes excellent solution mine which i like was to take one rope fold it in two so
we know it's a half hour take the other rope fold it in four places so we know that that one's 15
minutes and then you can just connect the two and burn it straight across i think they're trying to cover that by
saying they're not regular the ropes are have some irregularities maybe that's what they meant by that you couldn't do something like that
that's my solution below are the eight actual values of target variable in the train file so we have a training
file not to be confused with the train on the tracks we have zero zero zero one one one one
what is the entropy of the target variable we mentioned earlier that you should know your entropy and how to calculate
the entropy what is the entropy of the target variable so we have a couple options here we have minus five over eight
logarithm of five over eight plus three over eight logarithm of three over eight okay let's just see where they got those
numbers from we have uh one which is going to be five ones and
three zeros and then we have a total of eight okay and then we have the option of five
which is number of ones five eight logarithm of five eighths plus three eighths logarithm of three 8
and we also have 3 8 logarithm of 5 8 plus 5 8 logarithm of 3 8 and then we kind of reverse those
numbers around and let's see what you're going to get here which one did you think it was you should check the first one so what
is the entropy of the target variable the key there is a target variable so we're looking at the target in this
case is going to be one usually that's what you're looking for and so the entropy of that one we want to subtract
out the entropy of the non-targeted variable whoops i had that backwards we want to we're looking at 0 so we want to
subtract out the 5 8 from there so 5 8 logarithm 5 8 or negative 5 8 logarithm 5 8 plus 3 8
logarithm 3 8. and they have the hint on the bottom entropy equals i of p of n so we have a negative p plus p
and n times the logarithm base 2 of p over p plus n minus the n over p plus n times logarithm 2 of
n over p plus n we want to predict the probability of death from heart disease based on three risk factors
age gender and blood cholesterol level what is the most appropriate algorithm for this case
so we have three features and we want to know the predictability of death okay a little morbid there choose
a right algorithm do we want to use logistic regression for this linear regression k means clustering or
the appropriate algorithm and if you selected logistic regression then you've probably got the right
answer linear regression remember deals with like you take your line and draw a line
through the data and of course you don't necessarily have to use a straight line there's other means for that but you're dealing with a lot of numbers and
k-means means we're just going to cluster objects together with the logistic regression though you can mix those things together in buckets so
really the logistic regression is what you want to use in that model would be the most apt fit after studying the behavior of a
population you have identified four specific individual types who are valuable to your study you would like to find all
users who are most similar to each individual type which algorithm is most appropriate for
this study certainly identifying census and just about a lot of different markets is common so maybe they have a census or whatever it
means but let's take a look at some of the different algorithms we might use on this we have k means clustering
linear regression association rules and decision trees and i'll give you a hint we're looking
for grouping people together by similarities and by four different similarities so
very specific they gave you one of the values specifically the k value so k means clustering would be great for
this particular problem you have run the association rules algorithm on your data set
and the two rules banana apple is associated with grape and apple orange is associated with
grape have been found to be relevant what else must be true so this would change you to understand association
rules you could picture in this particular one you're going shopping and you almost always see
somebody who has bananas they usually have grapes in their bag also and somebody who has apples usually has grapes in
their bags and then apples and oranges is also associated with grapes and let's go and take a look at that
and we have a couple different options here first one is banana apple and grape orange must be a frequent item set
not so much banana apples oranges must be relevant rule grape is common with banana apple must
be a relevant rule and how about grape apple must be a frequent item set let's go back and take a look at that
and we notice that we have bananas apples to grapes we have apple orange to grape boy there's a lot of grapes and a
lot of apples in there and so if you said the last one grape and apple must be a frequent item set
then you got it correct your organization has a website where visitors randomly receive one of two coupons it
is also possible that visitors to the website will not receive a coupon you have been asked to determine if offering a coupon to visitors to your website has
any impact on their purchase decision which analysis method should you use
and so let's go ahead and start by giving you another hint and give you some limiting your selection we have a
one-way anova k means clustering association rules and student t test so obviously you should know what
each one of these means but let's take a look at the question again so you want to know which method should you use to see if the coupons
valid for their purchase well we're not clustering and we're not associating things together we want to know the end result
student t-test also drawing a little t in boxes and switch them around there's really only one answer that works in here and that's
a one-way anova in this video we will learn some of the important questions that are often asked in any data science
interview we'll discuss questions related to various aspects of data science such as programming
sql mathematics statistics linear algebra and probability finally we'll get an idea about the top
machine learning and deep learning interview questions that are frequently asked in interviews so let's begin our first question is
how do you get a list of all the keys in a dictionary now in python for data science data
structures play an important role to store and manipulate data dictionary is one such data structure
that is declared using curly braces and they are written as key value pairs
you can see on the screen i have my dictionary called d and it is assigned using curly braces
now to get a list of all the keys in a dictionary we use the dot keys function now let me take
you to my jupyter notebook so here is my jupyter notebook and you can see my first cell
i have my dictionary defined so the name of the dictionary is d and
it has the key values the keys are the country names and the values are the capitals of each country so i have
india and the capital is new delhi then i have usa the capital is washington
similarly i have spain madrid and we have germany and berlin now india
usa france spain all these are my keys so to get the keys i'll use the dot keys
function so i'll write my dictionary name that is d
dot i'll give keys close the brackets and hit shift enter
you can see these are the list of my keys in the dictionary d all right
okay now let's move to our next question so our next question is what do you mean
by list comprehension in python so list comprehensions are a sought and
easy way to define and create new lists based on existing lists so suppose
you want to iterate through a list and find all the even numbers in the list you can either use a for loop and an if
condition as shown here or you can write a list comprehension to do it in one line of
code you can see this is my list comprehension so let me show you both the ways
all right so here on my jupyter notebook let me declare my list as l equal to i'll pass in a few numbers
let's say 5 comma 12 10 we have 14 43
11 7 9 and let's see the last element in the
list is 6 all right let me run this list
okay now let me create an empty list let's say list 1
and let this be an empty list with square brackets then i'll use the looping construct to
find the even elements in the list so i'll write a for loop called for item
in my list l i'll give colon then i'll give my
condition if item percentage 2 is equal to equal to 0
which means if the remainder is 0 then
i'll append the items to my empty list which is list1 so i'll
write list1 dot append and i'll pass in the argument my item
now let me go ahead and print my list i'll write print
the final list using a for loop
i'll give colon give a comma and pass in my new list name which is
list1 now let's run this you can see
using this for loop and the if condition i have got all the even elements in my
list which was l you can see my original list and 12 10 14 and 6 where all the even
numbers present in my list now this task you can do it using list comprehensions as well
so let me show you how to do that so i have my list defined let me just copy this here i'll
paste it and now i'll create another variable
which is list 1 i'll write equal to now here
using square brackets i'll write item for item in
l and i'll give my if condition as if
item percentage 2 is equal to equal to 0
then i'll print my resultant list so i'll write print let's say
resultant list using list comprehension
i'll give a colon then a comma and i'll pass in my new list which is list1
so here whatever is there in my square brackets is called the list
comprehension technique let me just print this and i'll get the
same result so these are my even numbers all right so let's move to the third
question so our third question is given a list of numbers find the squares
of the numbers using the map function in python now the map function calls the specified
function for each element of an iterable and returns a result so let me show you an example to find
out squares of a list of numbers using the map function okay so let's first define a function
called square so i'm using my def keyword and i'll pass in my user defined
function name as square and i'll give my parameter as x
and i want to return the square of x which is x
star x all right and then let me define my
list of numbers so i'll pass in a variable called num and let's
say i want to find the squares of five numbers one two three comma four
comma five here i'll pass in another variable which will
store the resultant list so i'll write num underscore
squared equal to here i'll use my map function so map
function takes in two parameters first it will take in the user defined function name which is
square here and the list of numbers which is num
i'll run a for loop i'll write for let me just scroll down
i'll write for num in num underscore squared
print num so this is my map function so
it will map all the elements in the list to this function which is square i'll just
print it you can see we have the list of squares
from the num list which has the elements 1 2 3 4 and 5.
now moving ahead coming to our fourth question
the fourth question is on to write a program that will filter out the numbers divisible by
3 from a tuple using the lambda functions so lambda functions are anonymous
functions that are not defined by the def keyword in python so here you can see i have the
example so this is a tuple i have defined and using filter along with the lambda
function i am trying to find out the numbers that are divisible by 3 and finally i am printing those
numbers so let's do it on our jupiter notebook all right so we are on the jupiter
notebook so first i'll define a tuple let's say the variable name is my
underscore top which stands for my tuple and
using brackets i'll pass in my double values let's say 5 12 10
18 we have 43 9 7 11 and
let's say 6 so you can see these are the numbers present in my
tuple now i'll create
another variable called new tuple and i'll use my tuple function
and inside the tuple function i'll use the filter function that will filter out only the values that are divisible by 3
and now i'll write my lambda function so notice this clearly
how to use a lambda function i'll use the keyword lambda and then i'll pass in a variable
x now this should consider only those values
that are divisible by 3 so i'll write x percentage 3 should be equal to equal to 0
and i'll give my tuple
as the variable here all right now we have declared our lambda function
let's just go ahead and print my new tuple which is new underscore tup so here you can see i have my tuple
function inside the tuple function i'll use my filter function that will filter only
those values divisible by 3 and to filter those values i have used my lambda function here and passed in my
original tuple which is my tuple it's actually my tup so let's delete
the last two letters let's just go ahead and print this
there you go you can see we have 12 18 9 and 6 are the only numbers that are divisible by 3
from this tuple moving to the next question so our next question is how to create a
data frame from a dictionary now to create a data frame from a dictionary you can use the pandas
library and the dot data frame function now pandas is a popular library in python
that is used for data manipulation so let me show you how you can create an
employee dictionary and then convert that into a data frame okay so here on my jupyter notebook let
me go ahead and import my pandas library so i'll write import pandas as
pd and next let's create my employee dictionary so
i'll write emp which stands for employee and and within curly braces i'll give my key
value pairs so my first column would be the name of the employee
and let's pass in a list of employee names i'll write ami
let's say we have angela
the third employee's name let's say we have samuel and let danny be our
fourth employee i'll give a comma
now the second attribute in our dictionary will be the age of the employee so my key would be age and
let's create a list of age values i'm assigning random values to those
employee age 30 and let's say 32
and let's create another variable called city
here variable refers to the column name of the data frame let's say ami is from
new york then we have angela from chicago
let's say samuel is from boston and we have danny
from let's say seattle okay so if i run this i have my
employee dictionary created so if you want to see the dictionary you can write print
and pass the variable name which is emp here so this is my dictionary now to convert
this dictionary into a data frame you can simply write df and you can give the pandas library
as pd and use the data frame function so i'll write data frame
and i'll pass in my dictionary name which is emp here
let's run it okay so we have successfully created our data frame now let me just print the
data frame i'll write df there you go you can see i have a nice
table here which is actually a data frame here you can see the indices
and you can see our column names as name age and city and these are the
values or the rows all right now let's see the sixth question we have
so the question is what is the difference between loc and iloc in python
now as data scientists you would often want to analyze a chunk of data and not all the rows
in it so loc and iluc functions can help you fetch specific rows and columns
from your data frame loc function uses row or column labels to select and
slice data from a data frame while iloc uses integer index to select
specific rows and columns remember iloc stands for index location
i'll use a car data frame to show how loc and iloc works
now on your screens you can see the data frame so it has a column called brand which
has different brand names such as ford there's hyundai tata mahindra there's maruti the sky
motors then we have the year in which the car was manufactured
we have the number of kilometers the car has run we have the city and the mileage the car
gives so let me first create this data frame on my jupyter notebook
okay so here on my notepad i have my code written to create the data frame
that is car i'll just copy this code and i'll paste it to this cell
okay let me just run it if i go on top you can see we have successfully created
our car data frame now let me go ahead and print this data frame so i'll write
print in brackets i'll give my data frame scar let me run it
all right so here is my data frame now suppose you want to see the data
from second row till the fifth row for brand and city columns so you can use the loc function
so i want my data from the second row till the fifth row for brand and city columns so let me show
you how to do that so first i'll write my data frame name that is car and then i'll give my function as
loc and within brackets i'll give my
row index values so i'll give from two to five which is second row to fifth row and
i'll give my column names as brand you give a comma and my second column
which i want is city so here i'm using the
labeled beast indexing let me just run it there you go you can see we have the
information available for the second row third row fourth row and
the fifth row all right similarly let's see one more
example for index location based slicing
so suppose you want to see the data from second third and fourth row for year kilometers
and city columns so for that you can use the iloc function so i want the value for second
third and fourth row and the columns that i want are years kilometers and the city column
so for that i'll give my data frame name as car dot this time i'll use iloc
and within square brackets i'll give my row values and
the column values now one thing you should notice here is
if you use iloc python will exclude the last index which is the fifth row and similarly here it will
exclude the last index from the column which is the fourth column
if i run this you get the desired result now let's see another example
let's say this time i want to print the rows that have mileage less than 25 for brand and city columns
so you can use a print function and i'll pass in my data frame as car
and use loc here i'll give my condition as
car and i want my condition that is mileage should be less than 25
and i want the brand name so i'll use a square
bracket and give my brand column and i also want the city column
okay let me close it if you run this
now these are the brand names renault tata and kia which have mileage less than 25 let me just verify
this you can see all these values are less than 25 similarly you can use the
iloc function as well i can write print i'll give car dot
iloc within square brackets i'll give car
and my condition column is mileage now this should be less than 25
i'll use dot values and then i'll give my
column numbers as 0 comma 2 let me run it there you go
the same result we have got it using iloc function this time and you can see
the columns were 0 comma 2 which means we wanted the first column which is
brand and the second column which was kilometers
all right now moving to our seventh question in the list of questions that we have
the question is how is list dot append different from list dot extend
now list dot append adds its arguments as a single element to the end of the list
the length of the list increases by 1 here and list dot extend iterates over its
arguments and adds each element to the list the length of the list increases by the number of elements in its arguments
so let me show you the difference on my jupyter notebook okay so let's
create a list called list 1 equal to i'll create a list let's say it
has some country names like india then we have
usa and i'll consider one more country let's say canada
okay now let me print the length of this list also
i'll use the length function for that all right now
let me append one more list so i'll use the append function
i'll write list one dot append
and within brackets i'll give my new list which has
two more country names let's say france and spain
okay and here let me print list 1 and let's also print the length
of the list again i'll use the length function and my list name which is list1
so just mark we are using append here let me run this there you go
so here you can see my original list had three elements since the length is
three now when i appended this new list which had two elements france
and spain our size increased by 1 and the length of the new list is
4. now let me just copy this code
and instead of append let me write extend so that we can
see the difference between append and extend just scroll down run it
here you can see our original list had three elements but after we extended our
list with two more elements the length of the new list became five so these are the five elements in the
list and here you can see there were total four elements in the list
when we appended the new list okay now moving to our eighth question
suppose you have a car data set which has a mileage column you need to create a new field called
mlg that will accept two values if mileage is less than 25
then the value should be low mlg and if the mileage is greater than or equal to 25 then the
mileage should be high mlg or the value should be high mlg now while analyzing data data scientists
often need to create extra columns in the data frame to add more values that will help you make your data analysis more efficient
so this question is related to one such instance or one such idea
so here you can see my data set it has the brand name year kilometers
city and the mileage column now if mileage is less than 25 i'll add the value low mlg
to my new column else if mileage is greater than or equal to 25 i'll add high mlg to my new column so
here you can see my final output wherever the mileage is
less than 25 it should print low mlg else it should print high mlg
and now let's do this on my jupyter notebook so we'll use lambda functions that we
learnt earlier to perform this task i'll write a function let's say f is
equal to you can call this as a function or a variable
i'll give the keyword lambda and use x as my
parameter x should be low mlg
if x is less than 25 which is mileage here
else if it's not less than 25 it should return high mlg
now what i'll do is i'll give my data frame scar
and i'll create my new column that is mlg then i'll write equal to
car dot i'll give my column name from the original data frame
which is mileage and i use my apply function and pass in
the variable f now let's just print this data frame
if i run it you can see the output here we have created a new column which is mlg
you can see our new column name was mlg and these are the results so
for renault tata and kia for the last three rows since the mileage is less
than 25 it returned low mlg for the remaining it gave us high mlg
now moving to the ninth question so this is our next question we want to create a 4 cross 4 matrix and
find the sum of all the diagonal elements in the matrix data scientists often work with multiple
dimensional arrays or multi-dimensional arrays and matrices to build models this question is related to an operation on a
4 cross 4 matrix so first we need to create an array which
will be a numpy array of 4 cross 4 shape and then we'll see how to
calculate the diagonal elements or the sum of the diagonal elements
so i am on my jupyter notebook first let's import the numpy library so i'll
write import numpy or numpy as np
let me run it okay now i'll create my 4 cross 4 trick so i'll
write m is equal to
np dot array and i'll create my 4 cross 4
matrix let's say the values are 1 2 3 4 in the first row
and in the second row i'll pass 5 comma 6 comma 7 comma 8.
in the third row we'll have 9 10 and 11
also 12 and finally we'll have 13
14 15 and 16. the fourth row i'll just run it
now if i print m you can see this is a four cross four matrix so
this is one way of creating a four cross four matrix which is a bit tedious and lengthy
instead of that you can also use a range function so if i write m is equal to np dot
a range function and here i'll give my values starting from 1 till
17 so this will actually exclude 17 and take from 1 to 16
and i'll give my shape as 4 comma 4
let me print it there you go it has printed the exact same matrix
that we saw above all right now to print the diagonal
elements we'll use the diagonal function so i'll write print m dot
diagonal let me just run it so 1
6 11 and 16 are my diagonal elements you can see this matrix 1 6 11 and 16 are my diagonal elements
now to print the sum of all the diagonal elements i'll use the sum function
so i'll write m dot diagonal
along with that i'll pass my sum function
if i run it so the sum of 1 6 11 and 16 is 34.
okay so coming to the last question on this interesting session on programming interview questions for data scientists
our question is given a vehicle data frame find the average and maximum mileage of
the vehicles using pandas so here data scientists use statistical
measures to analyze that data so it is important to have knowledge of the statistical functions in python
here is my vehicle data set and for each brand of vehicle we want to
find the average and the maximum mileage so let me show you how to do it
okay so here on the notepad i have my code written to create the vehicle data frame i'll
just copy it and paste it on this cell
you can see we have the brand column the year column kilometers city mileage here you can see the
different brand of vehicles let me just create this data frame first
and let me go ahead and print the vehicle data frame so i'll write
vehicle and run it so here is the data frame
now i want to find the average and maximum mileage for each of the brands so for this
we'll use the group by function and the aggregate function that is available in the pandas library of python
so let me show you how to do it i'll give a variable called result
and then i'll use my data frame name as vehicle
followed by the group by function i'll write group by and since i want to
check for all the brands so i'll group all the brand of vehicles then
i'll use an aggregate function which is a g g and within curly braces i'll give
my column as mileage i'll give a colon
and pass in mean which is for average and i'll use max to find the maximum
mileage now let's print it i'll write print
i'll give a message as the average and maximum
mileage of vehicles
let me just print the result i'll run it
you can see it has resulted in an error the reason is in vehicle the v is
capital now if i run it there you go here you can see on the
left we have the brand names we have hyundai there's maruti and tata because those
were the only three brand of vehicles we had and for each brand you can see
the mean mileage and the maximum mileage for maruti it was 26.33
and the maximum mileage is 28 similarly you can see for data vehicle as well sql is a widely
used language for querying data from databases it allows you to store retrieve manipulate and update data present in
the form of rows and columns data scientists often work with structured data that is stored in mysql
databases microsoft sql servers oracle databases or even nosql databases such as mongodb
today we will look at a variety of sql questions we'll be using the mysql database for
our demo throughout the video we'll be using mysql workbench and the mysql command line to solve the
sql queries let me show you the database that we will be using for this demo and the tables we are going to use okay
so here i am on my sql workbench and below you can see i have a local instance created i'll
just click on it now it's asking me to enter the password i'll give my password
and hit ok now this will take me to the sql editor now this is the sql
workbench so let me first create a new sql query file all right
so first and foremost let me show you the databases we have for that i'll use
the command so databases if i hit tab it will auto
complete i'll give a semicolon and hit ctrl enter to run this there you go
here you can see the list of databases that we already have and from the list of databases we are
going to use the sql underscore iq database so to use this database i'll write the
command use sql underscore iq i'll give a semicolon
i'll select this and i'll click on this execute button if i run this
you can see now i'm using sql underscore iq database
now if you want to see all the tables present in sql underscore iq database we'll use the following command so i'll
write show tables now this will list all the tables present inside
sql underscore iq database if i run it you can see here it has executed
successfully and it has given us seven rows which means we have a total of seven tables
in this database you can see there's an author table there's a table called books we have customers email
employee players and weather table so all these tables we are going to use in the course of this video now if you
want to have a look at the data present in one of the tables you can just use the
select statement so i'll write select star from let's say i want to see
the data present inside the employee table so i'll write employee so this is my table name i give
a semicolon if i select this hit the execute button
there you go it shows me that it has returned six rows
and these are the column names we have the name of the employee the age of the
employee we have the employee's salary and the employee id so there are total six employees in the
table all right now i can do the same
operations on my sql command line as well now let me open the command line first
okay so this is the mysql command line client it is asking me to enter the password so
i'll give my password first all right now here you can see i have
come inside my sequel now the operations that we performed on the mysql workbench the same operations
can be performed on the command line as well so i'll write use
sql underscore iq i'll hit enter you can see the message
it says database changed now if you want to display the tables present inside this
database i can use show tables command
i have to give a semicolon and hit enter there you go you can see the list of tables we have inside the database now if you want to
have a look at the content of one of the tables i can just write select star from let's say players
if i give a semicolon hit enter you can see the rows and columns in the data set all
right and with that let's move on to our first question on sql
okay so our question is what is the difference between where and having clause in sql
so where clause is used to filter the records based on the specified condition while the having clause is used to
filter the records from groups based on specified conditions you need not worry we'll do the demo for
wherein having clause using the employee table so where clause cannot have aggregate
functions such as count sum min or maximum functions
having clause can operate on aggregate functions now where clause is implemented on rows
while having clauses implemented on columns now let me show you an example of how a where clause is different from an having
clause so i'll take you to my my sequel workbench
all right so we are on the sql workbench so suppose i want to get the records of the
employees whose age is greater than 30 for this i can use a where clause so
since i want to get the employees whose age is greater than 30 i'll use a where clause i'll write my
sql query as select star from employee where
i'll write age is greater than 30. if i give a semicolon
and run this it will return all the employees whose
age is greater than 30 you can see 36 36 and 35.
these are the employees whose age is greater than 30. now i have intentionally kept some
duplicate rows because we are going to use this in our next questions again i can also do the same task using
the having clause so let me just copy this query i'll paste it here now instead of
where i'll write having so you will see that even this query
will return the same result if i run it you can see it gives me the same result now
let's say i want to find all the records where the count of age for all the employees is greater than 1.
in this case i can't use a where clause it will throw me an error since where clause cannot be used with
aggregate functions so let's try using an aggregate function
in the where clause so i'll write select star from employee
where count of age
is greater than 1 so the idea is to find all the records where the count of age for all the employees is greater than 1.
so if i run this sql query you can see it has given me an error it says invalid
use of group function now instead we need to use a having clause
along with the group by clause to solve the problem so i'll group all the age values and then find the count of each each value
so let me just paste this and here i'll edit my sql query
instead of where i'll write having and just after the table name i'll use
the group by clause so i'll write group by each now let's run it
all right you can see our sql query has returned only those employees whose age occurs
more than once now let me just verify this if i run my
employee table you can see angela's age that is 36 has
has been repeated twice you can see angela here as well and similarly we have mic also repeated
twice all right now let's see the second question
now this is a multiple choice question where you need to answer the correct sql query based on the question
so the question is the correct sql query to select all the records of employees
with ari in their names is so we have a list of four options given
and out of that you need to find the correct option now
the answer to this question is this so if you write select star from
the table name which is employ here where name like then if you give this
condition it will return all the records where employees have ari in their names
now like is an operator in sql that is used to search a specified pattern
so here our pattern is we want all those employees who have eri in their names
the first percentage means that the name can have any number of letters in the beginning
followed by eri in the middle and the last percentage means the name can have any number of letters
in the end so let's do this on our my sequel workbench
let me first display all the employees in the table so i'll write select star from employee
i'll run this if you see my table i have one employee who has ari in its
name and that person is kerin i now want to get this particular record
so what i can do is i'll write a select query select star from
employee where name then i'll use the like operator where
name like within double quotes i'll give percentage
eri again a percentage close the double quotes and give a semicolon if i run this
you can see my sql query will return only karen now let's do one more example
suppose i want to find the employee whose name starts with d so the way to do is
something similar to what we saw just now so here in my condition i'll just
replace this with d this means the name should start with d and after
that it can contain any number of letters now let's run it
there you go so our table had one employee called danny whose name started with d
now moving to our third question
so the question is we want to write a query to find the players with the least number of goals so there
are two methods to do it one where you can use a sub query
and the other method to use the limit operator so let me show you both the methods
first let me go ahead and display all the tables present in my database that is sql or sql
underscore iq all right if i scroll down you can see
we have a table called players so i am going to use the players table to find the player
with the least number of goals so let me show you the first method that
is by using a sub query so i'll write select star
from my table name which is players where goals equal to
and in this i'll write my sub query using brackets so i'll select
minimum goals from the table that is players
i'll close the bracket and then i'll give a semicolon now let me go
ahead and run this you can see matt was the player from
scotland who scored the least number of goals that is three now
let's break it down so what we did was first my sql query found out
what was the minimum goal scored by a player so if i run this you will see it will
return me 3 so 3 was the least number of goals scored by one of the players
now in my outer query what sql did was it searched for
those goals where the value was equal to 3 so we wrote select star from players
where goals equal to 3 hence if i run this
you will find the player who scored only 3 goals and that player is matt alright
now the other way to do is using the limit operator so this is more simpler than the
previous method i can write select star from players order by
goals now let me first run this sql statement if i run this
you can see we have ordered this in ascending order so the person
who has scored the least number of goals appears at the top and the person with the or
the player with the highest number of goals appears at the bottom and from this table i only want the
first row and for that i can use the limit operator so if i write
limit 1 it will display only the first row
if i run it you can see it shows me only the record for matt moving ahead now
let's see the fourth question in our list the question is write the sql query to
find the player with the second highest number of goals now again there are two methods to do it
one is by using limit and offset and the other method is to use a sub query
so let's see both the methods i'll first write my sql query using
limit and offset so my query will be select star from
players order by goals descending
let me just run this all right so here you can see my table
has been ordered in descending order of goals so anthony has scored the most number of goals
then comes daniel followed by sam and at the end we have matt who scored the
least number of goals now out of this i want the player who scored the second highest number of
goals which is daniel so i want my sql query to return this row that has player id 103
name is daniel country is england and goals is 7. for that mysql has two features
so i can use limit 1 followed by offset 1
so what this query does is it will offset one row that is it is
going to offset the first row or it is going to skip the first row and then print only the second row because i
have set my limit to one so let me just run it you will see we get the desired result
the player name is daniel and the number of goals scored is 7. now the other method to do
is using a sub query now let me write my sub query first i'll write my outer query which is
select star from and here i'll write my sub query select star from
players order by my column name which is goals in
descending order here i'll limit it to 2
then i'll give my alias name as t then i'll use order by
goals limit one let me just run it first
if i run this you see here i get the same result now let's first break it down i'm going
to run this sub query first so here i have select star from players
order by goals descending and limit is to let me just run it
so the selected sql query returns the first two rows ordered by goals
and if you see here clearly we have again ordered the value returned
in ascending order which means from this table if i order this table in ascending order
daniel should appear first and out of that i am limiting my result
to 1 so it will print only daniel if i run this again you can see i have the desired output
now let's check the fifth question in our list given the below email table
write a sql query to find the distinct domain names from the email column so if you consider
this as my email table the domain names are gmail.com
in and we have hotmail.com these are the distinct domain names now to solve this task
again we have two methods either you can use the substring underscore index function or you can use the substring function
now the substring underscore index function returns a substring of a given string before a specified number of delimiters
so let me first use the first method okay so here on my sql workbench i'll write
select i'll use the keyword that is dextent then i'll
use my function which is substring underscore index
here substring underscore index will take three parameters the first is the original column name
which is email followed by the delimiter i'm looking
for that is at the rate so i'll put at the rate with single quotes since it is a
text or a character and then i'll give the number of times to search
for the delimiter here it is going to be -1 so you can either give a positive value
or a negative value if it is a positive number then this function will return
all to the left of the delimiter if it is a negative number the function returns all to the
right of the delimiter all right and then i'll give an
alias name as say domain name
from my table that is email if i run this
you can see i have the list of all unique domain names or distinct domain
names that we saw in our table this is one of the ways the other way is
to use the substring function now the substring function extracts the substring from a string
starting at any position so let me write my sql query i'll write
select dixtent
i'll then give my function as substring
here substring will take in two parameters the first parameter is the column name
which is email id here so i'll write email i'll give a comma
next i want to find my delimiter so for that i am going to use another function
called locate and here i want to locate the index position of at the rate so i want to
give my delimiter as at the rate followed by the column name which is email
and then i want to fetch everything after the delimiter and for that i am giving plus one
again let me give a domain name as the alias name from
email if i run this you will see will get
all the unique domain names gmail yahoodin and hotmail.com so these were
the two ways to solve the same problem now moving ahead let's look at our sixth
question so our question is write the sql query to find
duplicate records from a table without using a temporary table now one of the other variants of this
question is to delete all the duplicate records from a table without using a temporary table
now to solve such queries you can use the group by and the having clause let's look at how to do it
so here on my sql workbench i'll write my select query as select star
and then i want to count all the duplicate values
let me give it an alias name as frequency
from my table name which is employ here and then i want to group by
name having count star
greater than 1 let me just run it all
right you can see it has returned me two rows the name is angela in mic
and the frequency is two let me just verify this to see all the repeated
records that were present in my employee table so i'll write select star from
employee if i run this and scroll down here you can see angela
is present here as well as at the bottom similarly we have mic present twice in
the table and hence our sql query returned only angela and
mike because they were repeated twice you can see the frequency here so let's now look at our seventh question
given the below employee table i want to write a query to find all the even and odd records from the table
so as you can see there are two ways to do it one is to use a sub query and another
you can use the mod function so let me show you both the ways all right
so let me run this sql query to see all the values present in the employee table
so if you see the last column which is the employee id column has all the employee ids as
unique values so i am going to use this in my where condition
so to find out all the even records
i'll write my sql query something like this i'll write select star from employee which is my
table name where employee id
then i am going to give my in operator
and inside the in operator i am going to write my sub query which is select
employee id from employee where employee id
percentage 2 is equal to 0 this means wherever my employee id
is divisible by 2 it will return only those records so let me just run it there you go so
here we have easily fetched only the even records from the table similarly if you want to get only the
odd records from the table you can use the mod function also so i'll write select
star from employee where here i am using the mod function
mod emp id comma 2 is not equal to 0 so i'll use
the exclamation mark is equal to 0 so if i run this
it will return only the odd employee ids you can see it here so those were the
two ways in which you can fetch the even and odd records from the table now let's see our eighth question
so here is the question below is the customer table write a query to get the first purchase
of each customer now if you look at the table there are three customer ids
101 102 and 103. now customer101 made its first purchase
on 2nd october 2018 which is this one and customer 102 made its first purchase
on 5th of october 2018 and since we have just one record for customer id 103
so this is going to be his or her first purchase so i want to return 101
phone 102 tablet and 103 books to do this
we are going to use an inner join if you can see i have used an inner join
and i've also used a sub query let's write this query on my sql workbench
okay so this query is going to be a little complicated if you have any questions then please
put it in the chat section we'll be happy to help you so i'll start with select
i'm going to take an alias name as t dot i'll write cust underscore id
comma t dot purchased at
comma the next field i'm going to select is t dot item from
customers as t next i am going to write my inner join
and inside the inner join i'll write my sub query it's going to be select customer id
comma i'm going to take the minimum value from the purchased at
column which is my date column
close this i'll give another alias as min underscore
purchased underscore at from my table name is customers
then i am going to group it by one here one means the first column that you
have put in the select statement which is customer ids so i am grouping by customer id i close the bracket and everything
i'm going to give an alias name as t1 now i'm going to join on t dot
customer id is equal to t1 dot customer id
and my next condition would be t dot purchased
at is equal to t1 dot min underscore
purchased underscore at i'll give a semicolon
i would request you to go through this sql query in a detailed manner so that you understand
it line by line let me just run it now
if i run it okay it has thrown me an error which says table sql
underscore iq customer does not exist the reason is my table name is customers
all right so let me run it once again
okay so there's one more error it says unknown column purchases ad so we had made a spelling
mistake here it should be purchased at now let's try and run it
all right there you go so for customer id 101 the first purchase was made on 2nd of
october 2018 likewise for customer id 103 the first
purchase was made on 5th of october and similarly for customer id 102
the first purchase was made on 5th of october again now moving to the 9th question so
here we have a weather table we want to write an sql query to find all the date ids with higher temperature
compared to its previous dates so if you see we have a date id column we have
the date and the temperature using this table i want to find those
ids with higher temperature compared to its previous states so if you look for date id 1 the
temperature is 32 for data id 2 the temperature is 31. for
data id3 the temperature is 30.8 now if you clearly notice for date
id 4 the temperature is 31.5 which is actually higher than the previous date id which is 30.8 similarly for date
id5 the temperature is 33 which is again higher than the previous date which is
31.5 now if you look at the date values they start from 10th of march till 15th
of march 2020. now to solve this query we are going to
use sql joins again and i am going to use a function called
to underscore days now my sequel to underscore days function returns the
number of days between a given date and year 0. so let's go ahead and write
this sql query all right so first let me select
all the rows from the temperature table which we are going to use for this particular question
i'll run it all right there's some error actually
the name of the table is weather and not temperature so i'll write weather
let me run it okay so here you can see
these are all the rows present in the weather table and as i explained i want to return only
the date id 4 and this date id which is 5. now one thing i want to make it clear
here that this date id should actually be 6 so there was a mistake while creating the tables
please consider this data idea 6 and not 5 so i want to return data id4 and data
id5 alright so let me go ahead and write my
sql query so i'll write select i'll write
a dot date underscore id from
whether as a comma
i'll join weather as b where
a dot temperature is greater than b dot
temperature and i'll give my condition and
i'll use the function to underscore days and give my value a dot date underscore
val is equal to i'll use the function again 2 underscore
days b dot date underscore
val plus 1
now i would like to know from our viewers why have we used plus one in the where condition
so please put your comments in the chat section let me just run it there you go so here
you can see my sql query has returned the date id 4 and 5
which means these are the date ids where the temperature was higher compared to
the previous dates all right now coming to the last question on this interesting session on
sql interview questions for data scientists so there are two tables one is the
authors table and the other is the books table i want to write an sql query to find the second
highest author who sold the most number of books now this is going to be my sql query
again we are going to use joins on both tables author and books along
with limit and offset operators so let me show you how to do it
so first i'm going to join my author table and the book table so i'll write select a dot
author underscore name comma i'm going to find the sum of
copies sold from the books table so i am giving my alias name as b dot copies underscore sold which is my
table name i'll given alias as some underscore sold from
author as a join books as
b on i'll write b dot book underscore
name is equal to a dot book underscore name
now i'm using book underscore name as my
criteria because book underscore name is the common column to both the tables
then i'll use a group by clause i'll write group by a dot
author name then again i'm going to use an order by
clause i'll write order by sum underscores hold and i'm going to
order it in descending order so i'll write d esc i'll set my limit as
1 and offset as 1.
if i run this you will find
that mysql query has thrown an error the reason is we have used
a column called bull underscore name so it should be book underscore name there was an error
here let me run it again
okay there was another small error this would be some underscore sold let's run it
there you go so my sql query has returned author 3 because
author 3 sold the second highest number of books now let me just remove the last
few lines in the script let me just run it now
if i run this you can see author 1 sold the highest number of books followed by author 3 who sold the
second highest number of books and then it was author 2 who sold the
least number of books so if i write limit 1 and offset one
what this line of code will do is it will skip the first row and print the next row
so let's just run it and you can see it has written me the
author who sold the second highest number of books in this video we'll be discussing some of the top mathematics interview
questions for data science these questions are based on various topics in mathematics such as statistics probability and linear
algebra so let's begin with our first question the first question we have is what is
normal distribution now normal distribution is a probability distribution that is used to check how
your data is distributed your data could be spread out to the left or to the right
but in many cases the data tends to be around a central value with no bias left or right
as you can see on your screens now this is called a normal distribution where half of the points are to the left
and another half of the points are to the right of the center now such a distribution this contributes to 68 percent of the data falling within
one standard deviation of the mean then we have 95 percent of the data falls within
second standard deviation of the mean so if you add up 13.6 percent and the 68
percent plus 13.6 percent this will result to 95 percent of data falling within two
standard deviations of the mean and finally we have close to 99.7 percent of the data falling within three
standard deviations of the mean so this is what a normal distribution is
all about now moving on to the next question so what are the different
measures that are used to summarize the distribution now to get more idea about the data and
summarize the distribution you can use three types of statistical measures the first we have
central tendency measures then we have variation measures and finally we have shape measures now under central
tendency measures we have mean median and mode now the mean is the most frequently used
measure of central tendency in a data set let me take you to the wikipedia page and explain you what mean exactly is
ok so here you can see on the wikipedia page it has the formula to calculate
the mean of a distribution so this is basically the summation of all the data
points divided by the total number of data points so here you can see an example we want
to find the arithmetic mean of 5 values which are
will pick the fourth element from our ordered set of points that is 6 here
and hence 6 is the median for this ordered set of points now if you have an even number of
observations as you can see here total we have 8 observations
now the median becomes x n by 2 plus x n by 2 plus 1 divided by 2.
so your median becomes the mean of the middle two numbers that is 4 plus 5 by 2 which is 4.5
all right now the next statistical measure we have under central tendency
is called the mode so the mode is the most frequently occurring value in a set of data points so here you can
see an example we have a set of data points 6 three nine six six five nine three so your mode is going to
be six here since six is occurring thrice in the set of data points
okay now under variation measures we have variance standard deviation and
interquartile range so first look at what variance is so variance is the average of the squared
differences from the mean now this is the formula for variance you can see x i represents
the individual set of data points and mu represents the average value so mu is
nothing but the summation of all the data points divided by the total number of data points so this is your formula for variance
next we have standard deviation so standard deviation is a measure of the amount of variation
or dispersion of a set of values a low standard deviation suggests that the values tend to be close to the mean
while a high standard deviation suggests that the values are very spread out now here you can see the formula for
variance now this is denoted by sigma and your standard deviation is nothing but
the square root of variance so here you can see the formula for variance you have x i minus
your mu square which is the mean of all the data points divided by n
then we have the interquartile range which is also known as iqr so iqr is a
measure of variability based on dividing a data set into quartiles so an interquartile range measures where
the bulk of values lie so the formula for iqr is q3 which is the third quartile minus
the first quartile so here you can see we have a list of data points
then we are ordering the data points first and then we cut the list into quarters
so first you have the q1 then you have q2 and finally you have q3 so for these set of data points
your iqr is going to be 7 minus 4 that is 3 then
we have shape measures so under shape measures we have skewness and ketosis so skewness is a measure of
symmetry your distribution is symmetric if it looks the same to the left and the right of a center
point now they are majorly of two types one is called
negative skew or which is also known as left skew then we have positive skewed
or right skewed and finally we have ketosis so kutosis
is a measure of the tailless of a probability distribution it defines how heavily the tails of a
distribution differ from the tails of a normal distribution so if you consider these distributions the kutosis will
calculate the tailedness which is actually these areas of a distribution now let's look at a
third question so the question is create a sparse
matrix in python now in numerical analysis a sparse matrix or a sparse array
is a matrix in which most of the elements are zero now there are multiple ways to create a sparse matrix
i'll show you two ways to create a sparse matrix so let me take you to my jupyter notebook where i'll create a matrix first and
then we'll see how to create sparse matrix out of it so i am on my jupiter notebook here
first let me import the necessary libraries so first i am going to import numpy as
np and then from scipy will import
sparse i'll run this cell okay next let me go ahead and create a
matrix i'll use the np dot array function and within this function
i'll create a 3 cross 3 matrix and i'll assign the elements
so my first row will have 1 2 3 next we'll have 4 comma 5 comma 6
then we'll have
7 8 and 9 all right let me run this cell okay
now let me go ahead and print my matrix so i'll use the print
function and let's say i'll pass in a message that is original matrix
comma and then pass in my variable name that is matrix let's run it so here you can see i have
my original matrix it is a 3 cross three matrix and has nine elements in it
first let's see how using dictionary of keys you can create a sparse matrix so for
that i'll write the print statement and then i'm going to use a function which is
called dok that stands for dictionary of keys underscore
matrix and i'll pass in my variable name which is my original matrix so if i run
this you can see i have got my dictionary of keys sparse matrix
next i'll show you how using diagonal storage paste you can create a spot that i am going to
use another function that is sparse dot dia
which stands for diagonal underscore matrix and within this function i'll pass in my
original matrix variable name if i run this you can see we have created diagonal
storage based sparse matrix all right now moving to the next question
so given below is a matrix we need to find the inverse of this
matrix now the inverse of a square matrix a which is sometimes called as a reciprocal matrix
is a matrix a inverse such that the dot product of a and a inverse is always i which is an identity matrix now in
numpy we have a function which you can see here that is np dot lin alg dot inv
that helps you find the inverse of a matrix let me show you on the jupyter notebook
first let me print my original matrix all right now here i'll use the function
with min that is np dot l-i-n-e-l-g dot inv which stands for
inverse and then pass me my variable name that is matrix if i run this you can see
i have got the inverse of my original matrix now moving to our fifth question so this
question is related to probability you call 2 over x and 3 ellipse if the
time that each takes to reach you is iid what is the probability that all the lips arrive
first next we want to find what is the probability that all the uber x arrive first now this is fairly a simple
question first let's look at the lifts arrive first then first we need to find the
probability that the first car is a lift which is going to be three by five since we have three
lifts and total we have five vehicles next we want to find the
probability that the second car is the lift which is going to be 2 by 4 third we need to find the
probability that the third car is also a lift which is 1 by 3 now arrive first
is going to be the multiplication of the product of the above three probabilities that is
three by five into two by four into one by three which is nothing but one by ten
next let's say you want to find for all the ubers that arrive first so you need to find the probability that
the first car is uber which is nothing but two by five since we have two ubers
next the probability that the second car is an uber is going to be 1 by 4 so the total probability that all
the ubers arrive first is going to be the product of 2 by 5 and 1 by 4 which is also 1 by 10.
now moving to the 6th question the mean median and mode of a distribution are
30 25 and 20 we need to determine if the distribution plot will be positively
skewed or negatively skewed so you can see from the distribution graph the mean
is to the right median to the middle and more to the left so since the value of mode
is less than the mean and median the data will be positively skewed and similarly the skewness of the
distribution is on the left if the mean value is less than the median and the mode occurs at the highest frequency of the
distribution the next question we have is what are covariance and correlation
so covariance and correlation are measures that help you understand the relationship between two continuous variables
so covariance tells the direction of the linear relationship between two random variables
the direction means if the variables are directly proportional or inversely proportional to each other
so increasing the value of one variable might have a positive or a negative impact on the value of the other
variable now covariance can take any value between minus
infinity and plus infinity so below you can see the formula to calculate the covariance
so you have summation x i minus x bar x bar or x dash here represents the mean
x i represents the individual points then this is multiplied by y i minus y dash or the mean of y
divided by the total number of observations next we have correlation so correlation
explains the change in one variable leads to how much proportion changes in the second
variable correlation tells how strongly two random variables are related to each other
it takes values between minus one and plus one a negative correlation means they are
inversely proportional to each other a positive correlation means they are directly proportional to each other
if the correlation coefficient is zero then there is no linear relationship between the variables
so here below you can see the formula to calculate the correlation now moving to the eighth question
so what are the sampling methods used in data science now sampling is a statistical technique
that is used to select manipulate and analyze a subset of data points and find patterns and trends in the
large data set been examined now there are two types of sampling methods
we have probability sampling and non-probability sampling now under probability sampling we have
something called as random sampling so in random sampling every individual is chosen entirely by
chance and each member of the population has an equal chance of being selected then we have stratified sampling
so in stratified sampling subsets of the population are created based on a common factor and samples are
randomly collected from each other now talking about systematic sampling so
in systematic sampling a sample is created by setting an interval at which to extract data from the larger
population an example could be selecting every fifth row in a spreadsheet of 100 items
and creating a sample size of 20 rows for analysis and finally under probability sampling
we have cluster sampling so in a cluster sampling subgroups of the population are
used as the sampling unit rather than individuals so the population is divided into clusters
and a whole cluster is randomly selected to be included in the analysis
now let's look at the sampling methods that are present under non-probability sampling
so first we have convenience sampling so for convenience sampling the samples are selected based on the
availability so this method is used when the availability of samples is rare and costly
so based on convenience samples are selected second we have quota sampling so quota
sampling depends on some preset standard the proportion of characteristics in sample
should be the same as the population next we have purposive sampling
it is also known as judgmental sampling you select members of sampling on the basis of the objective of the study
it is very useful when you want to reach a particular or targeted sample quickly
finally we have snowball sampling so the snowball sampling technique is used in situations where the population is
completely unknown and rare now coming to the ninth question in our list so explain what
p value tells about statistical significance so p value is a statistical test that is
used in hypothesis testing it helps you to decide whether to accept or reject the null hypothesis
the null hypothesis represented as h0 is a commonly accepted fact it is the
opposite of alternative hypothesis researchers and scientists work to reject or disapprove the null hypothesis
they come up with an alternative hypothesis that they think explains a phenomenon and they work to reject the null
hypothesis for example null hypothesis or h0 could be the world
is flat and an alternative hypothesis could be the world is round
so lower the p-value the greater the statistical significance of the observed difference
now the p-value is just one piece of information you can use when deciding if your null hypothesis is
true or not the p-value lies between 0 and 1 as you can see here the threshold for
p is set to 0.05 when the value is below 0.05 the null hypothesis is rejected
now coming to the final question in our list of mathematics interview questions for data
science the question is what do you understand by type 1 versus type 2 error
now in statistical theory the idestical error is an integral part of hypothesis testing
in statistical hypothesis testing no test is ever 100 certain
that's because we rely on probabilities to experiment even though hypothesis tests are meant
to be reliable there are two types so type one error occurs when the null hypothesis is true and we reject it
it is also known as a false positive finding or conclusion an example could be an innocent person
is convicted and type 2 error occurs when the null hypothesis is false and we accept it
it is also known as false negative finding or conclusion an example could be a guilty person is
not convicted so here you can see we have a reality where your null hypothesis is true
and here we have the null hypothesis is false and this is where the type one error lies so this can also be represented as
false positive where the null hypothesis is true and be rejected and this could be
false negative where the null hypothesis is in reality it's false and we accept it in this
video we will learn about some of the machine learning and deep learning interview questions
so let's begin with our first question the first question is how to detect
outliers in data so in data analytics and machine learning you often find data points that
lie at an abnormal distance from other points in a random sample from a population those are called
outliers now outliers in data can significantly impact any prediction analysis
there are majorly three different methods to treat outliers first we have the univariate method it
is one of the simplest methods for detecting outliers the univariate method uses box plots a
box plot is a graphical display for describing the distributions of the data box plots use the median and the lower
and upper quartiles this method looks for data points with extreme values on one variable
next we have the multivariate method so the multivariate outliers can be found in an n-dimensional space having n
features we look for unusual combinations of all the variables in this method finally we have minkowski error
this method reduces the contribution of potential outliers in the training process the minkowski error is a loss index that
is more insensitive to outliers than the standard mean squared error now moving on to the second question
what is a confusion matrix so a confusion matrix is a table that is used to describe the performance of a
classification model on a set of test data for which the true values are already known the target variable has two values
positive or negative the columns represent the actual values of the target variable which you can see
here the rows represent the predicted values of the target variable which you can see here now there are
four important terms that are related to confusion matrix first we have true positive which is this one
so in true positive the predicted value matches the actual value so the actual value was positive and the
model also predicted a positive value then we have true negative which is also
represented as tn the true negative depicts the predicted value matches the actual value
now the actual value was negative and the model predicted a negative value next we have false positive now false
positive is also known as type 1 error in false positive the predicted value was falsely predicted
the actual value was negative but the model predicted a positive value finally we have false negative the false
negative is also known as type 2 error so in false negative the predicted value
was falsely predicted the actual value was positive but the model predicted a negative value
now moving to our third question which is explain the roc curve now the roc curve
is one of the most important evaluation matrix for checking the performance of any classification model
roc stands for receiver operating characteristic receiver operating characteristic or roc
curve is a method to compare the diagnostic tests the roc curve was created by plotting
the true positive rate against the false positive rate at various threshold settings so here on the y-axis
you have the true positive rate on the x-axis we have the false positive rate the true positive rate indicates
the proportion of observations that were correctly predicted to be positive out of all positive observations
similarly the false positive rate is the proportion of observations that are incorrectly predicted
to be positive out of all negative observations you can take an example suppose in
medical testing the true positive rate is the rate in which people are correctly identified to
test positive for the disease in question let's say the coronavirus testing
roc does not depend on any class distribution this makes it useful for evaluating
classifiers predicting rare events such as diseases or disasters now moving to the fourth question we
have what are the assumptions for linear regression so linear regression analysis is used for modeling
the relationship between a single dependent variable y and one or more feature or predictor variables
some of the important assumptions for linear regression are so first they should have linearity
so linear regression needs the relationship between the independent and the dependent variables to be linear
it is also crucial to check for outliers since linear regression is sensitive to outlier effects
next we have homocydasticity homoscedasticity illustrates a situation in which the
error term that is the noise or random disturbance in the relationship between the features and the target variable is the same across
all levels of the dependent variables third we have independence so observations should be independent of
each other finally we have no multi-collinearity so there should be little or no
multi-collinearity independent variables should not be too highly correlated now moving to our fifth question in our
list of interview questions the question is what is regularization
in machine learning explain the l2 regularization
so regularization is a machine learning technique that is used to reduce the errors by fitting the function appropriately on
the training set in order to avoid overfitting of data so overfitting happens when a model
learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data
so here you can see we have a nice plot which shows how overfitting of data can
be visualized and here we have a good fit line over
the same data points so this is also known as the regression line now l2 regularization is also known as
ridge regression so ridge regression modifies the overfitted model by adding the squared magnitude of coefficient
as a penalty term to the loss function so on the right you can see a set of data points plotted
and we have our linear regression line and here we are calculating the cost function for the
ridge regression line so our cost function is actually loss
plus lambda into summation of w squared where loss is actually the sum of squared
errors or squared residuals lambda stands for penalty for the errors
w is called the slope of the curve or line okay now consider a case where there are two
points passing through the linear regression line now if you calculate the cost
function we get the value as 1.69 so here we have assumed that loss is zero since the two
points lie directly on the line we have taken lambda to be one and w is one point three so if you use
this function or this formula you get the cost function as one point
six nine now moving ahead let's consider another situation where
we'll calculate the same cost function for the ridge regression line there is some loss
for both the points as they are not on the same line so here you can see the sum of squared residuals
is 0.05 is actually is the sum of 0.2 square i'm assuming this as
0.2 and 0.1 for this one so if you square both and add it the
value is 0.05 and lambda is again 1 and w we have assumed to be 0.6 now if you
find the cost function the value is 0.41 let's draw the linear regression
line and the ridge regression line with all the points we find that the ridge regression line as the best fit
since its cost function is less now coming to the sixth question what are the different
methods to split a tree in a decision tree algorithm
so there are three methods to split a decision tree first we have variance so reduction in
variance is an algorithm that is used for continuous target variables this algorithm uses the standard formula
of variance to choose the best split so here you can see the standard formula variance which is summation of
x that is all the individual points minus x bar which is the mean squared divided
by the total number of observations now the split with lower variance is selected as the criteria to split the
population now the steps to calculate variances you need to calculate variance for each node
and then you need to calculate for each split as the weighted average of each node variance
moving ahead the second method we have is information gain so information gain is used for
splitting the nodes when the target variable is categorical it works on the concept of entropy now
the degree of disorganization in a system is known as entropy so here you can see the formula for
information gain which is 1 minus entropy finally we have
gini impurity so gini impurity is the probability of incorrectly classifying a randomly chosen element
in the data set if it were randomly labeled according to the class distribution in the data set
so below you can see the formula for gene impurity so we have 1 minus summation of p i
whole square where n represents the number of classes and p of 5 represents the probability of
randomly picking an element of class i now moving to the seventh question
so the question is how do we find the optimum cluster value in k-means clustering algorithm
now there are two methods to find the optimum cluster value so first we have the elbow method which
is one of the most well known for finding the optimum number of clusters so in this
method you need to calculate the within cluster sum of squared errors for different values of k
and choose the k for which within cluster sum of squared errors first starts to diminish so in the below
plot of squared errors versus the number of clusters k you can see at k is equal to 4 the
squared error starts to diminish so hence our optimum k value is 4.
next we have the silhouette method so the silhouette method measures how
similar a point is to its own cluster compared to other clusters the average
siloid method computes the average steroid of observations for different values of key
the optimum number of clusters k is the one that maximizes the average solid over a
range of possible values for k the soloid score reaches its global maximum at the optimal k so
in our case the average siloid reaches maximum at key is equal to 2 which you can see here
so our optimum cluster value will be 2 here moving ahead the 8th question in our
list is how does the pooling layer work in a convolutional neural network
so the pooling layer performs a down sampling operation in order to reduce the dimensionality of the feature map so in
the pooling operation you slide a two dimensional filter over each channel of feature map and
summarize the features lying within the region covered by the filter it is a common practice to periodically
insert a pulling layer in between successive convolutional layers in a convolutional neural network architecture
so the pooling layer operates independently on every depth slice in the input and resizes it spatially
using the max operation so in the diagram shown here you can see we have a rectified feature map
we are using a 2 cross 2 filter and performing a max pooling operation
so consider this as the filter if you perform the max operation over the values let's say
0 5 3 and 1. so considering this one our pool feature map maximum value will be 5.
similarly for this chunk of data it is going to be seven next if you slide the filter over this
square frame you get eight and similarly here you get six so this is also known as a
pooled feature map moving ahead the ninth question in our list is how
does lstm network work so long short-term memory networks are a type of recurrent neural networks
that are capable of learning order dependence and sequence prediction problems so remembering information for long
periods of time is practically their default behavior now lstms also have this chain like
structure which you can see here but the repeating module has a different structure so instead of having a single
neural network layer there are four interacting in a very special way
now you can see these are called as gates these gates contain sigmoid activations
a sigmoid activation is similar to the tanning activation instead of squishing values between
minus 1 and plus 1 it squeezes values between 0 and 1 and ls dm has 4 gates
now these are called forget remember learn and use or output so if you see this in
the first step we use the forget gate that decides what information should be thrown away or
kept the information from the previous hidden state and the information from the current input is passed through the
sigmoid function values come out between 0 and 1. so if the value is closer to 0 it means
you need to forget that information and if the value is closer to 1 it means you need to keep that information
next we have the input gate so the input gate is used to update the cell state
first we pass the previous hidden state and the current input into a sigmoid function that decides
which values will be updated by transforming the values to be between 0 and 1. 0 means not
important and 1 means important you also pass the hidden state and current input into the tanh function to
flatten the values between -1 and plus 1. this helps to regulate the network then
you multiply the tanh output with the sigmoid output the sigmoid output will decide
which information is important to keep from the 10h output and finally in step 3 we have the output
gate this output gate is used to decide what the next hidden state should be
first we pass the previous hidden state and the current input into a sigmoid function
then we pass the newly modified cell state into the tanh function
we then multiply the tanh output with the sigmoid output to decide what information the hidden sheet
should carry the output is the hidden state the new cell state and the new hidden
state is then carried over to the next time step finally talking about the last question
in our list of interview questions we have explained the concept of gradient descent in deep learning
now gradient descent is an optimization algorithm which is mainly used to find the minimum of a function in machine
learning gradient descent is used to update the parameters in a model parameters can vary according
to the algorithm such as coefficients in linear regression and weights in neural networks
you can see we have these maps and on the y-axis we have the loss
on the x-axis we have the weight and here we are trying to find the local minimum
or the global minimum now this gradient decent method is used to minimize the cost function and update
the parameters of the learning model the gradient always points in the direction of the steepest
increase in the loss function the gradient descent algorithm takes a step in the direction of the negative
gradient in order to reduce the loss as quickly as possible to determine the next point along the
loss function curve the gradient descent algorithm adds some fraction of the gradient's magnitude to
the starting point now this process is repeated to find the global minimum
so that brings us to the end of this video tutorial on data scientist full course i hope it was useful and informative
thank you for watching stay safe and keep learning