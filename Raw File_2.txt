
SUPPORT VECTOR (CONTINUE)
Hello and welcome back to our discussion on Support Vector Machines.
So, we were looking at the optimization problem corresponding to the optimal separating hyper
plane. So, to solve this problem, so with one of the techniques for solving these kinds
of constrained optimization problems is to set up a Lagrangian, which essentially looks
at the original objective function, which is half beta square and the second component
corresponding to the constraints that we have. So, if you look at this quantity in the square
brackets here, so you can see that this is the term on the left hand side of the inequality
and that is the term on the right hand side of the inequality and we really want to make
sure that, this difference is not negative. If this difference is negative, then that
would mean that y i times x i transpose beta plus beta naught is actually less than 1.
So, we do not want this to be negative, so what we essentially say is, this we added
here as with a minus sign. So, this essentially means that, when I minimize
this whole expression, so this term will become as large as possible, as largely positive
as possible. So, that essentially means that I will go and try and make this as larger
than 1 as possible. So, this term here alpha i let us me control how much weight I want
to give to satisfying the constraints versus how much I really want to minimize the objective
function. So, we really need to satisfy the constraints as much as possible and since,
there are solutions that will satisfy the constraint and give you a good optima.
So, we should essentially be trying to derive this thing to as larger value as possible.
So, this is called the primal of the problem and your goal is to minimize the primal. So,
I am going to do something fairly technical right now. So, if you do not understand all
of it in the first goal that is fine, you might have to do a little bit more reading
on this side, but this is essentially give you an idea of how people go about solving
these kinds of problems. So, we are going to try and create, what is
called the dual of this, the primal objective function. So, the dual is a way to create
something that create an optimization problem, that is simpler to solve in some sense than
the primal and the dual at all points provides you some kind of a lower bound on the kind
of solutions that you can achieve with the primal and that the optima of the dual you
ideally like the optima of the primal also to be achieved.
So, we are going to create a problem called the dual, we are going to solve the dual and
when we reach the optima of the dual, you would like the optima of the primal to be
also achieved. The same solution that gives you the optima in the dual problem should
give you the optima and the primal problem and there are technical conditions under which
this is satisfied and we are not going to go in to the technical conditions and this
going to be give you a flavor of kind of results that will be looking at.
So, let us start by
setting the derivative of L p to 0, derivative with respect to beta and beta naught. So,
taking the derivative with respect to beta and setting it to 0 and solving for beta gives
me… So, you can figure there out by little bit of algebra here and likewise setting that
derivative with respect to beta naught to 0 and solving it gives me. So, you can substitute
these back into the primal problem and do a lot of algebra, do a lot of algebra really
and then I can simplify this and I will get what is known as the dual, we write the dual
here. So, this is just really obtained by substituting your beta into the expressions
here and then, using the fact that alpha i y i will be 0 at the optimum.
So, that is the thing, but then it is subject to be constrained. So, note that I said, so
your dual is always going to give you a lower bound on the solution of the primal problem.
So, really if you are minimizing the solution in your primal, it should be maximizing the
solution in the dual, so that the two of them can coincide at some point. So, essentially
you would be maximizing this subject to the constraint that, all your alpha i’s are
greater than or equal to 0.
So, if you think about it, this is the much easier constraint to wrap our heads around,
because it just says that you will only be doing it in the positive co ordinates and
while this had a more complex set of constraints. So, you kind of reduce the constraint to do
something easier and therefore, the dual problem is sometimes easier to solve. So, for the
dual and their primal to be at optima at the same time, so you really want them to satisfy
a set of conditions, which are essentially to with the derivative of the primal problem.
So, we required that this should hold, we required that this should hold, we write them
as 1, 2, 3. In addition, you required that this condition should also be required, that
this condition should also be satisfied, these are called the KKT or the Karush Kuhn Tucker
conditions. And so far, the optimization problem to have the same solution, we require that
the KKT conditions should be satisfied. So, once you have an optimal solution for
the dual and the primal problem, because these KKT conditions have to be satisfied, you can
make certain observations, especially we are working from condition 3 here. So, if alpha
i is greater than 0, so what does it mean. So, this has
to be equal to 0, then the term in the square bracket has to be equal to 0; that means,
y i into x i transpose beta plus beta naught should be 1. So, what does that mean?
It means that, it is exactly on the edge of the margin when it is equal to 1, because
it is greater than equal to 1 is what we needed to satisfy, so when it is equal to 1; that
means, it is exactly on the margin. Likewise if so, if the quantity in the square bracket
is greater than 1, then alpha i has to be 0, but that essentially
means is, if your data point is something; that is far away from the hyper plane let
us just more than the margin away from the hyper plane, then the corresponding alpha
is will become 0. So, what does this mean for us, so if you
think about it. So, the solution that we get, which is essentially beta that is the solution
that we want to get is formed by taking the product of alpha i, y i and x i. So, if saying
the alpha i is going to be 0, it essentially means that the corresponding x i has no role
to play in determining, what my beta should be if I say that it implies if x i is 0 that
implies that x i has
no role in computing beta. So, which are the data points, which will
actually effect the solution beta here exactly those points for, which y i times x i transpose
beta plus beta naught is 1; that means, these are exactly the points, which lie on the margin
. So, only these points will influence, how the solution beta looks like and all the other
data points that we have, which are farther away from the separately high per plane, then
these points do not matter in the solution. So, these points are called
support vectors. So, you don’t really have to solve this
optimization problem yourself there are enough tools that actually can do it for you the
whole goal of this lecture is to get you to appreciate, what is said that you are doing
when you are using a support vector machine for solving a problem. So, at the end of the
day all we are going to do is fire up tool that is going to tell you, what is the separating
hyper plane given a bunch of data, But, it is good to have an appreciation of how the
classifier is actually build. So, once you figure out the beta, then I can
substitute that I can substitute that into the KKT the third condition here and solve
for beta naught. So, typically what you do is that you use every x i that is a support
vector and you substitute that here and then, try to solve for beta naught and typically
end of taking the average value of that. So, the couple of things, which I want to point
out about support vector machines. So, one thing is we should be very clear that
the training data none of the training data will fall within the margin, but that it is
not to say that the test data might fall might not fall within the margin the test data might
fall within the margin it might actually fall on the other side of the hyper plane. So,
for all we know that the test data that could be errors on the test data it is just on the
training data it tries to fix something there is as far away as possible from the data points.
So, the idea here is that, so if I give as much gap between the classes as possible,
then the classifier would be more noise on either side. So, this is the assuming that
the noise could be in this class or in this class if you know for sure that one class
is noisier than the other or if one class is more valuable than the other. So, you might
want to actually modify your objective, so that the line does not go write in the middle,
but it is goes to one side or the other. So, having said that under the assumptions
of the support vector machines if assumptions hold good, then is a very, very robust classifier.
So, the reason is it pays attention only to the points that are closest to the class boundary.
So, you know I can have as many data points here I say want I can have as many data points
here I say want of the corresponding class it will be does not affect my classification,
because truly the once that are close the boundary are the once that need attention.
So, that essentially makes support vector machines more robust and on the other hand
if you are going to have some kind of stochastic process that is generating the data right.
So, if there are the few data points there are by chance or noise data points that actually
close to the hyper plane that will affect the support vector machines tremendously.
And therefore, it will try to reduce the margin by a large extent while classifier that looks
at the entire data and tries to find the distribution for the entire data might be a little bit
more robust to this kinds of noise. So, this is the, this is how you solve the basic optimization
problem for support vector machines.
 
 
 
Support Vector Machines for Non Linearly Separable Data
Now, we look at the case where the data is not so-well behaved as you wanted to be. Specifically
the data is not separable, right, is not linearly separable. So, you look at the non-separable
case. So, I am going to introduce some additional data points whatever have looking at so far.
So, this is a non-separable case, right, linearly non-separable case because some of my data
points are really mixed up here, right. Now, still we will like to have large margin, but
not only have I allowing data that is not separable, but I am also allowing data points
to fall within the margin. So, its essentially two sides of the same coin, right. So, if
I am allowing data to fall within the margin, so in some sense I am having the flexibility
to make some kind of errors as well.
So, I essentially look at how far away am I from the margin, right in the long direction.
So, I am going to denote these distances by which I am away from the margin by the symbol
zeta, right, but we still have same constraints here, but now going back to our optimization
problem, but I am really looking at here is, I am going to modify my constraints such that,
so y i times x i times beta plus beta naught, right is really greater than or equal to 1
minus zeta i, but zeta i is some kind of slack variable that allows me to satisfy this constraint
with some error in it. So, essentially if you look at this first data point that we
drew here, if you look at the first data point I drew here, right. So, it has the slack of
zeta 1, right and the second data point as a slack of zeta 2, right and this one of the
really fairly large slack, but I still it is possible under the circumstances. This
allows me to have a larger margin, right. So, if you think about it if I did not, even
if this data point was not there, if I did not allow these kinds of data point appear
in the margin, right. If I dint allow these things to appear in the margin, my classifier
would actually have been here, right. My classifier would have been here trying to separate this
x n from this o, right and margin would have been very small. In all likelihood I am fitting
a noise data point here which is not an optimal thing to do, right.
So, now I am allowing some amount of data points to fall within the margin, right. I
am able to expand the size of the margin that I can have and I can also incorporate linearly
the small number of errors that I have to make because the data is not linearly separable,
right, but then I donÕt want to this become arbitrarily large, right. I just cant say
that hey it doesnÕt matter you can have slack variable for every data point that we have
and the slack variable can be as large as you wanted. I need to have a control on this
as well and therefore I try to minimize that, right. So, are these conditions sufficient
for us to define a new problem now, but still one more condition that we need. So, we are
really measuring zetas in one direction and not in the other direction, right. So, we
have to be careful. So, I also need to add another set of constraints, let us say that
the zeta have to be positive. So, that is the complete constraint optimization problem
for us in the case where the data is not linearly separable.
So, you have to minimize beta square, norm beta square plus the sum of the zetas that
we are using subject to the condition that y i x i times for beta plus beta not is greater
than 1 minus zeta i and zeta is greater than or equal to 0. So, what happens to our primal
objective function now? So, what is that we need a component that corresponds to the actual
objective function, and you need a component that corresponds to the constraints that we
are using, right. So, the first part of it remains as it is, right, but then we have
to add the newer components that we are bringing in, right. So, this is the second part of
the objective function and this is the first constraint, right and this is the second constraint
and since these has to be applied to each and every data point, so you have the summation
over all the data point you have, likewise here.
So, how do we go about deriving the dual in this case? So, just like we did earlier, so
start differentiating the primal with respective of various parameters. So, you end up with
the same condition that we had earlier, and here you end up with the same condition when
you differentiate with respect to the zetas, you end up with so, one condition for each
i, right. So, alpha i equal to c minus mu i. So, I can put everything back into the
primal, do some algebra and derive my dual which turns out look exactly like this, except
that my alpha has to lie between 0 and c that you can actually see from that condition that
we have there, right. alpha i y i have to be equal to 0. That we already have as a condition.
So, the remainder of the KKT condition that we have, I am just going to go through this
very quickly because I donÕt want to do the complete derivation here.
So, the remainder of the KKT conditions will be, this is what we had last time except for
the zeta i part, right, but you also have. It is essentially or initial constraint written
in a slightly different form, right so the third constraint here which is the original
constraint with 1 minus zeta taking to the other side. So, what is that we notice from
here? So, let us go back and let us do the same argument. If alpha i is greater than
0, then y i x i transpose beta, beta naught is actually less than 1 and only, then this
will be zero because right. So, that would mean that for the particular choice of zeta
i. So, this goes to 0. That would mean that this is on the wrong side of the margin, right
or on the margin, right. If the zeta i is 0, then it will be on the margin and if the
zeta i is not 0, it will be within the margin. So, all the data points that are on one side
of the margin or all, right because again like last time, so beta depends only on those
vector for which alpha i is greater than 0, right.
So, if the data happens to be on the right side of the margin, right then your alpha
i have to be 0 as we saw earlier. So, those data points have no role in computing. So,
this is essentially kind of takes us over the entire optimal separating hyper plane
part, where it was linearly separable. So, we had a very easy solution, but when the
data is not linearly separable, so we have to allow for the possibility of data points
to lie on the wrong side of the margin. So, when we do that, we can essentially take advantage
of the fact and try to push our margin away by allowing data point which are correctly
classified, but are within the margin is small fraction of such data points are permitted
and therefore, we need to expand the margin a little bit. Therefore, we can come up with
the solution. So, you donÕt have to, at this point here
donÕt have to go in details of solving this optimization problem, but there are many powerful
optimization techniques it have been developed that allow you to solve these kinds of problems.
In fact, SVM have let to the revival of popular class of optimization algorithms called interior
point methods because these are very efficient in solving these kinds of optimization problems.
They are pretty robust classifiers and are very widely used for wide variety of applications,
but some of you will be probably thinking right now, but whenever I talk about a support
vector machines, people always tell me something about kernels, right. I thought to support
that machine all about kernels and I have not talked about kernels at all at any point
here, yes. So, the kernel idea is very crucial in support
vector machines, and I will be looking at that in more detail in the next module. What
you should remember is the basic optimization problem that you are trying to solve with
support vector machine is the one of optimal separating hyper plane. So, in fact the kernel
idea is called the kernel trick because it allows you to solve really wide variety of
problems which is not easily amenable to linear classification by using a very powerful idea,
but then the under lying optimization problem that you are solving is still this, the same
optimization problem that on the boards so far in the last two modules.
 

Support Vector Machines and Kernel Transformations
So far, we have been looking at the problem of the optimal separating hyper-plane in the
previous two modules, but then the idea of Support Vector Machines is to be able to use
it in data which is not nearly linearly separable or only working in that space of linear hyper-planes,
right.
One way of looking at extending this problem to complex settings is to think of taking
your original data, right and transforming it into something else, and then trying to
apply the same idea to the transformed function. This idea should not be new to you because
you have always seen this in linear regression where you can look at transforming the input
variables into some other kind of basis function, and then trying to do linear regression on
that. So, the idea is similar to that, but we are going to make use of a very powerful
technique here, right. So, look at how predictor is going to work. So, we are going to have
f of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is
lesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will
predict it as being of class plus 1, right. As on this is, so this is separating hyper-plane
that we have, fine. So, if you think about it, we said we have solution betas, right
or going to be of this form. Therefore, I can rewrite this, right. So, if you look at
it, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner
product of xi and similarly, if you look at how you are solving the optimization problem,
this is what we wrote down last time. So, the duel is essentially going to have again
x i transpose x k, right. So, you can see that the xÕs appears in our problem always
as inner products, right and if somehow you are able to compute this inner products efficiently,
then you should be able to solve the problem more efficiently. In particular, given that
you are going to be looking at this kind of transformation, right. So, I can now write
this as , where this denotes the inner products. This is what we can say inner product, right.
So, likewise the dual also can be written as
So, if you stop and think about it, it essentially tells you that I really donÕt need to know
h of x, right. If you have an efficient way of computing the inner product of the transformed
function, right, then you really donÕt need to know what the actual transformation itself
is, right. So, there is a class of function which I am going to call as kernel functions
here. It is called as kernel function which allows you to compute this inner product efficiently.
So, essentially it is going to say that right. So, the kernel corresponding to function h,
right when you give it as input x and x square, which is going to compute the inner product
of h of x and h of x square, right.
So, one of the things that we require for a function to be a kernel function, right
is there it should be symmetric. So, k should be symmetric, positive-definite. So, if we
do not really understand that, so k should be symmetric in the sense that if I give it
the set of x and x prime x prime, so the kernel functions for x and x prime should be the
same as x prime, x. The positive-definite essentially means that if I take any vector
x transpose K x, that should always be positive. So, there are technical reasons for why this
condition should be satisfied. For one rough way to think about it to say that this essentially
you would want this to be whole thing to be positive, so that your optimization problem
will work as you wanted to. So, that is rough intuition behind why you
need this condition. Some of the popular choices for the kernel functions are the polynomial
kernel. This is essentially 1 plus. I got the parameter d is something that we choose.
The other one is Gaussian or the radial basis function. The other one is sometimes called
the neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,
right? So, as I was mentioning earlier, you have a data there is given to in the original
dimensions, right. The data might be badly mixed up in that original dimension, it might
not be easily separable at all in the original dimension, but then when you look at the transformed
dimension, then the data becomes linearly separable, right.
Let us take the example of the polynomial kernel and see what happens. So, I am going
to look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I
am assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these
are like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.
So, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality
of the underlined space, but in this case I am assuming this has. So, if we take the
square of this, so I essentially end up with the expression that has six components to
it, right. So, if you think about it, this is somewhat
like taking the inner product in a very higher dimensional space than the original space.
Originally x and x prime were residing in a two-dimensional space, but if you look at
what is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,
h2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;
h4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine
that I transformed my original two-dimensional representation into a six-dimensional representation
is essentially taking all the second order terms along with original terms.
Now, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with
this expression, right. Inner product of h of x and h of x prime, right. So, the entire
six-dimensional vector if I take the inner product, we are basically going to end up
with this expression. So, what we have done here, we took the inner product in two-dimensional
space, right and performed the squaring operation. So, this is going to give me number which
is equal to the number I will get by first transforming the data point from two-dimensions
is six-dimension and then, taking the inner product in the six-dimensional space. So,
essentially this allow to work in much higher dimensional space than originally intended,
but by only looking at inner product computation in the original space, right.
So, why this is a useful property to have in the case of support vector machine, is
that all over operation here operate only within inner product whether we are finally
trying to predict the output f of x, or when you are trying to solve the dual problem ld.
So, all we really need to do is know what the inner product is, right. Now, I am able
to take the inner product in a higher dimensional space, but then do the computational only
in the lower dimensional space. This allows us to have a much greater advantage than simply
operating with the original dimension.
So, to see how this polynomial transmission really helps us, let us look at a very simple
example, right. I am going to assume that the single dimension, right and then I have
data points, let us assume this is 0. I have data point that look like this, right. So,
this is the dimension x1. So, obviously there is no single line that I can draw to separate
these into two classes neatly, right. So, I can assume my slack variables and try to
draw the line here that says ok I am not making too many errors bla bla, so on and so forth,
but still that is not the satisfactory solution, but let us looks at what happens if I try
to plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square
as my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that
will be x1 that is corresponding to this here and then looking at the square of that, but
then looking at these data points, so this is going to go here, this will probably go
here, right. Now, it is very clear that I can draw straight
line, right. I can draw if can find the linear decision boundary that separates these two
classes once I have done the appropriate transformation. So, this essentially allows us to solve a
larger class of problems. You see the kinds of basis transformation, then we could do
just by operating in the original space and trying to solve the linear optimal hyper-plane
problem. So, this is essentially what all your choices of kernel functions or all about,
right. So, if you look at any SVM tool, they will tell you to pick one kernel function
which is either the polynomial kernel. So, in which case you have to pick in the appropriate
d or you have to pick radial basis function or Gaussian kernel, in which case you have
to pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or
we can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa
2 which are the parameters that define how quickly this sigmoid function rises. We will
see more about that when you look at neural networks.
So apart from this, you still have one further parameter that we will have to worry about
which is this constant c, right. So, this tells you how much slack that you are willing
to tolerate, right. So, if you think about it, if c is very large, if c is infinite,
then we have to be in the completely separable case because even a very small value of zeta
will cost this objective function to become very large, right. Even for a small value
of zeta, right so you will find that this thing is actually very bad, right. c has to
be if c is very large, then zeta has to be very small, right and if c is small, then
zeta can be large. So, what this is going to tell you is that in the higher dimensional
space, where you are assuming that the higher the dimensionality that you are projecting
into, more likely is the data will be separated, right because if c is large, right because
you are going to try in fit more complex surface, right. c will look, the surface will look
very vigil, right and if c is small, then you will really get much smoother surface,
but the possibility of you making errors is also higher. So, thatÕs trade of that you
have to figure out empirically by looking at how you are doing the, how you are performing
in the actual data. This brings us to the end of the module on
Support Vector Machines and these are very powerful classifiers and quite often they
are the first classifiers of choice for people when they are trying to solve new problem
which they really donÕt know much. So, one thing I should point out that people have
come up with different kinds of kernel functions. So, I have given you three choices of kernels
here. These are essentially the most popularly used kernel choices, especially if you are
operating with text data, you would like to use linear kernel d is 1 and in most other
forms, you will be looking at RBF kernels, but then for special forms of data like graphs
and strings and so on and so forth, people have defined their own kernels and as long
as they are satisfying your properties of the kernel function, you can define your own
kernels and then have them help you solve the problem, right. That is topics for another
day or perhaps for another course. So, we will stop here. Just let me reframe that SVM
are one of the most popular and powerful classifier that are currently being used widely.

 
 
 
 
Ensemble Methods and Random Forests
 
 
So far, we have been looking at the problem of the optimal separating hyper-plane in the
previous two modules, but then the idea of Support Vector Machines is to be able to use
it in data which is not nearly linearly separable or only working in that space of linear hyper-planes,
right.
One way of looking at extending this problem to complex settings is to think of taking
your original data, right and transforming it into something else, and then trying to
apply the same idea to the transformed function. This idea should not be new to you because
you have always seen this in linear regression where you can look at transforming the input
variables into some other kind of basis function, and then trying to do linear regression on
that. So, the idea is similar to that, but we are going to make use of a very powerful
technique here, right. So, look at how predictor is going to work. So, we are going to have
f of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is
lesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will
predict it as being of class plus 1, right. As on this is, so this is separating hyper-plane
that we have, fine. So, if you think about it, we said we have solution betas, right
or going to be of this form. Therefore, I can rewrite this, right. So, if you look at
it, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner
product of xi and similarly, if you look at how you are solving the optimization problem,
this is what we wrote down last time. So, the duel is essentially going to have again
x i transpose x k, right. So, you can see that the xÕs appears in our problem always
as inner products, right and if somehow you are able to compute this inner products efficiently,
then you should be able to solve the problem more efficiently. In particular, given that
you are going to be looking at this kind of transformation, right. So, I can now write
this as , where this denotes the inner products. This is what we can say inner product, right.
So, likewise the dual also can be written as
So, if you stop and think about it, it essentially tells you that I really donÕt need to know
h of x, right. If you have an efficient way of computing the inner product of the transformed
function, right, then you really donÕt need to know what the actual transformation itself
is, right. So, there is a class of function which I am going to call as kernel functions
here. It is called as kernel function which allows you to compute this inner product efficiently.
So, essentially it is going to say that right. So, the kernel corresponding to function h,
right when you give it as input x and x square, which is going to compute the inner product
of h of x and h of x square, right.
So, one of the things that we require for a function to be a kernel function, right
is there it should be symmetric. So, k should be symmetric, positive-definite. So, if we
do not really understand that, so k should be symmetric in the sense that if I give it
the set of x and x prime x prime, so the kernel functions for x and x prime should be the
same as x prime, x. The positive-definite essentially means that if I take any vector
x transpose K x, that should always be positive. So, there are technical reasons for why this
condition should be satisfied. For one rough way to think about it to say that this essentially
you would want this to be whole thing to be positive, so that your optimization problem
will work as you wanted to. So, that is rough intuition behind why you
need this condition. Some of the popular choices for the kernel functions are the polynomial
kernel. This is essentially 1 plus. I got the parameter d is something that we choose.
The other one is Gaussian or the radial basis function. The other one is sometimes called
the neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,
right? So, as I was mentioning earlier, you have a data there is given to in the original
dimensions, right. The data might be badly mixed up in that original dimension, it might
not be easily separable at all in the original dimension, but then when you look at the transformed
dimension, then the data becomes linearly separable, right.
Let us take the example of the polynomial kernel and see what happens. So, I am going
to look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I
am assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these
are like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.
So, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality
of the underlined space, but in this case I am assuming this has. So, if we take the
square of this, so I essentially end up with the expression that has six components to
it, right. So, if you think about it, this is somewhat
like taking the inner product in a very higher dimensional space than the original space.
Originally x and x prime were residing in a two-dimensional space, but if you look at
what is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,
h2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;
h4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine
that I transformed my original two-dimensional representation into a six-dimensional representation
is essentially taking all the second order terms along with original terms.
Now, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with
this expression, right. Inner product of h of x and h of x prime, right. So, the entire
six-dimensional vector if I take the inner product, we are basically going to end up
with this expression. So, what we have done here, we took the inner product in two-dimensional
space, right and performed the squaring operation. So, this is going to give me number which
is equal to the number I will get by first transforming the data point from two-dimensions
is six-dimension and then, taking the inner product in the six-dimensional space. So,
essentially this allow to work in much higher dimensional space than originally intended,
but by only looking at inner product computation in the original space, right.
So, why this is a useful property to have in the case of support vector machine, is
that all over operation here operate only within inner product whether we are finally
trying to predict the output f of x, or when you are trying to solve the dual problem ld.
So, all we really need to do is know what the inner product is, right. Now, I am able
to take the inner product in a higher dimensional space, but then do the computational only
in the lower dimensional space. This allows us to have a much greater advantage than simply
operating with the original dimension.
So, to see how this polynomial transmission really helps us, let us look at a very simple
example, right. I am going to assume that the single dimension, right and then I have
data points, let us assume this is 0. I have data point that look like this, right. So,
this is the dimension x1. So, obviously there is no single line that I can draw to separate
these into two classes neatly, right. So, I can assume my slack variables and try to
draw the line here that says ok I am not making too many errors bla bla, so on and so forth,
but still that is not the satisfactory solution, but let us looks at what happens if I try
to plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square
as my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that
will be x1 that is corresponding to this here and then looking at the square of that, but
then looking at these data points, so this is going to go here, this will probably go
here, right. Now, it is very clear that I can draw straight
line, right. I can draw if can find the linear decision boundary that separates these two
classes once I have done the appropriate transformation. So, this essentially allows us to solve a
larger class of problems. You see the kinds of basis transformation, then we could do
just by operating in the original space and trying to solve the linear optimal hyper-plane
problem. So, this is essentially what all your choices of kernel functions or all about,
right. So, if you look at any SVM tool, they will tell you to pick one kernel function
which is either the polynomial kernel. So, in which case you have to pick in the appropriate
d or you have to pick radial basis function or Gaussian kernel, in which case you have
to pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or
we can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa
2 which are the parameters that define how quickly this sigmoid function rises. We will
see more about that when you look at neural networks.
So apart from this, you still have one further parameter that we will have to worry about
which is this constant c, right. So, this tells you how much slack that you are willing
to tolerate, right. So, if you think about it, if c is very large, if c is infinite,
then we have to be in the completely separable case because even a very small value of zeta
will cost this objective function to become very large, right. Even for a small value
of zeta, right so you will find that this thing is actually very bad, right. c has to
be if c is very large, then zeta has to be very small, right and if c is small, then
zeta can be large. So, what this is going to tell you is that in the higher dimensional
space, where you are assuming that the higher the dimensionality that you are projecting
into, more likely is the data will be separated, right because if c is large, right because
you are going to try in fit more complex surface, right. c will look, the surface will look
very vigil, right and if c is small, then you will really get much smoother surface,
but the possibility of you making errors is also higher. So, thatÕs trade of that you
have to figure out empirically by looking at how you are doing the, how you are performing
in the actual data. This brings us to the end of the module on
Support Vector Machines and these are very powerful classifiers and quite often they
are the first classifiers of choice for people when they are trying to solve new problem
which they really donÕt know much. So, one thing I should point out that people have
come up with different kinds of kernel functions. So, I have given you three choices of kernels
here. These are essentially the most popularly used kernel choices, especially if you are
operating with text data, you would like to use linear kernel d is 1 and in most other
forms, you will be looking at RBF kernels, but then for special forms of data like graphs
and strings and so on and so forth, people have defined their own kernels and as long
as they are satisfying your properties of the kernel function, you can define your own
kernels and then have them help you solve the problem, right. That is topics for another
day or perhaps for another course. So, we will stop here. Just let me reframe that SVM
are one of the most popular and powerful classifier that are currently being used widely.
Thank you.
 

Artificial Neural Networks
Hello and welcome to this module on Artificial Neural Networks.
So, artificial neural networks, are computing models inspired by biology. So, we have neural
network architectures that have been proposed for a variety of different analytics tasks
like regression, classification, clustering, feature extraction, etc. So, these architectures
essentially are networks of simple computing entities and so this is like a very simple
threshold entities that are connected together in the specific network architecture that
give rise to complex computing functionality. Now, oscillate there has been significant
resurgence in interest in artificial neural networks, especially under the domain of T
networks about which we will see in one of the later modules. So, for this module and
the discussion about artificial neural networks is concerned in this course, we will look
at only the classification task and many of the ideas we talk about here for classification
are generalizable to regression, like for while for the other kinds of analytics task
we need different architectures and, but we are not going to cover that in this course.
So, the inspiration comes from biological neuron. So, let us not worry about the complete
complex structure of a neuron, what we really have to focus here is on the input and the
output. So, the neuron receive inputs from the dendrites or from the dendrite branches
from other neurons and when the input signals is above a certain threshold, it is going
to produce an output, that is going to be transmitted via the synopses to neurons that
are further down the line.
So, these connections to the dendrites and synopses are going to be result in a very
complex network and even though, the computing done by each element is very, very simple
summation and thresholding. The sum total of this taken across the entire network can
give rise to daily complex computations, which we will see.
So, the completing unit is something that is very simple. So, it is going to take a
set of inputs x 1 to x n and it is going to compute some functional arm, it is function
is very simply incredible and then it will produce an output. So, we will look at what
this function is going to be in detail in the next few slides.
So, the initial model for this for a biological neuron was proposed by McCulloch-Pitts in
1943 it is called the McCulloch-Pitts unit, it is only binary signals, so 0s and 1s. So,
either an input is active, then these cases are represented by 1 or if it is not active,
in this case it is represented by 0 and the nodes also produced only binary results. So,
the outputs could be either 0s or 1. So, the edges between these different nodes were directed,
unweighted, they could be of two types that could be excitatory or inhibitory and again
I can mentioned earlier, the transfer binary signals.
So, what is the computation that happens here? So, I let assume that the McCulloch-Pitts
unit gets inputs x 1 to x n through n excitatory edges. So, these are positive edges and inputs
y 1 to y m through inhibitory edges. So; that means, these are edges that could produce
the depression in the function or could actually stop the functioning of the neuron. So, the
assumption that was made is, if m is greater than or equal to 1 that is at least one inhibitory
edge and if any one of the inhibitory edges is 1, so if there is a one inhibitory input
then the unit as a whole does not produce any output, regardless of what the inputs
x 1 to x n are. If none of the inhibitory inputs are 1 or
if there are no inhibitory inputs at all, the unit computes the summation of x 1 to
x n, let us call it x and if x is greater than the threshold that is specified for each
unit, if it is greater than the threshold theta then the result of the computation is
1 as the result is 0. So, it is very simple, so essentially you can think of it as adding
up all the inputs that come to the neuron and if the summation is greater than our threshold
theta, your output 1; otherwise, your output 0. So, the inhibitory edges in some sense
here acting act as a gating signal. So, if it is 1, the output is always 0, if the inhibitory
is only 0 then the output is the result of the computation.
So, it is essentially the McCulloch-Pitts unit, it is implementing just threshold function.
If the input is below theta you are going to see a output of 0, if the input is above
theta you are going to see a output of 1, that this is essentially a step function.
So, what kind of computations can you do with this? So, you can actually do almost all your
familiar Boolean operations with the McCulloch-Pitts neuron. So, you can think of doing an AND
operation, you have two inputs x 1 and x 2 and the threshold is set a 2. So, if only
both x 1 and x 2 are one, so it will be greater than or equal to the threshold and therefore,
the output will be 1 and for implement in a OR you can set the threshold that one. So,
if either x 1 or x 2 is 1 to the output will be 1 after complimenting a NOT unit it can
implement the NOT unit by having x 1 act as an inhibitory input. So, this circle here
indicates an inhibitory input. So, if x 1 is 1; that means, they neuron and
inhibitory output will be 0 on the other hand x 1 is 0 then the output will be whatever
according to the result of the computation. But, we can see here that the threshold for
this neuron set as 0 and that is for the output will be always 1 as long as there is no inhibitory
input. So, if x 1 is 1 then the output will be 0, x 1 is 0 output will be 1, that how
we have implemented NOT function. Now, once we unable to implement this kinds of AND,
OR and NOT then you know that you can connect neurons together and then implement any Boolean
function that we want and is this what really we are interested in.
So, we are not really interested in that because we want to be able to do more complex classification
problems, then we would like learn simple things like linear surfaces or more complex
surfaces that is separate two classes. So, that is has been the goal of classification
we have looked at so far. So, in 1957 rosenblatt proposed a very simple extension to the McCulloch-Pitts
model which we called the perceptron, the more crucial thing what the perceptron is
that a it introduced weights at the inputs, crucial differences from the perceptron from
the McCulloch-Pitts module is that the perceptron introduced weights at the input.
And then it the output could be either a one or a minus one depending on whether the weighted
sum of the inputs is greater than threshold that one that is the computing unit with a
threshold theta So just repeating it. So, the output of the neuron is 1 if the weighted
sum of the inputs is greater than or equal to theta is equal to minus 1 other wise.
So, what is the goal here in perceptron learning, when perceptron learning we are essentially
trying to learn a hyper plane, trying to learn a separating surface as we have done in the
past in the other classification problems, we are trying to learn the separating surface
that can separate one class from the other. So, what would the classes be in our case,
classes in our case would be plus 1 and minus 1. So, this essentially means if w i x i is
greater than equal to theta, it essentially defines the equation of a hyper plane as we
have seen in the previous modules. So, if this you can take the theta to the
other side. So, we like w i x i minus theta is greater than or equal to 0. So, we have
seen that was greater than 0 to some one side of the hyper plane if it is lesser than 0
it is on other side of the hyper plane and we are going to say that data points to one
side of the hyper plane belong to class 1 data points other side of the hyper plane
belongs to class minus 1. So, now, the question is given a set of training data that gives
you the x x the vector x and the decided output y.
How would we find these weights w i's such that the perceptron is actually implementing
that hyper plane, implementing the right separating hyper plane. So, the weighted all this is
follows, you start of the randomly initializing the weights to some value and then we look
at the prediction that is made by the way. So, the prediction that is made by the current
setting of the weights, let us call it o and the target is the two class of the data point
x. So, with this, it will be plus 1 or minus 1 and likewise o is also plus 1 or minus 1.
So, your goal is to make sure that here perceptron output matches the target value.
So, the perceptron training algorithm has a very simple rule. So, at every presentation
of an input point, we change the weights by an amount that is proportional to difference
between the target value and the actual output produce times that the input on the particular
it. So, w i changes by an amount that is proportional to t minus o times x i. So, eta here is a
small constant may be 0.1 or 0.01 as called the learning rate.
So, one thing to note here if for a particular input x I will produce the correct output.
So, the class is minus 1 and I produce minus 1, the class is plus 1 and I produce plus
1. This expression evaluates to 0. You can see that this expression evaluates to 0 and
therefore no changes in the weights will happen. So, essentially what happens here is you change
the weights only whenever you make a mistake and that to you change the weight proportional
to the input variable. So, if x i is say a small value say 0.1 or 0.2 then will be changes
in the weight will be small and as for as the poster when x i is the large value let
us say 1 or 0.95 and things like that then the change it be next will be large.
So, this essentially because the larger the input variable the more important it is going
to be in the production of the output at least the way we are set up this perceptron. So,
that is essentially the simple training rule. So, whenever you make a mistake, you take
the vector for which we have made a mistake add some small fraction of that vector to
the weights.
So, this looks like a very simple rule, but then back in 50's this perceptronÕs created
a lot of human cried the people saw that the perceptronÕs by the able to learn from scratch
trying to solve something which are considered hard learning problems and then they used
the perceptronÕs they were able to solve that, so much so you can see here the hype
was that they are going to build the computer that expects to be able to walk, talk, see,
write, weight reduce itself and be conscious of it is existence, such the significant amount
of hype and it is always hard to live up to any height that this proportionate and to
the actual effect that was achieve that point.
So, let us take a look let us just back and take look at what can of perceptron learning
is a news paper article really true or what are the limits to the perceptronÕs learning
ability. So, here is a very simple perceptron here, so it has a two input variable x 1 and
x 2 and that is the threshold of 1 and the weight w 1 is 0.9 and w 2's 2. So, if you
look at it essentially it implements this straight line here, so everything above the
straight line this light color regions belong to one class and the dark color regions belong
to another class. So, we know that these are data which are
linearly separable; you saw this in the case with SVM's. So, these are data that are separated
by a linear hyper plane or the linear separating surface. So, all data points for which the
w transpose x evaluate, so greater than x is 1 will get a class of plus 1 all those
that evaluate to lesser than 1 and get a class of minus 1.
So, again let us go back and look at the simple logic function that we saw earlier. So, it
can implement that OR. So, essentially OR requires you to have a hyper plane and this
passing here. So, everything to this side this become plus 1 everything to this side
becomes minus 1 and likewise you can implement and so you can draw a simple hyper plane.
So, everything to this side become plus 1 and everything this side becomes minus 1 or
0, I mean depending on how you wanted to predict the output.
And let us look at another one, look at simple problem just like OR and AND the XOR problem.
So, Minsky and Papert in 1969 in a famous monograph called the perceptrons showed that
well a simple problem like XOR. So, where the truth table is given here is the inputs
of the same output is 0, if the inputs are differently output of 1, the simple problem
like XOR is not linearly separable, you cannot draw a hyper plane that separates these two
classes. So, forget about walking, forget about talking
and doing all those wonderful things that was claimed to news paper article perceptronÕs
cannot even solve this as simple problem as XOR is essentially says that two things are
same, the output is 0, two things are different the output are 1 that we cannot recognize
the similarity between this simple inputs like 0's and 1's what kind it do to complex
computations. So, once Minsky and Papert showed this, it is a kind of you dampened the research
into neural networks for a long time until there was revival much later.
So, perceptronÕs can learn only linear decision boundaries that is the take away message here.
So, that is make that is whole idea of neural networks completely useless, because they
can learn only linear decision boundaries in case of SVM's we saw that we could get
it to do all linear boundaries by going into Kernel expansion this has something similar
that we can do here.
Let us look at how we can change the representations and try to do something more clever. So, if
you look at the original problem the XOR problem, so I have my inputs x 1 and I have my input
x 2 and now we can see that in this space the problem is not separable. But, let us
look to do a simple transformation on my data points, so instead of looking at x 1 I will
define my first variable as NOT x 1 and x 2 and similarly I will define my second variable
as x 1 and NOT x 2. So, if you think about it, so we can now plugging
different values of x 1 and x 2 here and see what the outputs will be and then you can
see that when x 1 is 0 and x 2 is 0, the output is going to be 0, when x 1 is 1 and x 2 is
0. So, the output here will be x 1 is 1 and x 2 is 0, the output here will again be 0
and x 1 is 1 and x 2 0 the out here will be 1. And we know that 0 1 the output has to
be 1, so that we get it here and likewise for the symmetric case this will be the output
and so you can see that this is again going to be 1 and when x 2 x 1 x 2 both are 1 again
the output will be 0 0 and therefore, this is the resulting point.
Now; obviously, this representation the data points are linearly separable. So, now, the
task becomes one of finding the right representation, such that the data becomes linearly separable
for the next level, next stage of computation. So, people realized this very quickly, so
even though a single perceptron cannot solve complex problems like XOR which are not linearly
separable, he could actually stack layers of neural neurons and then have the first
layer compute something that is simple. So, you can always compute NOT of x 1 we saw
that earlier and also can be computed by a single neuron. So, you can this get have layers
of neuron that exactly compute your features and of NOT x 1 comma x 2 and then have another
neuron, which takes the output of this neurons combine same together and produces the output
that you want. So, people very quickly realize that stacking this kinds of neurons into layers
allows you to do more complex computation. In fact, it is easy to show that stacking
these neurons into layers actually builds a universal function representation that learns
to a represent any Boolean function, you see a combination of neurons. So, what is a problem,
now we know how to solve this more complex problems, why did the research in neural networks
pick up again.
So, the question here is when I start connecting all of these neurons into layers. How do I
find the weights? So, perceptron learning algorithm might no longer work in this case
actually does not work in this case and people were struggling to come up with the mechanism
for training all these weights. So, you can see that the way of started putting these
things into layer. So, that is one input layer and one output layer, so there is one input
layer, there is one output layer and in between this you could have many layers of neurons,
they are typically called hidden layers because you do not observe their outputs directly.
So, now, we have this many, many hidden layers of weights and it is little hard to find out
what this weight should be and so in the mid 80's around 83 an algorithm was proposed called
back propagation which allow you to learn the weights of this and we solve this hidden
layers.
So, for the rest of the presentation, we will be looking at the standard three layer network.
So, there is an input layer x 1 to x d and the output layer which will denote by f of
x and one hidden layer of neurons. So, these take the inputs from the input layer do the
weighted sum do your thresh holding function and then produce an output and then the neuron
and output layer will take all this outputs of the hidden layers take their weighted sum
and take the threshold or not and that produce the output, instead of using hard threshold
we use a kind of a soft threshold like in order to do this competitions this is needed,
so that you can derive more efficient training algorithms later.
So, the output of a hidden units and it given by g of the bias term, this is the theta that
we had earlier. So, instead of theta so it is going to call it b 1 and plus w 1 times
x and the output of this will be note by h of x and the output of the final layer of
neurons is given by some function o of b 2 plus w 2 times h of x. And so now, the goal
here is to figure out what this w 1 and w 2 are going to be. So, this is called the
three layer network, even though there are only two sets of weights that we have to learn.
So, the layers here talk about the neurons here, so we use for each input variable we
are same that there is separate neuron that is activating the hidden units. So, this is
called the standard three layer network structure.
So, what are the different activation functions you can use? So, we already looked at one
which is the threshold function, we can also have just a linear activation function that
basically takes the summation of weighted summation of all the inputs and outputs as
it is. We can also look at the sigmoidal function, sigmoid logistic function which takes the
summation input and then squashes the input. So, that it remains between 0 and 1 and then
there is a steep raised somewhere around the threshold. So, that it transitions rapidly
from 0 to 1. When if you are interested in having signed
outputs then you can think of using a hyperbolic tangent, where the outputs are going to taxation
between minus 1 and plus 1 and again around the threshold. So, there are parameters at
control where the threshold would be and how steep the price would be. So, another transition
function some time gives is this squashing function, which is 0 before the threshold
and one at a certain distance higher than the threshold and in between you have a...
So, linear approximation adds to the step function, this called the squashing function.
So, typically in most of the neural network architectures that we look at will be looking
at either the hyperbolic tangent or this, the logistic sigmoid or the linear activation,
because these are different shape and this allows as to derive efficient training algorithms
for the same. So, if you are doing a classification problem then the output layer could be the
hyperbolic or a logistic sigmoid and if your solving a regression problem, the output neuron
could be a in linear neuron. So, that you can do appropriate regression fit.
The hidden layer almost always has to be a non-linear function and where the little bit
of what you can show that if the hidden units have a linear activation, like you might as
for not have them at all. And what is the function that is implemented is something
which can be as well implemented by a single layer of neurons. And the next module we look
at how you the exactly find out these weights given the assumption that they are working
with the sigmoidal logistic function. So, the function for the sigmoidal logistic
thing is given by f of net is 1 by 1 minus e to the power of minus net. So, that is the
function and look at, given that this is the activation function how we are going to derive
the weights of the two layer standard three layer neural network. So, that is in the next
class.
Artificial Neural Networks(cont\'d)
Hello and welcome to the module on back propagation or how you are going to train artificial neural
networks and determine the weights.
So, to keep things simple, let us start off with a single neuron and that is going to
have a logistic sigmoid as the output function. So, you are going to write here f hat of x
is equal to b plus w transpose x and then, you pass that through your sigmoid function
o. So, o in this case would be the logistic sigmoid, where we will say o of v is 1 by
1 plus e power minus v. So, we saw this in the last module, so the error measure that
we will be using is the squared error. So, the excepted error of the parameters w
going to be half times, the excepted error w is going to be summation i equal 1 to n,
where n is the number of training data points that you have of the squared error for each
training data point. So, the way we are going to use this for changing the weights is essentially
to compute the gradient of the error. So, essentially that will be the error times the
derivative of the output function times minus x i.
It is essentially taking the derivative of this with respect to the w function. Once,
you have computed the gradient of the error with respect to the weights, then we essentially
just change the weights in the direction opposite to the eta times the gradient of this weights,
where eta is the essentially the step says parameter. So, this was fine when you have
a single neuron. So, what about the case when you have layered networks like this?
So, this is our standard three layer neural network. So, the first layer is essentially
just the inputs. So, the second layer of neurons takes in the inputs and computes the output,
these are called the hidden neuron that is we discussed in the last module and then the
output layers finally, take the output from the hidden units, again do the appropriate
transformation and give you the final outputs. So, here we will denote the hidden layer outputs
by h and h is given us g of the weighted summation of the inputs and let me introduce the temporary
variable here called t, which essentially is the summation of the outputs of the hidden
units and f is finally, the transformation o of the outputs of the hidden units.
So, one thing to note here is that, so I have used different functions g and o for the hidden
layer and the output layer and typically for two class classification problems both g and
o are logistic sigmoid as we saw earlier. So, g as well as o would be of the same form
1 by 1 plus e power v, but then if you are having a regression problem you can essentially
use the same setup that we have here, except that o would be linear for the regression
problems, so in which case o would be essentially just passing on the inputs that it is getting.
So, suppose I have this multiple layers, how do I go about finding the weights of this
standard three layer output? So, in some sense finding training rule for w 2 is not very
hard. So, w 2 if you think about it, it is just like a single neuron network. So, I can
just take all the weights that come to f 1 and then, essentially use the same rule that
I used earlier here for a single neuron. I can use the same rule for training f 1, except
that instead of x i I will be using the output h, so that is clear. So, for finding w 2 I
really can just stick with what I did earlier.
So, I am going to write that here again just for clarity sake. So, I have rewritten this
in an appropriate fraction for working with this multi layer networks. So, I am going
to look at the gradient of the error with respect to a single weight that runs from
the m'th neuron here to the k'th output neuron, this weight runs from the m'th hidden neuron
to the k'th output neuron. So, this is essentially the k'th output minus the prediction given
by the neural network. So, that is essentially our error times the derivative of the output
function of the k'th neuron with respect to the input that it receives times the input
that is coming on the m'th line, which will be essentially h m i.
So, if you think about it that is exactly the update that we had here except that I
have change the notation to apply to the two net, the second layer weights in the three
layer network. So, now, the interesting partÉ So, this is fine, so this we can just get
from the single neuron update. So, what we do about the first layer weights? So, let
us see how you will do this, this essentially uses the very simple idea of chain rule from
differentiation, I am going to take a very specific weight here. So, I would like to
find the derivative of the error with respect to the first layer weight that runs from the
l'th input neuron to the m'th hidden neuron. So, it runs from some l'th neuron to the mth
neuron, so I am just looking at this one weight here, we are trying to find out what is the
gradient of the error at the output with respect to that one weight. So, I can rewrite this
as follows, so the derivative of the error with respect to the first layer weight is
essentially the derivative of the error with respect to the output of the m'th neuron times
the derivative of the m'th neuron with respect to the weight.
So, this makes sense, because the weight to the m'th neuron affects the output only via
the output of the m'th neuron, so it affects the overall output of the network only via
the output of the m'th neuron. So, I can essentially apply the chain rule here. So, let us take
this bit by bit. So, if you look at this, so you can see that this is immediately obvious,
because this is the function we are talking about here. So, if you take the derivative
of this with respect to any specific w here, so we will essentially getÉ
So, if we take the derivative of h m i with respect to w m l, then you essentially going
to get the derivative of g times the derivative of this expression with respect w m l which
will just be x l i. So, this is the second term in the derivative. So, the first term
in this derivative is the one that requires a little bit more work, so let us see how
we will do that. So, I am going to try and evaluate that expression here, so if you think
about it, so the different ways in which the output of the m'th neuron can influence the
overall error is essentially through each one of the output neuron that m'th hidden
neuron connects to. So, it can influence the error through this
output or it can influence error through this output. So, we are essentially summing over
all possible outputs k of the gradient of the error with respect to f k and the gradient
of f k with respect to the output of the hidden neuron. So, you can easily evaluate the derivative,
so the derivative of the error with respect to f k. So, if think about it, so this is
the expression we have. So, the derivative of the error with respect
to f k, so going from this expression derivative of the error with respect f k going to be...
So, negative of y i minus f hat k x i, because we are taking derivative with respect to f
k here. So, we do not have to worry about the further terms that constitute f k. The
second term is concerned that is essentially looking at this. So, derivative of f k with
respect to h m is essentially the derivative of o with respect to h times the derivative
of the argument of o with respect to h which gives us just the weight w k m complex expression
for this. So, before I put these things together to
make things a little simple let me introduce the small notational thing. So, that it makes
lie for little easier for us, so I am going to say delta and I am going to say delta k
for the input i this essentially the error term times the derivative of the last layers
output function and also define this term row m for the input i as the essentially the
derivative of g with respect to w times the summation of w k m into delta k of i.
So, putting these together now we can write our expressions more compactly essentially
we can say that dou. So, the error of the derivative of the error with respect to the
output layer weights, with respect the w to weights is essentially delta k i times h m
i, so likewise the derivative of the error with respect to the first layer weights, the
weights from the input layer to the hidden layer is given by row m i times x l i.
So, if you think about it, so this part corresponds to this and that comes from here and x l i
is what is left out here. So, this expression look pretty compact and now we essentially
have... So, you essentially update the parameters by w k m is just change by accept the gradient
here and w m l is again change by the gradient that we are computed here. So, one thing I
want to point out here that if you are function, the function g or your function o happens
to be a sigmoid, then g dash is
equal to essentially g of a into 1 minus g of e.
And suppose you are using a tanh function, because you want here outputs to run from
minus 1 to plus 1, suppose then g dash it is 1 minus g v squared essentially
this gives you the... So, you can see that the sigmoid functions have a very convenient
form for the derivatives. So, one thing that we have to be careful about here is the fact
that these are gradient following methods and therefore, and there is a good chance
that will get stuck in local optima and there are many techniques for getting out of these
local optima, but we are not discuss too many of them there one of the simplest one is of
course, to try this with multiple starting stage, starting points for your weights and
taking the other set of weights that gives you the best possible result at the end of
some kind of experimentation. So, that brings us to the end of this module
on training your neural network using back propagation. So, but this form of training
has it is own draw backs will see what is that in the next module.
English - NPTEL Official
Deep Learning
So, in the previous module we saw how we can use back propagation in order to find the
weights of a neural network; upper three layered neural network. So, one of the earning success
stories of back propagation was learning to recognize hand written digits in an address;
right. So, this is work done by Yann LeCun back in 89, and essentially they trained a
slightly more complex network architecture call the convolutional neural network; in
order to recognize hand written digits. You can see the variation in the digits, that
their network manage to handle; the fairly well.
So, another early success story of neural networks was the Backgammon Player built by
Gammon Tesauro from IBM, T. J. Watson labs in 1992. So, he built 2 versions of this backgammon
player one called the Neurogammon which came in 89, which was essentially neural network
trend using back propagation in supervise learning manner; in order to play a game of
backgammon. So, it is like Ludo of people know about it. So, you throw a dice, you throw
a dice and depending on that die roll; you move your coins around. So, that is a white
and black side. So, the idea is to move all your coins of the board by repeatedly rolling
the dice. So, the TD-Gammon player essentially used the 3 layer, standard 3 layer neural
architecture and was trying using a specific form of what is called reinforcement learning.
We look at reinforcement learning in the later module, but he used reinforcement learning
in order to generate the error signals and, but then use back propagation to train the
weights of the hidden unit. And you manage to build Backgammon player which was able
to beat human Backgammon champions in game play.
So, this was some of the early success and lot of interests was being spent on looking
at neural networks of solving variety of different problems. But then again people discovered
a 2nd drawback with neural networks. So, the one thing was that It was incredibly hard
to train a neural network, because you had so many parameters you had to treat them appropriately.
So, that they desired to deserved for obtained.
Further there was this problem called the vanishing gradient problem. So, people quickly
figure out that if we have many layers in the neural network; right. It was easier for
the network to represent more complex functions; even though 2 layer network or a 3 layer network
and depending on the how we contact. The standard 3 layer network was the universal approximator,
having more layers allowed more complex representation to be build. But see, number of layers in
the neural network increases, the gradient that we compute by doing back propagation
are becomes vanishingly small; right. And since there are not enough feedback for the
early layers in the network. The rates in the lower layer remains random wherever universalize
them ; right. And so, you are not able to learn any deep networks; right. So, the networks,
neural networks it will learn necessarily had to be shallow; because only a few 3 layers
could be, 3 or 4 layers could be trained meaningfully using back propagation. So, people had come
up with different tricks for training deeper networks.
So, and this discovery of these tricks for training deeper networks, is essentially what
is revive the interest in the field. And now we can build networks at have 8 layers, 10
layers and so on so forth. And this produce some fantastic performances in problems that
we are talk to be very hard to solve for AI and machine learning. And so, there has been
revived interested looking at neural networks. So, I will talk about one specific mechanism
for training multiple layers in the network, and then you can I mean if you are interested
in this follow it up with other material.
So, this is proposed in mid 2000 by Hinton 2006 and independently by Ashwa, Bengio and
others. In 2007 it is more like a greedy unsupervised layer wise pre training, so what we mean by
this? So, you start off with very simple network architecture called an auto encoder. So, you
have an input layer and you want to produce the same input at the output. So, the input
layer and the output layer need to be identical, but in between the connections will go through
a smaller hidden layer of neurons. So, the idea here is that the smaller layer is going
to learn some kind of a encoding or some kind of a reduce representation of the input; that
is sufficient for you to produce the output that you are looking for.
So, you could train this using back prop or other slightly more advanced gradient techniques.
So, if you think about it, there is no real supervision that is required here; because
as soon as have the input; right. The output is just the same input. So, I am going to
set it through a smaller hidden layer. So, that I am learning a reduce representation
of the input. So, once I have this 1st level of reduce representation;
right, I am going to iteratively deepen the network. So, what I do? So, once I have the
1st hidden layer of representation, the first hidden layer of representation I am going
to add another auto encoder network on top of it. So, this network takes the hidden layer
representation for the input and tries to produce the same hidden layer representation
at the output, but in the middle layer is going to have fewer neurons. So, in the effect
I am taking this larger input and reducing it to a smaller hidden representation here.
And I can repeat this; right. So, once I have trained this, I remove the outer layer; and
then take on another layer of auto encoders. So now, you can see that my training has gone
several layers deep. So, instead of just looking at one layer deep auto encoder which have
I was able to train using back prop efficiently. So, I am essentially using the same construction
again and again. So, at any point the training happens only on a one layer auto encoder,
but then because of this iterative deepening. Now I have actually taken this input representation
and progressively reduced it to a much smaller representation at the hidden layer. So, this
kind of an observation that you can use this layer wise pre training of the data, led to
resurgence in lot of deep architectures. So, why do we call this pre training? So, far
I am not talked about any kind of classification task that you have performing. we are just
trying to find features in the input space. So, once I have found these features in the
input space then I can take this, and I can tack on top of it.
On top of this features that I have learned I can tack on other neural network. And now
I have my full fledged deep auto encoder and then this hidden layer representation can
now be used as an input any learning task. So, so this part is call the fine tuning part
and the layer wise training part is called the pre training part; where I find the hidden
representation. And after that I can add it to any complex neural network, that can do
my classification task or my regression task whatever it is that I am looking at. So, this
kind of layer wise pre training allowed people to, drive more complex compressed representations.
That very useful in a variety of problem solving and I use.
So, this again revived a lot of hype in neural networks or in deep networks as they are called.
So you can see very human similarities to the news items that I showed you from the
1959 thing. So, a stimulated brain and you can teach context to computers, machines at
learn without humans, etcetera, etcetera. So, skate what you read in news paper with
a pinch of solve, but again there is significant renewed interest in deep networks and neural
networks as there was in the 50s.
But the overall feeling in the community is that, deep learning is come to stay; because
there are lot of nice properties about this generation of neural networks as compare to
the earlier generations. So, the things of more stable and things of reproducible; even
though significant computing power is needed. So, most of the successful applications of
deep learning that you see, would have had significant amounts of computational power.
But then the computational power is also cheap; and you are able to solve really complex problems
using neural networks.
And so, deep learning collects have yield at state of the art performance in the variety
of domains, like image classification, object detection, language modeling, machine translation,
speech recognition, image description. These are problem at traditionally considered very
hard for machine learning algorithms and deep learning seems to have significant in pattern
these areas.
So, some of the images I used here are taken from the neural networks book by Raul Rojas.
And so, I like to show you few demos of deep networks and action law. So, here is one algorithm
from company called clarify. So, given an image with different kinds of textual labels
it says that, here given this image is say that is coffee, at there is croissant, there
is a beverage in it, and this is probably breakfast and it is had in the morning and
overall hey this looks like food. So, it is able to derive all of this tags just by looking
at this image. These are all similar images it does manage to retrieve images, similar
to these. You can see that all of these are images of breakfast or continental breakfast
and all of these have a cup of coffee in there. And so, even though the variety of this is
able to cover is truly expounding. So, like wise look at these picture here,
is able to figure out the here suspension bridge here and there is a river here; even
though the river is not explicitly visible. And that is it night because it is lighted
up and it looks like bridge in the middle of the city. And likewise it has found similar
images, which are all suspense bridges in cities on top of rivers.
And. So, likewise it is able to work on variety of outdoor seems, as well as indoor seems
and it’s able to perform really well. And this is state of the art in terms of image
understanding and labeling at correctly.
And likewise you can look at how well it works in machine translation. So, the both the image
tagging and the machine translation tasks use some much more complex neural network
architecture that we have seen so far. But then so, here it’s a simple example from
here. So, I typed in the sentence what a wonderful idea in English and then it gives me the equivalent
in French. And not only does it do that, it tells me which words correspond to which word
in re translated language. Which words in the original English language, correspond
to which word of the translated language. So, I do not know French. So, I am not sure
that is a good translation, so let us not do this. So, let us stop with the previous…
So, it is then this thing actually you can learn translate, can do translations between
multiple language is not just in English and French. It first trained on the data from
United Nations and European parliament. So, it can do translation between the all the
European languages. So, that brings us to the end of this module on deep learning.
Thank you.
Associative Rule Mining
Hello and welcome to this module on Association Rule Mining or Frequent Pattern Mining, which
is essentially the basic problem underline association rule mining.
So, we had a very brief look at association rule mining with very beginning of the machine
learning section.
So, the idea behind association rule mining is to first mine frequent patterns that occur
in the data and based on the frequent patterns that you have mined, derive association rules
which is to form at if A happens, then B is likely to happen.
So, basically is like a conditional dependence relation that you are mining if A happens,
that makes B more likely to happen.
You could find such patterns in sequences looking at time series data, like financial
data or looking at fault analysis, where one thing causes fault to occur; or you can look
at in the transactional data column context which is where it was originally proposed
and that is what we will look at in more detail in the rest of the module.
And more interestingly you could also look at mining frequent patterns and associations
in graphs, which is appropriately used in social network analysis.
So, let us look at a mining transaction for the rest of the module.
So, transaction is a collection of items that were bought together.
That is the simple definition that we will use for the purposes of association rule mining.
And please note that the set or subset of items, is usually a denoted item set in the
association rule mining community.
So, the goal here is to find first find frequent item sets, then you would say that an item
set A implies item set B; for example, if you could say that somebody buys milk, then
they are likely to buy bread, if both A and the event A union B or frequent item sets.
That would mean that both A, sorry which is milk in this case and A union B which is bread
and milk both should be frequent.
In which case, I can say that, if you buy milk then, you buy bread as well.
Let us take a look at exists simple example here.
So, here is a set of transactions, so I have 5 transactions and each color here denotes
a different kind of item.
So, the first 3 transactions are 4 item sets, the 4th one is the 3 item set, and the 5th
one is a 2 item sets.
And let us assume that we have a frequency threshold of 3.
So, we essentially have the following as frequent 1 item sets.
So, we have blue, which occurs in all the 5 transactions and then purple which occurs
in 4 transactions.
And pink which occurs, big item which occurs in 3 transactions.
So, none of the other items occur in 3 or more transactions.
So, the frequent one item sets are just these.
So, the next thing you have to look at is the frequent 2 item sets, in the frequent
2 item sets you can see or essentially purple and blue which occur in 4 transactions and
pink and blue which occur in 3 transactions and so, none of the other combinations are
frequent.
So, the only are the things which we really have to look at this purple and pink; and
purple and pink occur only in 2 of the transactions together therefore, they are not frequent.
So, the goal here is to first find such frequent item sets.
And from these frequent item sets, how do we determine which are interesting association
rules.
So, the 2 measures of interestingness for association rules or essentially support.
So, the support of a rule is the percentage of item sets that contain A union B; right.
So, in this case and then the confidence of a rule is the other measure that we are interested
in.
So, we will go back and look at the data set once we have understood what support and confidences.
So, the confidence of a rule is a percentage of item sets containing A, that also contained
A union B. So, essentially this tells you how confident you are in making the association.
So, typically we look for rules with both high support and confidence.
If you think about it, if both there in the case of support and confidence we really need
to find the frequency of the item sets; right.
So, once we determine what are the frequent item sets and what their frequencies are,
then we can easily determine what are the relevant association rules.
So, more effort needs to be focused on counting rather than the association rule itself.
So, that is why I said there is frequent pattern mining part of this more interesting than
the association rule mining part.
So, let us go back and look at the pattern set we have mined so far.
So, if you remember the one item sets are in really interesting except to establish
the frequency part of it.
So, let us look at a rule which says that purple implies blue.
If you have bought purple then you likely to buy blue.
So, if purple occurs in your transaction then blue is likely to occur.
So, if you think about it, this is the valid rule to have because both purple as well as
purple and blue were frequent.
In the earlier slide we saw that both purple and purple and blue are frequency, so it satisfies
the A, A union B rule and, what about the support of this rule.
This support is essentially all the transactions the number of transactions in which both A
and B occurs.
So, that where A union B occurs divided by the total number of transactions.
So, A union B occurs in 4 fifth of this data set; and therefore, the support of this rule is 4 
5.
What about the confidence of the rule?
The confidence of the rule is, whenever A occurs; how many times A union B is occur.
And in this case it turns out that whenever A occurs, A union B occurs; therefore, its
confidence is 1.
So, likewise for the pink implies blue rule, you can see that the support is 3 by 5; because
that is how many times a pink union blue occurs.
And the confidence is 1; because whenever pink occurs, pink union blue also occurs.
So, what about this rule?
I mean is this the valid rule to look at?
Both the blue is frequent and blue and purple is also frequent.
So, that is a valid rule to look at, but is that the better rule then the one that we
saw earlier.
That is not necessarily in the case because the support here is 4 by 5 and the confidence
is 4 by 5 as well.
So, the earlier rule which is purple implies blue had a higher confidence than this rule.
But both of these are possible rules that you could consider, because both the rules
have a high support and a high confidence.
So, this is essentially the idea behind the association rule mining.
So, it has been applied to variety of applications, Market Basket analysis is one.
So, as I had mentioned earlier.
So, market basket refers to the fact that when you go to super market, you are going
to buy a set of items together and put them together in your basket.
So, essentially looking at what goes into the basket at the market.
So, these baskets are considered as transactions, and you look at frequent pattern mining in
these transactions.
You could look at co-occurrence of words and that can be used to derive certain kinds of
topic relationships.
And people are looked at plagiarism detection in terms of frequent pattern mining.
Now people have applied this to a biological data looking at bio markers and genes of proteins
versus diseases.
So, try to find out associations between co-occurrence of a certain protein, abnormalities, and diseases.
You also looked at in the contrast of time series, where co-occurrence of events can
be used to model trigger events and this identifies trigger events.
So, that brings us to the end of the first module on frequent pattern mining and the
subsequent module will look at techniques for efficiently mining frequent patterns.
Association Rule Mining (contd)
Hello and welcome to the 2nd module of Association Rule Mining. In this module we look at a very
popular algorithm that is used for association rule mining. As I said earlier the main problem
in mining association rules is counting part. So, typically how you would do, it is that
you generate candidate frequent item sets then, count how many times they occur. And
then you prune the candidates based on the count. So, if their count is above certain
frequency then, you are going to call them as frequent item sets and if they are below
a certain frequency you are going to reject that.
But then, the problem with doing such an approach is that you have a combinatorial number of
candidates. So, think about it. So, we have like 10 different items from which your transaction
can be drawn; and that is say you are considering 2 item sets. So, that essentially gives you
10 choose 2 candidates item sets; now this is a small number. Think about real applications
where these candidate sets can be in millions. So, somebody like Amazon or a Flipkart is
great are like millions of candidates. And how would you even generate candidate item
sets some more than is very small size of candidates. So, really need a clever way of
generating fewer candidates. So, the very first approach for looking at generating fewer
candidates came about late 90s which is the apriori property.
So, the Apriori property is very clever observations and just says all nonempty subsets of a frequent
item set must also be frequent. And even if one of the subset is not frequent, then I
do not have to consider the larger item set. You do not even have to count the larger item
sets. So, the first table that propose this Apriori
property and came up with the fast algorithms for mining association rules was purposed
in 1994 and it is being a very similar algorithm. First they introduced the problem of finding
frequent item sets and data base of transaction. It is said the tone for the entire field.
In fact, this use of the term mining association rules, almost was the reason for the whole
sub field of data mining. And it since then there have been numerous improvements it have
been propose on top of Apriori algorithm, that allowed to main really large data sets
scale up to will be Billions and so forth. But still Apriori algorithm swap we have to
start; alright explanation into association rules. So, in this module I will talk about
the Apriori algorithm and give you more like introduction by an example and so, I will
leave it you to look up other algorithms if you are interested in association rules.
So, here we will start up with a very simple example. So, look at very small transnational
data base. So, it has got only 4 transaction learning, so each of these have different
id. And there are total of 5 different items which could be figuring in these transactions.
So, we are going to call the A through E. So, let us look at the set of 1 item sets.
So, we have A through E and this complete frequency of these item sets. Let us suppose
that I have a minimum threshold for the support of 2. So, I have 4 transactions and item set
can be called frequently appears in a least 2 of these 4 transactions. So, we can immediately
see that item set D is not frequent. So, not only is the 1 item set D is not frequent,
any larger item set that contains D as element in it can also not to be frequent.
So, when I am looking at the candidates, I have to use for generating 2 item sets I can
completely ignore D. And now, all the candidate 2 item sets or those that do not have E as,
I mean do not have D as part of it. So, essentially the 2 item sets that will have to consider
are: AB, AC, AE, AC and B E and C E. So, these are essentially generated by looking at all
possible combinations of the frequent 1 item sets.
Now, that we have these candidates 2 item sets. So, we are now on the second scan through
the data then, we count them frequency of the 2 item sets. Now, we can see that A, B
and A, E on actually not frequent among these and therefore, we can thrown those away. So,
we have a list of frequent 2 item sets, which compares AC, BC, BE and CE. Is it clear so
far? So, we had we started out of with all the 1 item sets, these are the candidates
1 item sets. Counter that frequency we left out the 1 item sets, that was not frequent.
And then from the frequent 1 item sets, we generated a set of candidate 2 item sets which
could be frequent. Then, we counted the frequency of these 2 item sets and from there we have
proven the way the 2 that were not frequent. So, these are the frequent 2 item sets. From
these we can generate candidate 3 item sets. So, if you think about it. So, the candidate
3 item sets could be ABC, ABE, and BCE. So, can ABC be a candidate. It cannot be a
candidate because the sub set AB is no longer, is not frequent. That sense the sub set AB
not frequent, ABC cannot be a candidate item set; and likewise can we look at BCE. Can
BCE be a candidate frequent item set? Yes, because, BC is frequent, BE frequent, CE is
frequent. And what about ABE? Again that cannot be a candidate item set because AA, AE is
not a frequent item set. Essentially we have only 1 candidate 3 item set. And when we count
the frequency of the 3 item set and then we find that exactly frequent. And now we have
the complete set of all frequent item sets. So, one 3 items sets which is BCE, four 2
item sets and then four 1 item sets which are all frequent. You do not have look for
larger item sets, they both potentially possible because we do have 5 elements, 5 week events.
But, we do not have to look for any further frequent item set because that is only one
frequent 3 item set. So, the Apriori property allows us to proven
the number of candidate item sets that we will have to generate. So, but still there
is a problem in that. So, every time we generate candidate sets of larger size, we essentially
do another scan over the data. So, the more recent algorithm tries to minimize the number
of scans, you have to perform over the entire data sets; because that is very expensive
operation.
This is a big challenge but, there are one other Caveat which I want to point out. So,
scaling up to large data set is a big challenge, but there is a one other caveat which I want
to point out. So, high confidence is not always a good idea. So, we will have to be really
careful about what we mean by high confidence. So, I can say that somebody buys games, implies
they buy videos with confidence of 66 percent; the support of 37 percent. So, this is something
that was actually observed in a real data set from video rental company, which also
going selling video games with their shops. Because this, so from the real data they are
found that somebody buys games, hence they will also buy videos with the confidence of
66 percent and support of 37 percent. So, essentially it means 37 percent of the people
that came to their showroom, shop bought games and videos and 66 percent of the people to
bought games also bought videos. But then, if you just look at what fraction
of their customers bought videos there is 75 percent of the customers actually bought
videos. And so, even though this rule has a high confidence, you can see that if you
buy a game, actually implies negatively on buying videos. This is the video store after
all and so if we typically come into buy videos but, the occasional person who comes into
buy a game, is not that interested in buying videos. So, it actually imply a negative correlation
and if you had just blindly been using support and confidence to determine rules, then you
actually trip this out as a important rule in terms of having a high confidence.
So, one measure which people gives instead of confidence and support alone, is known
as Lift. Lift is essentially the ratio of the confidence of the rule to that of a default
rule. So, if you have a lift of 1 that means, that is really does not imply anything. So,
whether A happened or not, B is always going to happen with same frequency. So, if I A
implies B is the lift 1 and it is not a significant role, but if A implies B is the lift much
larger than one that would be in that. So, the things tend to truly indicator of B. But,
again if you think about in this case of games and videos, where lift will be lesser than
1 and which case is indicates a strong negative corporation.
So, lift is a useful measure to have, and that lift is not only measure that people
have to proposed the variety of different measures which people have proposed analyze
association rules and association rules mining is a very very active area of research. And
so this is essentially just an introductory module and if you are interested in association
rule mining you spend more time. And looking at the various modifications and additions
that people have come up with of viewers.
So, just to summarize we the major challenges in association rule mining is how do you extend
to these millions of transactions. So, there could be billions and billions of potential
item sets. So, how do you do the pruning efficiently? How do you minimize the number of passes?
How do you reduce the number of memory that is required when you are doing the counting?
So, there are many many rules and many issues that we have to consider in trying these large
data sets. So, we have talked about transnational data
now, so some sets easy to count and it was discrete event that are happening; but, what
about the continuous data like time series data and images? How would you go about even
identifying what are appropriate items more which will be doing this counting. And data
with rich structure like graphs. So, frequent pattern mining in graphs is a very important
topic which is you used in diagnose area like graph discovery and social influence prediction
and so on. So, how would you can you average a structure or can you make computation or
efficient and it handle in see structure data. So, that is something which is a very active
area of research and experimentation. And one important twist to this whole frequent
pattern mining issue is that, we are not sometimes not interested to in frequency within significance.
So, if A occurs let us say in 0.3 percent of all transactions; but when B occurs it
occurs in 1.2 percent of the transactions. So, it means B implies A and that is a significance
effect, because it improves the frequency of A from 0.3 to 1.2. And A might be that
in the A stack that we are looking for and finding B. And actually gives us greater evidence
for the presence of A. So, such patterns are significant, but if you just go by the frequency
a loan, these are very infrequent occurrences of a data. So, how do we actually look at
such frequent, none frequent but significant occurances. So, that is a big challenge in
the association rule mining community, again like it is a very activity process and people
keep developing, that brings us to the end of these.
Big Data, A small introduction
Hello and welcome to this module on big data analytics.
So, what I would do in the next two modules, is trying to introduce you to the problems
of big data analytics.
We will start of looking at what is big data.
Of course, I cannot hope to cover all the aspects of big data analytics in a couple
of modules in this course.
The whole idea is to give you a very brief or as I say a small introduction to big data
analytics.
So, when people started talking about data, organizing data, and managing data they initially
started with data base management systems.
You might have heard of rational data bases and other forms of data management systems.
So, the goal here was ease of access, and to be able to answer simple aggregation select
queries; essentially trying to do some form of analytics of the data.
But while looking at very specific, very static, amount of the data.
So, the volumes of data were not as large as we talk about now a days.
And then from data base management systems, when the data became much larger that you
could not make sense out of the raw data itself; and people started talking about data mining
or data analytic systems.
Where the goal was to detect?
Patterns in the data and people used lot of ideas from machine learning and applied statistics
in order to do this kind of data mining, try to understand the data better.
So, what was happen now a days?
So, this whole system is evolved to a point where you cannot just hope to run your machine
learning algorithms directly on the raw data.
But you have to worry about the data management aspects of it; as well as the analytics aspects
of it.
So, that is essentially the crux of what people call big data in now a days.
The fact that the data is so large, that you cannot look at analytics divide of data management.
So, give you a feel of what this big data, it is court from rich man oppose the executive
chairman at Google.
So, from the dawn of civilization until 2003, human kind generated five exabytes of data.
Now we produce five exabytes of data every 2 days, and the pace is accelerating.
So, if you can just think about it, what we have produce for 1000s of years since till
2003, we are able to produce that kind of data every 2 days.
That is a volume of data that humans are producing and obviously, we cannot make sense out of
this data until some kind of organization and analytics goes handle with us.
So, what are the main challenges that come about, when we are talking about data of this
scale.
So, this is the some out of other the coming at shape that view of big data analytic, but
one that is popular.
So, I thought I should present it as part of this model.
So, the challenges are essentially in capitulated under the 4V’s of big data: that is volume,
velocity, variety, veracity.
And I would like to add a 5th V to it; is it called value, which is essentially goal
of all of data analytics and subsets.
So, I will briefly introduce you to each of these ways and another may be the end of the
first module.
So, why you said that we are able to generate such huge volumes of data.
So, one of the reasons is that all aspects of modern life has been digitized, all aspects
of modern life has been digitized.
And nowhere is it more a parent than in your social activity.
So, we have Facebook; lot of peoples friend significant portion of their breaking hours
on Facebook and possibly significant portion of the sleeping hours reviewing about Facebook.
And then we have such a volumes of data that is probably Youtube, and WhatsApp and twitter
and flickr and all the web sharing totals at.
So, if you think about it at almost every aspect of your life is now being recorded
somewhere or the other.
And the next thing is just not the social activities that you perform social media,
any kind of interactions you are having.
This with very high probability is being recorded; for example, phone conversations.
We call costumer support calls they are being recorded any feedback and surveys; that will
have full fill out of the form anywhere as being that preserve for posterity, any kind
of online conversation, chats or sub transcript and other things they are also recorded.
And so, it seems like everything that are working at a, it is actually being recorded
at some point of other.
Another aspect is the increasing availability of cheap storage and cheaper bandwidth that
allows us to share multimedia content at rates and volumes that is never imagine possible
even 5 years back.
So, here are few examples that I took from the internet.
So, Instagram is growing at more than one billion photographs.
Instagram has more than one billion photographs.
Youtube has 100 hours of new videos uploaded to the site every minute.
Over one billion unique users vsit Youtube each month.
At Facebook adds 30 billion pieces of content every day and likewise you could take any
of these online portals and you can come up these mind boggling numbers, that tell you
how much data is being shared on these portals.
So, apart from these the aspects of everyday life and regenerating this data, there is
being significant advance in technology in other area; for example, in biological data.
So, 3 gigabases of a human genome can now be sequenced in a few days.
This is like 3 times 10 power 9 base pairs which is significant volume of data, but then
this is for one human genome.
And you can essentially get your genome sequence for few thousand dollars in the US and so
it should be made accessible at much lower price range.
And so the data has being generated at every enormous unbelievable mind boggling pace.
Protein data banks have 10s of thousands of structures amounting to several terabytes
of data and again people are continuing add to this experimentation.
So, to put this in prospective; Donald Knuth one of the fathers of computing algorithms
and so on so forth.
Has this to say about computational biology.
I think the most exiting computer research, now is partly in robotics and partly in applications
to bio chemistry.
Biology is so digital, incredibly complicated, but incredibly useful.
Biology easily has 500 years of exiting problems to work on, it is at that level.
And not only is it in the data analytics appear which is most relevant to us, but the variety
of other area has to well including data storage and also in computing hardware and access
technologies; biology is challenging computer science at levels never seen before.
So, this is about volume but, what about that rate at which this data is being accumulated.
So, data is generated at tremendous volumes, but tremendous rates.
Instagram in may 2012, 58 photographs were being uploaded and a new user was being gained
each second.
In Facebook adds half a petabyte of data every 24 hours, and then let’s not even talk about
sensor networks; which are generating data most of the continuous rate; for example,
at IIT Madras we get GPS readings from about 300 buses every 30 seconds.
And if you think about cities city wise transportation, this is really a small scale.
And if you are able to instrument all the buses that run in Chennai city we can get
data at a much higher velocity.
Just hold on one minute, why where we worried about all these big data.
So, we are not really, I mean not every one of us is going to work for Facebook or Instagram
or Google.
So, tremendous amount of data is already available in the public domain.
Either through in Facebook APIs or twitter APIs or through Government data that will
being made available on open forums; and we can obtain significant insights from analyzing
this public data.
And you could think about sentiments, you could think about trends and the variety of
things.
It is the kind of insights you can derive is limited by your imagination and access
to the data; and there is the significant amount of public data available, that we do
not really have to work for Google or Facebook to think about big data.
The second thing is that sensors are becoming ubiquitous.
So, a lot of everyday I think that sees are being instrumented, and then raw sensors,
and buildings, there are sensors on bridges, and other kinds of infrastructure out there.
And ubiquitous of internet of things, this essentially the whole idea is to connect different
kinds of equipment through sensors on low power wireless networks.
So, once you have these kinds of data being generated by sensors, then you do not have
any depth of volume or velocity of data.
And so you really need to develop technique like this; that allow us to work with data.
Can I say all ready mentioned and large volumes of public data is already available.
Government is pushing for more access to public information.
So, the all of these factors really makes sense for us to look at big data and we do
not have to work for such large company; is even though the examples I gave you through
make you appreciate the volumes that we have to look at, where drawn from this kinds of
online social media companies.
So, the next V, I am talk about a little bit this variety.
So, traditionally we talk about relational data bases, we talk about structure data with
very well defined fields; may be of a few types in all could have strange, you could
have numbers, you could have categorical variables things like that.
But now with big data, data coming from the different kinds of sources, you have many
variations of this.
So, the first and very widely available source of data is unstructured text.
So, we can get things from the web we can get scientific articles, news paper clippings,
variety of sources review unstructured text.
We keep these are not really marked into sink, this is the heading, this is the most important
keyword here and these are all the attributes of this keyword.
I mean so you do not get this kind of organize data, essentially you have to just read free
from text and from that from some kind of a representation which you can then use for
your data analytics form.
Second source of information now a days is multi media.
As I am mentioning you get pictures, you get videos, and you get variety of different sources
of videos and pictures, and as well as audio song clippings, and so on so forth on the
internet now.
And so, you have to come up with technology for analyzing all of this, especially at a
scale.
And mention about sensor data and again this is going to look like unstructured data.
So, it is going to be company wise time varying signals that we would be measuring from these
sensors.
And how do you even record these, how do you make sense out of this data?
That is the other main challenge and then you have scientific data.
And scientific data comes in variety of different forms.
We are taking about medical data, it could come in terms of reading from instruments,
it could be black and white images, it could be thermal images or here whatever seeing
here is snap shot from a micro array data used in computational biology a lot.
And so, making sense of this kind of data its requires not only new computing techniques,
but also significant understanding of the domain in which you are operating, and therefore
we have to work about close being collaboration with the this scientist; who are handling
this kind of data.
And data analytics cannot be performed in isolation here.
So, lastly I would like to talk about link data; and this is essentially data that comes
with some kind of structure, but not the kind of structure that we are normally used to.
this is data and that comes with the network structure.
So, we talking, you can talk about the entity; let us look at the small close up of this.
So, each of this nodes here this very complex graph and each of this node here is essentially
an entity; and the links tell you how are these entities connected.
So, such large volumes of link data which give you relationship between entities, already
available in the public domain.
And apart from that many sources of data that you generate now a days, have this kind of
links structure associated to it.
So, now the question is how would you mine such large volumes of big data?
So, variety that is tremendous lot of variety and each of these come with its own challenge
and quite often significant requirement for domain knowledge.
So, on the next V, we look at this veracity.
One of biggest problems in social media data is figuring out if the data that is given
to you is true or not.
So, the falsification data I am pretty sure that many of you have actually given false
information to create email ids and other things online, and this being cases of people
maintaining multiple profile on Facebook; and so, that selectively share information
etcetera.
So, it is hard to disambiguate when you are giving false and when you are giving true
information.
So, a significant amount of resources are spent by many of these companies in order
to verify the information which is provided to them.
Or if you take sensor of data, data could be noisy, that could be missing values.
So, how do you account for all of this?
And it is how do you trust the data that you are getting from the sensor is it; or we sure
that the sensor is not malfunctioning.
So, how do you account for that?
Maybe there are few sensors; you can do that manually, but suppose you are talking about
million of sensors diploid over high rise building.
Then the question of manual verification is moved, just cannot do that.
And talk about how biological data is one of the biggest sources of future problems
for us, but even though there are huge volumes of data available more often than not.
These are estimated interactions, estimated data; and again you are not sure about the
veracity of the data.
not sure with that lets if you say protein interact with another protein, you are not
100 percent sure interaction happen.
You can say with 80 percent confidence, I am sure this interaction happens.
So, in such cases how would you handle this kind of unsure data?
So, this is just example.
So, if we take any source of data coming from any kind of domains will always find that,
there are issues of noise and trust worthiness in the data.
So, the verification the people still work with this kind of data already.
We can, some of you if you already worked or working in some kind of a data analytics
company you know that; you have to work with this kind of data already.
But then verification becomes hard due to the scale that we are talking about.
So, current technology people typically end up doing some kind of internal consistency
checks and some amount of manual verification but, we need something more scalable to work
out this a big data scale we are talking about.
And at the last V, I wanted to mention very briefly this whole idea of value.
So, I have all this big data; how do I monetize the big data?
So, we have to think about problems where substance are returns or higher than the investment.
And now, the investment is no long at trivial because we really have to put in the lots
of resources in order to just manage this data at the scale; and then run analytics
on top of it.
So, all the 4 V’s play a role here, that is so basically have to figure out what is
the right balance interns of the effort, that you put in and the kind of monetization that
we can do.
So some example which people actually try out or on recommendation is trying to build
better recommend assistance for different domains.
Looking at sentiment analysis trying to get a sense of what people feel about new products;
may be movies or what do people feel about a particular candidates in election.
So, we are looking at that and then other examples include business process optimization,
surveillance, healthcare, smart cities; there are many many domains in which people have
trying to find out monetization for big data.
So, in the next module we will look at some of the challenges in running data analytics
at the scale that we have talked about today.
That brings us to end of this module.
 
Big Data - A small introduction (contd)
Hello and welcome to the 2nd module on Big Data.
That is we saw earlier that the variety and the volume of data; requires has to have a
new paradigm for handling all the computing.
And one of the most popular paradigm that is used for handling big data computation
is Map-Reduce.
So, this was a programming model that was pioneered by Google which were reusing initially
proprietary implementation.
But later on a Hadoop an open source platform, that implements Map-Reduce became very popular.
And so, in the next few slides I will give you a very brief on introduction to Map-Reduce.
So, Map-Reduce as we will see, is a very intuitive approach to parallelize computation typically
on commodity hardware.
But the big problem when you are using, especially using commodity hardware is that machines
tend to fail a lot.
So, fault tolerance becomes a big problem.
So, for example Google runs typically a million machines in one of the data centers and assuming
on average 3 years lifetime for 1 machine.
You would expect about 1000 machines to fail per day.
So, Hadoop is a distribution frame work, that fundamentally uses Map-Reduce as that computing
paradigm, but on top of it takes care of data distribution, communication, and fault tolerance;
and makes it very convenient to use this kind of a distributed; set up and solving everyday
problems.
And hence Hadoop is being wildly deployed commercially, and variants of that are continuing
to be very popular even today.
Let us look at the Map-Reduce computing model.
The Map-Reduce as you can imagine consist of 2 stages: the map stage and the reduce
stage.
And in between there is a group and redistribute phase.
So, the map stage, so the each mapper which runs on a individual machine takes a block
of the data; that is that you have to originally process, extracts some information from the
data and output these as key value pairs.
So, the key as you know, as identifier the value is any value that you would like to
send along with the key.
So, we will look at examples later.
And then before sending to the reduce stage, we sort and shuffle all of these key value
pairs and you group them by the keys.
So, that all the pairs which have the same key values, will go to the same reduce stage.
In the reduce stage you essentially compute aggregate statistics corresponding to the
key.
We could add things up; we could summarize them, we could transform them, we could do
any kind of filtering that you want based on the key, whatever it is we can compute
aggregate values based on the keys.
Look at this an example of a very simple word count, it is like the hello world of the Map-Reduce
programming.
So, let us say that you have a document.
So, what you would do is you would divide the document into different blocks as shown
here.
So, the 1st block I have colored it red, the 2nd block green and the 3rd block blue.
And we send it to the different mappers.
So, what the mapper does is?
It reads each word in the document, that outputs that as a key and the value as the count of
the word in the document.
So, every time it encounter a word, it just going to output it as that word with the count
of 1.
So, the red mapper is going to output; concept, one, of, one; and then the green mapper is
going to output variety of things on this case is going to say for one, and concept
one.
And the blue marker again outputs for one, various one, weighted one so and so forth.
So, in the group stage what happens, you group things by the keys.
So, all the key value pairs which have concept of the key get grouped and they are send to
one reducer, likewise all the key value pairs which have for as a key get grouped and are
send to another reducer and so on so forth.
And finally, the reduce stage aggregates all these counts regardless of which mapper they
come from and then outputs the total count.
So, in this case you would get (concept 2) (of 1 for 2 various 1 and weighted 1.
This is very simple way of doing word count and your document could be very very large
as long as we have sufficient number of machines we can compute this very efficiently.
That is not only in working with documents and the other things.
Even simple items, even simple computations is you would have thought they were straight
forward can become complicated when you are dealing with it scale.
So, here is an example I would like to compute degree of a node in a graph.
On very large graphs computing even degree of a node becomes little harder.
Why is that, because the data itself is not available to you at one time.
Suppose I am trying to build the graph as of who called who.
So, I am going to get things like A called B at time t and spoke for m minutes.
So, that is an event that arrive to me after the call has finished.
So, at no point of time do I have all the calls of A, stored in some place and we just
going to find the degrees.
It is not like a single graph that already being constructed firm.
Or if you could think of trying to create some kind of an interaction graph on Facebook;
so user x posted on the wall of users y.
Now these events even if they are available to you post facto that are so numerous that
aggregating them and to finding the degree of the graph is could take a while.
So, you could actually use Map-Reduce to efficiently aggregate the each event into a graph.
Here is a simple program that would do now.
So, the mapper essentially takes each edge event, each event, each interaction could
be the posting of a message on Facebook; could be the making of a call, takes each of those
events and then it creates 2 key value pairs for every event.
That has of A call B it will say, A 1 and B 1.
and on the reducer essentially now takes in every event or every key value pair that have
the same key; that essentially means is going to gather all the node A’s.
Add up the events corresponding to the node A it will output the degree of node A. Fairly
simple, fairly straight forward and so you do not really have to create a adjacency matrix
or adjacency list representation of your graph.
We will be able to answer to queries like beginning.
We can stick with the edge list representation and still answer these kinds of queries.
So, let us look at some other questions that not necessarily need Map-Reduce; but become
harder when you are looking at large data.
So, you looked at k nearest neighbors in one of the earlier modules.
So, how would you find k nearest neighbors, if you have huge volumes of data?
So, linear search is impossible; because any index structure should be small enough to
fit into memory for you to do linear search.
That is not going to happen if data is very large.
So, there is a new approach for finding neighbors in data call locality sensitive hashing introduced
by Andrei Z.
Broder and others.
So basic idea here is to, find hash functions.
So, you remember hash functions, they are functions that take a key as an input and
then the hash it into one of n buckets.
So, here what we do is?
We want to find hash functions, such that 2 elements x and y, if their distance is less
than a certain threshold the hash to the same buckets or same bin.
Two elements, x and y; if the distance is greater than a certain threshold, then x and
y hash to different bins; and we would like this to hold with very high probability.
We would like this to hold for sure, but then it is hard to get something that will work
always.
So, you would like to hold for this to hold with very high probability.
So, now, if I want to find if y is nearest neighbor of x, then all I need to do is look
through the bin into which x hashes.
And since this is only with very high probability, so you might actually miss your nearest neighbors.
But you will certainly get some neighbors that is close enough because since the data
set large, even close neighbors are usually sufficient.
So, depending on the kind of distance function that, you want to use on your data.
If you remember we talked about different distance function in the various neighbor
depending on the distance function you want to use on your data; you are going to have
to define different hash functions.
And so for example, if you want to look at distance measure between sets; this has groups
of words and so on so forth.
You probably like to usage the Jaccard distance, which is 1 minus the size of intersection
by the size of union.
Or if you could say Euclidean distance between points or between vectors you could want to
see cosine distance; and for all of these people have worked out what are appropriate
locality sensitive hash functions.
So, we are not going to get in the details of the locality sensitive hashing, just wanted
to give you a feel for how hard it can be, when you are looking at large volumes of data.
Even what you thought are simple operation become harder, when you are looking at large
volumes of data.
Here is another example; we talked about frequent pattern mining and association rule mining.
Imagine counting frequent items in amazon’s transaction data; we have millions of transactions
a month.
So, just blindly running apriori is just not going to work.
So, we need a different approach to the problem.
We can still use apriori property, but we have to think of how you would handle the
memory more efficiently.
So, here is a very simple approach, to do distribute counting; you divide baskets randomly
among compute nodes.
So, here we are talking about market basket data.
So, essentially what I mean here is the transactions are randomly divided among all the nodes you
have.
You run apriori in each computing nodes separately.
And whatever turns up as frequent item sets in each of those nodes are now candidate frequent
item sets.
So, what we have to now do is, go back and count the actual frequency of these candidates
item set; because the original candidates were determined on a small subset of the data.
You will have to go back and count the frequency on the entire data to determine if these candidates
set are frequent.
So, remember that when you are doing this in the distributed fashion the threshold that
you are using for defining frequent item set should be lower.
So, if you are splitting your data at 10 ways then the threshold that you had for frequency
should be lowered by 10 times.
So, once you have this candidate frequent item sets you do not have to count it in a
single machine, you could still do this in a distributed fashion.
So, again each node will count the frequency of just this one candidate item set and then
at the reducer we could basically combine the frequency of frequency reported by each
node, and then report frequency of the item set on the entire data.
So, there are other computing models; I just spoke about Map-Reduce and this top.
And the other thing has Spark which is Map-Reduce variant with local memory and then there are
GP-GPUs, which are multi core massively data parallel computations.
And then there are other models with shared memory multi core repetition.
So, a depending on what is the use case that you have, we will have to use different computing
models.
It is not that Map-Reduces one solution for all method.
So, depending on the solution, depending on the problem that you have, you will have to
pick the computing model that the shows you.
One thing which I would like to point out is that data visualization is still a challenge
for big data.
So, visualization is a challenge for normal data analytics, but it is a challenge for
big data.
So, people are working on adaptable interfaces or interactive visualization, but more often
than not people are still trying to fit old visualization ideas to big data and that is
not working.
So, it is very active area of the research and several new generation methods are needed
here; I just wanted to draw your attention to the fact that it is something that you
could work on.
So, in summary; data analytics has matured to a certain level and so now, people are
looking at big data challenges.
So, it is some sense evolving to a new discipline of data science, where just the analytics
techniques are not themselves sufficient, but we need more understanding from the modeling
front.
It is very exciting time to be in this space, availability of vast amounts of data and the
internet of things like picking up as going to be a lot of work, more data available.
The new computing models and that could very well be a high impact on society if you are
able to come up with solutions, handle make sense of this big data.
That brings us to the end of this module.
Clustering Analysis
Hello and welcome to our lecture, our first lecture on Clustering Analysis.
This is the first of two lectures that we will have on this subject, this one is an
introductory lecture.
It introduces the basic concepts and algorithms behind clustering analysis and in the next
lecture; we will actually go into two specific algorithms and see how those two algorithms
work.
So, deriving to the subject, what exactly is clustering?
Clustering is the idea of dividing data into groups and we do that, because sometimes there
is inherent meaning to doing such an activity.
And in other cases, it serves as the first step, it is fairly useful to do this.
Now, you might notice that you might have come across clustering, with some other names
as well, which such segmentation or partitioning.
So, that is kind of why we have written out all three for you in this first bullet point.
But, the core idea is the same, it is fairly interesting, it also partly what terminology
wind up using also has to do with the context or the background.
So, typically something like segmentation is used more in a marketing sense.
So, when you grouping people or customers, sometimes people use the word segmentation.
Partitioning, again interestingly comes more from the computer science community, where
you are breaking a graphs, so you are partitioning graphs.
But, again the core idea is that you have some data and taking the data and based on
certain properties of this data, you choose to group it.
So, you can think of it as actually grouping data objects based on various attributes associated
with this data.
The idea behind such an activity is that, the data itself is represented through various
attributes or features and some data points are similar to others based on these attributes
or features.
And you want to group those that are similar and put them into one cluster or group and
differentiate that from other groups or clusters, which are similarly formed based on some measure
of similarity.
So, the core idea is to put things that are similar together and therefore, as a result
the different groups are as dissimilar from each other as possible.
So, data points within a group are similar and data points between groups as a result
are dissimilar to each other.
Now, the core idea the clustering often relates to and sometimes it is often confused with,
is classification.
I think an important thing to recognize here is the clustering is primarily seen as an
unsupervised learning technique, whereas classification is supervised learning technique.
The idea is that you have some, in classification you have some input data and you use the historic
input data and the specific output and in the case of classification, the output actually
has classes, it is a categorical variable.
And, so you used some kind of supervised learning technique to look at the relationship between
these input variables and the task there is to make a prediction and assign it a class.
In the case of clustering, you are again dividing the input data space in some sense into groups,
but the groups are not based on labels that have been explicitly given to you.
So, in some sense there is no output variable with clustering and what you have is only
the input data space and it is not even clear that, there is another there is a classification
scheme like in the sense of the, in the classification there was this output variable and this output
variable had 1, 2, 3 or more categorical states.
So, there were the states that are labels that was explicitly given to you and you are
now trying to create that relationship between these labels and the input variables.
In the case of clustering, not only that you do not have the labels or the output variable,
there might be no meaning to having such labels.
So, in some sense with clustering you are really breaking the data, input data set based
on the inherent relationship between data points and how similar they are to each other,
you do not have external label that is given to you about the data points.
So, if two data points are very similar across these attributes, then you group them together
if they are not you group them apart.
So, in the end you might create a few clusters and you can call them cluster A, cluster B,
cluster C, but that is not the same thing as classification, which is more of the supervised
learning process.
So, why really do this and the best way to see that is this brought the two major reasons,
one is clustering itself might just be useful to understand the universe of the data that
you are dealing with and we are going to look at some examples in that light.
The other reason and sometimes the clustering is used is that, it serves as a precursor
to further data analysis.
So, it has some utility from a machine learning sense itself to do a clustering.
So, now, let us look at the first case, which is that in some sense you get a better understanding
of a data when you do clustering.
A fairly common example of this is in marketing or sales, your business is for instance collect
you know lots of information about a customer.
So, the way you should be thinking about it is, each data object is essentially a customer
and the customer has for instance various attributes.
They could be things like gender, age and you know it could extend all the way to the
kinds of products of the person tensed to buy and so on and so forth.
And you have a full data set of a lot of customers or potential customers and you might be interested
in grouping that.
So, there is no explicit output variable, but certain types of customers are very similar
to certain other types of customers.
They could be very dissimilar to the third type.
So, creating groups out there could help for instance like a market research initiative
into looking into a particular group and coming up with ideas, so how to specifically target
our market to them.
Another example you know, another very common example is in terms of just communicating
information; take a simple example such as Googling or you know searching for a movie
on the internet.
Now, there are lots of things related to a movie.
For instance, there could be a reviews of a movie, they could be trailers and videos
of a movie, they could be a ratings of a movie and they could be information on which theatres,
the movie is running in or where you can purchase a movie.
Now, a search itself, the first search itself across the web might give you a huge large
bucket of things that could belong to any of these categories.
But, if you have an ability to for instance look to see which pieces of information are
similar to each other, you might be interested in representing one of each type.
So, you might have a cluster; that is created, which talks about essentially reviews of a
movie and another cluster that gives of potential links that talk about, where this movie is
playing.
And, so in some sense you have you could create this clusters and present the viewer or present
the person, who is searching with representative of each cluster or typical value in each cluster.
So, that a person, who is searching for a review of the movie does not get 20 links
on the first page that give him trailer, give him or her trailers, which should be quite
frustrating.
So, in terms of information retrieval, communication of the information it might be fairly useful.
Clustering is used a lot in biology mainly in taxonomy, where if you just said the wild
universe of mammals or insects or something like that, you kind of want to group animals
or mammals that are similar to each other and give them some taxonomy that is different
from animals that are different from each other.
And here for instance each animal would be the data object and the attributes would be,
you know various animal related attribute such as, how they feed, what their family
or genes or species and so on and so forth.
It is also used in climate, many times understanding ocean temperature ranging from ocean temperature
to hurricanes to various other things, it can be better done you know if you cluster
the data on weather patterns.
Medicine of course, again a certain types of disease similar to other types of disease
or certain medicines similar to other medicines, it can simplify the word in some sense and
help people understand, how for instance certain medicine interact with certain disease and
so on and so forth.
Now, these give you an understanding and these kind of talk about, where clustering is useful
or has been used or just to get a better sense of the data.
But, there is this completely different and other utility sometimes to clustering and
that is mainly in the form of serving as a precursor to further data analysis.
The idea here is that, clustering for instance can be used as a great way to summarize data.
Especially, when the algorithm that a person needs to use.
Let us say a person is interested in performing some form of regression or another more complex
supervised, unsupervised learning tasks.
Sometimes with more and more data the task just becomes computationally harder and harder
and it might be sometimes beneficial to perform a clustering analysis on the data which could
be in some cases it is computationally easier to do the clustering.
And then, just have a representative from each cluster as a data point, so you could
have a representative or you could have essentially a cluster center the clusters representative.
So, was the data point for all the data points in that cluster?
And, so you are now, doing that same machine learning task be it a regression or a factor
analysis.
So, whatever it is you are really doing it on the prototypes on the representatives rather
than the full data set.
It is also used an extension of that is also how it could be really useful like say in
a nearest neighbor task.
We in an earlier in earlier lectures we spoken about this algorithm called KNN: K nearest
neighbors when you think about the problem that is involved with that for any given point.
If you need to find it is nearest neighbours you need to actually try and look at the distance
between the point under question and every other point in the data set.
So, you need to evaluate the distance between the point you are interested in and every
other data point and then pick the K closest neighbours.
Now, that can become computationally very, very, very hard and you need to keep either
the memory and you need to do the computation from scratch, a simpler approach could be
that you just take the K you do the clustering on the data.
And if you look to see, which cluster center is closest and once you identify cluster center
that is closest you now, take your point under question and only evaluate it is distance
to all the points in that cluster.
And the assumption here is that the points the points under that cluster are the once
that are going to be closest to this point, because this cluster center was the closest
for instance.
It is also used in certain other forms of you know compression of data specifically
something called vector quantization, which is used a lot in image or sound or video data,
where you typically find that in a particular image, if you break it up into pixels there
is the whole host of data points that will look very, very similar to each other.
Let us say you take a photo of a person almost 20 to 30 percent might be the background behind
the person and that might have the same color.
So, it would be more efficient to just acknowledge that there is a very good chance that the
pixel on all four sides are going to be the same color as this pixel.
And, so you kind summarize the data you reduce the data to few a number of pixels in a memory
values and; obviously, would this the some loss of resolution either some loss of information
essentially, but that might be acceptable then the data size itself get substantially
reduced.
So, we spoke about, what clustering is and why use it, now let us just briefly talk a
little bit about the different types of clustering.
The major classifications tend to be one hierarchical versus partitional and the major idea here
is that hierarchical clustering is essentially a nested form of cluster.
So, think of it this way you can start with this one mega cluster, which is essentially
a single cluster of all the data points and that is on one end of the scale and then,
you go to the other end of the scale where you have as many clusters as the number of
data points and each data point it is a cluster is its own cluster.
Now, some where hierarchical clustering works on creating this tree between a single cluster
of all data points to each data point being its own cluster now either of these extremes
are useless.
Because, the single cluster of all data point is not really clustering.
You just created you put all the data points in one group and its called it group one that
is not very useful.
And neither is it useful to call each data point its own group.
So, somewhere in between these two extremes is the real value at, but hierarchical clustering
works on some form of nested or kind of tree form of clustering when you start with this
one you can you can start it either end, but you might start with one large cluster and
then, break that into two.
And now, you have these two clusters, now you go into either of these two clusters and
break that into created division there.
So, now, you will have three clusters, but the three clusters is strictly a division
of the two clusters.
So, it is not like you are going to take some points from cluster when you have two clusters.
Let us say you had some cluster A cluster B it is not like you are going to take some
points from cluster A and some points from cluster B and call it cluster C. It is in
that sense nested it is in the sense that you make one split and very, very similar
to decision trees you keep making further splits and this is contrasted to an approach
of partitional clustering.
Partitional clustering is not of this kind of nested approach it is just that you explicitly
decide on the number of clusters in some sense and you go and partition the data.
its it is It is simply a division of the set into non overlapping set, so it is essentially
clusters.
And, so in that sense there is no tree diagram or anything of that nature with this.
So, for instance a partitional cluster that if I decide to do the partitional cluster
and create four clusters and I contrast that to a partitional clustering approach, where
I had three clusters.
The four, need not be a further division of the three they just completely you know the
one that said decided to break in into four has a very little or no relationship theoretically
to the division the separate exercise of the separate effort into creating a partitional
cluster with three clusters, whereas the same cannot be said for hierarchical.
Because, hierarchical went in sequence, it could have started top down or bottom up meaning
you could have started with all in hierarchical all the individual clusters, where it is each
day you know each cluster is a data point and then started grouping them or the other
way around.
But, essentially with hierarchical one is nested into the other and so on.
The other major type of clustering that people talk about is exclusive verses overlapping
versus fuzzy.
The idea here is with exclusive clustering each object is assign to a single cluster
and that object therefore, cannot be assigned to another cluster.
And, so there is no there is no notion that one can simultaneously belong to multiple
clusters, where as in overlapping, in case in clustering algorithm that allow for over
lapping, you can.
Basically it is not exclusive a data point can choose to belong to more than one cluster
at a given point and decision on which, of these are really depends on the underlying
system.
And some cases it might just make sense to have some data points belonging to multiple
clusters and in some in some cases that that notion of division is just not sensical.
And finally, you have you have the notion of fuzzy clustering fuzzy clustering kind
of takes this over lapping even further, where each data point is not really assigned to
a cluster, but it basically gets a number between 0 to 1 that that talks about the weight
associated with that data point belonging to the different clusters.
So, for a data point each data point gets total weight of 1 and it takes that data that
weight of one and says, I am going to assign 0.3 in belonging to cluster A, I am going
to assign 0.7 in belong to cluster B and I am going to assign myself 0 belonging to cluster
C. So, the constrained is that the sum of its weights in terms of belonging to the different
clusters adds up to 1, but it can use its weight of one in any way chooses to belong
to the different clusters.
So, that is to give you some idea between of exclusive versus overlapping versus fuzzy
the last that is worth mentioning is you can also have a complete process partial clustering.
A complete clustering basically just assigns every objects to a cluster, where as a partial
clustering does not it starts with the data points chooses to you know cluster as many
points to different clusters and data points that do not really help in terms of belonging,
so clearly to given cluster just not cluster they are just left out.
And these might and the and the motivation there is that you are more interested not
in kind of pigeon holing each data point into a cluster, but you are more interested in
the cluster formation itself.
And there you do not want data points are not, so clusterable, that do not really belongs,
so clearly into 1 of 2 clusters to kind of ruin the cluster center or ruin that nice
division that you created.
So, these are set different types of clustering, now the algorithm themselves and these are
more descriptive of the type of algorithm that goes into it.
Now, another area focused could be the kind of clusters that you are forming and while
this has everything to do with the algorithm also here we are just looking at the end product.
So, the algorithm that we are using what kind of end product does it can it give you.
So, there is the first one is the well separated idea the idea the well separated is that you
want to create clusters, where get the objects where each object is similar to every other
object in that cluster.
So, the way you defining well separated clusters are is more from the core idea that you are
very interested in looking at the relationship between each data point with respect to each
other data point and you are measuring or your quantifying the clustering itself by
looking at how similar that data point is to that other data points that are in that
cluster.
And therefore, how dissimilar this data point is to the data points that are in the other
clusters and this kind of thinking you know is very useful especially when the data itself
can be very nicely separated.
When the distances between the clusters are fairly significant, then this conception becomes
very useful.
The prototype based approach really talks more about how each data point is close to
its cluster representative.
So, that the commonly used terminology is to the proto type that defines the cluster
and when I describe it I can try to think of it as there is a cluster and there is there
is the main representative of the cluster, the prototype the one that kind of signifies
that what the cluster is in the center of that cluster.
So, in the prototype based approaches, because its representative is in some sense so central.
It is also sometimes called center based clusters, but the idea is as following, where each data
point is looked at more from the prospective of how close this data point is to the cluster
center to the prototype.
And, how far this data point is from the prototypes of the other clusters or the other representatives
and the classification is done in this fashion.
We then, have the graph based clustering and this is really useful if the data is represented
as a graph and you have nodes, which represent each data point or object and you have this
links or edges, which represent some notion of connection between the data point.
And this notion of connection could be the one that talks about how similar a data point
is to the other data point you could have some kind of a threshold value or especially
when sometimes you are variable itself is not quantitative variables.
So, you could have some notion that two data points are connected and the idea here is
to do some graph based clustering, where you really looking for a high density of connection
this between the data points that belongs to a cluster and a very low level of connectivity
of the data points between two clusters.
And for that reason you know this is whole language that comes from the graph theory
community, where you define things calledÊcliques, which essentially just means that at the set
of nodes in the graph of a completely connected to each other.
And, so a lot of that that kind of clustering ideas that go in to graph based clustering
are once that kind of look out for cliques and say these guys are all connected to each
other, so they must be a cluster.
Finally, you have density based clustering the idea behind density based clustering is
that a cluster is essentially a dense region of objects that are surrounded by regions
of lower density.
So, the idea here is that, because there are not such well defined clusters like in the
case of the well separated idea that you are not really looking too often do a complete
clustering and there is a lot of noise in the data and you know the clusters themselves
are irregular or intertwined and there are lots of outliers and so on.
So, the idea here is that you acknowledge all of that, but you just try to identify
spots of extreme density, where once you identify that spot that dense region becomes a cluster
and so that is fairly useful in defining a cluster, where there is a lot of noise and
so on.
With that we will conclude our lecture on lecture that introduces clustering and the
different types of clusters and where it is useful and so on.
In the next lecture we will looking to basic algorithms one is the K means algorithm and
that really belongs to that partitional camp and we will look at another algorithm called
the hierarchical clustering algorithm which belongs to the hierarchical camp.
Thank you.
Clustering Analysis (contd)
Hello and welcome to our second lecture on Clustering Analysis. In the first lecture,
we introduce the idea of clustering, differentiated it from say classification and other supervised
learning techniques. And we explain, what clustering is, how it is useful, where it
can used and also gave you brief overview of the different types of clustering and the different types of clusters. In today’s lecture we are going to introduce
two very popular clustering techniques. The first is K mean clustering and the second
is called the hierarchical clustering, where both these techniques are fairy old, this
still enjoy immense popularity in terms of being actually used. The first one, the K
mean clustering and this choice of K mean and hierarchical, you should find to be also
fitting. The overall, the first dichotomy that we used when we talked about clustering
approaches, which is partitioning based approaches and hierarchical approaches. So, the K mean
clustering is essentially the partitioning based approach. And, what we will do is, we will dive into the algorithm. So, we have already mentioned
that is a partitioning based approach. So, it is one, where you partition the entire data set into K clusters essentially and you, it is also very useful to think of the K means
as a prototype based approach, where that is also something that we discuss in terms
of how clusters are formed. And the prototype based approach is one where, there is like
this representative for each cluster and you use these representatives in some fashion
to express, what a cluster is and to also form the clusters.
So, it is essentially this prototype based approach, where you create K clusters and
it is also noteworthy that it is an iterative procedure. So, even right at the end of the
first iteration you already have K clusters and they might not be good clusters, because
it is just the first iteration. And then, like most optimization procedures like steepest
descent, any of these other iterative producers you will find that over time you are just
refining a solution and at some point, it does not make sense to refine any more. You
are not, your clusters are not changing essentially prototypes or not changing either location
or who they are and, so you stop at some point. This procedure is ideal when all variables
are quantitative, whether what we really mean is that you should be able to take each data
point and you might have some other data point or some other location and your location is
defined across the multiple attributes associated with the data point. So, a data point, again
the conception you have in your mind is these rows and you are basically grouping data points,
that is your job. And each data point is described by some attributes, which are these columns in a table and each column means something. So, with the K means procedure, what you doing
is you want to say that that you want to take this conception and you want to actually,
you want actually be able to compute distances between two points across these dimensions, which means that attributes themselves need to be quantitative. This in a lot of ways
to remind you of this K nearest neighbors, examples that we took up especially in the
case of K nearest neighbors regression, where each of these dimensions for those each data
point is a continuous quantitative variable. And once you do that you just able to be compute
distances, typically whenever it comes to computing distances we always use Euclidean distance, you could also use other measures of distance and you would actually, it would
still be a K means procedures, but with different approach, so common once are Manhattan distances,
Euclidean distances and so on. So, how does it really work? So, let us just go through
the algorithm in some senses and then, we will see a graphical representation of it, the idea is to initialize some clusters centers K.
So, what you mean by initialize cluster centers? What we mean is, essentially think of this
and it is easy to think of it in a graphical sense or you can think of it in a mathematical sense, but the idea is that each data point can be expressed in terms of an x axis, y
axis, z axis, w axis, so on and so forth, depending on the number of attributes there are that represent each data points. Now, while each data point is represented based
on these different attributes, you can create a cluster center also on these attributes.
And often it really make sense to put cluster centers, where you think the clusters are going to be form, but you see how even if you do not do that perfectly sometimes the
clustering the K means clustering will adjust for it. But, the most important thing to take
away from this is that, the K means cluster algorithm therefore, could be sensitive to
where you place the cluster centers. If you do not place them in sometimes the right places,
you could get to a solution, which is not necessarily a good solution.
So, the idea is that you initialize some cluster centers, so specifically you would initialize
K cluster center, so you will, because it is K mean clustering. So, K is usually a number
between 2 and may be 10 or 20 depending on the… The higher limit is a little more vague,
most problems you looking at between 2 to 6 or 7 clusters at the most. But, there are
contexts, where your data set is really large or what you intend to do with the clustering
is such that you do not mind having more clusters. So, the algorithms initially you drop in K
cluster centers and then, what you do is you take each data point and if you figure out, which of this cluster centers is closest to the data point and then, you assign the data
point to the closest cluster center. And, so essentially it is like if you kind of give
names. So, just to give you some inside, this is where we are, where the second bullet point.
So, what we are doing is we basically saying if you wanted to give those K-cluster centers name as cluster 1, cluster 2, cluster 3, then what we would do is, we would go to each data
point and say, which cluster center is closest to you and then, I am going to assign you to that cluster center. Now, once all of these points have been assigned, what we are going
to do is we are going to recompute the cluster center to be essentially the centroid of this
assignment of data points derived from step 2. So, what we are going to do is essentially in step 2, what you did is you assigned a
whole bunch of data points to different clusters. So, what we will do is, we then, for instance
take all the data points that were assigned to cluster 1 to the centroid 1 essentially,
to the cluster center 1. And we gave them all the labels that you are assigned, the
data point x you are assigned to the cluster sector 1, data point y you are assigned to cluster center 1. So, all of them that were assigned to cluster
center 1 we take those data points and compute the centroid of those data points. Centroid
essentially is in many senses like an average. It is, again it depends on exactly what measure
of distance you choose, but if you are choosing Euclidean distances, the centroid is essentially like the average. And we say it is the average of these data points, but it is called the
centroid, because it is the average across all those dimensions. The number of dimensions are the number of attributes, so across those dimensions, what is the center.
So, once all these points have been assigned you kind of compute a new cluster center based
on the centroid. And it is essentially like you forget the old cluster center and now, this new cluster center is your cluster center and you do this for each assignment. If 10
data points were assigned to the cluster center 1, then you do this step for cluster center 1 data points. And then, you go to the next 15 data points set for perhaps assigned to
the cluster center 2. So, for these 15 data points find out the new centroid and call that the new cluster center. So, you got new K set of cluster centers.
What do you do next? You iterate, for these new K cluster centers you go to each data
point and it is almost like you forgot, which cluster center that data point belong to,
because remember that data point was assigned to say cluster center 1 or 2 based on the
old centroid, based on the old cluster center. Now, because your cluster centers are moved
based on the centroid, you now assigned them all over again and once you done the assignment all over again, you find the centroid. Once you find the centroid, you do the assignment
that is how it is an interactive process. Essentially you repeating the steps 2 and 3 until the centroid is not moving any further or in some cases, you might say the centroid
is moving by an amount smaller enough that is within your tolerance.
So, this was fairly abstract in terms of bullet points, so let us actually take a graphical
look at this if you taken an ultra simplistic example, where there are only two attributes.
So, let say attributes is x 1, x 2 always remember that with clustering, which is unsupervised
there is no y there is no output variables, what we doing is trying to grouped the data and here is the data. The data is this blue squares and you are trying to grouped this
squares. Now, fairly naive look at this can kind of
make it obvious that perhaps this is one cluster and this is one cluster. So, the blue dots
are actually the data points, now that is to the naked eye and the things to remember
are that you know if for instance there were more dimensions that were there were more
attributes beyond x 1 and x 2 it would be harder to show you this visually. So, that
point the math kind of take over in 3, three dimensions I can show you x 1, x 2, x 3, but
after that whatever I do if you have more attributes. So, the squares are essentially the data points and let say the K means clustering algorithms
started with two centroids here obviously; that means, K is equal to 2. So, you initializes
two centroids and you know this like I said the algorithms itself to some extend could
be sensitive to how you initializes the centroid. So, there is no right way and the wrong way. Ideal would be if you could actually plot the centroid in the middle of the clusters,
but often we do not know that yet that is why we are doing the clusters. So, what is the first step in the clustering algorithms the first step is to take each
data points and see, which cluster center is closest. So, let us say we take this data
point 1 clearly this the yellow cluster center is closest and at least hope for the set of
data points is yellow is closest. Obviously, for another set it looks like the green is closest, so each data point basically gets an assignment either gets assigned the yellow
color or a green color and that is essentially, what I have done. This diagrams are more conceptual
not the scales. So, I kind of high balled it and said it looks like this 4 data points are close to the yellow that is this 3 this 4 are close to the green,
so that is how they have been assigned. What is the next step? The next step is given this assignment the new cluster center for the yellow data points is the centroid of the
yellow. So, what is the centroid of the yellow the, so you can think of it as the essentially
the average of the yellow and average; obviously, needs to be on this axis as well as it needs
to be on this axis. So, where you would centrally places yellow, so that is you know minimizing the squared deviation, which is Euclidean distances to
each data point and that is the definition of centroid. And, so we move the yellow yellow
circle to be the new centroid and it is probably going to come somewhere here and the green
circle, now the green circle has a little bit of problem and it just cannot move to the center of this space out here. Because, it is; obviously, has an assignment.
So, this is going to bias where the green moves, so perhaps it will move somewhere here and that is what I do in the next step actually move the yellow and the green. But, once I
move it is like the assignments are completely lost you forgotten the assignment, because
remember that the assignments were made with the old centroids. Now, with the new centroids you just have the new cluster centers and no assignments
you redo the process when you redo the process you see now, which blue dots are closest to
the yellow and which ones are closest to the green and we repeat the process and as you can see we have already gotten pretty good results out here with the yellow and green
classification, which matched our intuition about what the two clusters of this graphs was. Clearly in the next step for instance is green
will move into this cluster center the assignments themselves won’t change too much perhaps
the yellow will move little bit out here. But, essentially after maybe one or two more steps the centroids will stop moving your assignment will probably the same. So, even
if you stop the 1 or 2 stages earlier you would have gotten the clusters that you were interested in. So, I hope this gives some idea, what the
K means algorithms is, now another very popular prototype based approach, which can work and
go beyond K means in some senses, which can go beyond a quantitative attributes. Because,
remember for you for this whole algorithm to work you had to able to be compute distances
and it is makes senses that you attributes x 1 and x 2 for quantitative and continue straight x 1 and x 2 there is a little there is a medium there is more and this is continuous
quantitative variable, now many times you do not have that.
So, very useful alternative is K medoids in that case, now K medoids allows you to go
beyond the quantitative variables. So, when you have categorical or rather more importantly
nominal variables, where the variables is are things like male, female and the attributes
is like male female or things like that you can use K medoids. But, more importantly K
medoids allows you to go beyond attributes altogether, where x 1 and x 2, where attributes
to a point, where all you need is some dissimilarities matrices.
What do you mean by this dissimilarity matrices? What I mean is you have all this data points, let us start calling them 1, 2, 3, 4. Now, with K means the distances between and I am
going give you the same data points in the columns, now with K means I can compute the
distances between the data points 1 and data point 2 through some form of Euclidean distances
and mark a value of x. But, what if, so the whole process of using Euclidean distances
require that I go into each attributes look at how different it is and then compute that square distances takes the square roots. But, what if I did not have that process,
what if I had a process, where I gave you just the differences between each data points.
So, I have something like a dissimilarities matrix, so when I have a dissimilarities matrix,
that I am giving you and you can get the dissimilarities matrices through completely differences ways
it can be user survey or it can be something extremely subjective, where you say I feel
like 1 and 2 are different by this much and I can tell you how different 1 and 2 are and
I can tell you how different 1 and 3 are you know and I can tell you how different one
and four are and so on. But, I cannot really break it down into five different attributes and give it to you that way. So, people do this lot of times in social
science research whether some kinds of a question I have that is given to people to tell, so
please tell me how different this two are. And people are able to say that there are not able to break it down into a set of quantitative variables and say how different they are in
each of those dimensions. So, K medoids is very useful when you do not have these attributes,
but you just have dissimilarities matrices of how each data points is different from every other data point. So, you will essentially just need some kinds
of leading diagonal of data. So, the data either on this side or data is on that side, because what is different from two the same amount that two is different from one there
is no different between those two. So, in cases like that K medoids primarily winds
up not having this arbitrarily cluster center, because you really cannot any longer compute
things like centroids there is no centroids there are no dimensions on which. So, what winds up happening is you nominate A data points to be the prototype and you
use the same core concept of K means in that you first nominate a data point and then,
you do an assignment, where each data point chooses between the nominated data points. And once you have one assignment you find the new nominated medoids; that should become
the representative data point. It is kind of like the idea of being the cluster center.
Now, while this works an important point is that the computational intensity associated
which such an approach. Now, when you had 10 or 20 data points which
had an assignment, so let us take all these data points. The computing the average through
sum of square just was essentially like using sum of square minimization was essentially
computing an average of this points across each of this dimensions. Now, you do not have
something like that with K medoids once you have an assignment to choose which, for this
assignment, what should be the prototype requires you to see the dissimilarities between each
data point to each other. So, the number of computations that you need
to assess really grows. So, K medoids is often seen as a very computationally intensive approach.
So, this should give you some idea about K means and K medoids. The next approach that we are going to talk about is hierarchical clustering with hierarchical
clustering you do not really windup fixing fix number of clusters. And essentially the approach itself starts with either one very large cluster and breaks it down step by step,
so the first one, so one way of doing it is called the divisive way, which is you have
one large cluster, which has all the data points in it is and then, after that I break it in to 2 clusters and then, after that I break it in to 3 clusters.
So, I look at the two clusters that were broken up as choose, how to break them down further.
So, and that is called the divisive approach you also have the other approach, which is
agglomerative and definitely the more popular approach, which is bottom up, where each data
point becomes a cluster, so you have all these. So, you have the number of actual cluster
you have is equal to the number of the data points, because of each data point is a cluster.
And then, after that you choose the two closest clusters and merge them together and we will
talk about how closest is defined. So, you choose the two closest clusters and merge them together. So, the end of the first step you are essentially having we had n data points
you having n minus 1 data points, so the end of one step and after that the next step you
have the n minus 2. Because, now you have n minus 1 cluster where all of them expect
one have one data point and one clusters has two data points. But, you are just treating them as clusters and you are saying, now order this n - 1,
which are the two closest clusters that I can merge and that is seen as bottom of approach.
Now, because of how we do this essentially you are going to have nested clusters. Think
of this divisive approach if you created two clusters. Now, when you go to create three
clusters in this divisive approach you are going to take either cluster 1 or cluster
2 and break it further and the others is going to be same, so one break up could be that the cluster 1 stay the same and cluster 2 gets broken up into two pieces.
So, in many ways the clusters that you are creating are nested within one another you
can think of them is parent and daughter. And this same of approach goes for the agglomerative, where you started with many individuals clusters and you choosing to group them. It is never
like you are breaking one grouping you rethinking an action that you did in the previous step
you never in some senses going back in time and recorrecting a decision to either group
clusters or break clusters, so in some sense you can think of this also partly greedy approach.
Now, we spoke about, how we are going take in with especially agglomerative, which is
what we focus on the rest of lecture, because it is the more popular one and for very specific
reason that we will talk about you need to take in the first step. For instance you have n data points the n clusters and you need to take two closest clusters and merge them,
how are you defining closest clusters. And the way you would kind of define them is through
some measure of dissimilarity between the clusters. An example is for instance that there are many definitions of the dissimilarity one
the three of them that are listed here really talk about it more in terms of come more from
graph theory. And the first one is called single linkage and the idea behind single
linkage as the means of talking about dissimilarity is that it is essentially when you take two
clusters you take the minimum distances between two data points that can be in the two clusters.
So, do not think of it as much in terms in the first step with in each cluster there
is one data points, so it is not a very interesting case. Now, think of a case, where you got
this cluster and you got three data points; that is what I have shown here and let me give you concrete example, so here is one cluster and there are three data points within
this cluster. Here is an another cluster, four data points five data points. Single
linkage basically takes each combination and sees, which combination is minimum, so in
this case probably this would be minimum. And, so you know it is for that reasons it
is actually called min as a definition of dissimilarity. Complete linkage in contrast
takes the maximum this is which, of this combination I need you take one from cluster A and you
take from cluster B, which combination of points can I take get the maximum distances and I am probably guessing that this connection that is just I drew would have the maximum
distances this group average, which basically takes every combination of every point to
every other point and takes the overall average. Now, in addition to that you also have some approaches that try the more prototype based ways and there you would for each cluster
try to create a centroid and from that centroid you look at the distance from one centroid to the other and so on. And there are some other approaches there is a there is a wards
method, so on, where again trying to take some kinds of a more prototype based approach
to define the dissimilarity. But, essentially hierarchical clustering it is all of these
still come under broad umbrella of hierarchical clustering. So, there are two important things in connection with the hierarchical clustering as you can
see, because it is doing this nested clustering, where you start from with agglomerative you
start with each data points being a cluster and you keep merging them till you have this one mega cluster of all the data points. And in the other way around with this divisive
you start this one mega cluster with all the data points and keep breaking it till each data points is it is own cluster. So, therefore, you have not really committed
to creating clusters of a specific size K and you can therefore, take you can kind of
look at the results over all and make it make a decision in terms of what here size should
be. One important property that we see with all agglomerative when you know in some divisive
methods is that they possess this property called monotonicity. And the idea here is
that dissimilarity between merged clusters is see there is some way of measuring the
dissimilarity of a cluster. So, that that dissimilarity between merged clusters is monotonically increasing with the level of merger and that should be fairly
intuitive. When you think of the dissimilarity of a cluster which has just one data points
there is no dissimilarity it is only when you have two data points you can say this
two data points at different by this much. So, with agglomerative clustering in some
senses the more number of data points that you have in to the clusters the greater of the amount of dissimilarity there and that monotonicity is strictly maintain with agglomerative
clustering. So, a very useful way of representing it therefore,
especially when you have this monotonicity you can graph quickly represent the agglomerative clustering through something called the Dendrogram and what is shown on this slide is Dendrogram
and it is essentially this binary tree, which is plotted. So, that the height of each node
is proportional to the value of the dissimilarity between it is daughters. So, what is node
here? Essentially you can think of each partition is being a node and the height of this node. So, let us take something very simple height of this nod as to do with this height as to
do with the degree of dissimilarity between b and c between see you are forcing b and
c to be in a cluster now that is what you doing by this part of the graph that is what
it is doing. This height as to do with dissimilarity of b and c, which is why for instance perhaps
d and e was merge first. So, with agglomerative clustering think that you are going bottom up and you are sequentially making decisions. And, so if you choose to put d and e together
first; that means, d and e would have been less dissimilar to each other than b and c and that is why d and e was done first and then, b and c was separately done perhaps
later. Now, the really good thing about this dendrogram is that, now you have a full picture
you can, now choose to say I am interested in this situation where there are three clusters and you can essentially draw this horizontal line. And you have the three clusters cluster
1 is just data point a cluster 2 is the data point b and c cluster 3 is data point d e
f and g you basically see what goes in each of this limbs and that is that is essentially
your cluster. So, at any level when you draw a horizontal line you can the number of vertical line cuts across is the number of clusters that you
have in your thing. So; obviously, if you had a line out here you drew it here this
is 1 cluster and here it is the 2 clusters does, so, on. So, this should hopefully give
some idea about the two very popular algorithms K means clustering and hierarchical clustering.
In hierarchical more specifically K agglomerative. Thank you.
Introduction to Experimentation and Active Learning
Hello and welcome to our first lecture on Introduction to Experimentation and Active
Learning. This is the first lecture of a two part series on these topics and in this lecture we intend
to motivate the use of these techniques as well as the concept of reinforcement learning.
So, the reinforcement learning is a lecture, a separate lecture that you will have from
Professor Ravindran. And in this lecture we will motivate the need for these topics experimentation, active learning,
reinforcement learning, why are we talking about them in a data analytics course and we will also briefly introduce the topic of experimentation or design of experiments and
in the next lecture, we will continue with experimentation and end with active learning.
So, let us get into the subject, where we take a broader look into data science and
analytics. The core idea is that data science and analytics need data and if you were to go with the big
buzzwords now, we might even need big data, you know where we can really gain useful insights
and this is quite fairly motivated with the easy availability of storage, the easy availability
to process data and the internet of things generating a lot of data.
It is quite easy to get lots of data and analysis the data and come up with useful insights.
But, in not every situation do you start with a data base full of data and in not every
situation, is it easy to create this data. It might either be costly or it might not be, the data that you might have is not the
relevant data that you need, and in many cases, you just have not started the exercise.
So, for all those cases the big question is, basically do you have no scope for data analytics
and, the answer really is that, there is this whole other set of tools and techniques, the
quantitative tools and techniques where which focus really are not just the analysis part
of data, but have something to say about what data gives creative, which then goes and gets
analyzed. And that is the focus of these lectures on experimentation active learning and reinforcement
learning, which is that data science is not confined or data analytics is not confined to lots of data that are already available, but it is moreover an iterative process, where
sometimes the question of creating the data is also intrinsically looked into this grand
problem statement. Now, sometimes we will not even give a second thought to this dichotomy of creating data
and analyzing data, because some problems just inherently come with this creation.
So, if you take a look at many of the, you know inferential statistics techniques that
we saw earlier in this course. Let us take an example where we used a two sample t test, you essentially had to sample
n number from, you know class a and class b.
So, and you know compare the means, this whole process or sampling was in some sense creating
the data. So, let me give you a concrete example, one of our favorite examples might have then that
our 10th standard girls. Is a average height of 10th standard girls higher than the average height of 10th standard
boys and there we said, we need to go to take a sample of 10th standard girls and sample
of 10th standard boys and we came up with a conclusion that and we did a hypothesis
test there. So, when you do a sample, you are essentially creating the data and in other situations
as well, where you would doing some kind of an engineering experiment perhaps and somewhere
we did not really use a word experiment excessively, but the idea is that sometimes we never thought
about it, but those are fairly the simpler cases. There are a lot more cases, where you need to explicitly think rule, the creation of
the data before any kind of analysis can take place and that is what we will focus on in
this lecture. Now, when we speak about creating data, a very important factor becomes whether this
is an online context or an offline context for creating the data.
What do you mean by that? Online and offline do notÉ Online does not mean that you are in the internet, it means
something different here. What it means is, essentially when you are in a online setting; that means, you are experimenting
or you are creating data on a system that is currently creating a product or producing,
you know involved in performing a service or a task, which is going to the end user
and for the purpose of getting this data, you are not just starting a passive observation
exercise. So, you are actually either querying the system or you either playing with the system in order
to get the data that you need. Now, that can be a problem sometimes, because you are actively interfering with the system
to create the data that you need and this system is right now either producing a product
that is going to the end user or this system is a live system, which could influence the
experience an end user is having and I am using the word end user in a fairly broad way.
If you experimented on a traffic signal, the end user or the motorists need to go through the traffic signal.
If you clear on with a manufacturing process which is producing a product, the end user
is the ultimate user of the product that goes and reaches the, you know the customer.
If it is a process, then again the process itself has some outcomes. It either gives the end user some information or the process itself performs a task for
the end user and all of these things could be getting compromised, if you are playing with the live system.
And so, a major area that we would be looking at is the one of the reinforcement learning,
where you are looking at an online system. So, you care about learning, where you care about this learning in a very supervised learning
frame work, where you have some outputs and you want to understand how these inputs effect
the output, that is one side of the story. But, this is second side of the story which is, you do not want toÉ You are interested
in doing as well as you can, even while you are experimenting. Because, some consumer or end user is experiencing the effects of your experiment and you yourself
could be, the experimenter could also be the consumer. So, we are going a little abstract out here.
So, but the advantage of going this abstract is that you can really envision any scenario and any domain and this kind of a frame work should apply there.
So, we will see that primarily, the online setting or I liked to kind of call it the
live setting, live experimenting gets covered in reinforcement learning and those lectures
will support that. Now, in going forward now, in this lecture and the next one we are primarily going to
talk about an offline setting. What we mean by an offline setting? Just the opposite of the online, which means, that the unit that we are experimenting on
is not producing products that are going to go to the end user.
So, you either created and you can do this in many ways. So, if you want to conduct an experiment and you want to gather data, you can create a
model of your system and go experiment on that model, you can artificially create a
lab setting. Let us say I want to experiment on what fertilizers work on my fields.
I do not have to go pour those fertilizers on my fields, I can actually create a green house and choose some specific plans and try these fertilizers out and I can essentially
create this lab setting, which is suppose to mimic the real world. But, it is not always creating models I mean it could be creating models, it could be creating
artificial environments, it could be creating computer simulations and then you go experiment
on that. But, it could also be the real process except, now if turned off the real process.
So, let me give you an example. Let us say you wanted to do an experiment on a machine and this is a machine that used
to manufacturing and you wanted to know, what how to set various boiler plate stuff on this
machine. So, you wanted to know what the speed should be, what the turning radius should be, different,
different parameters associated with this machine. Now, if the machine is currently manufacturing the product which is going to the end user
that is the problem. But, what if you stopped the entire manufacturing process and you said, we are going to now
exclusively commit some resources to experimentation. So, we learn about this system and so you clear on with the machine, clear on with various
settings on the machine, make it you know create products and then you measure the products
and see how well you did or how badly you did and you learn about the systems, you created the data, you analyze the data, you learnt about the system, but these products that
are sacrificed in some sense, they are not going to the end user. In other words, you do not care how well you do when you are experimenting and that is
the core of offline experimentation. A bad experiment is not one that gives you poor results, but a bad experiment is one
where you cannot learn about the system, your focus is about creating data to learn about the system and that is not mean there is no cost to experimenting.
There is a cost, but you can think of it as a fixed budget that has been sacrificed or
you can think of it as there being no cost at all. However, you want, but it is not that while you are experimenting you are trying to perform
as well as possible that would be the online setting. So, now, that we have understood this difference.
It is also important to understand the difference between observational data and offline experimental
data and I use the word in DOE. So, for the first time you started using the word DOE and here we mean design of experiments,
it is a more formal way of talking about experimentation of talking about statistical experimentation.
And the idea here with this difference between observational data and offline experimental
data is the fact that with observational data you are not interfering with this system.
So, let us go back to the original problem. We said, hey how do you do data analytics when you do not have data.
One approach is to say, so I need to start collecting the data and so perhaps I will
turn on a few sensors or put some sensors in certain places and I have start collecting the data.
Now, that in itself is not the subject we are talking about here, that is just turning
on this switch of collecting data and once you have the data, you analyze the data and that is fair game that is data analytics in some sense.
But, that data analytics has coming from observational data.
What we focusing on right now is, is there some way for me as the agent that wants to
collect the data to actively engage for the system and choose, what data need gets collected
and that is what we will be doing both in design of experiments and active learning. You are in an offline setting, meaning that you are right now committing all your resources
to collecting the data and you can collect whatever data you want. But, the point is that you are going to be controlling what data gets collected and as
a result, the only way to do that is not to passively observe the data that get generated,
but do actively in the case of design of experiments you will actively go and change some of the
settings in a system or very specifically go query certain points and that is how the
data gets generated. A typical problem statement and experimentation would actually say, go set the machine to
setting a and setting b and setting, setting c and then let us see what the output is.
And in active learning is seen a little bit more as that entire data, the input space
is available and you get a query a particular point in the input space and then get the
answer. Now, these are just two difference ways of describing it, because they come from slightly difference context of application, but the core problem statement is the same which is
that you as the user and experimenter or learner has the choice of generating an output at
a pre decided point in the input space and that is has some critical difference over
observational data and lot of advantages just to give you some intuition, the biggest advantage
is one concerning multi collinearity, if you just observe the data it is possible that
two input variable are so highly correlated to the point where there is almost a perfect
correlation. In which case you would never know if the output was increasing or decreasing as a result
of variable input variable a or input variable b, because input variable a and b are so highly
correlated. So, in the case of design of experiments or active learning you would come to that realization
at some point without passively observing data and say o we need to try out an experiment
or we need to query a point, where input variable a is high and input variable b is low and
vise versa to try and understand which of these two input variables is having an impact on the output.
So, let us start with focusing our first lessons on experimentation and design of experiment.
The core idea design of experiments is that as long as you can conceptualize the operation
of a systems has some combination of inputs which when use together results in outputs,
you have the scope for this kind of black box creation and a black box I mean essentially
and understanding of the way the inputs and the outputs relate to each other. So, the inputs themselves can be anything you know they can be really broad, the only
important thing is you want to be able to quantify them in some way. The black box that you are experimenting or can be anything, it can be a process, it can
be a system, it can be an organization which is performing certain organizational functions,
it can be an actual product. And typically what are the outputs you are interested in?
The result of the black box could be the some products being created and you can measure these products and therefore, measure how good or bad they are.
The output could be some services or tasks that are created by this black box and again you should be able to evaluate the output.
And the third is a little bit more abstract which is the information is getting created, again as long as you quantify these outputs and quantify the inputs you have a system
where there is scope for experimentation. So, formal experimentation what is it and how is it different from just observing data
and analyzing. Formal experimentation essentially involves systematic, it is a very important word systematic
and purposeful changes that you make two input variables in an attempt to gain knowledge
about this system and or find the ideal settings that result in the best output.
So, that sentence is little long wind it, but let us kind of break it down, so the idea is that in experimentation you proactively go and systematically or purposefully change
the input variables it would mean that you actually go and say, oh I need to understand
about little bit about the systems. So, I am going to try setting input variable a to a particular value input variable b to
another value and there I am going to go look at what output I get and the purpose of this.
Now, in some rare cases it could just be that you are not physically changing the input
variable, but you are physically choosing the input variable that you want to observe.
Because, you do not have the information about how the output is going to look at every point
in the input space. So, you are not making a modification to the system there exists this large enough repository
of information, which is very expensive to query. Because, if it is not all you have is a huge data set which requires a supervised learning
task. But, for some reason if this is very expensive to query you could also think of experimentation
and light of choosing very carefully the input variables that you want to gain knowledge about.
But, more often than not the typical context is that you have this system where you go actually change the input variable set them to different values.
So, think of this perhaps this foundry process, where you are trying to create castes and
your input variable could easily be the temperature of the molten metal that you pouring in, the
pressure that is being applied the kind of material that the cast is made off and your
output could be the number of defects that you see in the cast.
Now, in an experimental process you will go and systematically purposefully try different
temperatures, different pressures, different materials to make the cast, different practices
in the cast, how long be you weight before you open out the cast, what kind of a room
do you place it, you would go actually physically change these settings to different values
and observe what happens to the output, which in this case is the number of defects you see.
So, the emphasis here is in the systematic and purposeful changes to the input variables. Now, why do you do it, you could do it for two reasons, you could do it to gain knowledge
and you or you could do it to find those settings that you want to set the inputs to get the
best output. Now, the gaining knowledge could just be an independent process that could be the final
goal, sometimes the approach is to gain knowledge and once you gain the knowledge.
And what knowledge we are talking about here? We are talking about the knowledge associated with how the input variables affect the output
variables. Now, you could then once you gain this knowledge use that information to figure out what are
the ideal settings for the inputs. But, you also have a algorithms which say you know what I do not care about the knowledge
my goal is to right now find out what those settings should be. So, that I get the best output and that is takes on a different form.
So, that is the core idea of experimentation. So, this point a very natural question arises see you got some two or three or four input
variables and you got an output variable. What is the problem?
Why do not we just clear on with each input variable, you know one at a time and see what
is the best setting for each input variable and do this sequentially, turns out it is
not that simple. This is simple problem with that and we I am going to illustrate that with this diagram.
Let us say that this plot that you has two input variables, input variable one x called
x 1 and input variable two called x 2 and the output is nothing but, a hill that is
projecting out of this green and this hill is shown with a contour plot.
A contour plot is basically is one which connects which is often seen in maps, where you basically
draw a line, where the height about sea level usually is the same.
So, here imagine that there is flat surface which is the base the rectangle and then there
is hill projecting out of this flat surface, out of this screen and coming towards your
face as if you are looking at this screen and these eclipse is that you see on this
screen these kind of circular looking things are nothing but, the contour plots.
So; that means, everything that is on this line is 70 meters or feet or inches whatever
you choose to think of it. So, this is an example of response surface are basically try to characterize in this
picture, how the variables x 1 and x 2 have an influence on the output, why which is the
hill coming out of the screen. So, it is an abstract concept, now let us take a look at what happens, when you just
play with one variable at a time and we are going to call that adaptive one factor at a time experimentation.
The idea behind this algorithm is that what I am going to do is at any given point of
time I am going to play with anyone variable. So, I am going to start with let us say x 1 and I am going try different values of x
1 and I am going make a conclusion at some point by saying at what value of x 1 did I
see the best y and I am going go with that I am go on a fix x 1 now to that value and
play with x 2. Now, sequentially do that with all the input variables until I come to a conclusion.
Now, take this example where I stop playing around with x 1 and I arbitrarily fix some
value for x 2 and it turns out that I fix the value right here. So, I put a star there, so I started with x 2 set to this value and I just started playing
with x 1. So, what is that mean when you playing with x 1; that means, you keep changing x 1 and
here we are actually just changing x 1 to different values and as you go to through different values you see different heights.
For instance, when you set x 1 to this value you seeing a height of 70.
Why? Because, this is the contour line of 70 when x 1 is equal to this value you see a height
of 80 and so you keep going through this process and until you find the highest point and the
highest point is here, because of this point you are touching 80 for instance at this point
at x 1 you are not touching 80 your some across the 75. So, you conclude that this is the best value of x 1 and then you set x 1 to this value.
So, x 1 gets set to this value and then you go about changing x 2.
So, and basically when you changing x 2 you are staying on this grey line out here and
as you keep going through x 2, you find that the point where there is a star is the peak
and you conclude that is a highest point. So, you choose to set x 1 and x 2 such that you are at star.
Clearly, what is the problem with this, your problem is that you miss the peak, there it
two problems on this, one is that you miss the peak the peak was really out here, if you will take look at this contour map in your understand the contour plot, the plus
sign is where the peak is and you erroneously concluded that the star is where the peak
is. And the reason for it is fairly simple, the way this hill is drawn it is clear that x
1 and x 2 have some interaction effects, there is x 1 is really good out here, when x 2 is
set to this value. Now, if you said x 2 to another value, let us say x 2 here the highest point of x 1 is
probably somewhere here. So, this value of x 1 winds up being highest.
So, it really what we mean by an interaction is where you conclude x 1 to get the best
output depends on where you set x 2 to. So, this is interactive effect that you can sometimes gets fooled by, there is an another
thing that we have not really discussed here, which is that few do an experiment at a given
x 1 and x 2 setting are you always going to get the exact same value and the answer is
probably not. Experimentation is typically carried out in sarcastic setting meaning that even if you
understand that your output y is some function of your input variables, in this case there
are two input variables. So, there is some mathematical function that is associated with input variables x 1 and
x 2 and the output variable y, but on top of that if you said x 1 and x 2 to specific
values are you going to get the same y. The answer is in a stochastic system you do not, because there is another factor which
is just you can call it noise, you can call it irreducible error, you can call that luck
whatever it is, it is just concept of just there being uncertainty above and beyond you
are and this kind of uncertainty is what we deal with in supervised learning is what we deal with in much of what we have discussed and in this course.
So, that aspect also can throw you off in an approach like this.
Now, this has been illustrated to you in a continuous frame work, where you are able to change x 1 continuously and see different, but in more practical settings you do not
have infinite experiments. So, you might just set x 2 to a particular value and sample x 1 at someone two or three
predetermined values, more often than not in design of experiments you will see that
we are only interested in linear effects most of the cases you are interested in linear effects.
So, you would really look at trying out each variable at two different points.
Because, where two points you can draw straight line and the two points are typically get
coded. So, what is another approach that you can do, that you can employ to overcome this problem?
The other approach you can do to overcome this problem and one of them is defined is called is a broadly called as orthogonal arrays and a specific type of an orthogonal arrays
called the full factorial which is what I show here. So, take a system where variable A can take on two states.
So, let us go back to this casting problem and let us say you are interested in variable A which is let us say the temperature of the molten metal that is poured in and so the
temperature could be something like 250 Fahrenheit and you might be interested in studying the
effects at 350 Fahrenheit. Now, I am not an expert in this I do not know those are reasonable temperatures for molten
metal I am guessing it is a little too low actually, but who knows. Now, typically what you do is when you have just these two settings, you kind of lay code
them and you called them 250 as minus 1 and 350 as plus 1. Now, you do the same thing with the second variable input variable of interest, now variable
B could be something like pressure, where you have lope or let us say the time that
you weight before you remove the cast. So, that could be 1 hour or 2 hours again I do not know those are reasonable numbers,
they to illustrate a point is to 1 hour again you call it minus 1 and 2 hour you call it
plus 1. So, what you go about doing in a full factorial is you try every combination of every variable
with every other variable. So, you try the minus 1 minus 1 setting, you try the minus 1 plus 1 setting.
So, and plus 1 and minus 1 setting and you get the picture, you essentially try every
possible combination and that is called a complete enumeration of the designed space. Because, you have discrete data points and you might choose to do some experiments at
each of these combinations of points.
In this example of shown you two replicates these are called replicates and these are both the same output variable y, but you choose to take two readings, it is actually unfair
to call it two readings, because it is not like you do the experiment just once and just use the same measuring device and just take two readings, you actually redo the experiment
and the reason you do that is because of the concept we just discussed which is you acknowledge
that your y is some function of in this case variables A and B.
But, on top of that the sum error, this error often you sought of is being Gaussian which
sum with mean 0 and standard deviation equal to sum value. But, even without going to there.
There is just some noise and you can see that even though you set A to minus 1 and B to minus 1 first time around you got 57, second time around you got 56 and you see different
levels of uncertainty. Now, the same concept extended to three variables is what shown here, on the right hand side
and you have so a, b, c three variables and I am also showing you a case where you are
not just interested in two levels, but you might be interested in three levels. So, variable A has 3 levels, variable B has 2 levels and variable C has 2 levels.
So, complete enumeration of them would be nothing different than three times, two times, two which is equal to 12 and we call the 12 treatment combinations.
So, you can have 12 treatment, 12 rows out here and again here I am choosing to take
two replicates just to get a better idea of the noise that is there in this system as
well. But, the hope is that to taking on such an approach would enable us to perform a comb
of analysis on the data which helps us understand which helps us not get strict by this Gaussian
this noise that is there on the system which can make you conclude the wrong things, if you an art where of it and at the same time also understand that they can be some interactive
effects between A and B or A and C or B and C and that is about we will be focusing on.
So, in this part we are focused little bit more and how the experiments are itself designed and this is just one way of designing experiments and this is called the full factorial design
and there are other ways of doing the same thing. In the next lecture we will be briefly talk about, how do you analyze the data that you
gets from such an experiment and we close to be talking about active learning. Thank you.
 
Introduction to Experimentation and Active Learning(contd)
Hello and welcome to our second lecture in the series, where we talk about Experimentation
and Active learning.
In the first lecture we briefly spoke about, we motivated the idea of we using active learning
experimentation or re enforcement learning in a broader data analytic setting, where
we said these are some approaches that are fairly relevant when you do not have data,
when data does not exist or the data exist and it is not enough or you have only partial
data.
In the first lecture, we also went into design of experiments and spoke about, how perhaps
an approach, where you sequentially change one variable at a time need not be the best
way to conduct an experiment.
And we also spoke about something called orthogonal arrays and specifically, there we spoke about
a form of experimentation called the full factorial design, where it is essentially
a complete enumeration of a discrete input space, where even if you have continuous input
variables you break them into 1, 2 or may be sometimes 3 discrete points.
And you essentially do a complete enumeration, which means that every variable is set to
every possible value it can be set to with relation to every other variable being set
to all their possibility.
So, all the possible points in the input space are essentially looked at and that was essentially
looking at the design meaning, what points in the input space to you choose to experiment.
In today’s lecture we will briefly take the just full factorial, which is a very basic
design and talk about some approaches that are used in analyzing such an approach, such
a design.
So, jumping into the subject in this slide, what we have is a full factorial design associated
with three input variables A, B and C. And the idea here is that what we have is a full
factorial design, because A can take on three values minus 1, 0, plus 1 and B can take on
two values and C can take on two values just minus 1 plus 1 and that gives you 12 combinations
totally and we have taken, the output variable is Y.
Now, I call them Y 1, Y 2, Y 3, Y 4, only because they are Y 1, Y 2, Y 3 and Y 4 are
replicates, so it is a same core variable.
But, essentially you conduct the experiment at, the settings for instance minus 1, minus
1, minus 1.
You conduct that experiment 4 times; again it can be a parallel effort or a sequential
effort either way.
What we mean by that is, when we say we conduct the experiment 4 times, you might have one
experimental unit and you separately conduct the experiment 4 times on it or you could
have 4 experimental units and you might, you choose to parallely try the same setting of
minus 1, minus 1, minus 1 on those four different experimental units.
But, essentially this is almost a setting, where you conduct 48 separate experiments
and in design of experiments, language that just called 48 trails or 48 runs and these
are essentially your results, these are your outputs.
And the convenience of this is sometimes you can look at this raw data or you can look
at the average Y and this is nothing but, the average for instance for the average 80.75
comes from taking the average of these four numbers.
So, each row is average and it is represented and that is the, those are the results that
we haven, this is essentially what we look to analyze.
So, what is one way that you can take these results and arrive at a conclusion of what
values A, B and C should be set to, because that is kind of the goal.
The goal is to figure out, what is said A, B and C to, I mean accurate one of the goals
can be on to what values to set A, B and C; such that you get best Y and here we are going
to treat best as the highest value.
So, how should I set value A, B and C, so I get the best Y?
One approach is called the classical analysis.
The idea behind classical analysis is to take each variable individually, each input variable
individually and ask the question, at what setting am I getting the best results.
So, A is set to minus 1, A is set to 0 and A is set to plus 1.
So, if I am ask the question, what is my Y on average when A is minus 1, what is my Y
on average when A is equal to 0, what is my Y on average when A is equal to plus 1 and
I look at these three numbers and I will choose the value of A, where my average Y is the
best.
I will similarly do that for to B and C and I will come up with a recommendation on that
basis.
So, what is that look like for this data?
It is a fairly straight forward calculation, when A is set to minus 1 you essentially get
81.81.
So, it just means you can think of it in many ways, you can think of it as the average of
these data points.
Right here, what I circled and the average of these data points or you can think it as
the average of these data points, because ultimately each row comes from an average
from that respective row and the sizes are equal.
But, you can think of it either way, either way this is the average Y when A is set to
minus 1 and that is 81.8.
And similarly you get an average for 0, you get an average for plus 1 and you would basically
say, I like A at minus 1 it is giving me the best result.
Similarly you do it for B at minus 1 and B at plus 1.
Now, when you do it for B at minus 1 and B at plus 1, how do you take the average?
It is the same principle.
So, you would for instance at B at minus 1 you would be interested in taking the average
of these points, B is minus 1 at these points, so it would ultimately be the average of these
points.
So, essentially that average would be B at minus 1 and that is 77.83.
So, you do the same process and it is clear that A at minus 1 is the best, B at minus
1 is good, because B at minus 1 is greater than B at plus 1 and C at plus 1 is good,
because that is better than C at minus 1, so that is essentially classical analysis.
And it is a fairly heuristic approach and it is like a first cart approach.
Now that is one way of going about the analysis.
Another approach is to take the best.
The take the best essentially says, I am going to look at that treatment combination, which
gave me the average highest average Y.
So, would that be at here?
So, it looks like 85.75 is the highest average Y and that setting is I believe A 1, 1, 1
and, so we would essentially go with that recommendation.
So, take the best would have selected A is equal to 1, B is equal to 1and C is equal
to plus 1.
So, here are two fairly contrasting approaches and the question you need to ask yourself
is, where would you want to use classical analysis and where would you want to use take
the best.
And to answer that question we need to answer that question in terms of, if you want to
take a one factor at a time approach, what are two reasons that experiments that particular
approach fails or for that matter, what is the two major difficulties with design of
experiments.
The two major difficulties are the following.
One is that, you could have some interactive effects between the input variables, which
means the effect that n input variable will have on the output variable really depends
on how some other input variable is set.
So, that is called an interactive effect.
The other reason is that sometimes you get fooled, because yes, Y is some function of
A, B and C from top of that, there is also some noise.
So, that noise would have given you results, which you are erroneously interpreting and
you are essentially over fitting and you are getting fooled.
Now, the question is, under what circumstances would classical analysis work and under what
circumstance would take the best work and the quick answer is, a classical analysis
essentially works really well in an environment of high error or noise.
So, when this, when the noise, which is so we said Y is equal to f of a, b and c, but
it is also got this noise component and when this noise component, although this there
could be no bias to this noise.
So, this noise could be something like it is normally distributed with mean 0 and standard
deviation, some standard deviation.
Then, if sigma e is very high and, so it is a very noisy environment, then classical analysis
would work quite well because it is averaging a lot of data points.
Now, where it move to work well is when there is lots of interactions, so you need low interaction
effects for classical analysis to work, because it just taking the average at A and average
at B. In contrast, take the best would work only
when there is very low error or noise.
It would not work well when sigma e square is high, but because it just takes the best
combination, it is almost like it does not care about the interaction.
So, high interactions work very well in its favor and, so you would you take the best
there.
Now, the reason we talked about these two heuristics is to really motivate the statistical
way of doing things, which is this dichotomy that you see between high noise versus low
noise and fitting to any shape you want versus not being able to fit any shape you want has
a lot to do with something you seen before, which is the bias variance dichotomy.
And this bias variance dichotomy is what you seeing in these two extreme approaches.
The statistical way can sometimes balance that out and, so the truth is almost any supervised
learning algorithm could be fair game, the only context is that the data set is fairly
small.
But, any data set, any supervised learning algorithm that is not require a very large
data set that requires a very small data set, but they still give you meaningful results
could be a fair game.
And in that regard step wise regression is usually popular in analyzing designed experiments.
You start with the basic model that Y is equal to linear function of all the inputs a, b
and c and then, you also try to incorporate two way interactions in the form of saying
a times b, a times c, b times c and you could even have a three way interaction a times
b times c.
And, because your variables are coded to minus 1 and plus 1, just multiplying a and b as
an input can have a meaningful interpretation.
Now, one thing that can be said about this process of design of experiments is the way
we have done it with full factorials and the way we explained it makes it essentially a
one shot approach.
It basically means you decided even before you see any results the entire set of points
that you want you look at the input space.
So, if you look at this you already decided, so each treatment combination is a point in
the input space.
We already decided the all the points and how many times you want to look query each
of these points even before you look at even one result.
So, it is ideal in some sense for a parallel deployment, but if you could do the sequentially
is there a better way of doing it, which is can you react to the data your seeing to say
in want to query this point more, because I am less certain about this point or variable
verses saying o I am very confident about something else.
So, I do not want to waste my resources in conducting experiments in a particular place
and that is primarily the motivation of sequential experimentation this idea that you can you
can conduct experiment sequentially and that gives you an opportunity to react to the data.
Now, in many ways while sequential experimentation has been an effort from the statistics community
active learning essentially is this same core idea and it is been motivated more often the
machine learning community and the context in which, the problems are applied to do sometimes
differ as the result.
Active learning is often seen as a semi supervised learning approach and understandably it is
also called is optimal experimental design.
Because, active learning is a process where the system sequentially chooses to query the
design space and therefore, be able to build knowledge, on what we see and how it can be
better understood.
The key different, especially in a in the typical context of application except while
design of experiments often focuses a lot on the sole idea of starting with 0 data often
active learning will start with this notion that there is some amount of data and it is
not enough and you might want to sequentially query the system to improve upon your understanding
of the system itself.
And, so in many ways it is kind of seen as semi supervised learning, because there is
an abundance of unsupervised learning data just means there is an abundance of states
of the input space and you sequentially get you choose points in the input state for,
which you really want to get answers.
And therefore, get outputs for and by doing that the whole idea is that you can get away
with much less data on the output space and you can still come up with predictions that
are meaningful.
So, what are some prominence strategies in the whole active learning frame work is that
in a sense all of these strategies at we going to talk about now, rely on one thing.
It relies on you know evaluating the, how informative different points on the input
space can be if you query them and you got an answer, what you mean by queried is you
choose a particular point in the input space and say can you please give me an output for
that.
And that, now goes into your data set, where you can apply some kind of supervised learning
approach to make sense of, what you have.
And the strategies that are involved with active learning can be broadly classified
in these four and you know this these is an area of you know where there is a active research
and, so there they might be some strategies that also follow fall that are new that might
fall out it these have been historically the more popular ones.
So, let us take look at the first one which, is uncertainty sampling, now this is perhaps
some more simplest and also therefore, very fairly common frame work.
And in this frame work essentially the active learner chooses to query instances, which
it is least certain about, how to classify or how to predict.
So, if it is a classification problem it says I am going to ask questions about I am going
to choose points on which, I want answers on which, I want you to give me an output
and I am going to choose points, where I am least certain currently given the information
I already have I am least certain about if it is a classification problem 0 and 1.
I am least certain about whether to classify it as 0 or 1 in those instances I really want
you to I want to conduct my experiment in that point, where I am least certain.
Now, if you can also think of this as regression problem, where at a certain point in the input
space you might have a prediction, but that prediction is not cast and stone some kind
confidence bound.
You have some kind of bounds around that predication is some amount of uncertainty associated with
particular predicted value and you might choose to pick the point where your uncertainty is
the highest.
Another approach also fairly popular one is this idea of query by committee, let me just
mark, where we are we finished this and the second is query by committee.
Query by committee approach essentially involves having committee of different models, which
are all trained on the current data set and when we say data here we are talking about
the data set for which, we have the outputs you have a data set with the outputs in those.
So, you have a some kind of supervised learner, which is capable of interpreting the data,
but you might have a committee of models, which are all trained on the current data
set, but they might represent competing hypothesis.
Now, each of these members is now, allowed to vote and you basically choose to take you
choose to get a data point from the input space, where the committee members have the
most disagreement.
So, think of it you know one way to one example that I always kind about liked about this
is it really maps on to lot of ensemble methods something that we saw earlier in the course.
So, if you had a set of methods set of supervised learning approaches all trying to predict
the same thing.
So, think of it as a random forest, where we have multiple trees trying to predict same
thing.
So, for a given input vector each tree does not going to come up with the exact same class
predication or it might not come up with exact same predication even if it is the output
is continues variable.
So, this method simply says let us go with let us go get an answer for a data point,
where the trees disagree most about what to classify it or what where are the highest
variance in the predicated value of the models.
The next approach is expected model change the idea behind expected model change is to
see if we knew the label of a particular data point.
Then, which label of the input space if we knew would contribute to the greatest change
in the current model we have based on the current data we have of the inputs and outputs.
So, we have some data on the inputs and outputs and you basically extrapolate to this to the
broader question of asking the question saying you could get data of you could get an output
for any point in the input space, which point would essentially lead to you making the largest
change in the model.
And the idea is to kind of query that that point very specifically the last two sets,
which is the expected error reduction and various reduction use the following approaches
the approach is to basically say with error reduction the idea is there some deviation
of the predication verses actual.
So, you can think of it as essentially the residuals in a regression case and you know
in other in every other case other it is bias or variances for whatever reason you are unable
to predict you are unable to predict the exact value at a particular location.
The question we need to ask our self is and answer to which, point in the input design
space could lead to the largest expected error reduction.
In that sense its it is fairly close to a uncertainty sampling, but it is not just uncertainty
sampling is the where it really differs from uncertainty sampling is, uncertainly sampling
ask this question about each data point in the input design space with respect to that
data points.
So, I go to one data point in the design space and ask the question saying, how much uncertainty
there in that point.
In terms my predication at that point, where as expected error reduction does not talk
about a single point it talks about, how if I got and answer to a question at any particular
points.
So, I go to a particular point to the input design space and I query the oracle and I
get an output.
Now, that output if I now, refit my supervised learning with this extra data point there
is going to be overall reduction in my residuals across the boat, because this new data point
could change the entire regression line fit.
Now, this new regression line fit will create new residuals and, so I am looking at the
overall reduction in error between the data points and the fitted model.
And, so I choose to query that data points in the input space and get an output for that
data point, which lead to an overall reduction in error.
Now, the variance reduction approach is a deviation from that it is a deviation in that
it is you do not look at the error between the fitted model and the data points, But
you just look to see reduce the overall variance in the output space.
Now, that is done probably because it is much easier to do this, but what the core idea
here is that you have some variance associated with the outputs.
And you can still continue to reduce generalization error indirectly by minimizing output variance
and that also can sometimes you do that because it is mathematically a easier you can kind
of get close forms solution an and that is essentially the core idea.
So, I hope this gives you some feel for experimentation and the whole idea of active learning and
at least some strategies that we could use an experimentation of active learning.
Thank you.
