
Short introduction of regression 

Hello and welcome to our class that today on Regression. Today’s class is going to 

be a very short introductory level treatment of the topic regression analysis and the idea 

here was to a kind of position just towards the end of your modules and descriptive in inferential statistics, so that you are at point by you know enough to appreciate, how 

regression uses these concepts. But, we will also be revisiting regression 

down the road, where we will actually talk about the mechanics about, how you implement it and that is something we will do, right after you get an introduction to machine learning 

and more specifically to supervised learning. So, jumping into, what a regression is. Essentially a regression analysis and I say regression 

analysis here, because the word regression itself in different fields it is used differently 

and in fact, even in statistics there is a concept called regression towards a mean, 

which has a lot to do with regression analysis, but it is also quite a different from, how we understand, how we use regression analysis today. 

So, again the idea is to you know it use the word regression analysis. So, regression analysis 

essentially is the study of relationships between variables, more specifically it looks 

at understanding the relationship between 1 or more input variables and their relationship 

to an output variable. And the exact nomenclature is sometimes different, we use the words I 

am using the words input and output, you might also come across the term response to describe 

the output or dependent variable to describe the output and for the input variable you 

might come across the terms explanatory variable or independent variable for the input. 

So, the idea is to study this relationship between some set of inputs or one input variable 

to this output variable that you have in mind. And, so right from the minimum you are looking 

at two variables and the whole idea is, have you come across this kind of studying relationships 

between variables, so what is regression doing newly. And the idea is yes, you have, you 

studied, when we started looking for instance two sample tests, two sample t tests for instance. 

Right there, you for the first time you encountered scenarios, where there were two variables 

involved to jog your memory. An example of the two sample t test that we looked at was, for instance blood pressure and the idea of the average blood pressure was lower when 

given calcium supplements versus when given placebos. Placebos are just sugar pills that 

kind of it look like, the original medicine. So, the idea was there were two variables involved; there was your notion over output variable or response variable, which was your 

blood pressure. It is a quantitative variable and you had another variable and this other 

variable was, you can call it calcium. And this other variable calcium, it is a variable, 

because it can take on two states. It can take on the state yes, meaning you have given the actual calcium supplement or you can take on the state no, which I am using as proxy 

to mean that you were given instead the placebo. So, you now have two variables and you can 

think of your, this variable calcium is often called the treatment. But, you can think of 

it as the input variable, which takes on two possible states and you are looking at essentially that to see, if there is any difference in blood pleasure for people, who given calcium 

versus people, who have given placebo. Right there, you are looking at a relationship between 

two variables, you are looking at the relationship between the variable calcium and the variable 

blood pressure. Now, another example that we saw on the two sample case was boys and girls in 10th standard in public schools to see, if the average height 

of boys was equal to the average height of girls. Again, here your output variable is 

the height and your input variable is gender, whether you are a boy or a girl. You are looking to see, if there is a relationship between gender and height, yes the answer is; obviously, 

there is, but may be in 10th standard is one higher than the other in public schools. 

So, you went even beyond that, for instance with something like an ANOVA, Analysis of 

Variances that we discussed in the last class. You go beyond just having two categories of your input variable, you can have multiple categories. We saw the example, where we looked 

at states as the input variable, we looked at height of 10th standards, to make it simple 

I just say boys. But, you looked at the heights of Tamil Nadu, Karnataka, Maharashtra, there you looking at the relationship between state as your input variable and height as the output 

variable. Is there any relationship? Is there any difference between boys of different states and their heights? And finally, we also looked at something like 

test of independents, where here you were looking at two categorical variables and you 

were looking to see looking to see there was a relationship between them. But, so the reason 

for going into all of these and kind of revising some of the topics is to say that, you have studied the relationship between variables. But, for the first time you are going to go 

beyond dealing only with categorical variables. Regression analysis is especially powerful, 

because it creates a relationship between two variables, but this relationship can, 

both these variables can be continuous and quantitative and therefore, the scope of application 

for regression really becomes much greater. So, to give you an extension for instance 

of some of the previous examples we have looked at from the top of my head, one thing I can think of is we will looking at the relationship between blood pressure and you would given 

calcium plus or placebo, so there is just two possible states. Now, what would happen 

if I gave, if I had 20 different types of terms, which started on one end from the placebo, 

which had 0 calcium and went all the way to the other end, which has 100 percent calcium. 

And in between, the 20 different types of pills, I am not saying there are 20 pills there are 20 different types of pills. The different types of pulse had varying degrees 

of, varying percentages of calcium. So, now, you have at least 20 states and those number 

of states can, if especially if we are randomly sampling, that can be infinite number of states, 

but you just have a range. So, it is a continuous quantitative variable between 0 to 100 percent of calcium and your output variables still remains your blood pleasure. 

So, you can actually create a relationship between these two variables, it can be a mathematical 

relationship, it can be a graphical relationship, but it goes beyond just 2 states or 3 states 

or n states. It has the potential to go to infinite number of states of the input variable 

or variables and, so it can describe you know, richer things. The idea again behind regression 

analysis is to not just unlike most of what we studied with inferential statistics. It 

is, the goal is not just to establish that there is a relationship, but it is to quantify 

that relationship as well. So, a lot of emphasis in a regression is given 

to the model itself, that mathematically equation that describes this relationship between the 

input and the output variable and that can be either linear on non-linear relationship. 

That, the relationship between amount the calcium that is given and the drop in blood 

pressure if there is 1 can be a linear or non-linear relationship and you can extend this to all the other examples. Now, as with most supervised learning techniques and that 

is something that we briefly discussed in the course overview. 

But, it is also something that you will hear a lot more often the upcoming lecture. The 

goal of a regression can be two fold, it can be towards prediction, the idea that you now 

create this linear relationship. And, so I can now use it to predict, what the drop on 

blood pressure would be for a given amount of calcium supplement and that particular 

amount of calcium supplement might not have even been something that was given in the 

original data set from which I built the line. But, the fact that I used this data to create 

this relationship in the form of a mathematical equation, which represents a line, which represents 

the relationship between calcium supplements, the amount of calcium given to the drop and blood pressure allows me. This, the fact that I have created this allows me to use this 

tomorrow to predict, how much the drop and blood pressure would be. If someone came and 

told me, I am thinking of giving this amount of calcium supplement, it allows me to make 

predictions. The other thing that it also does is, given that you are now able to create a mathematical formulation, it gives you an understanding 

of the word. It gives you an understanding of, how far how much extra calcium that I 

gave, how much of a blood pressure drop will I see. So, it is an understanding of how these 

two variables are actually related and that is, what we essentially call as one prediction, 

which is one goal, which is the kind of predict and the other is interpretations, which is the kind of interpret the world that we level to through the lens of this linear relationship. 

So, let us just talk about just to give you some feel for where a regression analysis 

is useful of what kinds of contexts. Here are some examples that we have listed. For 

instance, so how do wages, how do salaries of employees get affected with variables, 

such as experience, education, promotions, etc. So, the idea here is that salary is the 

dependent variable, it is the output variable, it is the response variable and variable such 

as experience, which might be measured in number of years, education again measured in terms of number of years after high schools or something and promotions, again a numerical 

value, how do these variables affect the output variable. 

Another example is how does the current price of a stock or a share depend on it is past 

values and perhaps, also on values of the market indices or other stocks. A subtle difference 

here that you might notice and we will circle back on this is, in the case of the stock 

for instance you will notice that the output variable is the price of the stock, but the 

input variable is also the price of the same stock on previous days. So, how do it is current 

price depend on it is past values? So, here the past values become the input variable and the current value becomes the output variable. So, we call that time series and we briefly 

revisit that concept as well, but regressions are also used in that context. So, the next 

example is, how does sales revenue get affected as functions of advertising expenses and comparative 

advertisements. So, the more money through into advertising you might have belief that 

the sales goes up, but what is their exact relationship. Can I look at past data on different 

advertisements that have different campaigns, advertising campaigns that are made for different 

products and what my competitors did and how that affected my sales revenue and create 

this relationship? And also something else from the mechanical engineering, how would be relationship between speed of a car fuel efficiency within a certain 

range, when you are driving on a certain gear, what is a relationship between the speed, 

which you drive in a fuel efficiency and you could take a sample of 10 cars, make them drive at different speed or you know, take the same car, make it drive it at a certain 

speed and get data and try to fit a line in this data to understand the relationship. 

And finally, another example could be how does the price of a house get affected by the number of bedrooms, the square footage of the house, you know it distance from the 

center of the city and so on and so forth. Again some of these input variables need not neatly fit into being continuous quantitative variables, but the idea is that a regression 

is not constrained by that. A typical application of regression would use continuous variables, 

but the whole selling point associated with a regression is that, it is a technique that 

also capable of dealing with continuous quantitative variables, whereas a typical ANOVA or a t 

test or any of the tests that you seen so far are built around the idea of using a categorical 

input variable and that is, what they meant to do. So, let us talk about some more concepts associated with regression. Just for reference, I have 

shown you a typical graph, graphical representation of regression, where the x axis is your explanatory 

variable, your y axis is your response or output variable. And each square is a data 

point, by square I mean each of these small points, it is a data point and the idea is 

loosely the idea is to kind of fit a line through this data point and that is what we would call linear regression. If you are trying to fit a line through the data point, we would 

call it linear regression. The word actually linear regression could also be used, when you are doing a linear combination of variables. But, some of these 

variables themselves represent a non-linear transformation. I think, the simple way to 

put this is whether you are looking to have a linear relationship between the input and 

output variables or a non-linear relationship. You could still use the core concept of regression, 

but you should be a little careful in terms of what gets called linear and what gets called non-linear. Because, often the camp of linear regression 

could include variables, the input variables themselves which are non-linear transformations 

and, so it would still be called a linear regression, but it will not exactly look like the graph that you are looking at. You also have the categorization of simple versus multiple 

regression. The idea behind simple regression just means that there is one input variable, 

whereas in multiple regression you have more than one input variables. So; obviously, in the graph that you see in the slide, you looking at a simple regression 

because there is one explanatory variable, which is the x axis, if you have more than one that becomes hard to represent on a two dimensional slide, you would have to use a 

3D model for two variables and then, after that it becomes much harder. But, the idea 

is that if you have one input variable it is simple regression, if you have multiple input variables more than one input variable it is multiple regression. 

We then come to the next concept that we briefly discuss which is cross sectional versus time series. The idea behind cross sectional is that, it is not a function of time. Your data 

is collected across the board and that temporal, the time component of basically means that 

it is not either function of a time or it is not a function of it is previous values. So, you can put all the variables in the basket and think that they were all created at the 

same time or at least that you do not care, whether they were created a different points of time. You are still going to treat them it is just different data points and look 

at your relationship and that would be cross section, whereas time series is the idea that previous values in time affects subsequent values. 

So, your modeling techniques themselves become a little different and classic example of 

the time series was the stock example that we spoke in the previous slide, where previous 

price of the share influence tomorrows price, where as the equivalent of the cross section 

analysis of that would be to completely ignore the stock price of this to completely ignore 

the previous days or previous time periods stock price and just say can I predict what this stock is this stock is value is going to based on other stocks or other indices. 

So, can I use the, NIFTY index can I use another stock to kind of say based on the stock this 

is what the stock should be this is what the output the response variable stock value should 

be. Next we just come to the idea that this is notion of response variable or it is called 

dependent variable and the input variable is called explanatory variable or independent this is terminology that you will keep hearing and, so it is kind of important. 

So, the next important point is that we learnt of about something called scatter plots during 

the descriptive statistics phase and regression analysis in many ways essentially captures 

mathematically, what does scatter plot tries do graphically. So, scatter plot tries to 

graphically show you that relationship between two quantitative variables, where is regression 

analysis tries to go beyond that and tried to actually fit a line to this or fit a model 

to be more general fit a functional from to this data and therefor that is the improvement 

on scatter plot that regression analysis does. But, often even before getting into regression 

analysis scatter plot could be very useful graphical window into what you should expect 

to even see if you should try to fit a linear equation or non-linear equation, because just 

visually you might be able to say you know this does not make sense to fit a linear equation, 

because that is not the co relationship. Another big advantages scatter plot is that if there 

these outliers in by outliers we mean essentially there are data points, which look like they 

are essentially just errors or something. So, you let us say you had this data point 

that was in some where completely unrelated that could come from an error in that it completely 

ruin in a regression analysis along, but if you see it in a scatter plot you might choose this say I want to ignore this data point that is idea a behind scatter plots and outliers. 

The next concept is an unequal variance we going to talk about this little bit, but the 

idea could be that they could be a very strong linear relationship or non-linear relationship. 

But, the variability at different points of x or your input could be different you could 

have something that looked like this in terms of the data. So, this is the line that your 

fitting, so there is this central line is still the line that you are fitting and it is a linear line. But, the data points in the lower end, so in this side of x the data 

point a closer to the line, where as once you go to this side of x once you go to the 

right side of data points of far more spread out. And that influences, how the line gets created in there is in techniques that available to 

kind of counter this problem it is a problems, because again visually you my just see this speaker phone kind of shape and say still I am just going of fit a line in the center, 

but this higher variability on one side and lower variability kind of makes does not always 

result in the correct line being fit and you need some kind of transformation to understand 

the relationship better. So, the last concept associated with the slide 

in what I want to talk you about is with co relationship. So, co relationship you can 

think of is a more advanced form of summary of descriptive statistics we didn’t speak about in detail or in the descriptive statistics part. But, the idea is that when you have 

two variables just like a scatter plot can be used to graphically describe these two 

variables, correlations is a single number that describes that a linear relationship. 

And its essentially a quantitative indicator of that linear relationship, what you will discover is that in a regression analysis you will also be coming up with certain numbers 

that quantify this linear relationship and if you have not already heard if this you 

will come across this something called R square and that has a direct quantitative mapping 

correlations. So, it is not a new concept essentially R square is nothing but, correlation 

square, but you will come across it under the term R square, where as you might of heard 

is of the word correlations more colloquially used or used more in the concept context of 

descriptive statistics. Now, let us just briefly talk about the exact roll of descriptive and inferential statistics. 

If we start first with descriptive statistics the whole idea there was either graphically 

or to quantitatively summarize the data that you see. And one way of summarizing that one 

essentially summarizing that you do of the data in regression is to capture the linear 

relationship, which is there in the form of the actual data into line. And, so you essentially summarize this relationship in the form of an equation and many of you 

might know that if there is if the simple line in two dimensions you have this formula 

y is equal to m x plus c you might of come across that in high school in some form or the other, where c essentially represent the intercept of the line on the y axis and m 

represents the slope. So, you can think of ms that angel m represents 

that slope and you can in regression often we use the terms beta and here I am just using 

b not and b 1, because I am representing the sample based of the sample that we have we 

have some estimate of b not and b one. But, the idea is that this is some form of summarizing 

the data, because you taking the data and summarizing it to the single equation. 

So, what is the concept between behind, how you do it and the idea is there is some form 

of optimization it is the form of optimization, because you have a set of y s and x s. So, 

you are given a series y s and x s and that is your data and for a each y and x you essentially 

plot it on this graph you than go had an you take line and this is that line that we are 

going to play with and you choose some criteria to fit the line through the data points. 

So, you might say that my goal is to fit a line through the data points and you know 

I do not want to draw a line out here that has nothing to do this data. So, what kind of a line as something to do the data I am its say well I want the line to split the 

number of data points equally above and the below it. So, this will do be overlap the, 

but we get the point, which is that you might want you might want that line to split the data points another idea could be its say I want the line to go through the two extreme 

points. So, there are two extreme points I want two extreme points to be connected by a line and you can have various other criteria and another 

very common one could be to say I more advance one could be to say I want to minimize the 

distance between each point to the line and that incidentally happens to be what we call 

is the ordinary least squares. But, we will get into that later, but that could be one criteria, another criteria could very well either I want to minimize the perpendicular 

distance to each points to the line. So, you could have various criteria and you 

know you can choose some criteria and you can say I want to achieve that criteria and 

that is how a want to fit a line. And, so at the root of doing that is the series of 

techniques and optimization where you say I want to do some form of optimizations. So, 

you might ask the question, what are you relay optimizing and the answer is your really optimizing 

this objective of minimizing some metric and let us stick to the metric of minimizing this 

distance this sum of this distance. So, how are you optimizing that, what it can 

you change what you can change is this line and what I am going do is I am going to change 

the two parameters associated with this line and the two parameters are m and c. So, c 

is, where this line intersects on the y axis, so I am going to try and move this line above 

and below keeping that is slope this same. And try to see, where should this line b such 

that I minimize the sum of these distances those red lines such you see my goal is to 

minimize the sum of these distances. And that is an optimization to see that is my objective function my objectives is the minimize the some of these distances. And 

I am going do that by changing two variables I am going to change the variable c by moving 

this line above and below and I am going to change the variable m by rotating this line, 

so by rotating this line, so this way and this way. So, simultaneously I am trying to 

change two variables and thereby find that line by changing my m and c I find that line, 

which minimizes this deviation the deviation of each data point to that line. 

And there are different ways of doing there I am just giving you one objective that am 

I choose to optimize. But, at the root of it you need to realize that ultimately the 

process of regression fitting a regression line is process of optimizing and you might 

different criteria ordinary least squares, which is one type of regression uses the criteria 

of taking this distance of each point to the line and squaring it and trying to minimize 

the some of those squares. Because, each point will have some distance to the line and then, you can take that distance square it and then, you can take go to the 

next point in do this same and then, you can sum all those squares and least squares tries to minimize that objective function. But, that need not be the only objective function 

there can be many other objective functions, but at the root of it the idea behind putting 

line through many data points is to say I am interested in maximizing or minimizing 

some goal associated with the process of fitting the line. And I am going to do that by changing the two variables set I can, which is m and c 

or in other words you are b not and b 1. So, that is essentially the summary statistics 

the process of describing the data through line and that is how that line gets created. 

Now, where is the concept of inference the concept of inference again comes from the 

score idea there ultimately these data points are samples they do not represent the population. 

So, any b naught and b 1 that you calculate are coming from this samples, so there essentially 

the sample statistics. So, but just like x bar in some sense is the sample mean, which 

is a point estimate of mu, which is the population mean these this b naught and b 1 essentially 

represent beta naught and beta 1. So, you can you essentially have the population parameters 

beta naught and beta 1 yes these are the population parameters let us just beta naught in beta 

1. Now, the idea is that, which statistically 

inference what you doing is your trying to see if either of these terms beta naught or 

beta 1 are actually equal to 0. Because, if they are then, they do not have any business 

in being a part of this equation this equation that you have out here now the this equation 

that you have out here that is an equation that you started with before you even fit anything. Now, you a just in the process of finding out to b naught and b 1 through some 

optimization procedure, now imagine a situation, where x has nothing to do with y, x has nothing 

to do with y. Now, you calculate the sample of x s in the sample of in y s if you had infinite number of sample you might arrive at the conclusion 

that b 1 is thoroughly equal to 0 therefore, this terms itself becomes 0 on should not 

be there in this equation. But, you still only have a sample and the sample is 5 data 

points 10 data points 20 data points. So, it is perfectly possible that even though true beta 1 is equal to 0 that is the null hypothesis even if that is true you it is 

possible that you just take a sample of y comma x s and you get some beta b 1, which 

is not equal to 0. So, b 1 winds up not being equal to 0 even 

though beta 1 is equal to 0. So, you might be erroneously thinking that x has this relationship 

to y, where is in reality beta 1 is 0. So, the whole idea is to do a statistical test 

to see to test the hypothesis the beta 1 is 0. So, the null hypothesis would be that beta not is 0 and you can think of it as also beta 1 is 0 and it turns out that under certain 

assumptions of normality and even when those assumptions are violated to some extents central 

limit theorem it terms out that the distributions of the betas of these coefficients winds up 

being t distribution, so like t distribution. And, so you can use essentially t test to 

test the hypothesis that each of these coefficients are actually could the 0 not and the idea 

is that if you wind up you need to reject the null hypothesis and reject the hypothesis 

that beta 1 is equal to 0 and only then can you actually have the term in the model and 

this become really useful when you have you know multiple input variables. So, you might have something that is beta naught plus beta 1 x 1 plus beta 2 x 2 and so on, and it can 

go on you can have many variables. So, which of these terms actually get to stay 

in the modeling, which do not is not just the function of this magnitude of beta 1 and 

beta 2, because that is the magnitude becomes really function of it becomes also a function 

as magnitude x 1 x 2 in the units and so on. But, what really determine whether these things 

stay in the equation or not is the inferential statistic test that you do for each of their 

population parameters, which is for beta 1 for beta 2 and so on. And in, so for is you 

can reject the null hypothesis that these are equal to 0, then you can leave them in 

the model. But, if you can if you if you cannot the reject the null hypothesis than these do not these terms do not have any business being in the 

model. So, that is the idea behind doing statistical inference on the individual parameters you 

will also notice there in a typical regression analysis or regression analysis output independent 

of the software that you use there is you will find an inference for the overall model. 

So, in a side from the inference such you see for beta 1 beta 2 separately. 

For given model you will have an inference and that inference again uses the concept 

of an ANOVA and the way it does that is by looking at if you remember the ANOVA in the 

past you have the concept of mean squares between and mean squares error, where the mean square between was trying to quantify the effect of each treatment or each state 

of the input variable and mean square error was trying to quantify the inherent noise that was there even with in each state. Here; however, you do not have finite number 

of means square between, because they could be infinite states of the input variable x. 

So, you have a single term that tries to capture how much of the variability is being captured 

by this linear or non-linear model that you built, how much of variation in x is being 

captured by this equation, how much of the variation in, how much of the variation in 

y is being captured by this model versus how much of the variation in the data points is 

not captured by the model. So, 1 becomes the mean square of the model 

essentially and the others becomes the mean square error, which is just the general, which is some notion of general noise that is there in the system and essentially the ANOVA out 

here. Again will be using the F test the same ways use the F test when we were describing 

the ANOVA in the previous class for categorical variables. And there by rejecting that null 

hypothesis would mean that this model explains variability in y, where as if you fail to 

reject it the idea is that you cannot say that this model essentially you cannot say 

that this model is any different than a few word to just randomly choose y s completely 

unaware of what the model was. So, no variability in y is being explained 

by different points in x. So, an essentially when you think of that nothing but, the concept 

of straight line it’s essentially concept of something were not describing the variability 

in y at different points of x. So, various points of x you looking at the same position 

same position of y I hope that made that gave you an introduction to the concept of regression 

and like I mentioned after we introduce the idea machine learning and supervised learning we will be revisiting regression to look at more end up treatment of how you actually 

do it and give you an understanding for how the co derivation are made. Thank you. 

English - NPTEL Official 

   

 

Introduction to machine learning 

Welcome to this module on Machine Learning. 

So, till now we have mostly looked at data analysis. 

So, all the tools and techniques that we have looked at have to do with analyzing data and 

trying to understand the data better with possibly the exception of regression. 

From now on for the rest of the course, we will be looking at how you can infer models 

about the process that generated the data by looking at the data alone. 

This is essentially the idea behind machine learning. 

So, what we call, even though it is a kind of a fancy name when you have a visions of 

Robots and terminated suiting around in your head, but machine learning is essentially 

trying to learn models about the process that generated the data from the data itself. 

So, you could look at examples where you learn about, how the rainfall pattern varies over 

a season. 

So, you could say that rainfall pattern is very intermittent over the season or you could 

look at, how the temperature of certain equipment is varying with time of operation. 

You could say that the temperature showing a linear growth, it is possible to do this 

kind of learning from the data by a machine and this kind of machine learning algorithms 

becomes essential, when we move away from data analysis into either predictive, descriptive 

or prescriptive analytics. 

For example, I can ask what will be the temperature in 15 minutes of a particular equipment know 

and if I know that the pattern is that of a linear growth, I should be able to tell 

you what the temperature would be in 15 minutes and likewise, I can ask you what are the areas 

that are likely to receive rain in the next season. 

Then, if I know what the variation has been seasonally with the data, then I should be 

able to tell you, what are the areas that will likely to receive rain in the next year 

and so, when I talk about prescriptive analytics, so the first two questions I was asking you 

or more about questions on the system and prescriptive analytics, you will be asking 

questions about what I should do in response to the patterns that you are describing. 

For example, I can find out patterns in stocked data and I could ask a question, should I 

invest in this stock or not. 

For all of these kinds of analytics, so we need to have a technique tools and techniques 

from machine learning. 

So, what exactly is machine learning? 

So, I will fall back on this old definition from Tom Mitchell. 

So, Tom Mitchell said an agent is said to learn from experience with respect to some 

class of tasks and the performance measure P, if the learner’s performance at tasks 

as measured by P improves with experience. 

There is lot of qualifications here, in the first case he did not say a machine he just 

said an agent. 

So, in Tom Mitchells opinion, this applies to all learning agents, it could be humans, 

animals or machines. 

So, an agent is said to learn from experience, so you have to have experience in trying to 

solve something and now, you measure this experience with respect to some class of tasks. 

I mean, it is not that you can learn every things you have to be very specific about, 

what is said that you are trying to learn and the performance measure P. So, you need 

to know how well you are doing in that particular task that you are learning about it. 

And then, if you are said to learn if your performance as measured by P keeps improving 

with experience the very, very inclusive definition of learning. 

So, you have to be very careful when you use this, because you could even apply it to desired 

scenarios. 

For example, you could think of a slipper, a new slipper that becomes more comfortable 

as you keep wearing it. 

So, you cannot really say that the slipper is learning to fit your feet with experience. 

So, you have to be careful about, how you apply this definition, I mean it is a fairly 

serviceable definition as we will see as we go long. 

So, the rest of the course we will be looking at three different machine learning paradigms. 

So, the first one is called supervised learning, where you expected to learn a mapping from 

certain input variables to output variables. 

So, we already looked at one example of such a supervised task, when you looked at regression, 

so where the output variables was a continuous valued variable. 

So, and then the input was described by a set of attributes and if the output is a categorical 

output, where it could be one of many classes. 

So, you see, is the patient sick or is it, see he is not sick, will the customer default 

on the payment or will they not default on the payment. 

So, these are like categorical attributes, both these examples, where the output could 

be either 0 or 1, you could think of outputs with multiple such levels. 

In such cases, the learning problem is called the classification problem. 

So, but what distinguishes supervised learning from the other forms of learning is that, 

in the form of experience that you will get. 

So, whenever I give you an input, the sample input I will always have an expected output 

for this input. 

So, I am going to, I will give you a set of samples which consists of an input vector 

and an expected output for that and that is what makes you supervised learning. 

In unsupervised learning, the goal here is to discover patterns in the data that need 

not necessarily be any output that I am trying to produce. 

So, there are many different unsupervised learning problems, we will be particularly 

looking at two in this course. 

One of them is called clustering, but the idea is to find cohesive grouping among the 

data points that are given to you, so in order to find any patterns that are occurring. 

So, will see how this works in a little while, so but you can readily think of the rainfall 

task that I was talking about earlier. 

So, you can group regions that somehow are similar in their rainfall behavior. 

And the second unsupervised learning task is known as the association mining or association 

rule mining sometimes and the goal here is primarily to find the data points that occur 

together or co occur frequently. 

So, in association rule mining you essentially you are trying to figure out, which data is 

associated with which other data. 

So, how it is which co occur frequently. 

So, in both of these cases, as you can see there is no real output that you are expected 

to produce except to find the patterns on the data. 

And the third class of machine learning problems which are called the reinforcement learning, 

essentially has to do with learning how you would control a system. 

We will talk more about it towards the end of the course, but roughly you can think of 

the following problem. 

So, how did you learn to cycle? 

So, it is not just discovering patterns, there is nobody giving you an input output pairs, 

that tell you how to cycle and somebody actually does give you an input output pair, then probably 

not learn to cycle. 

If your cycle is tilting by 30 degrees to the horizontal, then you should push down 

with your right foot with so many Newton’s of pressure. 

If somebody gives you directions like that, you will never going to cycle. 

So, you have to do some kind of style and error learning. 

So, that is essentially what reinforcement learning talks about. 

So, you will do a little bit of this towards the end of the course. 

So, the different machine learning tasks that we are talking about, so classification, regression, 

which are essentially supervised learning problems and you remember, I told you that 

you really need a measure by which you are going to decide whether the algorithm is performing 

well or not and the measure in the case of classification and regression is just going 

to be error. 

In the case of classification, it will be the classification error that is the how many 

mistakes you make in predicting the categorical outputs and in the case of regression, it 

is going to be the prediction error which is, how far away you are from the actual value 

that you need to predict. 

In the case of unsupervised learning problems, it is a little tricky as to what the measures 

should be and there are, let say when we look at clustering and association rules, we will 

talk about many such measures in detail. 

But, roughly, in clustering one of the measures is how tight your clusters are or how scattered 

they are and I am talking very roughly here, but we will formulize as we go long. 

And in the case of associations, it is more on how confident you are that these two items 

are associated and what is the fraction of the population in which these associations 

appear. 

So, that is what we mean by support and confidence. 

And again as I said, this is just to give you an idea that for every task you are going 

to have an associated measure and we will elaborate on the actual measures as we go 

along. 

So, having said this, there are many challenges that we need to address. 

So, one, the first challenge in any machine learning problem is to figuring out, how good 

is your model. 

If somebody gives you a machine learning algorithm and say, there, so here is a model that has 

been learnt by the algorithm, how do you decide how good that model is. 

So, you could use the measures that I showed you on the previous slide, but that could 

be other ways of deciding, how would the model is. 

So, I will be elaborate on this as we go long. 

For example, you can build a very, very, very, very detailed model that gives you almost 

zero error on all the data that is given to you at the beginning. 

But, then this model might essentially be useless, when it starts looking at unseen 

data. 

When I actually wanted to make predictions on data that I have not seen before, this 

model might not perform well at all. 

So, how do I look at that kind of a trade off? 

So, there are different ways of measuring, how good a model is and then, given the data 

and given the class of models, here we are going to look at how do I choose the right 

model. 

So, that is the second challenge and many different machine learning algorithms are 

all about answering this, the choice of the model question, but then, it is not all about 

modeling. 

So, you have to be very, very cognizant of the data that you are operating with. 

So, the first question that plagues all of us is, do I have enough data. 

That might surprised some of you, who have heard of terms like big data and when having 

excess of data, data delusions, so on and so forth. 

But, then getting in a supervised learning problem, getting label data is incredibly 

hard and so, you have to have an expert that this looking at all the data and then labeling 

them for you. 

So, getting such labeled data is incredibly hard and so, do you have sufficient labeled 

data or do I, can I make use of unlabeled data in a clever way. 

So, these are all kinds of question that you will have to think about. 

Is the data of sufficient quality that could be errors in the data? 

For example, age could be recorded as 225 or there could be noise in very low resolution 

images, that you are feeding your algorithm and so, the algorithm is not able to make 

out, what is that in the image or it could be that some values are missing or not been 

recorded in your data and then, your algorithm has to deal with that. 

So, it is a very important question that the data has sufficient quality and in any almost, 

in every large scale, real life machine learning insulation, a lot of effort has to go into 

cleaning the data. 

So, making sure the data is of a sufficient quality to feed into your system and, so how 

confident can you be of the results at the end of it. 

It is both the factor of the data quality, the data volume and as well as the machine 

learning, exact machine learning algorithm that you end up using. 

So, all of these factors are together influence, how confident you can be of the results and 

at the end of it, there is one very important question, which typically you know it not 

address that carefully is, am I describing the data correctly. 

So, for example there are two classes of questions I could ask you. 

So, the first thing is like, do I have enough information about what I am trying to classify 

or age and income enough to describe all my customers or should I look at gender also, 

how would I answer such a question. 

A lot of it actually comes from the data analysis that we have been doing so far or age and 

income alone sufficient to explain all the variance that I see in the data or should 

I include another variable, in order to make my model more accurate. 

So, such questions will tie back into all of the analysis that we have seen so far. 

And the next question which is more often, it is partly an engineering issue and partly 

a theoretical issue is, how I should represent my variables you know, how do I represent 

age, that age can be a number. 

Well, age can sometimes be merely a number, but sometimes you can classify age into different 

levels as young, middle aged and old. 

So, what kind of an encoding would you choose for representing age and again, it goes back 

to the data analysis if you are talked about and… 

So, these are things that you have to pay a lot of attention too and the more often, 

than not when you are looking at a simple exercises that you could do on the web, you 

end up looking at the data that is already where this questions have already been answered 

and they are given to you and you are able to do it very easily. 

But, when you go out and you are trying this, all your first real problem using machine 

learning you will find that these are very important questions that we have to answer. 

So, in the next module we will look at more detail at the different learning paradigms 

that we talked about today. 

English - NPTEL Official 

 

 

Supervised learning 

Hi and welcome to this module, where will be I am introducing you to various machine 

learning tasks. So, we already saw in the previous module and that, machine learning 

is essentially improving the performance of artificial agent with experience. 

So, today we are going to look at the first supervised learning, where the experience 

we are going to call the experience as training data here. So, in this case I am showing you 

data points that this distributed in a 2D plane, so it could be let us say age and income 

that describe customers that come to a particular store. So, in the case of supervised learning, 

this training data is going to carry labels. 

So, it could be that whether the customer is going to buy a computer or not going to 

buy a computer. So, the customers marked in green are going to buy a computer and the 

customers marked in red are not going to buy a computer. So, this is the kind of experience 

that is going to come to you and in the case of classification task, which is this, so 

we are going to call it label training data, where the labels are drawn from small discrete 

set, in this case it is just yes or no. 

So, what is your goal here? So, your goal in this problem is to figure out, how are 

the yes’s and no’s distributed in this two dimensional play. So, the simplest model 

for this is going to be to draw a straight line. So, what does this straight line mean 

here? So, it’s essentially saying that people, who have an income below a certain level are 

not going to buy the computer; people, who have income above a certain level are going 

to buy the computer. So, if you think about it, we are really coordinate 

more or less correct, there are a few points like that and that, which are incorrectly 

classified and here is another one, who is actually going to buy a computer, but we are 

classifier is going to say, it is not going to buy a computer, because it is on the wrong 

side of the line. So, you can do better, so that is a slightly better classifier. So, 

the two, the red points that we had incorrectly classified previously that one and that one 

are now correctly classified, but we still are making a few errors. 

So, what has happened from the previous classifier to this one is that, you have made it slightly 

more complex. So, if you think about it, the previously classifier was essentially say 

x equal to some constant, so it is just saying income is equal to some number, if it is less 

than that, it is essentially no, if it is greater than that it is yes. So, now, what 

has happened is we have added a slope to the line, it is no longer; just based on the income, 

but the age also has to play a role here. So, there are more parameters that are needed 

for describing this classifier, earlier we just needed to have one parameter, now we 

need at least two parameter to describe this classifier. So, can we do better? Obviously, 

so we can do better we can draw a parabola like this, so in this case you need more parameters. 

So, you have a x square plus b x, so you are now going to get additional parameters that 

you need to describe this classifier. So, are you doing good here or should can 

we do better, it looks like we could do better because we still have an error. But, it starts 

looking a little weird, so we are essentially now, this is going to acquire a lot of parameters 

to describe what this classifier does, you know that little wiggle there that goes out 

and gets that additional red that you missed and say now, becomes increasing complicated. 

So, what really was happening is probably that data point is a noisy point, it could 

be noisy due to variety of factors, it could be that person came to the shop to buy a computer, 

but for some reason he received a call and then, had to leave immediately without buying 

one or may be the data has been erroneously recorded you know person actually bought a 

computer, but it has not been recorded. So, in there are several situations, where 

which says noise could enter into the system. And we have to be very careful that we do 

not end up modeling such noise in the data, which will lead us to make really wrong predictions 

in the feature data. So, probably the best solution to this problem is this parabolic 

curve, because it gives us a good balance between the complexity of the classifier verses 

the accuracy that we have on the training data. 

So, remember that this is all evaluated on the training data and what you really are 

looking for it is, how well is this classifier going to work on the test data on the deployment. 

So, I am going to train this classifier and then, I am going to use this to predict whether 

the new customers who come to my store are going to by a computer or not. So, when I 

am actually operating it with this new data I want this classifier to do well and therefore, 

over fitting the classifier to the training data might be a bad idea. 

So, one thing that you should remember here is that my goal is not just to do well on 

the training data if I am only interested in doing well on the training data, what is 

the best way to do it yes I just have to remember all the data points it was given to me. So, 

I can never make a mistake ever again and I can be perfectly accurate in making predictions 

on my training data, what I am interested in is to be able to perform well on data that 

I have not seen before and therefore, I need to be able to generalize to unseen data. 

And the only way I can generalize to unseen data is by making assumptions about the model 

I have to make some kind of assumptions about, what kind of lines in this case, what kind 

of lines should be separating the buyers from the non-buyers. So, whenever I have this kind 

of need to generalize it translates to assumptions on these lines. So, such assumptions on the 

models that you are generating are called inductive bias. 

So, the whole paradigm of learning that we are talking about is called inductive learning 

and the assumptions that allow us to get this kind of generalization are known as inductive 

bias and there are two forms of inductive bias generally. So, one is called the language 

bias, which essentially is the restriction that we had on the kind of lines if you say 

I am going to only consider straight lines that are parallel to the x or y axis that 

is one kind of a language bias or if you say that am going to consider only parabolas that 

is another kind of language bias and the other kind of bias is search bias. 

So, given that you have picked a language in which, you are going to represent your 

classifier, how do you search among the possible classifiers in that language in order to find 

the right one. And that again influences what kind of classifier that you are going to find, 

because you typically will end up with the first classifier that you find that has an 

acceptable performance. And therefore, that will be influenced by your search bias and 

we will elaborate on this as we go along and this just to give you a feeling of what is 

involved in trying to do this kind of learning. 

So, I like to elaborate a little bit more on the whole process of this supervised learning 

and this is kind of common to the other algorithms you look at as well, but am just going to 

talk about supervised learning little detail in this module. So, you start off with training 

data that is given to you. So, training data consists of vector x and an output y, so if 

you think about it. So, in this example that we just saw on the previous slide, so x would 

be looking like a the tuple of income and age. 

So, the first data point could be that person has a 30000 rupees per month income and he 

is 25 years old and he did not buy a computer and the second person had a 80000 income he 

is 45 years old and he buys a computer. And, so he is essentially going to have a series 

of data points like these that are given to you and you have to use your now, your training 

algorithm in order to learn a classifier. But, if you stop and think about it, so for 

us to be able to implement this in a numeric fashion, so you are going to have to encode 

your data. So, you probably take your income levels and 

then, do some kind of a normalization, so I have normalized it between 2 lakhs and 0 

income. So, and that gives me like the first person has a income of 0.15 the second person 

has a income of 0.4 and so on, so forth. And likewise I have normalized the age between 

0 and 100 and I get numeric value for the age. And the labels are encoded again, so 

not buying a computer is now represented as minus 1 and buying a computer is represented 

as plus 1. So, whether you use a minus 1 or a plus 1 

or whether you use 0 or 1 of whether you can use y and n depends on the kind of algorithm 

that you are working with and as we go long as we different machine learning algorithms 

you will find you will learn about the importance of the selection. So, once a have this encoded 

data, so the training algorithm is going to work on it and it is going to produce the 

classifier, but I need to know how good the classifier is and I need to know if I can 

stop. So, if I go back on the training data and 

then, ask how good the classifier is typically we might end up over fitting the data like 

I was showing you in the couple of slides ago. So, what we do is set aside a testing 

set, so which is not something, which was used to build the classifier right and then, 

we validated the classifier on this testing set. And then, if the validated if the validation 

tells us the classifier is good enough, then we can go ahead and output it or if the validation 

process tell us known the classifier is not yet good not yet good enough. 

And then, I can go back and modify the parameters of my training algorithm or iterate over the 

data again until I converge to something that is acceptable. There are many different ways 

from this validation can be done and again you look at some of this later. 

So, what exactly goes on in the training module, so the input, which is denote by x comes to 

the learning agent the learning agents uses the current setting of the parameters and 

it the outputs a value, which is y hat, which is its guess of what the output should be 

for this input x. And if you remember in the training data we already have a target y, 

which is the actual output that you expect from the agent. 

So, I am going to compare the target y with the guess the agent output, which is y hat 

that allows me to form an error, which is the error in the prediction and it gets fed 

back in to the agent and then, the agent uses the error in order to modify its parameters. 

So, this is something, which you have already seen to some extent in the regression setting, 

but we will look at a more of a learning approach regression in the next few classes, but you 

can also see that this is exactly, what the classifier needs to do. 

So, there are many, many applications for classifications, so that a credit card fraud 

detection, which is a very I know that a few years back this was talked as a marquee application. 

So, when a person swipes a card you can say whether that is a valid transaction or not. 

And the other application which there a lot of buzzer around it nowadays a lot of startups 

and many companies are focusing on it is sentiment analysis or some time called opinion mining 

or buzz analysis etcetera this is looking at social media data whether the tweets or 

whether they are blog posts, so on, so forth. And then, trying to figure out whether they 

are saying something positive about you or something negative about you, so by negative 

about by you I mean whatever is of interest to you it could be a movie, it could be a 

new album, it could be a product for those release. So, instead of marketers going door 

to door asking people what they feel about the products we are now able to analyze the 

post that people put in the public forum and automatically figure out whether the opinion 

is positive or negative. Another application which has lot of commercial impact is one 

churn prediction. So, a churner is somebody is your user from 

your service who is likely to leave it is like am going to switch from Vodafone to Airtel. 

So, all I am going to switch from a windows machine to a Mac. So, these are these people 

are churners you know they are habitual users of a of a particular service who are going 

to leave it and move to another service. So, many companies are very interested in identifying 

such churners and then, taking measures to retain them. 

Because, attracting newer customers to your services is little harder than keeping the 

customers that you already have and people are willing to spend money to do that, so 

that is another interesting application. And of course, increasingly medical diagnosis 

is proving to be a very fertile ground for classification algorithms though it is the 

medical communities really not in a position to completely accept, what machine learning 

people tell them. But, the machinery people any way keep working 

on lot of medical domains and there are many competitions that are run that ask people 

to build classifiers that can predict whether a patient is sick or whether a particular 

blotch that they see on a x-ray or a scan whether that is a cancerous thing or whether 

it is benign. So, all this kind of diagnosis questions are asked of machine learning algorithms. 

Then, we fair amount of success, but then still quite a bit of way to go and another 

place, where machine learning is used in medical diagnosis is in risk analysis. 

So, I will elaborate on a little bit more when I talk about regression and there are 

in supervised learning. In fact, the classification is one of the most widely studied machine 

learning paradigm. So, that many, many approaches for classification and some of them are very 

famous a few or few might recognize the names here they can produce learning supervised 

learning can produce different architectures. So, it could be artificial neutral networks 

which we will talk about later support vector machines, decision trees or just sets of rules 

you know and other popular methods such a nearest neighbor methods, where you remember 

all your training data and then when a new data point comes in then, you try to make 

a prediction based on which, of the training data looks like the new data point right and 

also problestic methods are typically based on Bayesian approach to learning. 

So, we will be covering at least the first three in this in this course in detail the 

other supervise learning problem that we are going to talk about is regression or prediction. 

So, here the data is going to have continuous outputs, so the input in this case this is 

simple example I have is temperature. So, the x axis is the time of day at which, the 

which temperature is measured and the y axis is the temperature, which is the output. 

So, we could see that typically day time temperatures are higher and night time temperatures are 

lower and now I can try and fit a curve to this, so ideally what you would like to fit 

well all of us know about linear regression. So, what we are going to do first is try to 

fit a straight line, but then you can try to be little cleverer and you can try to fit 

a more complicated curve like that that sounds its looks a little reasonable or it could 

over fit the data like we talked about earlier. And then, try to make the curve really complex 

and then, try to follow all the micro variations in the temperature. So; obviously, in the 

night time you see a data points that is temperature is almost as high as the day time that is 

noise, but if you try to over fit it you are going to get incorrect results. 

So, we already looked at the regression little bit of detail, so that we know that we are 

essentially trying to minimize the sum of square errors when you are trying to make 

a prediction. So, how do I fit this line I just figure out the difference between the 

line the prediction made by the line that I fit and the actual data point that was recorded 

take the square of the error and try to minimize that, so that is the typical approach that 

we use for fitting lines. 

So, as we have already seen with a sufficient data doing linear regression is simple enough 

just a set of matrix operation. But, then if we have many, many dimensions in the last 

slide our data had only one dimension is essentially, what was the time of day that was my x vector. 

But, then if you have many, many, many dimensions and then, you really need a lot of data points 

in order to avoid over fitting, because if I have like a 1000 dimension data and I have 

only 100 data points it is very easy to over fit to those 100 data points, because I have 

a 1000 parameters. So, how do I avoid that kind of over fitting 

is by adapting something called regularization. So, we will ensure that of all the models 

that can fit the data to a certain extent we will try to try fix find something that 

is simple enough, so that we do not over fit the data. So, here we are talking about linear 

regression, but suppose I want to do a higher order regression. So, if you remember in the 

previous slide, so we looked at an example where, so again it its looked like a quadratic 

curve was slightly better fit to the data than a linear fit. 

So, how do we handle higher order functions one simple way of doing this is to look at, 

what are known as basis transformations. So, where you take the input, so input could be 

say x 1 and x 2 there are two variables at that describe the data and then, I translate 

that in to a much larger description by adding second order terms. So, I take x 1 x 2 I create 

x 1 squared x 2 squared the product x 1 x 2 and then, the original variables x 1 and 

x 2, now that gives me a five dimensional data, so I took the one two dimensional data 

and I converted it in to a five dimensional data. 

And now, I can do linear regression on this five dimensional data and this is going to 

end up giving me quadratics. So, I can use the same technique of linear regression and 

I can get a more complex fits by doing this kinds of basis transformation. So, linear 

regression is not really that weak of a method because I can do more complex fix using linear 

regression. 

There are many. many different applications of regression, so the one thing, which I already 

mentioned earlier was an time series predictions. So, you could try to build a model that predicts 

rain fall in a certain region or you can try to build a model that predicts, how much money 

a user is likely to spend on a particular service let us say calling right sometime 

you can even use linear regression to do classification. So, instead of saying whether the person will 

buy the computer or will not buy the computer you can try to predict what is the probability 

the person will buy a computer and you can try to solve the cerebration problem and is 

roughly. So, I have more I have more no answers to that right and you can use regression as 

a data reduction tool. So, instead of giving you like a 100000 data points I can just fit 

a low dimensional line low dimensional curve to that it could be even a straight line. 

And then, I can just tell you that these are the parameters of the line instead of giving 

you the hundred thousand data points and that allows me to have a very, very large data 

reduction and the other thing is to look at trend analysis. So, it is slightly different 

from the time series prediction problem that I was mentioning earlier, because I am not 

really interested in making predictions here. But, I am more interested in the data analysis 

part you know not necessarily in the predictive analytics part here. 

So, I would like to know whether the growth rate is linear or exponential or somewhere 

in between. So, those kinds of questions can be answered by suitably solving a regression 

problem and then, going back to what I mentioned earlier about risk analysis or in the classification 

case. So, you could think of risk analysis here, so if you remember in linear regression 

you learn coefficients for each of the input variables. 

So, the input variables that has a larger coefficient is the one that is going to influence 

the output the most. So, that way you can look at risk analysis by looking at the factors 

that contribute most to the output. So, this is essentially on supervised learning method, 

so we looked at two of these regression and classification. 

English - NPTEL Official 

 

Unsupervised learning 

So, in unsupervised learning, so what is our experience going to look like. 

So, we have looked at label training data during classification. 

In unsupervised learning, there is not going to be any labels, it is going to be completely 

unlabelled training data and, so the points of this going to look… 

The input data is going to look like points in n dimensional space. 

So, in this case it is a two dimensional space. 

So, now, whatever I interested in doing here, so in the task of clustering what I am interested 

in doing is to find groupings of similar data points in my input space. 

So, for example, this could be possible clusters, so that could be one cluster. 

So, you can see that, there is some kind of a gap between the data points in that cluster 

and the others and not a huge amount, but some amount and then this could be a another 

cluster and that could be another cluster and that could be another cluster. 

So, you could see that, a couple of things that we wanted to notice here. 

So, one is that, the clusters all seen to be ellipses, there are some kind of ovals 

in the input space and that is a choice that we have to make. 

So, that again gives you one kind of an inductive bias, you know. 

So, so you have to make this kind of a language choice, even when you are doing clustering. 

And the second thing I want you to notice, there are the few data points that do not 

belong to any of these clusters. 

It could be because, they lie equidistance from both clusters or they lie far away from 

all of the clusters, but then, they do not belong to any of the clusters. 

For example, look at this point, which seems to be far away from all of the clusters that 

we have identified so far. 

So, such data points which do not fall into any of the clusters are called outlier. 

So, what I would like you to note here is that, this particular outlier that I am pointing 

to actually lies in the middle of the input space. 

I mean, there are data points left of it, right of it, above it, below it, everywhere. 

So, usually there is a conception that an outlier is something that is far away from 

the other inputs, it need not necessarily be the case. 

An outlier is something that does not fit into the current patterns that we have discovered 

in the input. 

It need not necessarily be something that is very far away from the input, so that something 

which you have to keep in mind. 

There are lot of different applications, so you could look at, you know again customer 

data you could discover classes of customers or in image processing, people try to discover 

regions in the image like shown in the figure that, which by doing clustering on the image 

pixels and it could take words, look at the occurrence of words and the context in which 

the words occur, try to cluster those context together and find synonyms or you could do 

even better cluster documents together and find topics or you could do the flip of it. 

You know, you could cluster data together and try to find outliers. 

So, outlier mining, which is the very important task in several situations, where we have 

to find, say anomaly in a data, you know. 

So, you would like to build a secure system and you want to find a figure out if somebody 

is trying to track the system. 

So, any kind of anomalous behavior should be flagged. 

So, then you would not want data points that lie in clusters, but you want to find data 

points that lie outside clusters. 

So, this is called outlier mining. 

So, there are many different applications are for clustering and when we look at clustering 

in detail, we will see some more of these. 

So, the other unsupervised learning task I want to talk about today is association rule 

mining. 

So, so the idea behind the association rule mining is that, I want to figure out what 

kind of entities are frequently, you know co occurring in my input and so, then I can 

say that there are some association between these. 

So, this typically goes in two stages, so the first stage I find frequent patterns. 

So, this is typically the stage, where you analyze the data closely and this is where, 

this is essentially the quote unquote analytics part of it. 

And in the second stage I want to derive associations of the form that, if A occurs, then B is likely 

to occur. 

So, A implies B, from these frequent patterns. 

So, this is essentially a two stage operation. 

First find the frequent patterns, once I have found the patterns, then try to find these 

kind of associations. 

So, more often than not, the challenging part here is finding the patterns, finding the 

frequent patterns. 

So, once you find the patterns, then deriving association is not too hard and as we will 

the, see when we look at association rule mining in detail, but the performance measures 

that you are looking at, typically are associated with the association rules. 

I mean, how useful is this particular association that I have discovered is and more than, another 

frequent pattern part of it. 

So, you could find patterns in sequences, you could find like time series data or a 

fault analysis, you could find patterns in graphs you know, where people typically use 

this in computational biology or social network analysis and other domains or you could find 

patterns in transactions, which is essentially the first domain in which people introduce 

this association rule mining problem, you know. 

So, association rule mining in some sense is an interesting problem, because that was 

the first problem to which the word data mining was properly applied to. 

You know, in some sense you could say that association rule mining kick started the whole 

world of data mining. 

So, what is, what I am mean by transactions here. 

So, so transaction let say is a collection of items that are bought together. 

I do not know, you just go to super market and you could buy a set of items and then, 

so all of the items are go together in the basket that you bring to the check out would 

form a transaction. 

So, so this, the original form of this problem as post as a market basket analysis, so it 

is essentially what goes in to your basket when you do your shopping. 

So, you look at the items that are there in the basket and then, try to figure out which 

all items that people buy often together. 

So, it could be, there need not be individual items it could be sets of items and then this 

community, these sets of item are called item sets, just as a terminology. 

So, I will be using item sets frequently when I talk about the association rules. 

So, the goal here is to find item sets that are frequent, once you have the set of frequent 

item sets, then I can go ahead and form rules of the form that, item set A implies item 

set B, if both A and A union B are frequent. 

So, A is frequent. 

So, I say I buy milk, I also buy bread, so if A is frequent and milk and bread, which 

is essentially if milk is frequent, so lot of people buy milk and lot of people buy milk 

and bread together. 

So, I could say that milk implies bread, so if you buy milk, you are likely to buy bread. 

So, this is the whole idea behind mining transaction data. 

So, lot of applications here again, but just to highlight a few. 

It is, the first one is market basket analysis. 

People have actually tried to extend this to a point, where they try to look at the 

arrangement of items in a store based on what is frequently bought together and people could 

try to design promotions that take advantage of this kind of market basket analysis. 

And the second thing is looking at predicting co occurrence and where the things tend to 

occur together, this is the very generic application. 

And in the context of graphs, so mining of frequently occurring sub graphs in the graph, 

allows us to have a lot of insight into a kind of, you know behavior you would seen 

biological data and social networks and so on and so forth. 

And you could also look at this in time series looking at frequent co occurrence of events 

in a time series can be looked at as identifying trigger events. 

So, if this event has happened when quite likely another event is going to happen very 

soon. 

So, those kinds of a trigger event modeling, all of these can be done using frequent item 

sets or association rule mining. 

Why we have been looking at machine learning in such detail. 

So, data analytics in particular data mining in my opinion is machine learning really, 

but applied to very, very large data sets, very noisy data sets and very real data sets. 

So, what do I mean? 

So, classically machine learning has been more concerned with getting accurate parameter 

estimation from small volumes of data and trying to do the best that you could do with 

little data. 

But, now with very large volumes of data available, some of the focus is moving away from handling 

small data to things like, you know how do I make sure that my algorithm will finish 

running in, you know in a few weeks. 

And, so looking at scaling issues, looking at things like data selection, do any to run 

my algorithm on the entire data, looking at feature selection is spoke about that a little 

bit earlier and looking at the actual design of your data base, so how are you going to 

represent the data and so on and so forth. 

So, all of these issues now have come to the forefront. 

So, when you are working in these kinds of domains, you call it data mining and you then 

you go back and look at specific algorithms and error measures and so on and so forth. 

Then, you call it machine learning, but then the lines are the kind of a, you know getting 

a very fuzzy between the two. 

And then, the second difference error’s mentioning is on noisy data sets. 

So, again I mention a little bit of this earlier, so you have to worry about cleaning the data, 

you have to worry about missing values and you have to make sure that your algorithm 

is robust, when something goes wrong that you did not expect earlier, you know. 

Something goes missing or some data gets corrupted, you should have some kind of a failsafe mechanism. 

And the last thing is a little more technical which is that, real data typically does not 

satisfy some of the nice theoretical assumptions, many of the machine learning algorithms were 

operating under for several decades. 

So, typically the assumption is that the data is independent. 

So, one… 

So, we looked at the training data earlier, so their assumption was that each of those 

points in that age incomes space was sample independent of the other and then, there were 

sample from an identical distribution. 

So, it is not like each data point came from a different population, they are all sample 

from the same population, the same kind of age income distribution and then, they were 

sample independent of one another. 

It is not going to be true, more often than not, because your friend goes to a shop and 

he likes a computer and buys something there, you are likely to go there as well, but it 

is not like the fact that the first guy bought a computer is not influencing, whether you 

are going to visit the shop or not. 

So, these kind of independent assumptions are typically not valid in real data. 

So, you will have to think about that and then, so we looked at the classification problem, 

where we saw that we had, you know positive and negative classes like buys a computer, 

does not buy a computer and more or less equal, I mean that is about 60 40 split between buying 

and not buying computers. 

But, in reality it is not so. 

For example, you take any medical domain, then fraction of people who are sick is very, 

very small thankfully, but still it is very, very small and therefore, it makes a machine 

learning problem very hard. 

So, if I tell that everyone who comes to the hospital, I mean or everyone I see in the 

population at large is healthy, I will probably be correct with, say some 97 percent accuracy, 

which case it is not a bad predictor you know, but that is useless for us. 

So, we have to worry about handling this kinds of class imbalance to what actually make sense, 

when the data is so imbalanced and the another thing is, in many real worlds scenarios like 

in medical domains and also in security domains, like you cannot be happy with the acceptable 

levels of performance, you know. 

They have to be near perfect for you to be able to deploy these things in practice. 

So, this causes a lot of trouble, you says no more the case being able to do the best 

effort and get away with it, you really have to be the best. 

So, so such issues also are slightly different from what people worried about in a machine 

learning community. 

The last thing is, you have to operate under the resource constraints. 

You know, maybe want them things wrong on your hand, your handheld device not that there 

is a much of resource constraint any more, but it could still be and I have to work with 

limited computational power and you might not have the luxury of all the time in the 

world to produce an answer, you might have to produce answers in real time. 

So, how do you go about doing that? 

So, there are lots of issues, so but at the core of data mining is machine learning. 

So, machine learning gives you the kind of algorithms and the data mining addresses all 

these issues on top of those algorithms. 

So, that is one of the reasons while we look at machine learning and we will continue to 

do so in future modules in greater details. 

Thank you. 

 

Ordinary Least Squares Regression 

Hello and welcome to our second lecture on Regression. In the previous lecture we provided 

you with motivation for why linear regression could be a very useful data analytic tool. 

And today we are going to take the ordinary least squares regression, which is one type 

of regression and actually step through the process and in some sense, derive the formulas 

or the math that enables you to convert the data to performing a regression analysis and 

the context in which, we are going to do that is, we are going to do a simple regression, which just means that this single input variable only involved and we are going to step through 

a mechanics doing that. So, what is the broader context of this exercise, so we introduced, we gave a motivation for 

linear regression in the previous class. And since then, you should have had a few classes 

by Professor Ravindran talking about machine learning in general and also a module on supervised 

learning and we purposefully choose to straddle regression before and after supervised learning. 

Partly because, it is important to realize that you know regression, linear regression 

the whole process or any other form of regression is a supervised learning tool. You know supervised 

learning being a more an umbrella term, definitely encompasses a regression and regression based 

approaches. And this is despite the fact that for instance regression is something that has existed many, many, many years before you know even the 

terms machine learning or supervised learning or artificial intelligence was even thought off. So, you know the context that you often learn regression could be quite different, 

where you learn it from statistics course, whereas in a machine learning course the emphasis 

sometimes might be on other tools or not I mean depends on where, what the focus is. 

But, the important thing is to acknowledge us, while regression sometimes stand alone in your statistics text books, not sharing pages with the other machine learning techniques. 

The regression, linear regression is just as much as supervised learning tool or anything else or any other supervised learning tool. Another source of confusion that I just wanted 

to clarify before proceeding is, supervised learning techniques tend to get broadly classified 

as regressions type problem versus classification problems and there, what people I meaning 

is quite different from what we are learning as regression at this stage. Out there, what people I am talking about and it is just a definition is, when they 

say it is a regression problem in supervised learning, all they are saying is that the 

output variable is a continuous quantitative variable, whereas when they say they dealing 

with a classification problem, they are saying that your output variable is a discrete or categorical variable. And you know within those two broad classes you have many techniques 

and some techniques comfortably handle both types of data. But, that is sometime gets confusing people saying, it is a regression problem does not 

mean I am doing regression no or there people, what people are meaning is that the output variable is continuous quantitative. Having said that let us proceed with, what we wanted 

to do today, which is deriving the ordinary least squares regression. 

So, the goal out here is to fit a line essentially of the form y is equal to m x plus c. So, 

that is the form you might be heard of more frequently, what we are going to use in this 

class and in most classes is y is equal to beta naught plus beta 1 x and that is you 

can readily see those are both the same, I just replace m and the c with two other terms. 

So, the coefficient is beta 1, the intercept is beta naught, so a line once someone comes 

and tell you the values of beta naught and beta 1 or m or m and c whatever you prefer, 

but someone comes and gives you those two values, then you can define a line. If someone 

says draw a line, you can draw different lines you can draw line like this, you can draw 

line like this, you can draw line like this these are all lines, now these are all as 

straight as you see them lines. But, once someone comes and gives you the 

exact beta naught, there is the intercept and the slope that is a very specific line, 

only one line will have that exact beta naught and beta 1. So, that those two terms are what 

define the line and to give you some intuition beta naught is nothing but, where the line 

intersects the y axis. So, if you wanted different lines with the same beta naught, but different 

beta 1s, then you can think of many lines that go like this, go like this, that go like 

this, these are all lines and just keep in mind I am trying to draw as straight as possible. 

So, these are all lines that essentially have the same intercept beta naught and different 

slopes beta 1s. Similarly, you could have different lines that have the same slope, 

but different intercepts, so that would look a little bit like this. So, these lines all 

at least in, what at least in terms, in theory have the same slope, but different intercepts. 

Now, if but once you defined a slope and intercept there is only one line that has that, so that 

is the idea and what we are trying to do out here is saying, what should that slope and 

intercept be; such that you feel like that is going through lot of your data points. 

Now, I have said that in a fairly vague way, but I am going to define that more formally. 

To define that more formally you want to have a concept of the actual data point, this is 

the actual data points, all those squares are the actual data point and what the estimated 

value of those data points are. So, this data points has a value a particular value, so 

we will call that y 1 and this data point has another y 2 and so on. 

And, so we call those the actual values as y i and for each of these, now if I chose 

to fit a particular line that I feel is like going through this data, I am going to have 

some predicted values. So, what I will do is I will fit this line that is I will put this line here and I will say, my prediction of this y 2 is y 1 is nothing but, for that 

value x 1, where is my line. So, I push this value x 1 up to the line, what value am I 

getting a y and this is my predicted y 1 and it is represented usually with the small hat 

that you put on top. And this same process for y 2 I will I will 

try to write a this actually this line is not perfectly correct, so let me just erase 

that. Essentially, what I will do is this is my y 2 I will draw a line there this is 

y 2, but my prediction for y 2 is here. So, I am going to put a dash line here and this 

is x 2 and my prediction is y hat of two out here and you guys can see what I have done 

here. So, I have basically said look this is value of y 2 and it corresponds to some 

x 2 and I am going to take x 2 and see, where my line goes through in terms y values. 

So, this is in some sense my actual value and this is in some sense what I would wind 

up predicting for y 2, because I have tried to kind a fit some line through a data and 

you might ask a question. So, if this is x 2, then why do not we then just predict y 

2 in the sense y is not y hat of two but equal to y 2 and the answer is fairly simple you 

do not want to predict the exact data point because you are getting a sample we discussed 

how this is not a population. So, the population here for y 2 would be a for the same x 2 I had entire universe of possible y is and we know that for this same 

x value if you what take a other sample that might not fall exactly on this data point. 

The next one could wind up falling somewhere here, the next one could wind up falling somewhere 

here, the next one could wind up. So, we do not have the entire population of possible y is at the x values at the input variables x 2 and, so what we wind up doing instead 

is not predicting exactly on top of that value that you got. But, instead trying to fit this line acknowledging that there is going to be some noise above 

and below and you might do better of predicting at this point, where x intersects with the 

line and that is your predicted y 2. This is, so that you do not wind up getting fooled 

in some sense by just some amount of noise or uncertainty there is above and beyond the 

exact that the trend that you are setting the line in some sense indicative of the trend that is, which is in general when x seems to go up y seems to go up and that is, what 

the line is showing at least this particular line with the positive slope. And you want to capture that, so that tomorrow someone say, what will, what do you expect 

when x is equal to this value you go to the line rather than you going actual in individual 

data point. So, let us see an idea you now have the concept of actual y i and y i hat 

and the goal in terms of what we are trying to do is that we are trying to minimize the 

squared deviation between the actual and the estimate. So, we are actually saying this 

is measure which is y i minus y i hat and sometimes y i minus y i hat is going to be 

positive this case this case is positive, because actual is greater than the estimate. 

In some case it is going to be negative, but you take all these positive thing and negative 

numbers for each number and square it, then they all become positive. And then, you sum 

it. This is the sum of the square deviation between the actual and the estimate and it 

is a measure of how close the line is to its data points and what we are going to try and 

do with an ordinary least squares regression is figure out that line. And, how do you define 

a line you define it with beta naught and beta 1 once you fix with the beta naught and 

beta 1 the line gets fixed. So, we are going to try and figure out the goal of this exercise to figure out that beta naught and that beta 1, which defines the 

line, which results in minimizing the squared deviation between actual and estimate. Because, 

may be this line with another beta naught and beta 1 is not is very far away from your 

points. So, the square deviation between actual and estimate is going to be huge or take another 

example this line, which is like this is also not going to work very well. 

Because, look at the kind of deviation that you have between actual values and estimated 

values. So, this line with it is beta naught and beta 1 this line in red with its beta 

naught and beta 1 might again not do to well. So, what whatever we trying to do is we are trying to figure out that line when I am saying we are trying to figure out I am saying we 

are going to figure out that beta naught and that beta 1, which is what represents the line. So, we are trying to figure out that line, which minimizes deviation between actual 

and estimate, so does that kind of make sense? good. 

So, how do we go about doing this the way we go about doing this is the process of the 

derivation. We start by saying this is the functional model we have we have the model 

which says that y i is nothing but, beta naught plus beta 1 x i, which is the line that we 

are creating we do not know beta naught and beta 1 is yet. But, if you had beta naught and beta 1 your line would nothing but, beta naught plus beta 1 plus some amount of error 

just going back. For instance to this, what we are saying is each y i, which is nothing, but this value this is y 1 is nothing but, is equal to, where 

you can get to in the line this is equal to this distance and this distance can be defined 

as beta naught plus beta 1 x 1, because this value is x 1. So, this distance y 1 is nothing 

but, this distance beta naught plus beta 1 x1 plus some amount of deviation, which I 

am going to call as error this is the deviation between actual and estimate that y i minus 

y i hat that distance I am calling as the error. So, ultimately y i is nothing but, beta naught plus beta 1 x 1 plus e i and I shall really 

say that beta naught plus beta 1 x i plus e i, which is what I have done out here, as 

said y i is nothing but, the model plus the noise. So, we will call that model or you 

can call that y i hat and the noise, now all I do is just rearrange the terms such that 

e i is on one side and we have said that our goal is to minimize. So, this is the deviation 

between y i and y i hat and our goal is to minimize the square of the deviations for 

each data point. So, I am go through i equals 1 through n I 

am going through each data points 1 through n all the way and for each data point I am 

trying to look at the deviation between actual and the model and this is your estimate or 

you can think it as y i hat. So, this is what you are estimating and this is what is actual 

value you are taking the difference between them and squaring it and it is the you get 

the two minus signs because you can think of it is minus and put the beta naught plus beta 1 x i into the brackets, then you open the brackets minus comes in front of both 

terms. So, you are ultimately just taking the summation of the square the square term is here the square of the deviation between actual and 

estimate. And; that is, what we are going to call as a sum of squares error and that 

is what we are going to try and minimize you essentially want to minimize the squared deviation between actual and estimate. And you can also kind of think of it this is one way of getting 

into sum of squares you can also think of this definition, which is I started by saying I want to minimize the actual minus estimate square and we know that the estimate is nothing 

but, so y i hat is beta naught plus beta 1 x i. See notice the difference y i is beta naught plus beta 1 x i plus the error term, where 

as y i hat, which is the estimate of y i is just beta naught plus beta 1 x i this basically 

defines the actual and this defines the estimate. So, you can just plug in this beta naught 

plus beta 1 x i and I am using the word beta, but really these terms are still b. So, b 

naught plus b 1 x i will be more accurate. So, b naught plus b 1 x i and when you plug 

and expand that this is exactly what you get the same notion of sum of squared error, which 

is cycle through each data point and look at difference between estimate and actual. 

So, our goal in determining the beta naught and beta 1 see the y i and x i are data points 

that you collected from the field x i represents the input variable y i represents the output 

variable. So, you have 10, 20 or a 100 or 1000 of x and y pairs, so for a particular 

x there was a particular y and there are i such there are n such x and y pairs and i 

is just the index that represents a particular combination. So, x and y are actual data points 

b naught and b 1 is what we are going to determine and the way we are going to determine that is by finding, what values of b naught and b 1 minimize this function. So, that the exercise 

that we are embarking upon. So, how do we do that, like I said a goal is to minimize this term and the way we going 

to do that is to take a very basic idea from calculus, which is that you take the first 

derivatives of this term and equate the that first derivative to 0, why do we do that it 

is a very basic idea from calculus, which is when you if you take on different values 

of beta naught and beta 1. And right now, these are the two variables of interest y i and x i are actual data the idea that if you fix one let us say you fix beta 1 and 

you keep on changing beta naught. For a given beta 1 there exist a beta naught, 

where this error will be the lowest and, so for a fixed beta 1 if this was the variable 

beta naught I am plotting the beta naught here and I am plotting the sum of squared 

error this is SSE out here. The core idea is that you are going to get as you keep on 

changing beta naught for a fixed beta 1 you might get a function that looks, let me make 

more smooth I will just try again am I get beta naught you might get function smooth 

function like this, which basically says like this there exist a particular beta naught 

value, where the sum of squares is minimum. Now, how do you go about finding that, it 

is a simple idea if you take slope of this function the slope of this function at different point at the point at, which the sum of squares error is lowest that is slope is equal to 

0. So, the idea is that the slope is nothing but, the tangent to this function just like 

the slopes always are and this is, what is considered a positive slope a flat line is 

considered as 0 slope and this is negative slope on the left hand side. So, the idea is that if I take the first derivative, which is nothing but, the derivative of a 

of a particular function is nothing but, the slope of the function and if I take the derivative in equate it to 0 and I should be able to find out that value of beta naught, which 

gives me the lowest value, now remember I of course, said beta naught for a given beta 

1. So, I might get that in the form of in the form of beta 1, but then what I can do 

is then I can do this same exercise that I just did for beta 1 I can say for a given 

beta naught as I keep changing beta 1, what is that value of beta naught that minimize it. So, essentially you wind up having a concept 

two equations with two unknowns the two unknowns are beta naught and beta 1 the two equations 

are, what we get are what you get when you derive with respect to beta naught and what 

you get when you derive with respect to beta 1, so b naught and b 1 again. So, what you get when you derive with respect to b naught and what you get when you derive with respect 

to b 1, then you equate that to 0. And then, you have two equations with two 

unknowns; that is just a simple form of simultaneous equation for you to solve it. And as you can 

see what you have done in each of these two is to take the first derivative and these are partial derivatives, because you are clearly deriving with respect to beta naught but, 

you also have an another variable in this equation beta 1 same here these are partial derivatives. Because you deriving with respect to b 1 and you have another variable which 

is b naught in the second equation. So, but the core idea is this, which is you 

take two partial derivatives of sum of squares error with respect to b 1 b naught and b 1 

and solve for the values of b naught and b 1; such that you will be able to get that 

b naught and b1, which minimize this sum of squared derivation in a sense. So, we have 

explained a principle lets actually go to the steps that how we will do it. Now, first we are going to take care of first equation that we saw, which is this equation, so let 

just call this one and this is two, now we are going just to do the process for 1. 

You take the first derivative and all, what you might notice, now is that I have essentially brought this differentiation in because it is a sum of that we would looking at the first 

derivative of a sum of terms, which should logically be the same thing as sum of the 

first derivatives of those terms and what I do here is I am differentiating with respect 

to the beta naught. So, you can do this in many ways you can just basically take this 

square and just expand it and say what is y i minus beta naught minus b 1 x i times. 

Because, it is a square times the same term again and then, you get many terms and then 

break them up or you can use something fairly simple called the chain rule, which is just that this is the function of b naught. So, I will first take this square of the function 

and do you know the usual idea that I difference the first difference of x square the derivative 

of x square is 2 x. So, the 2 moves out and the derivative of minus beta naught is minus 

1, so the minus also comes out. And, so essentially use can use whichever the approach in differentiation that you like, but this is the answer to this step, now all 

I am going to do is equate to 0 and then, I am going start solving it. So, what I do 

is there are many separate term here again, so summation of a minus b minus c you can 

basically say it is summation of a minus summation of b minus summation of c I have done that and I have reshuffled the terms in this equation such that beta naught comes to one side, because 

we are interested in beta naught. Now, beta naught is essentially a constant 

it mean it is a variable in this equation, now but it is take on one value it is not 

like x i, which takes on different value depending the value, what is the value of i is . So, 

if I am doing the summation from i equals 1 to n each x i will be a different value, 

but beta naught is not a function of i it is a same beta naught for whatever value of 

i you pick. So, it is essentially out here is all you are doing is here adding n such 

b naughts and that is nothing but, n times b naught. 

So, what will do in the next step is to just isolate b naught and therefore, if you notice 

we took that n that was coming up on this side and we moved it as the denominator. So, 

that is, what you are seeing here in terms of moving from previous step to the step. 

And finally, we realize the sum of y i divided by N, which is the number of times y different 

y is nothing but, y bar is nothing but, the sample mean the sample mean is nothing but, the sum of your data points divided by the number of data points, so this is the easiest 

representation. Again we just simplified equation 1 of a 2 

equation combination with two unknown variables hence, b naught is described as a function 

of b 1. Now, we are going to solve for b 1 by substituting values of b naught with the 

term on the hand side, so let us do that. Here is the derivation of b 1 again a same idea, which is you are doing partial derivative 

over the summation and again you can take this term inside, which should be fine. And 

again you can use the chain rule for deriving it or just basically expand this whole set 

of terms expanded by the square and you can do it that is your convenience. But, one additional 

thing is it, which won’t look exactly at b naught, because the coefficient for b naught was just this minus 1, where is a coefficient for b 1 is you have the minus, but you also 

have x i, so minus x i. So, the answer is also going to look it different, the result of this derivation is this value and essentially you have an x i in brackets 

y i, but this derivation again should be fairly straight forward once you do this derivation 

you get this and you again go through the process of breaking this down or simplifying 

it. Again you had a summation over the entire set you can break up into many summations, 

so that what we have done here in the next step. Now, reshuffle things such that the b i the b 1 comes to one side, so that is what we 

have done here b 1 come to this side and on this side you would have had only these two 

terms. But, the problem is, so this b 1 I just shifted to this side, which is what you 

are seeing here, but the look the right hand side is looking different and the reason for 

that, because in the right hand side only these two terms should have been there. But, again look it is the function of b naught and we know, now from the previous exercise 

we know that b naught is equal to y bar minus b 1 times x bar that is was the conclusion 

and actually show you that value that is just erase we conclude that b naught is equal to 

y bar minus b 1 time x bar and let me also erase this and that is exactly what we are 

substituting, what we are doing is we are going ahead substituting this b naught with 

that term. So, yes we have shifted this to this side 

and that is how you get the left hand of the equation when the right hand side in addition 

to this y i x i we substituted the value of b naught with another term. Again note this 

summation y i divided by N is nothing but, y bar summation x i divided by n that the 

two term are here and here are nothing but, y bar and x bar, so we said y bar minus b 

1 x bar and of course, this x i out here just stays out here. 

So, that should give an idea, where the expansion is and, now again what we are trying to do 

is we trying to keep all b 1s to one side. So, this guy out here also gets shifted here 

and that is what you are having on the left hand side of the equation and the right hand 

side this step gets unaffected. And finally, you can just simplify this is just basic algebraic 

simplification to get to the final form of b 1. So, what would you do when you given 

a whole bunch of x and y and you need to fit a line through it you use this formula essentially 

to get slope b 1. And if you look there is nothing in this that has b naught in it is just a function of y’s x’s and N and x i squares, but again that 

is from x i and N is nothing but, total number of data points and you can use this and get 

b 1, which is the slope of the equation. And then, you can go and substitute b 1 out here 

and you know x bar and y bar from the data and get b naught and as you know once you have b naught and b 1 you have a line on your hands. And we essentially use this is the 

process of ordinary least squares regression where you take a bunch of data x’s, y’s, 

x and y pair of inputs and outputs and fit a line through that such that you minimizing 

the square deviation between the line and the line represents your estimated values 

y for a given x and the actual data point y. I hope that was clear and that is your 

ordinary least square derivation. Thank you. 

 

Simple and Multiple Regression in Excel and Matlab 

Hello and welcome to our series of lectures on the topic of Regression. Over the past 

few lectures we discussed the idea; we motivated the use of this approach called regression. 

And in the last lecture we even looked at the idea of deriving the ordinary least squares 

regression and we did that in the simple linear regression case. Today, we are going to look at how you can implement a regression through software and 

we will show to you in both excel as well as MATLAB. But, the important thing is to 

just understand the core concept and the terms, so tomorrow you would be able to implement it with any software of you are choosing. 

Now, with respect to the main topic that we discussed last time, which was simple regression. 

So, when we did this whole analysis on the derivation for the ordinary least squares, 

we took a case of simple regression which just means that there is one input variable; 

that is involved. Today, we are going to extend that, we are going to look at the case, where 

there are multiple input variables. So, let us start by setting up an example 

and I will show you a demonstration in excel. For performing the multiple regression, we 

can talk about some terms that you might encounter when using this software. 

So, this is the sample data set and what you have in column A, there is serial numbers 

to give you an idea of how many data points there are. I have chosen an example, where there are 88 data points. The variables B, C, D and E as you can see selected on the 

monitor are actually the input variables and column F, which is shown with, which is highlighted 

and also labeled as y is your output variable. And, so for the first time you are interested 

in creating an equation, regression equation that maps these input variables to y, the 

output variable. So, the way you would do that and I am using Microsoft excel out here 

would be to go to, you see the topics on top that say file home insert page layout where 

my mouse is and in that, you select data. So, usually your screen might be at home, 

you would go click on data, then you would go to data analysis, click on that. 

And that will pop up a set of possible tools that you could use analysis tools. And for 

some versions of excel you might not have this readily; that is you will not have this data analysis button already and there you usually need to go and add it in and it is 

called an add in and it should be there, it is there in the software, but it is just not installed for you. So, here you would choose the topic regression and you would click on, 

now it is asking you to tell it, where the data is, so that is what I am going to do 

I am going to show where the data is and the way I did that was to kind of select that 

whole column. And if you are bigger familiar with excel, 

you can just use some hot keys to do that as well. Just note that I am also selecting 

the titles of the variables when I am in putting the variables. And, because I do that I need 

to check this box versus labels, meaning that I have also provided you with labels. The 

idea here is, the constant is 0 basically means should the regression line be force 

to go through the origin and we do not want that, so we will not check that. That basically means in your equation y is equal to beta naught plus beta 1 x 1 plus beta 2 x 2 and 

so on, your beta naught is going to be forcefully said to 0 and that is not something you want. 

The second also gives you out here you have something called confidence level and that gives you the option of choosing a confidence level or essentially an alpha; that is different 

from 0.05 or 95 percent, but we are quite happy with 95 percent, so we will leave that. 

There is a couple of other options, the thing is I said that I want my results in a new 

worksheets, so that is what I am going to select. So, I say and this is the output that I get you might notice put in to a new sheet. So, 

let us just look at this output and try to process this. So, what it says out here is 

that the important things we have to note is the R square, the R square is in some sense 

a measure of how could your over all fit is. Essentially, it tries to say, how much of 

your variation in y is actually being explained by the model that you created versus how much 

of the variation in y, it is just beyond noise. So, the R square is essentially a number that 

goes from 0 to 1 and it is essentially the square of the multi, but it is called the 

multiple R. There is another term called adjusted R square is a very important term and we will come to that in a minute. And you finally, have 

standard error out here, which is been highlighted and standard error is nothing but, for a given 

predicted point, how much is the general deviation of the actual point from the fitted model 

for on average, so that is, what is being captured in standard error. So, an observation 

is just the number of observations there are, below this top table that we have just discussed 

you see two tables. Let me first talk to you about the bottom most table and then I will come to the table above, the idea here is that here you have 

the intercept and the four input variables. Now, what is called as the coefficients correspond 

to the betas? So, when it says intercept the coefficient 9.29 essentially talks about beta 

naught and corresponding for A, B, C and D are their respective betas. Because, just 

remember that we have a functional form of the nature, I am just going to type it out 

here y is equal to some constant, let us called beta naught plus beta, let us call it 1 times 

A plus beta 2 times B and so on; that is the functional form that we have and it goes all 

the way from C and D. The big question is, what is beta naught, beta 1, beta 2 and that is what these coefficients represents. You can call them beta 8 times 

A if you want, but essentially the coefficient corresponding to A is what we are talking 

about here and these are the coefficients. These actual values that you see here, excel 

has done this regression for you and it gives you these results. The standard error that 

is being discussed here is the standard error around this coefficient, what is… So, this 

coefficient is nothing but, an estimate based of sample going back to a topics in inferential statistics. And the standard error pertains to the uncertainty 

or the standard deviation using quantifying it with standard deviations, uncertainty around 

that coefficient, which is being quantified by the standard deviation associated with 

this estimate. Because, this is an estimate and there is some uncertainty around an estimate just like you had a sample mean and the sample mean, which you got from the data, which is 

supposed to represent the population the sample mean essentially is a random sample from a distribution. Because, each time you take a sample and you 

take a mean you are not going to get the exact same value. So, you are getting a value from a distribution and that distribution has some standard deviation we for instance earlier 

discussed about this standard deviation of the sample mean it is something that you can get from the distributions. So, similarly the standard error quantifies the uncertainty 

around this coefficient around each of their respective coefficients and it does through 

the measure of standard deviation great. So, now finally, from standard error we go 

to the t statistic and the reason for that is that this estimate, which is this coefficient. 

The distribution associated with this estimate just like the distribution associated with 

the sample mean was t distributed if you did not know the standard deviation of the population 

that is the same core idea here, which is that it is t distributed and excel calculates 

t statistic for you and gives you the p value associated with the t statistic. 

And what you have out here are a setup p values and what they represent is that they represent 

the p values in the t statistics correspond to the hypothesis that this coefficient. So, 

let us take one example, so I am just on a highlight this example, so we got some coefficient 

for A and that is 0.2420, what we than chose to do is test the null hypothesis that this 

coefficient is statistically different from 0 and this p value is the probability that 

comes about as a result of performing this hypothesis test and as you might as you already 

know the p value is nothing but, the probability of seeing this data if the null hypothesis 

is true. So, if the null hypothesis that that the sample coefficient is equal to 0 is true, then the probability of seeing the data that we have 

seen is this value, which is 5.21 times 10 to the power 17, now that is the very low 

number that is a very, very, very low probability. And, so the idea would be there out here we 

would reject the null hypothesis that this coefficient is statistically no different 

from 0. So, if you take a look at this all of these p values at least are fairly low the sign 5.1 e power minus 17 just means times 10 to 

the power of minus 17, which means it is a very low number and the only value that is 

high is out here that is 0.33. And; that is the kind of value on which, you might not 

be able to reject the null hypothesis or reject the idea that the coefficient is actually 

indistinguishable from 0 the null hypothesis that is that this estimate this parameter 

could actually be equal to 0. Going in line with the idea of confidence 

interval, so you also get confidence bounds, so this is the lower 95th percent confidence 

bound the upper 95th percent. And if you had mention values different in the original pop 

up screen saying I want something more than 95th percent you would have gotten the 95th percent bounds as well as those new numbers, but if you have not then these numbers just 

repetition. Obviously, noteworthy thing is that given 

that these are 95 percent confidence bounds you can reason from inferential statistics 

that, whenever the p value is less than 0.05 or even actually technically less than 0.1, 

because it is the lower and upper confidence bound if it is anyway less than 0.1 you should 

have the lower and upper on either side of 0. So, here the lower bound is 3.6 and the 

upper bound is 14, so it does not intersect 0. So, whatever sign your coefficient is your lower and upper bound are is going to be on 

that side of 0. Obviously, when a p values as high as something like 0.3, which means 

you potentially could not reject the null hypothesis, then you would have a lower bound 

that could be less than 0 and you will have an upper bound greater than 0 and that should make sense in terms of how we understand confidence bounds and p values. So, this gives you the 

inferential statistics or the background associated with having each term in the model. 

So, in this model, which I am just going to highlight with some colors, so it is clear to you in this model that you have out here you now, know, which terms could potentially 

go and which terms could not or which terms you can justify putting in there in which 

terms might just be a result of random noise. Now, in addition to these individual statistic 

associated with each individual term you also have an overall statistics that is being captured 

through this ANOVA, so I just give that a different color. So, the idea with the ANOVA is that you are trying to say how much of the variation comes 

from the regression model and this is the this cell is equivalent of the mean square 

between I am talking about d 12 and you have cell d 13, which is the idea of how much of 

the variation still exits even after you fitted the regression model and you use the same 

concepts. So, this these the these two terms are kind of like a mean square between and mean square error they are the equivalent from the ANOVA the traditional ANOVA that 

we study and you can calculate and f statistics and get a p value for the f statistic and 

what that p value says is over all as a model. Never mind that you have the statistics associated 

with individual terms, but overall as a model can I test the null hypothesis that the model 

explains some variation or is the variation explain by the model equal to 0. So, it should 

give you an idea of that and in many instances you are really looking to make sure that this 

value is as low as possible and you want here R square and adjusted R square values to be 

as high as possible, so great. So, we discuss how you can do a multiple regression 

in excel, but at this point we are going to also go a little bit beyond a standard multiple 

regression the software kind of allows you to do it conveniently, but you could also 

for instance perform a multiple regression by hand. And the way you would do that and 

this is really useful is, because you might not want to do an ordinary least squares and 

that is what we are going to show you how you do not have to do ordinary least squares. Just to jog your memory the idea of ordinary least squares was to say that I want to minimize 

the sum of the square deviation of each point from the line as measured along the y axis 

line. So, I want to measure the square of the distance of each point from the fitted 

line and the rational for using the square was of course, to say that, that you might 

sometimes have data point that is above of the lines sometimes have data points that is below the lines. So, you have positive and negative values 

and you want to represent all deviations in the this same light you do not want the positive 

and negative values to cancel each other you just want to minimize deviation. So, you said if I squared all those deviations and just minimize the sum of those squares your it 

is sum of the squares, because each data point deviates from the line by some amount. So, 

you want to minimize the sum of the squared deviation of each data point from the line. 

So, that is the core idea and, so now, what you might want to do is and you know doing 

that conception allowed for this very neat derivation; such that you had of close forms solutions for beta naught and beta 1 and so on and it is very easy to implement. But, 

you know now, we have computers that are reasonably fast and you might not want to have these 

neat closed form solution, but you might just be willing to take an excel sheet and do the regression through some other metric that you want to minimize. 

So, instead of minimizing the square deviation of the data points from the line you might 

just want to take the absolute deviation, what we mean by absolute deviation is if the 

data point is 7 and if the fitted line at that value of x predict say 5. This difference 

between 7 and 5 I am just going to take it as 2 I am not going to think of it is positive or negative. So, whether the data point itself is above the line or below the line I am just 

going to measure the magnitude of the deviation and put them all as positive values and minimize 

the sum of those. So, whether you want to do that or whether 

you want to say my particular problem I want to penalize it by using cubes instead of squares 

or cubes would not really work, but I want to use the 4th power or something like that 

you can do it yourself. And, so this is what I call as they do it yourself regression and the idea here is that I have copy pasted the same data out here in the sheet. 

And, what I am saying out here is that these are the coefficients, so this value at b 3 

is beta naught this value its c 3 is beta 1 and we can just see those with some initial 

values I am just going to see that with 0, 0.5 you want to start with some reasonable 

values. And let us just put some dummy numbers out here and 0 here, so these just some dummy 

numbers with reasonable range and the ideas when you do that I ask the excel sheet, what 

is my model predicting. So, it is essentially like saying that if 

these were the betas then what would my model predict for these inputs. So, if these were 

my betas I am just going to repeat that if everything in row 3 from b through f my betas, 

then given the input, which is starts at row 6 and goes on. But we will take one sample 

input, which is shown in row 6 from c through f given these inputs, what would my module 

predict and the math of that is fairly simple it is just of the it is just the math of beta naught plus beta 1 x 1 plus beta 2 x 2. So, that is, what is I do say b 3, which is 

nothing but, beta naught plus the some product of these arrays in 3 and 6 and sense the word 

you know some product is nothing but, saying you give me to arise and I am going pair vise 

multiply the terms and add them all together. So, what is doing is its doing b 3 plus c 

3 times c 6, d 3 plus d 3 times d 6 and so on all the way till the end and that is was 

the model is predicting. So, I have the actual value, which happens 

to be 21 and I have something that the module predicted and from that I can very easily calculate the residuals. So, what shown in i 6 is nothing but, the difference between 

the actual y and the predicted y. So, we do that and we than we could square it or we 

can take the absolute value, which is what I am interested in and, so I have a set of square deviation and I have a set of absolute deviations it looks like I am in general chronically 

over predicting, but that is just because I have given dummy betas for now. And out here, what I have in cell o 3 is the sum of the absolutes the residuals sum of 

the absolutes and in o 2 I have the residual sum of squares. Now, remember in an ordinary 

least squares regression what you have in o 2 is what you are trying to minimize by 

changing the values in b 3 through f 3. So, you want to change the values in b 3 through 

f 3 and minimize what you getting in o 2. But, in our particular example, what we want 

I want to illustrate you today I want to try and minimize, what I am getting in o 3. So, the way I would that is to again go to data and click on solver again if you do not 

have it in variable you need to add it from add ins and I tell solver saying I want to 

set this objective, which is o 3 I want to minimize it. So, I can either maximize it 

or set try to set it to value, but I want to try and minimized and I want to do that by changing this cells in b 3 through f 3 and a given the kind of optimization approach 

you taking you might have to either give some constraints, which just means giving it some lower bounds and upper bounds. So, give it something fairly reasonable, which 

is what I have already done here and the way you do that just go click on add and then mention this cell in give it to lower bound then give it to upper bound and that is what 

I have done here. And you haves a couple of different methods of solving this optimization 

problem I choose the evolutionary one you can play around with the others, because the I just feel like it takes time, but its I know for a fact the there is safe at its not 

it is in terms of computational time it is; obviously, not going to be the most effective. 

But, I am more than happy to pay that price this is not this particular example is not 

too hard problem to solve and I just click on solve. And then, as you can see right now, excel is doing the computation, so you know it takes 

couple of minutes and what we would do is we will come back to this excel sheet and look at the results in a minute. So, going back to the slide, so that, so in today’s 

class we have seen two demos one is using excel to perform the multiple regression. And the second one is just going beyond the ordinary least squares, where you might have 

some other objective function that you want to minimize and you should be able to you know fairly is re do that in excel by just doing at the way have demonstrate. 

Now, we come to our next topic, which is the whole idea of subset selections and it goes 

back on I think we have the excel results may be we will just take look at that and 

it says you want a keep the solve a solution and I say yes and this is it. So, and if you 

notice this is little different from the output that you got it is not the exact same output 

you know perhaps if you let the solve go on longer it mate up come up to something may 

be more similar, may be more superior in either case. Because, these objective functions different you should expect different answers and this 

is the linear equation that solver gave you one if it wanted to minimize absolute deviation. 

So, going back to our slide we spoke about, how if you in the regression analysis you 

saw, so; obviously, the do it yourself regression does not give you any inferential statistics it is just is way of getting coefficients. Now, there is the other important part, which 

terms do you leave in the model, which you take out and when looking at that we look at this fairly simple idea of measuring each individual term through the p value of the 

t statistic. Just you jog your memory I am talking about this output and I am talking 

about these p values that you see in column e from 17 through 21 and you could have an 

idea saying anything below certain values acceptable anything above is certain value is acceptable and that that would be this most simplistic way of doing it. 

But, the problem becomes that I mean there your using only inferential statistic to decide 

which gets included and which does not. And the idea is that sometimes having certain 

term inside, which might be statistically significant could affect other terms, so the 

other variables could also get affected. So, the problem is not I mean you could use inferential 

statistics and make one time decision, but what happens if you choose to add a variable 

and that changes the p value of another variable. And also you might you might be of the opinion 

that what you care most about is your prediction accuracy whether terms its significant or 

not that you care about getting the best prediction accuracy in which, case it is still possible 

that adding certain terms is detrimental and removing them is might be the best. So, often 

times, what we wind up doing is we wind up having some metric that measures, how could 

be performing as a model as a whole not as the individual term. 

And we already saw that we for instance solve this solve that in the ANOVA that we have, so this is measuring the model as a whole. And we might say all I care about is this 

p value that you see in f 12 that is all I care about. So, can I make a decision on which, 

terms should be in the model and which terms should not based on this one metric you might 

have another metric, which says that I want to look at adjusted R square we have not talked about that at, but will we will come back to that, but you could have this one metric 

that you want to minimize. And as we get more advance you look you will 

we will realize how there are many other criteria, which we could use to say, what is a good 

regression and what is a better and have a standard frame work to compare two different 

models. Now, assume that you have the standard frame work or for instance assume that you 

care about f 12 you care about minimizing f 12, which is a p value associated with the f statistic associated with the whole model. Now, it is perfectly possible that if you 

choose to add some terms that your overall model becomes better. And if you choose to 

remove some terms of, so may be if I removed d as an input variable as a whole I might 

get a better p value or it is also possible that if I added another variable e it could 

be a lower p value. But, how do you go about searching through the entire possible set 

of input variables to figure out, which ones should be in there in which, one should not. 

And this becomes very important question especially in light of one what we have already discussed, which is prediction accuracy I mean. So, imagine a problem that you faced with, which has about 

20 input variables and one output variable, which subset of the 20 and I am including 

that set, which is all 20 and I am including that set, which is none of the 20 the null 

set, but which subset of that of those 20 variables should be there in the model. 

So, as to give you the best performance, where performance is measured for instance to the 

p value of the f statistics that is one side of a this the other side also, which is that 

if you had a huge set of variables sometimes the interpretation of the model might not be, so clear. Because, these variables might be related to each other and the coefficient 

that one of them takes up would compromise the coefficient that the other takes up and so on. And it would be much easier you know if you 

could interpret the model with just the smaller set of variables at least the ones that are 

the strongest and in some sense that you know enables you to get the big picture of the 

entire process. So, what we are going to talk about now, is that process of choosing that 

subset of variables, which is the best in terms of creating a model. The goal standard 

in doing that is this process called, let me just make that full screen the goal standard 

of doing that is this process called best subsets regression best subsets regression 

basically says. So, you have 20 variables or let us say I 

had ten input variables let us try every subset of those 10 variables. So, you will start, 

where having none of the variables or and the end you will move you will have 10 c 1 

combinations of having one variable, which essentially means having variable 1 variable 

and that is one module having variable 2 that is one model. And then, we will move to the 

twos the 10 c 2 combination of two input variables are in the model I have 1 and 2 in the model 

I have 1 and 3 in the model I have 2 and 3 in the model I have 4 and 5. So, all the combination of two variables and then, you will have all the combinations of 

threes and you will go all the way to having all the 10 variables in the model that is a lot. So, that is 10 c 0 plus 10 c 1 plus 10 c 2 plus 10 c 3 combinations, but you know 

what with good computational speed and reasonably handle able problem you might just be able 

to use of brood for approach and look at every subset that is possible and your done with 

it. So, best subset regression of in its kind of seen as the goal standard of doing it and it allows you to you know evaluate every subset. 

So, it is essentially like creating that many different regression you possibly cant manually 

do it in excel, but what you can do is you might be able to do it in another language 

like you might need a little bit of programming document we are able to do it in other software. 

But, essentially that whole process of doing tools data analysis choosing a regression 

and getting that new output sheet you would do that for various possible combinations 

of the input variables for every possible subset of the input variable and that is called 

best subsets selection. Now, when that is not possible we tend to go towards the sequential 

methods and the problem with that is that there is a certain impact of adding and removing 

variables on each other. So, I can never say that I am going to add this variable and its 

going to cause this effect. And then, choose to remove another variable at a certain different point of time, but the effect of adding or removing a variable 

impacts other variables and for that reason no form of a sequential approach is mathematically 

going to be exactly the same we able to always recreate a best subset selection. But, for 

the greater part these methods that are sequential just work are just find and in that light 

I would say the more popular approaches are often called step wise regression. 

You have forward step wise selection, which means you kind of sequentially start adding individual variables to your model and you keep adding till a point, where the model 

does not improve any further in fact, the gets worst and any take a step back in say this was the best stay. You have back word step wise selection where you start with all 

10 input variables in your model in a sequentially removing them. So, for instance one way to 

kind of show you how you could that in excel is to say. So, I had all A, B, C, D in the model now D is a worst I am going to remove that and 

then, say does that improve my p value of the f statistic in f 12. So, it would literally be like repeating the data analysis going back to the data going back to data analysis 

saying I want to do a regression, but this time please do not include D in my range, 

so that is what I am going to do I am going to include the label. So, it is now, I have 

done that again and now I have a different analysis and low be whole this p value is 

much lower than the p value of the previous analysis. So, may be d should have been remove and to kind of do this kind of a process sequentially 

removing variables and getting to the point, where removing another variable is not good 

thing and you could use for instance the p value of the individual variables to tell you in which order to remove the variables and so on. Now, again this is fine as long 

as you have a reasonable number of variables if you have 50 an odd variables it becomes 

a very cumbersome. So, you could just essentially use software package to do the same thing and we will briefly demo that. But, before we do that you finally, 

have something called high bridge step wise which essentially is a combination of forwards 

and backwards step wise you start with nothing in your model in a sequentially keep adding terms. But, it each state you see whether which term to add next or which term to delete 

next and you put them on a common platter and make a decision on which to try. So, that is hybrid step wise and what we are going to do now, is demonstrate how you can 

actually do hybrid step wise selection in MATLAB this is the MATLAB screen if you have 

not seen it this is the version R 2013b the path that I am selecting essentially is called a command window and that is a place, where you can just type instructions and get outputs 

and we are primarily going to be dealing only with that space. So, what I have done is I already done in this, but I just show you what I did, which 

is I just selected the data and I just copy pasted it in MATLAB. So, I just to copy this 

I went to MATLAB and I said oh by the way this variable y is equal to open square bracket 

paste the data close square bracket enter and MATLAB is created a variable for me called 

y with the data and it the same process for x and all I am going to do now is present 

MATLAB the with the simple command call step wise and I will given x comma y as. If you 

noticed prompted me in terms of what do already put in there. 

But, you can also provide MATLAB saying I want some terms mandatory they need to be 

in the model and you can tell that and you can also give a criteria for entering and 

removing terms. Just like for instance you could have used some criteria out here to 

look at these p value is to decide, which order to go with MATLAB you can essentially 

say if its below certain range I wanted to go on and if its above a certain range I wanted to come out the default I believe this 0.05 and 0.1. 

But, we are not going to give any of those terms we are going to keep it simple and just hit n and what happens is MATLAB look at this data and it essentially comes up with an interface 

like this and anything that is red basically means is not in the model. So, now this is 

nothing in the model and this is the intercept and there is no f statistic, because is nothing 

in the model and so on. But, MATLAB gives you a suggestion what to do a next let’s just move x 3 in. So, all you will do is you will go to x 3 

and you will click on it and its moved in and immediately you notice that you have a f statistic you have a p value in, then says please move x 1 in see you go to the red dot 

and click on x 1 and that is moved in it looks like the p value even going to lower then 

it as x 2 in and you still doing better and now, it says do not move any more terms in. 

So, we went through a case, where the sequentially said add a add b add c and you know smartly 

it said do not add anything more and do not do anything more. But, you could see situation, where it will ask you to then add something in then, go 

back and delete something, but it kind of not only does it give you the interface do it. If, so you could for instance add d and c was the performance how the performance 

changes and you can see performance becomes worse after adding d. So, it is 1.76 if I 

take the d out again it is the p value is even more. So, essentially MATLAB not just 

I mean it allows you to do whatever you want, but it also gives you a recommendation by telling you what the next steps are. And at some points if you say export it will 

export the model for you, but you can also visually see it its basically saying this is the coefficient of x 1 a this is the coefficient of b this is the coefficient of c that that 

you can see under the column co f and it is also tell you what the intercept is it will tell you what the R square is and it will tell you the route means squared errors and 

so on. So, it is the very convenient tool to clear around with if you really had like lost of variables and you just wanted to play around more than do like brute force approach 

to visually see, what happens if you add one variable and remove a variable this could be very convenient great. So, with that we conclude our lecture on kind 

of showing you how to perform a regression and interpreting results through software as well as tackling the problem of variable selection or subset selection. 

Thank you. 

English - NPTEL Official 

Regularization/ Coefficients Shrinkage 

Hello today we will continue our lecture series in the topic of regression and in our last 

class, we primarily focused on, mainly ended, fine with the ideas behind subset selection. 

So, essentially we were talking about this problem, where you have a lot of input variables 

in, that you could use in your multiple regression model, how do you go about picking, which 

one should stay in the model and which one should go out. And in discussing that we would 

discussing a more broader topic which is, how do we measure how good a model is and 

how do we do things in our regression practice, so that we get good predictive accuracy and 

it is also useful for interpretation. In that regard, we will continue our lecture 

today and talk about two other small measures, which is the R square and the adjusted R square. 

The R square has at least been introduced to you, you know you have an idea of where 

you can find that in your analysis. We will talk a little bit about that and we 

will talk about them more as other matrix of measuring, how good our regression is and 

then, we will move on to this topic called regularization, which has to do with how do 

you fine tune the datas, the coefficients of your regression model. 

When we were discussing the topic of subset selection, which is, how do you go about choosing 

which variable should stay in the model. We said, we would go about making those choices 

based off of some metric, not of the individual variables, but of the model as a whole, correct 

and in that context, we primarily discuss the use of the p value from the f test that 

you do with your ANOVA. So, for instance I have copy pasted the same results from our 

examples that we took in our last class and so, this stable out here kind of… 

It is just a copy paste from that previous excel sheet and we were focusing on this p 

value primarily saying, this ANOVA is essentially a measure of how good the overall regression 

is, not individual input variables, but can we use that, can we use this p value as a 

guide to see which variable stay in and which variable stay out and that is the context 

in which we discuss the topic of best subset regression, we spoke about backwards, forwards 

and hybrid stepwise regression. But, today what we are going to talk about 

is, we talk about two other measures which are also available in which is, something 

that you could use and they primarily the R square and adjusted R square. So, the first 

thing that we are going to start by saying is the R square is not a very good measure, 

in terms of how the overall regression model is, especially in a multiple regression context. 

I will explain that in a second, what we mean by… 

Again just as a reminder, you know simple regression just means there is one input variable, 

multiple regression means there are multiple input variable. So, the R square essentially 

measures, is a measure of how much of your variation and here, you are talking about 

variations in terms of y, your output variable. So, how much if your output variables variation, 

the sum amount of variation and how much of that can I explain using my model and how 

much of that can I not explain using my model. So, if I take the overall variation of y and 

I break it up into two chunks, the amount that can be explained by my model, which is 

my regression equation and the amount that cannot be explained by that. If I put those 

two together I should get the overall variation in y irrespective of x. So, R square is nothing, 

but this ratio how much variation is explained by the model divided by the total variation. 

So, depending on the software that you use and the text book that you use, people usually 

have sum of squares total and that is the total variation and sum of squares model that 

is the model variation. But, I have also seen sum of squares regression 

as a proxy for sum of squares model and the sum of squares… If you take, if you say 

the total variation is sum of squares total and the model variation is either sum of squares 

model or the sum of squares regression, then the only the other quantity that is left is 

the sum of squares error or you know, as some people again call it the residual sum of squares. 

You might have seen it RSS or sum of squares error. 

So, this, your R square value therefore, is nothing… You can just basically look at 

your ANOVA output, even if your R square for some reason disappears. It is nothing but, 

the ratio of this number to this number, in which case I think that looks like it is about 

0.8, 0.9 invalid. To give you a little more mathematical intuition and also kind of giving 

you some kind of formulas that are used for calculating these three values. As I mentioned, 

the sum of squares total is just essentially the variability, in some sense the total squared 

distance of each data point from the grand mean, whereas sum of squares model is where 

you go to each, not each data point, but here y of i is each data point. 

We look at the deviation of each data point, actual data point from the grand average. 

With sum of squares regression or sum of squares model, you look at each predicted value y 

i hat. It is nothing but, what value of y are you predicting for each x and look at 

the deviation of that from the grand mean and finally, y i minus y i hat is how much 

is each data point deviating from each predicted value. So, that should hopefully give you 

some intuition us to how these values are calculated. 

So, now, going back to a bigger topic, as a metric to see how good a regression model 

is especially in a multiple regression, where there many variables, many input variables. 

This R square is not great, because R square by definition will only increase as you keep 

on adding more and more variables. So, we will be discussing this in great detail in 

our lectures on the bias variance dichotomy, which is going to come up, but the core idea 

just to kind of give you some feel for what we are talking about is that, if you keep 

on adding variables and if you keep on adding complexity, at some point you should be able 

to explain away all the data that you have or in another words, take the example where 

you have 10 data points and you had only 10 data points. 

You should technically be able to fit a line or essentially fit a function that will go 

through all these 10 data points. If you just choose, you know if you say I am willing to 

fit a 9th order polynomial, essentially if the fitted model can get more and more and 

more complex, then for a finite set of data point you should be able to explain the way 

everything, but that is not necessarily great. Because, when you go and try and predict with 

that model, you are not going to do too well, you just do kind of over fitted to the data 

by just constantly increasing the complexity of the model that you are using. 

Now, given that is the base and given that we were talking about this idea loosely when 

we in our previous lectures spoke about, how it is really important to try and figure out 

which of the subset of the variables you want in the model and you want to throw the others 

away. This R square does not help, because R square will never decrease as you keep on 

adding more input variable. So, let say I had a model with 5 input variables, I suddenly 

come up and say, maybe I should have the 6th variable and add it, it is almost it is definite 

that R square will increase and so, R square itself is not a great measure of how good 

a model is. Now, as we discussed you could definitely 

use the p value from the f test of the NOVA, but another metric that is also popular is 

called the adjusted R square and that is essentially nothing but, a modified version of R square 

that is shown in the formula here. So, it uses essentially to compute it, you can use 

your R square values and the idea is that n is nothing but, the number of data points 

and k is the number of independent variables that are there. 

So, for instance in our example, I believe we had 88 data points in the excel example 

that we were discussing yesterday. It is 88, I can infer that also from the total degrees 

of freedom and k essentially is 4, because we had four input variables. See, you can 

use k, the k will be equal to 4, so 88 and 4. So, the idea out here is that, when you 

use the adjusted R square, it kind of penalizes modules, where the number of variables you 

are using to explain it is too high. So, you can compare the adjusted R square 

of one model versus the other, where you used different numbers of input variables and yes, 

the higher your R square the better it is. So, if you can get a very higher R square, 

you are going to get a, you know that is one way of getting a higher adjusted R square, 

but not at the cost of, you know having too many variables. You want to keep k as small 

as possible and have a very high R square. Notice that, there is a minus in front of 

the k, but this whole thing also has a 1 minus, so just be careful when you are trying to 

get an intuition of, how increasing or decreasing these values is going to effect the R square, 

adjusted R square. So, that is more just, again introducing you to the concept that 

there is, it is not the p value of the f statistic as we discussed in the best subset selection 

and stepwise regression, but this is the concept of adjusted R square as well and that is just 

one of the them, there are other metrics also that can be used to measure regressions, a 

regression model. 

We now go on to the topic of regularization and you can also use the term coefficient 

shrinkage to express the same concept. What regularization does is also, you know as a 

huge over lap what you try to do with sub set selection, which is the core idea being 

how can I simplify my model in some sense. Because, I care about predictive accuracy, 

I care about interpretation, I do not care about just trying to get this function to 

go though all my data points. So, how can I simplify the model in some way 

and one way of simplifying the model is to use fewer variables, that is what we saw in 

subset selection. But, once you fix the numbers of variables, once you say I have fixed these 

are the variables that need to go in the module, is there some way to more smoothly determine 

the coefficients. We saw how the coefficients were being determined through an optimization 

process, but somewhere can I go directly in there and put in my constraint of saying, 

I do want you to optimize something, but at the same time I want you to trying keep it 

as simple as possible without kind of over fitting the data. 

So, in some sense the problem of regularization is a problem of saying I have an algorithm 

and I have a fixed number of input variables. So, I do not, I am no longer bargaining to 

throw variables out and keep them in, but is there some way of in my fitting methodology 

itself preventing this problem of over fitting or preventing this problem of trying to get 

the functional form to be so hardwired to the data, that it does not do such a good 

job of predicting and kind of, can I impose penalties upon myself to kind of over fitting. 

One way in which over fitting happens is, in problems where the problems of what we 

call as multicollinearity. Multicollinearity is the idea that many of my input variables 

are correlated with each other and when you have that problem, the coefficients themselves 

through a regular regression process can get, the determining their betas can be kind of 

poor. Meaning that, there can be a lot of variance between one samples to the next. 

So, it is like, ideally if I am doing a certain regression process and I take a sample of 

data and I then do this regression fit. If I take other sample of data and I get completely 

different set of betas, then that is not a very stable process and that tends to happen 

with least squares regression, when you have multicollinearity, meaning you have lots of 

highly correlated input variables. I mean take this example that we have on the slide, 

you have this idea, where you get this equation which says y is equal to 4 A plus 2 B. 

Now, imagine a word in which A and B was so highly correlated to the point, where they 

were practically the same variable. Then, a fitted model that says y is equal to 10 

A minus 8 B should also a kind of give you the same results, in the sense that A and 

B are practically the same data points. They correlated to 1 and now assume that they are 

also equal in magnitude, the 4 A and 2 B, you can substitute B with A and you net getting 

2 A, which is the same net you are getting in the second equations. 

So, our both equations the same, now the idea with something like ridge regression is, you 

in cases like this, you want to have the simplest possible equation and you want to have the 

lowest possible magnitudes for your variables. So, you would be very happy with the y is 

equal to 2 A, I am just keeping it as simple as that or you can take it as 2 A minus 0 

B. So, in addition to choosing which variables, you want to keep their magnitudes, the magnitude 

of the coefficients, you want to keep them as low as possible. 

So, if you have off setting coefficients, you are not making a regression equation that 

says y is equal to 10,000 and 2 A and you know, you know really large say 1000 and 2 

A minus 1000 and B. Your kind of blowing things out of proposition, it will not generalize 

very well and so on and so forth. So, how can we achieve this? How can we achieve this 

goal of and really simple terms, not just allowing highly correlated variables to take 

up opposing sides and you know, have really large coefficients. 

And the way we will do that is, by taking a standard least squares minimization problem 

and that is essentially what have written as the objective function here. So, the objective 

function here is no different for the ridge regression. At least the way have written 

it out here is no different than for least squares minimization, because all I am saying 

is let us minimize for each data point, the actual y minus the predicted y and the predicted 

y is nothing but, beta not plus beta 1 x 1 plus beta 2 x 2 plus beta 3 x 3, goes on. 

So, essentially j goes from 1 to p, meaning p is the total number of independent variables, 

n is the total number of data points, so you do it for each data point. You look at the 

deviation of the fitted value, the actual value to the estimate or the predicted value 

and the goal is, this deviation is what needs to be minimized. The square of this deviation 

is to be minimized in an ordinary least square regression, but in addition to that and what 

we do with the ridge regression is, we do this optimization with a constraint and the 

constraint can be written like this. The constraint said it is the subject to some 

beta, you take each beta which is each coefficients, square it and the sum of those square should 

be less than sum value s and that value s is essentially, it is not like you have particular 

number in mind. What winds up happening is that you can rewrite this optimization by 

just not having this constraint, but essentially out here, just adding minus lambda and summation 

of beta j square. So, this is kind of like, you might have come 

across this with like Lagrange multipliers, when you have a single constraint on the coefficients. 

You can just rewrite the objective function to have that constraint integrated in to the 

thing and essentially, it is like solving just the minimization without the constraint. 

But, essentially the solution to that is what will ensure that your data’s themselves 

are stable and you know, not taking really large values. 

Another way to achieve the same thing is, again even with the lasso regression, this 

should not be ridge, this should be lasso. So, even with the lasso regression what you 

see is the same thing, you have the same objective function, but now it subjective to the constraint 

that the sum of the betas is less than some value s and again, out here you can just wind 

up rewriting your objective function. But, out here rewriting it does not do you too 

much good, because you cannot do the same trick as with standard calculus with Lagrange 

multipliers, where you have a beta square. When you have a mod beta, it just becomes 

computationally harder, but you know just like we saw in the simple regression case. 

If you got an excel sheet or a MATLAB, you can just do the optimization and put the sign 

as a constraint and as long as your optimization technique is good enough, you should be fine, 

you should be able to redo it. I hope that gives you some idea of regularization techniques 

and look forward to see you in the next class. Thank you. 

English - NPTEL Official 

Data Modelling and Algorithmic Modelling Approaches 

Hello and welcome to this lecture on K-Nearest Neighbors Techniques. So, in this lecture 

we will introduce a new supervised learning approach called K Nearest Neighbors and a 

broader thing that we are trying do to with this lecture is by introducing this technique 

and comparing it to something that you are already familiar with, which is regression 

analysis. By doing that comparison, we are hoping to kind of illustrateÊand help you 

appreciate two very different styles of performing the supervised learning analysis and you know 

and therefore, the process of prediction itself. Two very different approaches, two predictions 

and it is a kind of important to know that, because up until now you heard of what supervised 

learning is in theory and the first technique that we gone into and explained is regression, 

which is the certain way of for instance performing a prediction. The hope is by introducing k 

nearest neighbors, you see a very different way of achieving the same goal and it is important, 

because you will realize that a lot of other machine learning techniques, take inspiration 

from this approach. 

So, this dichotomy if you will was something that was first pointed out by Leo Breiman, 

a very famous statistician, where he said there are two very prominent cultures to statistical 

modeling and out here, he primarily talking about supervised learning approaches and he 

highlights and you know, there is some amount of overlaps and so on. And I would not really 

say these two cultures and two different, it is not like a classification as much as 

two very different styles. And the idea is that, he says look there is 

the data modeling approach, which is what you for instance in the standard regression, 

where you have some you know output variable y and you kind of envision this output variable 

has some function of your input variable or variables x. So, for instance you would say 

y is equal to some, you know f of x and plus there is of course, some noise. In the case 

of linear regression and here a kind of shown you an examples of multiple regression, this 

f of x becomes fairly straight forward, it some intercept plus beta 1 x 1, beta 2 x 2 

for how many ever input variables you have. Suppose, you just have one input variable 

that would be beta naught plus beta 1 x 1, it is fairly straight forward. So, the point 

he makes and it is generalization, because he says us about many other statistical methods 

which he says, you have these whole breed of approaches to supervised learning which 

capture a functional relationship between y and x and that function is in some sense 

cast in stone and the only job you are left with is just go, get the data and figure out 

what the parameters are of the function. These betas, the job is to take some data 

and figure out the betas, this functional form itself which is said in this case of 

the linear regression, that it is a linear combination of the different inputs plus some 

Gaussian noise that is for instance cast and stone. And he says that you have those approaches 

and then, you have an alternative breed of approaches and he calls them the algorithmic 

modeling culture. So, the first one he calls is the data modeling culture, which is your 

and the best examples of the data modeling culture could be is the multiple regression. 

So, we covered that in good detail in the previous lectures. So, he says now look there 

is another approach, another approach is what he calls the algorithmic modeling and there 

he says essentially, the focus is not as much on a rigid mathematical model that you have 

presupposed that have you apriori and that creates it is relationship between y and x, 

which is already cast and stone. But, instead you really have a set of algorithmic instructions 

or algorithmic ideas that relate the independent and dependent variables. 

So, y is in the loose sense of the word of function of x, but that is really captured 

by algorithms rather than rigid mathematical models. Now, these two, there is a reason 

why in the start I said these are not like two classifications, but there are more two 

styles only because you can describe any algorithm in a mathematical form and perhaps you can 

describe the math in an algorithmic form. But, what I am going do is I am going to explain 

to you the k nearest neighbors approach and we are going to talk about the k nearest neighbors 

approach as an example of the algorithmic modeling and hopefully at that point, it becomes 

really clear us to what the stylistic difference is between these two approaches. 

So, let us look at how a prediction task happens with your linear regression approach. The 

way it happens is you have some data and right now I am focusing on the graph on the left 

hand side of the screen, you have some data, you do a regression analysis and what is the 

regression analysis do, it fix the line through the data, it fix a line and if you are using 

ordinary least square regression, if it is a line that minimizes the square deviation 

between the data points to the line and so it chooses the line that achieves this target. 

So, you have you fit this line. Now, what you do when you need to make a prediction? 

And someone comes along and says, oh great, so you done a regression analysis. Could you 

tell me what, y I should expect for a given x? So, they come and give you an x, so they 

give you this x and let us call it x, let us just call it x 1. So, they give you an 

x 1 and say can you tell me what why I should expect and what you do is you draw this straight 

line up like it is already there and you say, let me see where my what value of y I would 

get based on my fitted model. So, I basically take this x value, draw a 

line up to my regression fitted line and then see what the height of that point is. So, 

this is my predicted y you know, so may be y hat of 1. So, if somebody comes and gives 

me this x all I do, I do not really I have used all of this data to create a line, but 

then after that I can lose these data points, this data points can be erased from my memory. 

All I need to do to make a prediction at this point is I need to know this line and this 

line I have already created with the regression. So, I have created this line I have it my 

regression and someone comes and ask me a question is to what y would you see with particular 

x, as you know this line has a form y is equal to b naught plus b 1 x. So, someone comes 

and gives me a particular x. So, call it x 1 I will just substitute this x 1 out here, 

I know the values of b 1 and beta naught from the regression, that is how I was able to 

plot the line. I will then just get a y, because I know all the three terms on the right hand 

side. So, I will get a predicted y and that is my 

prediction of what why I will see for a given x. So, that is how a prediction task takes 

place of the regression approach. Now, let us take a look how the k nearest neighbors 

approach works, the way the k nearest neighbors approach works is that I basically I do not 

fit a line. So, I do not have line or I do not have mathematical form, the idea behind 

k nearest neighbors is that if you come to me with a question as to hey can you predict 

for me what y I will see for a given x. I will take the given x and I will ask myself 

who it is nearest neighbors are. So, somebody walks up me and says can you give me a prediction 

of y for this x 1 let us call it x 1, let us call it x star maybe. So, x star for this 

x star, because x 1 tends to have the connotation that it is the first data points. So, I going 

to call it x star, so for this x star can you tell me what my predicted output would 

be. Now, remember I do not have a fitted line, this is a completely different approach to 

making predictions. But, what I do is I go to this line and on 

my x axis, on my input variable axis I try to find the nearest neighbors of neighboring 

data points to the x under question. So, clearly this data point is kind of close to this line 

and this data point is may be close to this line and so on. And the idea is that this 

K-NN approaches has a parameter which is k. So, let us say I have chosen five as a parameter 

and we will understand what the five means, it is means I am looking for the five nearest 

neighbors defined by the distance from my point under question, so this distance. 

So, I am going to see the five closest data points and if I do that for instance, I see 

that these five data points are the ones that are closest. So, what do I do once I have 

identified the data points I take their average wise to make a predictions. So, I take the 

average of this y. So, let us call that y a this y, y b and so on. So, I do the same 

thing for this data point, this data point and I take the average of all those five specific 

wise and that is my predicted y, the predicted wise the average and there are many modifications 

to this, sometimes you do not take the average, you might do like the localized regression 

there, there are other ways, where you do not just take the arithmetic mean you might 

take the median you might do other things. But, the core approach with k nearest neighbors 

is that you have not fit a line, you not created any mathematical abstraction of the data, 

you not abstracted away from the data. For instance, in the regression remember I told 

you if I need to do a prediction task, I do not even need to remember the data points 

I can throw all my data points wave. Once I used the data points to create this line, 

I now only need the line to make a prediction I now have a y is equal to f of x. So, I have 

this functional form and I just need to plug in my values of x that I want to use to predictions 

and now I will automatically get a predicted y. 

Here we are not doing that abstraction, we are retaining the entire data set of points 

in the k nearest neighbors approach, we need all our data points. Now, you come and ask 

me a question about a particular x, I am going to go and look at the nearest neighbors of 

that x and if it is five nearest neighbors I am going to look at the five nearest neighbors, 

if it is ten nearest neighbors I am going to look at the ten nearest neighbors in all 

these cases I am going to look at nearest neighbors and you know either take a vote 

if it is a classification problem or if it is a regression problem am I choose to do 

choose to take something like an average. So, that is fairly clear in this line that 

I show is kind of what is approximately the arithmetic mean and we use that and finally, 

that is the output that we going to predict. 

Now, that should kind of illustrate k nearest neighbors for you it is a fairly simple technique 

a lot of the focus in using this approach to solve problems is based on how, because 

it is computationally very hard. Because, it is storing all the data and you need shift 

through all the data to find the nearest neighbors good amount of the focus is on the data mining 

aspect or the computational aspects using something like that, but it is also very convenient 

when you have no ideas to what the functional form is. 

So, suppose the linear regression approach can be very useful if you believe the relationship 

between y and x is the straight line. But, if you do not want to make that assumptions 

at all k nearest neighbors approach is fairly flexible to any function of form that y and 

x could have. And there are two points to note here just an addition to what we have 

discussed which is this k nearest neighbors approach; obviously, works when you have multiple 

input variables. So, the examples that we took in the previous case there was one input 

variable and then there was this output variable, this is the output variable. 

Now, what if had multiple input variables, the same idea. So, here we have one input 

variable in the x axis, one input variables in the y axis. So, where is the y, where is 

your output variables, one way to think of it is that it is coming out the screen essentially 

we are not able to in a two dimensional screen show you the three variables. But, if you 

assume that y is kind of coming out of the screen, you could graphically represented 

that way. But, the important point that I wanted to 

make here is that if you needed to take like a five nearest neighbors approaches on two 

input variables, you can still do that let us say you are interested in this particular 

point marked with the x then your nearest neighbors to this x again get defined on the 

two dimensions. So, it could be this point, this point, perhaps this point and what you 

can see is we are taking some form of like may be Euclidean distance, because the distance 

itself is not just on the x 1 axis, it is not just on the x 2 axis, it is on both x 

1 and x 2 axis. So, you can still use the concept of the input 

space and once you have more than three year of when you have four or more input variables 

you are talking about hyper space. But, just you could still use some simple measures of 

Euclidean distance or some measure or some other distances some famous once I call them 

and Manhattan distance some of them are called the Mahalanobis distances, Mahalanobis distance 

as well. Now, the good thing with this approach is 

that you can even use it for classification problems, if you briefly remember in the previous 

lecture we distinguish between supervised learning tasks, which basically just means 

you have an output variable you need trying to predict that output variable from the input 

variables. But, this output variable can be continuous quantitative which is where we 

primarily talked about regression and so on. But, it could also a categorical variable. 

o, if taken an example out here on the right hand side, where you have two input variables 

x 1 and x 2 are two input variables and your output variables is categorical variables 

with two classes. So, think of it as male or female or buyer or non buyer in marketing 

contexts, defective product, not defective product in a manufacturing context. So, let 

us say this output variables which is a categorical variable is represented by either square circles 

or squares. So, the orange circle are one class of the 

output, the blue squares are another class of the output and x 1 and x 2 are just your 

two input variables again out here you might be interested in for instances making an prediction 

out here and you might take the five nearest neighbors perhaps it will be this and may 

be this. So, these might be the five nearest neighbors and because you need to predict, 

whether it will be class A or class B you might want to take a voting approach or if 

your approach is to predict the probability of it being circles or squares that is what 

I am call class A and class B then you might just take the ratio of the circles to squares 

in your nearest neighbors. But, the idea is that this is also works perfectly 

well, you do not need to just take an average, you can take you know ratios or you can just 

make them all vote essentially the majority win. So, you have three squares and two circles, 

so I am going to predict this is going to be a square, you can using voting approach 

to say belongs to a particular class. So, I hope that gave you an idea of k nearest 

neighbors, but also more importantly motivated to you the idea that you have this regression 

style approaches, where you got this explicit data model. 

So, this is your regression style approaches by you have a functional form and then you 

try and figure out the parameters. But, you can also take on very different approach to 

the process of predictive analytics, which is through the process of prediction, where 

the importance is not as much on the functional form which is cast and stone which is really 

an assumption you are making about the relationship between your input and output variables. But, 

it really goes beyond that right like you do not want to make those assumptions and 

you just want to take more algorithmic approach to this entire process. 

You will get encounter a lot of machine learning techniques that we are going to be discussing 

later in this course really belonging to that class. So, I hope that gave you an idea both 

k nearest neighbors and this dichotomy that you be kind of seen machine learning. 

Thank you. 

English - NPTEL Official 

Logistic Regression 

Hello and welcome to this module on Logistic Regression. 

So, we have looked at the problem of classification earlier and here is an example from one of 

the earlier modules. 

So, the users not in brown here are those who bought a computer and those marked in 

red are people, who did not buy computer. 

And the goal of classification we said earlier is to find a decision surface that would help 

us separate people who buy computers from those who do not buy computers. 

There are different ways in which you could have these decision surfaces and we looked 

at a few. 

Now, let us step back and ask the question, what exactly does this decision surface mean. 

Specifically let me ask the question, what is the data point that lie on a decision surface 

belong to, is it buy computers or does not buy computers. 

So, one way of thinking about it is to say that this decision surface denotes all the 

data points for which the probability of it being red is equal to the probability of it 

being brown. 

This essentially means that for the points on the boundary the decision boundary you 

are not able to make a decision as to whether you will buy computer or does not buy a computer. 

So, what is it tell us about the points that lie to one side of the boundary? 

So, the points that lie to one side of the boundary are those, where the probability 

that the person will not buy a computer in this case is higher than the probability that 

he will buy a computer. 

So, the one way of thinking about the decision boundary is that it models all the points, 

where both the classes are equally likely or equally probable to occur. 

So, if you want to go beyond classification, so you might be interested in knowing what 

is the actual probability of a specific class given a data point. 

Not just in finding the right classification, you really like to know what is the probability 

that the person buys a computer given the age and income of the person versus the probability 

that the person will not buy a computer given the age and income of the person. 

So, why would you want to know this kind of probability or the class label? 

So, one example is you could think of in medical domain. 

Suppose I say that, you have a specific disease or the patient walks into the hospital and 

the doctor says that the patient has a specific disease and you would like to know if the, 

how confident is the Doctor of the prediction. 

So, the Doctor says I am 95 percent sure that this patient has the disease, then you certainly 

would go into the treatment. 

So, like wise when you have a classifier that is going to give you a class label you would 

like to know, how sure the classifier is of the class label and that is one application, 

where you would like to see these kinds of probabilities. 

So, one way to approach predicting probabilities instead of just the class labels could be 

to treat it as a regression problem. 

So, let us stop and think about how you would treat classification as a regression problem. 

So, normally in classification, so you have labels, who does not buy a computer or buys 

a computer. 

So, instead of using these labels you could use an indicator variable for the class. 

So, if the user is or the customer is going to buy a computer I would say the output is 

1, if the customer is not going to buy a computer I would say the output is 0. 

Now, your data gets transformed into a regression problem now instead of a classification problem, 

where you have 0's and 1's as your response variables and the actual attributes of the 

data has the predicted variables for the regression problem. 

And you could use linear regression here, we all know about linear regression now; you 

could use linear regression here. 

And the finally, their function that if it f of x can be interpreted as the probability 

that the output y will be 1 given the data x, that seems like a reasonable way of doing 

classification. 

So, whenever the probability is greater than 0.5, you would say that x belongs to class 

1, the probability is less than 0.5 you will say x belongs to class 0, that it is actually 

a valid way of doing a classification using linear regression, but there are some problems 

with that. 

So, what are the problems? 

So, linear regression is not really limited in range, the output can go from minus infinity 

to plus infinity. 

So, typically this output cannot be interpreted as a probability, when you troublesome it 

is the fact that the output can be negative and therefore, this certainly cannot be interpreted 

as a probability even if you think of doing some kind of normalization. 

Having said that I should say, it actually works in practice, if you do not really want 

to treat it as probability, but just as a classifier, you know if it is greater than 

0.5 it take it as 1 and lesser than 0.5 take it as 0 it works well, it works in practice, 

but not that well and there is way of doing better than just using simple linear regression. 

So, I want to use linear regression still, but I am going to do that on a transformed 

function. 

If the transformation that we are going to talk about here is called the logistic function 

or the logit function, so let us have some notation here. 

Let p of x denote the probability that the output y is 1 given x, then the logit transformation 

is given by the logarithm of p of x divided by 1 minus p of x. 

So, if you think about the binary problem, so p of x is the probability of the output 

being 1 and 1 minus p of x is a probability of the output being 0. 

So, essentially you are taking this ratio of the probability of success to the probability 

of failure. 

So, this is known as an odds and so this sometimes known as the log odds function. 

So, now, what are we going to do in logistic regression is essentially try to fit a linear 

regression model to this logistic function as the output. 

So, essentially we end up saying that your log of p of x by 1 minus p of x can be modeled 

as some linear function, which is beta naught plus x times some beta 1. 

So, if you think about it you can solve for p of x from this kind of an expression and 

then you end up having p of x looking like a sigmoid function. 

So, e power of beta naught plus x beta divided by 1 plus e power beta naught x beta and you 

can simplify that and the functional form that you are going to get is something like 

this. 

So, you can see that the it behaves like a probability function. 

So, it transfer only from 0 to 1 and by varying the value of beta what you are going to do 

is your going to vary the slope and by varying the value of beta naught, you are going to 

vary where the function is going to rise. 

So, this gives us a very valid way of fitting probabilities, there is no problem with interpreting 

p of x fitted in this fashion as a probability. 

So, earlier we trying to interpret f of x in a linear regression model as a probability 

had problems, so we could not do that, because it could be a negative as we saw earlier. 

But, in this case since p of x is going to be limited between 0 and 1 he might as well 

interpreted as a probability is it that right model for doing it that is an open question, 

it depends on the domain that your working in, but it is fairly widely used and it is 

very power in resolves in a very powerful classifier and which you can use in variety 

of different settings, whether this assumption is actually supported by the data or not it 

seems to work well in practice. 

So, that fig did with the linear regression case we will predict the classes 1 and the 

probability of x is greater than 0.5 and 0 otherwise and this essentially you can show 

if this minimizes the misclassification rate given the form of the predictor that we had 

on the previous line one thing to note. 

So, even though p of x is given by this exponential function, the actual classification boundaryÉ 

So, what is the decision boundary? 

Decision boundary is the point, where the probability of class 1 is equal to the probability 

of class 2 or class 1 and class 0 probabilities are equal. 

So, you with the little bit of thought you can see that the decision boundary is still 

given by a line which essentially beta naught plus x beta 1 equal to 0. 

So, that gives you the decision boundary of the logistic regression classified as well 

and hence this is also a linear classifier and I mentioned earlier it is pretty powerful 

and works well in practice. 

So, let us look at an example of what happens when we fit data using logistic regression 

versus linear regression. 

So, here is a two class problem, so the data points or either in blue or in red and the 

shading in the region indicates, what is the class label that would be predicted by the 

classifier in those regions. 

So, on the right hand side you have slides I mean you have the prediction made by fitting 

a linear regression to the indicator variable on the left hand side you have the output 

given by logistic regression. 

So, you can see that linear regression actually makes a certain errors closer to the boundary 

that is because linear regression is essentially limited at the rate at which the curves can 

climb and when closer to the boundary when there are points that are bunch together from 

one class, but little further away from the rest of the class linear regression is not 

able to model those successfully, while logistic regression by virtue of the fact that you 

could have a steep climb from 0 to 1 is able to capture those data points. 

So, this is essentially the difference between linear and logistic regression. 

So, far I have been talking about binary classification problems, because they are easier to illustrate 

and kind of understand the basics behind. 

But, then logistic regression can be extended to multiple classes as well, suppose there 

are k classes then I would say that each class gets the different set of parameters beta 

naught and beta for that specific class. 

So, in that case what happens is your probability of... 

So, the probability that particular class is the right class for a data point is given 

by e power beta naught of c which is the e power beta naught of c plus x beta c it is 

our essentially the parameters specific to the class and divided by the total the normalizing 

factor, which is essentially the numerators sums for all the data points. 

To make the problem somewhat easier traditionally the parameters of one of the classes, it could 

be either the first class by numbering from 0 to k or it could be the last class which 

is k is set to 0 and you can think about it, it really does not affect what the classifier 

the decision boundary that you are going to learn it will change the parameters that you 

are learning, but the decision boundary that you learn will not be affected. 

So, in a sense you will be left with fewer parameters that you have to estimate that 

is because you are talking about probability distributions here and we know that as soon 

as you fix n outcomes in a discrete probability distribution of the n plus 1 the outcome is 

automatically fixed in total of n plus 1 outcome. 

So, far we have been looking at the basic model and logistic regression and I will end 

this module here and for the next module we will look at how will actually learn the parameters 

of this logistic regressions classifier. 

English - NPTEL Official 

Training a Logistic Regression Classifier 

Hi, so we are looking at the module on Training Logistic Regression Classifier now. 

So, in the previous module we looked at the basic idea behind logistic regression, which 

is essentially to do linear regression with a logistic transformation. So, you took the 

log odds function, which is p of x by 1 minus p of x, took the logarithm of it and try to 

fit a linear curve to this transform function. So, how do we find these parameters beta, 

beta 1 and beta naught? So, we optimize the likelihood of the training 

data with respect to the parameters beta, so that is essentially the way we are going 

to be training this. So, this is slightly different from some of the earlier methods 

we have looked at identifying the parameters, mainly because we are looking at here the 

probability of classification, not just getting the classifications right or wrong, but we 

are actually looking at the probability of classification, that makes more sense to try 

to optimize the probability of seeing the training data with respect to the parameter 

beta. 

So, what is the likelihood? So, the likelihood is the probability of a training data D given 

a particular parameter setting beta. So, you should note here that, it is the function 

of the parameter setting, because the training data D that is given to you is usually fixed. 

So, here is an example of the likelihood of some kind of classification tasks. So, I am 

going to assume that the data is given to you in the form of x i, y i pairs as we have 

done in the past. So, for each x i there is going to be an outcome 

y i, which will be either 1 if it belongs to class 1 or 0 if it belongs to class 2. 

So, let us look at one term in the product that I have written down there, is you can 

see if the output corresponding to x i is 1, then the first term in the product will 

be p of x i and the second term in the product will be 1, because y i is 1 and 1 minus p 

of x i is going to raise to the power of 0, which essentially reduce to 1. 

Likewise, if the output corresponding to x i is 0, then the first term in the product 

is going to be 1 and the second term in the product will remain as 1 minus p of x i. So, 

this essentially means that depending on, what the output variable is I am going to 

either take the probability of the data point occurring, probability of the data point having 

a label of 1 or the probability of the data point having the label of 0. So, to do recall 

that p of x i is the probability that y equal to 1 given x i. 

So, now, for this is for one data point and if I want to look at the probability of the 

entire data, I just take the product over all the data points, so the product runs from 

1 to n. So, this expression now gives me the probability of seeing the training data given 

a specific parameter setting. So, where do beta and beta naught appear on the expression 

on the right hand side, so p of x i is specified in terms of beta and beta naught. 

So, implicitly, so beta and beta naught are appearing on the right hand side of the equation 

and like I said, likelihood is the function of the parameters and hence we denote it as 

L of beta. So, now our goal is to optimize this likelihood and, so that, so we get a 

good estimate of the parameter, so we have to find the right set of beta, so that this 

probability is maximized. So, now, we look the term, the term looks a little hard to 

optimize, because lot of products here and, so we have to be little careful. 

So, the usual way that we operate here is to take logarithms of this likelihood and 

you can see that lower case l here is used to denote the log of the likelihood function 

and then, you just walk through this log likelihood expression a little slowly. So, now, that 

I have taken the logarithms in the first step. In the first step, so I have taken logarithms 

and therefore, the products that I had earlier have become summations and the exponentiation 

that I had earlier have become products. So, that corresponds the original exponentiation 

I had in my expression and now they have become products. 

Now, we can do a little bit of simplification here and you can see that, what I have essentially 

done is taken the… In the second step I have expanded the product term in the second 

term in this summation and then, I have gathered terms together which have a coefficient of 

y i. So, that gives me the second term in the summation and the first term is just essentially 

one times log of 1 minus p of x i. So, we know what log of p x i by 1 minus p x i is 

and that is essentially the function that you are trying to fit from the beginning. 

So, we replace that with our linear fit that we had, the linear regressions fit that we 

did. And then, we do further simplification in order to come up with the expression given 

on the last line, that essentially writing out p x i and then, evaluating 1 minus p x 

i and that gives me the negative logarithms term on the last line of the expressions. 

So, now, what do we do? We have the log likelihood, so what we do to maximize this log likelihood. 

We essentially take the derivatives of this log likelihood with respect to beta and then, 

we should be equating this to 0. So, the first line here is essentially taking the derivative 

of the log likelihood and it simplify to a very nice form, which is y i minus probability 

of x i times x i each individual component of x i and now, we set this equal to 0. But, 

we really cannot solve this that easily. Why? Because, p x i is actually a transcendental 

function, so it is not very easy to find the close form solution for these kinds of expressions. 

So, we have to actually look at numerical methods for solving these kinds of optimization 

problems and we essentially look at class of algorithms, which are known as interior 

point methods. So, I am not really going to get into the math behind all of this, but 

I assume that many of you have actually come across very simple optimization technique 

called Newton Raphson method. 

So and here is what the expression for Newton Raphson method is going to look like. So, 

I start off with a guess for my initial solution, the beta and start of the guess beta naught 

and I would typically like my beta naught to be close to the true solution. And once 

I have the guess for beta naught, then I keep updating the solution by essentially subtracting 

the first order derivative of the likelihood divided by the second order derivative of 

the likelihood. Take the ratio and then, subtract it from beta in order to give me my next estimate. 

So, this has fast convergence under certain regularity condition, so for one thing is 

that second derivative should like this and should be positive. And as you can see the 

first derivative is going to be 0 you are not going to be changing the value of your 

guess and when would the first derivative be 0 it will be 0 at one of the optima whether 

it is the maxima or the minima and you will also like this since the second derivative 

is going to positive to approach the minima. So, you can till when you approach optima 

you can be sure there is going to be the minima. So, this is essentially the basic idea behind 

the Newton Raphson method and when Newton Raphson method is applied specifically to 

the logistic regression problem you come up with the iterative technique, which is called 

iterative re weighted least squares approach for training for finding the parameters in 

logistic regression. And, so most of these statistical packages that we have especially 

R in particular of front trust was have a very simple function that loves you to fit 

logistic regression to any data set that you have and they will essentially be using Newton 

Raphson by way of iterative re weighted least squares techniques. 

To summarize this couple of modules logistic regression, so logistic regression is very 

powerful classifier build on the idea of doing linear regression on a logistic transformed 

output variables and the logistic regression is related to exponential family of probability 

distribution that rise in the variety of problems and that is the one of the reasons that make 

them very, very popular classifier. And apart from that they work really well 

I mean, so that is the another reason that logistic regression is classifier of choice 

for many people’s especially in medical domains, because they allow you to perform 

what you know as sensitive analysis. So, you can look at dependence of class labels on 

features by looking at the, the regression coefficient of specific feature in the fit 

that you obtain. Thank you. 

Training a Logistic Regression Classifier 

Hi, so we are looking at the module on Training Logistic Regression Classifier now. 

So, in the previous module we looked at the basic idea behind logistic regression, which 

is essentially to do linear regression with a logistic transformation. So, you took the 

log odds function, which is p of x by 1 minus p of x, took the logarithm of it and try to 

fit a linear curve to this transform function. So, how do we find these parameters beta, 

beta 1 and beta naught? So, we optimize the likelihood of the training 

data with respect to the parameters beta, so that is essentially the way we are going 

to be training this. So, this is slightly different from some of the earlier methods 

we have looked at identifying the parameters, mainly because we are looking at here the 

probability of classification, not just getting the classifications right or wrong, but we 

are actually looking at the probability of classification, that makes more sense to try 

to optimize the probability of seeing the training data with respect to the parameter 

beta. 

So, what is the likelihood? So, the likelihood is the probability of a training data D given 

a particular parameter setting beta. So, you should note here that, it is the function 

of the parameter setting, because the training data D that is given to you is usually fixed. 

So, here is an example of the likelihood of some kind of classification tasks. So, I am 

going to assume that the data is given to you in the form of x i, y i pairs as we have 

done in the past. So, for each x i there is going to be an outcome 

y i, which will be either 1 if it belongs to class 1 or 0 if it belongs to class 2. 

So, let us look at one term in the product that I have written down there, is you can 

see if the output corresponding to x i is 1, then the first term in the product will 

be p of x i and the second term in the product will be 1, because y i is 1 and 1 minus p 

of x i is going to raise to the power of 0, which essentially reduce to 1. 

Likewise, if the output corresponding to x i is 0, then the first term in the product 

is going to be 1 and the second term in the product will remain as 1 minus p of x i. So, 

this essentially means that depending on, what the output variable is I am going to 

either take the probability of the data point occurring, probability of the data point having 

a label of 1 or the probability of the data point having the label of 0. So, to do recall 

that p of x i is the probability that y equal to 1 given x i. 

So, now, for this is for one data point and if I want to look at the probability of the 

entire data, I just take the product over all the data points, so the product runs from 

1 to n. So, this expression now gives me the probability of seeing the training data given 

a specific parameter setting. So, where do beta and beta naught appear on the expression 

on the right hand side, so p of x i is specified in terms of beta and beta naught. 

So, implicitly, so beta and beta naught are appearing on the right hand side of the equation 

and like I said, likelihood is the function of the parameters and hence we denote it as 

L of beta. So, now our goal is to optimize this likelihood and, so that, so we get a 

good estimate of the parameter, so we have to find the right set of beta, so that this 

probability is maximized. So, now, we look the term, the term looks a little hard to 

optimize, because lot of products here and, so we have to be little careful. 

So, the usual way that we operate here is to take logarithms of this likelihood and 

you can see that lower case l here is used to denote the log of the likelihood function 

and then, you just walk through this log likelihood expression a little slowly. So, now, that 

I have taken the logarithms in the first step. In the first step, so I have taken logarithms 

and therefore, the products that I had earlier have become summations and the exponentiation 

that I had earlier have become products. So, that corresponds the original exponentiation 

I had in my expression and now they have become products. 

Now, we can do a little bit of simplification here and you can see that, what I have essentially 

done is taken the… In the second step I have expanded the product term in the second 

term in this summation and then, I have gathered terms together which have a coefficient of 

y i. So, that gives me the second term in the summation and the first term is just essentially 

one times log of 1 minus p of x i. So, we know what log of p x i by 1 minus p x i is 

and that is essentially the function that you are trying to fit from the beginning. 

So, we replace that with our linear fit that we had, the linear regressions fit that we 

did. And then, we do further simplification in order to come up with the expression given 

on the last line, that essentially writing out p x i and then, evaluating 1 minus p x 

i and that gives me the negative logarithms term on the last line of the expressions. 

So, now, what do we do? We have the log likelihood, so what we do to maximize this log likelihood. 

We essentially take the derivatives of this log likelihood with respect to beta and then, 

we should be equating this to 0. So, the first line here is essentially taking the derivative 

of the log likelihood and it simplify to a very nice form, which is y i minus probability 

of x i times x i each individual component of x i and now, we set this equal to 0. But, 

we really cannot solve this that easily. Why? Because, p x i is actually a transcendental 

function, so it is not very easy to find the close form solution for these kinds of expressions. 

So, we have to actually look at numerical methods for solving these kinds of optimization 

problems and we essentially look at class of algorithms, which are known as interior 

point methods. So, I am not really going to get into the math behind all of this, but 

I assume that many of you have actually come across very simple optimization technique 

called Newton Raphson method. 

So and here is what the expression for Newton Raphson method is going to look like. So, 

I start off with a guess for my initial solution, the beta and start of the guess beta naught 

and I would typically like my beta naught to be close to the true solution. And once 

I have the guess for beta naught, then I keep updating the solution by essentially subtracting 

the first order derivative of the likelihood divided by the second order derivative of 

the likelihood. Take the ratio and then, subtract it from beta in order to give me my next estimate. 

So, this has fast convergence under certain regularity condition, so for one thing is 

that second derivative should like this and should be positive. And as you can see the 

first derivative is going to be 0 you are not going to be changing the value of your 

guess and when would the first derivative be 0 it will be 0 at one of the optima whether 

it is the maxima or the minima and you will also like this since the second derivative 

is going to positive to approach the minima. So, you can till when you approach optima 

you can be sure there is going to be the minima. So, this is essentially the basic idea behind 

the Newton Raphson method and when Newton Raphson method is applied specifically to 

the logistic regression problem you come up with the iterative technique, which is called 

iterative re weighted least squares approach for training for finding the parameters in 

logistic regression. And, so most of these statistical packages that we have especially 

R in particular of front trust was have a very simple function that loves you to fit 

logistic regression to any data set that you have and they will essentially be using Newton 

Raphson by way of iterative re weighted least squares techniques. 

To summarize this couple of modules logistic regression, so logistic regression is very 

powerful classifier build on the idea of doing linear regression on a logistic transformed 

output variables and the logistic regression is related to exponential family of probability 

distribution that rise in the variety of problems and that is the one of the reasons that make 

them very, very popular classifier. And apart from that they work really well 

I mean, so that is the another reason that logistic regression is classifier of choice 

for many people’s especially in medical domains, because they allow you to perform 

what you know as sensitive analysis. So, you can look at dependence of class labels on 

features by looking at the, the regression coefficient of specific feature in the fit 

that you obtain. Thank you. 

Classification and Regression Trees 

Hi and welcome to this module on Classification and Regression Trees. So, today we will look 

at a very simple, but powerful idea for building a both classifiers and regressors. 

The basic idea is that, you are going to partition 

the input space into rectangles. So, let us imagine that you 

have a two dimensional input space x 1 and x 2. So, you are going to try and partition 

this into rectangles by drawing axis parallel lines. So, why are we drawing access parallel 

lines here? Because, these lines can be with specified very easily by just comparing against 

one of those dimensions of the input data. So, for example, to draw this line all I need 

to specify is the intercept at the x 2 axis. So, likewise to draw this line I need to specify 

the intercept of the x 1 axis. So, one way of thinking about these kinds of partitioning 

of the input space for using axis parallel lines is to think of making a series of decisions 

as to, which side of a specific line is your data point line. So, you can think of this 

as following, we will call this say t 1, this point is t 2, this point is t 3, this is t 

4. These are the intercepts along the respective x 1 and x 2 axis. 

Then, one way of representing this is to think of this as a series of tests or decisions 

that you are making, so I can start of by asking the question, is x 1 less than or equal 

to t 1. So, that is essentially saying here is a line that represents x 1 equal to t 1 

that is your data point lie to the left of the line or to the right of the line. So, 

if it lies to the left of the line, so this will be an x, then I ask the second question, 

which is essentially is x 2 lesser than equal to t 2. 

So, x 2 equal to t 2 is this line and I am asking the question if the data point is above 

this line or if the data point is below this line. So, if it is below the line, so then 

I get a yes for this question as well and I will denote this by R 1, so this region 

is R 1. So, since this represents both x 1 being less than t 1 and x 2 being less than 

t 2, it is essentially bounded in this region. So, likewise if x, if the point is actually 

greater than t 2, so in this case this will evaluate to no and say I get to a region, 

which is called R 2. So, what would this region be if you can think 

about it? This is essentially the region, where x 1 is greater than t 1, but x 1 is 

lesser than t 3. So, I am going to call this region R 3, so here is x 1 is greater than 

t 1, but lesser than t 3, so that would be R 3 and if x 1 is greater than t 3, so in 

this case I am again splitting in to two regions. So, essentially, so I am testing on x 2 and 

t 4. So, what is that you notice about this tree that we have drawn here? 

So, every point I am asking you a binary question, is this yes or no. So, essentially it is a 

binary tree, at every point you divide into two branches. And once I have divided into 

one of these branches, once I go down one of these paths I am, from here on I am only 

concerned about data points that has satisfy the first question that I asked. So, from 

this point on in the tree I am only worried about data points that are to the left side 

of the line here. At this point in the tree I am worried about 

data points that are to the right of the line here. So, that is a couple of distinguishing 

features of the decision tree that I am making binary decisions and I am also looking at 

some kind of a divide and conquer approach great. Now, what we have done is that we have 

a representation that allows us to split the input space into different regions. 

So, depending upon the kind of problem that we are solving whether it is a classification 

problem or whether it is a regression problem, so we would like to fit a single value to 

each of this regions. So, if it is a regression problem, so regression problem we will be 

outputting a single value for the entire region. So, if your data point falls in R 1 regardless 

of, where in R 1 it is falling, it is so the data point could fall here, it could fall 

here, it could fall here, it could fall here regardless of where in R 1 the data point 

lands up, I am going to predict the same output. So, it would be the 

same real valued output for each region, so in the case of classification, what you expect 

it to be, it will be the same class label for the region. So, regardless of where in 

R 1 the data point falls I will always output the same class label for the classification 

problem. So, now, so we have two questions that we have to answer in the case of decision 

trees. So, the first one is how do we form the regions and the second question is, having 

formed the regions, how do I decide what is the output that I am going to produce for 

that region. So, we will look at each of these problems 

in turn, but the first thing if I wanted to mention before I go on to look at, how we 

solve them is that decision tree is a fantastic, because they are the most interpretable of 

all of the classifiers that we are going to look at, even more so than linear regression 

at some point. Because, if we think of the way we constructed the decision tree, it seems 

like a very natural way to map it to how humans think about making decision. 

So, that way the interpretability, so the interpretability of decision trees are very 

high and in fact, that makes it one of the classifiers or regressors of choice in a very 

wide varieties of problems. And the second advantage of decision trees is that they can 

work well with mixed mode data. So, here the example I gave you assume that x 1 and x 2 

are actual numbers and you could pick arbitrary comparison points x 1, x 2 need not be numbers 

they could be a categorical variable like color or it could be age, but represented 

as young old and middle age. It did not necessarily be a number on which, 

you have to run this kind of test I could compare whether the color is red or not red 

or I could look at whether the person is young or middle aged verses the person is old. So, 

I could have any kind of binary test among categorical attributes and then, I can still 

construct the decision tree. So, the first advantages one of interpretability the second 

one the, which you can hand mixed mode data. So, now, let us step back and let us look 

at regression trees specifically, so what we know about regression. So, regression the 

goal of regression is essentially to minimize some squared error. So, this is one of the 

goals of regression is to minimize some squared error will stick with that of course you can 

build regressors for whatever objective that you want optimize, So I want to fit a function 

f says that I minimize this some squared error. Let us, suppose that I have a tree that has 

split by input space into m regions, which I denote by R 1 to R m, so I have m regions 

in the input space. And then, for each of these regions, so I am going to output a specific 

value, which I denote by C m. So, C m is the value that have output for any data point 

that lies in region R m and I here is an indicator function that denotes whether the data point 

lies in region R m or R naught. So, essentially, what this summation tells 

us is that if the data point input data point x that is come to us is going to lies in one 

of these regions 1 to m, 1 to capital M. I do not know, which region it is going to lies 

in, but this indicator function will tell me, which region it lies in. So, this summation 

will be non zero for only one term essentially the region in which, the data point lies in. 

And therefore, the output will be the C value corresponding to the region in which, the 

data point lies in. Suppose x lies in R 2, then the output will 

be C 2, suppose I am given this tree already this region split has been decided for me, 

then we know what is the best value that we have to output for C m, so what would be the 

best value you have to output for C m. 

Essentially I go through my training data I pick out all those x is, which lies in the 

m th region pickout all the x is that lie in the m th region look at the corresponding 

y and take the average of those and that will be the value that I output if the data point 

lies in the region R m. So, why is this a reasonable choice well, 

so one way to think about it is when I am trying to minimize the error in a specific 

region when I am trying to minimize the error in a specific region, let us say R 4 I do 

not have to worry about any of the training point that lie outside of R 4, because the 

value I have to predict for them is completely independent of right all these other data 

point. So, I only have to worry about the data point that lie within R 4 when I am trying 

to make a fit for the value there in output in R 4. 

And among all the data points with lie in R 4 the best prediction that I can make is; 

obviously, the average in terms of minimizing the some squared error. So, if I have a different 

criteria let us say among to minimize the median I mean I want to minimize absolute 

deviation, then I probably have to predict the median not the average. So, this part 

is fine we know we are solved one of the two problems. So, what were the two problems one 

is given the region split, what is the output that have to predict for each region. 

So, that we know how to do that at least in the case of regression, now comes the harder 

question, how are you going to find the regions, how are you going to find the best R m’s. 

In fact, finding the best R m’s finding the best region split is actually a combinatorial 

problem when it is actually infeasible and it is going to take a very, very long time 

to find the exactly the right set of regions. So, quite often what people do is they adopt 

a greedy approach to finding this regions. 

So, what is that greedy approach to, so you basically start of by considering 

basically start of by considering a split variable, so what is the split variable. So, 

in the case of the example tree here, so the split variable at this level is x 1 and the 

split variable at here is x 2 the split variable here was again x 1. So, essentially you consider 

some split variable and then, try to find try to find the best split point. 

So, what is the split point again, so in the tree that you saw earlier, so in the first 

level the split point was t 1 likewise, so at each level we have to find out what is 

the appropriate split point is. So, let us take us a simple example, so I am going to 

define going to define two sub regions R 1 and R 2. So, R 1 is that part of the space, 

where the variable x the j th co ordinate of the variable x is lesser than or equal 

to some chosen value s. Likewise R 2 is that sub region, where the j th co ordinate of 

the variable x is greater than some chosen value s. 

So, now, what we are really trying to do is trying to find j and s; such that we can solve 

for the best possible split just to give an intuition here. So, in this case think of 

the original data, so I have chosen a split point, which is t 1. So, the s in my choice 

is t 1 and this part is wherever x was less than x 1 was less than t 1 and this part of 

the space was wherever x 1 was greater than t 1. So, in our new terminology this will 

correspond to and this will correspond to, so the one here is, because we are looking 

at x 1 the and the t 1, because that is the split point of you are considering. 

So, now, we have to find j and s both; such that this expression is minimize, what are 

this expression. So, we know this. So, C 1, is the prediction I am going to make if the 

data point lies in the sub region R 1, so y i, so these are all data points that lie 

in the sub region R 1, so prediction I make for this C 1. So, this is essentially the 

squared error for all the data points at lie in R 1 and this is like wise this squared 

error for all the data points at lie in R 2. 

So, we already saw that, so we can basically find the C 1 that minimizes this error likewise 

find the C 2 that minimizes that error. Now, my problem is to find j and s such that this 

entire expression is minimized some on a little daunting in the beginning, but if you think 

about it is not that hard, why because we are operating with the finite training set, 

like the data that is given to us at the beginning from, which we are going to build this tree 

is going to be finite. So, what does it tell us for every x j that 

I can choose as my splitting variable there are only finitely many points at which, I 

have to consider a split. So, the essentially tells me for every j I choose there are only 

a finitely many s that I have to try, why is that because that can only be a finitely 

many different values that the variable, x j can take in your training data. So, essentially 

what we do is that at every level in your decision tree you basically look at this expression 

for all possible split points for every possible splitting variable that you have in your data 

and then, decide on which is the best possible splitting point. 

Once the best j and s is found, so what you do next you essentially go hide and split 

the data into two parts one corresponding to the R 1 of the best j and s and other corresponding 

to R 2 of best j and s. And now, you repeat this process in both R 1 and R 2. So, now, 

if you think about it a problem is become much simpler, because the ranges that you 

have to the number of data point that you are looking at this much lesser and, so likewise 

you just keep going until you come to a point where you are happy to stop. 

So, this essentially I will stop here as in this module and the next module look at till 

when we will grow the tree and how are you going to handle the classification. 

English - NPTEL Official 

Classification and Regression Trees(contd) 

Hi, so in this module will continue looking at Classification and Regression Trees. 

So, in the previous module we looked at how to build the tree, but we never looked at 

when to stop. So, if we continue growing a very large tree, so we might end up making 

a lot of decisions and essentially end up specializing your tree to individual data 

points. So, it will lead to over fitting of your tree to the data and that is not a good 

place to be in. On the other hand, if you stop growing your tree to soon, you might 

be sought on finding interesting patterns in the data. 

So, for example, a very famous XOR case, so there is no single attribute on which you 

can split and get any kind of improvement in your performance, because whether you split 

on x 1 or whether you split on x 2. So, half your data is going to be positive, the other 

half is going to be negative. So, you really cannot hope to get any improvement by only 

splitting on one attribute. So, you just kind of stop there saying that hey there are no 

one individual attribute that gives me an improvement in performance and therefore let 

me stop. So, you might lose out on interesting patterns 

if you are going to stop too early. So, you cannot leave a tree that is very, very specialized 

to small data points and you cannot stop early as well. So, what we do? So, you typically 

grow the tree until you come to a point, where there are few data points in the leaf. So, 

the leaf here would correspond to region. So, I keep going the tree until I come to 

a point where each region has only a few data points, typically you would like to have bought 

four or five data points at least per region. So, for using some of the standard tools say 

something like weka. So, weka actually stops by default, when the number of data points 

in a region go down to two. So, it will stop only at that point, that is still a significant 

over fretting. So, you grow a large tree, where there are very few points per region 

and then you prune the tree, you basically start trying 

to collapse the internal nodes in the tree, such that you come up with the better tree. 

So, we have looked at how you need a validation sets. So, you have a training data and you 

have a validation data. 

So, one way of looking at pruning which is to say that, I will look at the performance 

of the tree on the validation data. So, look at the performance of the tree on the validation 

data and then, I will start collapsing some of the internal nodes of the tree. So, what 

do I mean by that? Suppose that I have a tree that let us think of the tree that we had 

earlier. So, this is the tree that we had earlier. So, one way of pruning something 

like this could be to say that, I am going to collapse one of the internal nodes and 

replace it with a single region. So, there I have pruned the tree, so this 

corresponds in the picture, this corresponds to doing something like, so that is our four 

prime. So, now, I can look at the performance of this pruned tree on the validation data 

versus the performance of the original tree on the validation data. So, now, if there 

is not a significant dip in the performance, when I have removed one of the internal nodes, 

then I will keep this new tree. So, there are essentially tells me that whatever 

I did in terms of the previous test on t 4 or something that was peculiar to the training 

data. So, now, I look at it in the validation set, where you really does not look like I 

need to make that split. Or on the other hand, there might be a slight decrease in performance, 

when I go from the original tree to the prune tree. Now, the question you have to ask yourself 

is, given the fact that I have reduced the size of the tree is it to pay the penalty 

in terms of the slight reduction in performance. So, there is trade off, so there is this what 

would call 

the cost complexity trade off. So, that is the cost that you incur in terms of the misclassification 

that or the misprediction in this case in that you are going to make in terms of the 

reduced reduction in the number of regions. And the complexity is essentially the number 

of test that you have to perform on the size of the decision tree that you are operating 

with. So, this is the cost complexity trade off and depending on which side of the cost 

complexity trade off that you want to be on you can get more complex or less complexities. 

So, that many ways in which this kind of pruning techniques can be implemented and one simple 

way which is essentially to choose a validation set and then have a tradeoff between the prediction 

error of the pruned tree in the validation set verses some measure of complexity of the 

tree. So, one measure of complexity of the tree is the number of internal nodes that 

you have in the tree. So, you can basically trade off the two and try to come up with 

a good smaller tree. So, far we have been talking about regression 

trees and we looked at how you would split regions and once having split the found the 

regions how do you fit, what is the best prediction that you have to make in each of those regions 

and how would be find the regions, we adopted a greedy approach by picking once variable 

at a time and then trying to find out which is the best point to split the variable. 

And we looked at the squared prediction error and essentially now if you want to look at 

classification trees, you only think we have to ask our self is, what is the appropriate 

measure that we will have to look at in the greedy such procedure. So, once we have decide 

on what the appropriate measure is, so the rest of the framework that we need to build 

classification trees are already in place. So, we can do the greedy algorithm and then 

once we know, what is the appropriate measure that we are going to use we can optimize that 

measure in the greedy algorithm and we can use the same setup that we have for pruning. 

So, we have a validation data and then we have a cost complexity trade off, where the 

cost instead of being measured by this squared error is going to be measured by whatever 

metric we choose for a classification. So, I am going to say that I will build the classification 

tree where the prediction. So, which we will called as p hat, so p hat here is the probability 

that a data point in region m belongs to class k the data point in region m. So, one of these 

regions, what is the probability that this data point belongs to class k. So, I am going 

to look at all the data point that is fall in the region R m. 

So, I am going to look at all the data points if fall in the region m and look at whether 

their class labels in the training data, whether it belongs to class k. So, if a point x i 

in region m, if the label is k then this will be 1. So, the summation is essentially going 

to count the number of data points in region m that belongs to class one or number of data 

points in region one that belong to class two and so on and so forth this expression 

is for class k and then I am going to divide it by the total number of points that fall 

in region m. So, N m is the total number of points in region 

m. So, this is essentially gives me an empirical estimate of the probability of a data point 

falling in region m belonging to class k and if I am interested in actually returning that 

class label then... So, the class in region m is going to be 

the class that I will assign to data points in region is essentially that k that gives 

me the maximum p hat value that make sense. 

So, you look at the different error measures that we can use. So, what is the most popular 

error measure in classification or which is the most appropriate error measure in classification 

is a simple thing called a misclassification error, the misclassification error is given 

by... So, the number of times the actual label does not match the prediction that we make 

by our classifiers. So, class m is the prediction made by our classifier in the m'th region. 

So, for all the data points in the m'th region whenever the class label the true class label 

does not match the predictor class label. So, I am going to sum it up divided by the 

total number of data points in that region. So, this gives me the misclassification error 

this for a specific region and I can do this over all regions and that gives me the total 

misclassification error. So, if you think about it this is essentially... So, the number 

of times this is should have been correct is given by p hat m and the class that I am 

going to predict. So, this is the given this gives me the maximum from the previous expression 

that we had. So, the total misclassification error will be 1 minus p hat m. So, I can use 

this as my error measure. So, once I estimate p hat the once I estimate 

the arg max over k of p hat m k then 1 minus that gives me the misclassification error 

for that region and I can sum this over all the regions. Whether there are a couple of 

slightly more common error measures that are used especially in the decision tree literature. 

So, one of these measures called cross entropy or deviance that leads us to something called 

information gained measure for looking at the splitting criteria. 

So, the cross entropy is given by... So, this is p hat m k is the label distribution that 

we have inferred is a label distribution that we have inferred from the data that was given 

to us, but we stop and think about it, if you have sufficient training data. So, that 

p hat m k is actually the true output label distribution as well. So, this cross entropy 

term actually gives us in some sense the amount of information that we need to encode the 

true label given that you are in the region m. So, this gives raise to this measure called 

information gain that tells us that if I have not split into the following set of regions. 

How much information would I need to represent the labels of the data verses having split 

into all these regions? How much less information do I need? If I do not know anything about 

where the data point lies on this entire plane then I have some amount of uncertainty about 

what the label is. Once I know that the data points lies in R 1 or R 2 verses the rest 

of the plane have less uncertainty about what the label should be and... So, given that 

when I have split the data into two regions I have gained some information about what 

the label should be and that leads to this notion of an information gain measure. And 

so it comes from this cross entropy term and we can use this also as a measure for a splitting 

the tree in the greedy growing stage. Last one is a Gini index is given by the expression 

p hat m k into 1 minus p hat m k and it is actually a measure of the disparity in the 

population of distribution of variables. So, if you think about it, so this is the probability 

that the label k appears in the distribution in a particular region m and this a probability 

that the label k does not appear in the distribution m and this expression will be maximum when 

they are equally likely like the problem there label k appearing and the label k not appearing 

in the region m or equally likely. So, that is really a bad situation for us to be in. 

So, in the some sense having a high value here essentially is a indicator that this 

is not a great way of splitting the data into different regions. So, when you could use 

one any of these three measures in terms of going your decision tree and they have their 

own advantages and disadvantages. And so the cross entropy and the Gini index or more sensitive 

to note probabilities than misclassification rate, but then 

quite often we find that these lead to much better trees than using the misclassification 

rate directly. So, that brings us to the end of our discussion 

on classification and regression trees, but there are few points that I would like to 

make about the use of tree based classifiers or regressors. So, the first thing is I glossed 

over a little bit the problem of handling discrete valued attributes I said could choose 

red or not red, but then if you think about discrete valued attribute that has k possible 

values, you can see that there is a combinatorial many ways of dividing these attributes into 

two groups. So, you need to have clever way of handling this and then many of the decision 

tree packages do handle this in a meaningful way. 

The other thing which we have to be very aware of when you are dealing with trees is that 

trees are notoriously unstable. So, what do I mean by unstable is that if there is a very 

small change in the training data that you give to the trees the tree that you build 

out of the data could be very different. So, you could delete a few data points and you 

might end up having a tree that looks very different. So, this is in contrast to some 

of the earlier thinks that we are looked at like logistic regression or support vector 

machines, where these things stable to deletion of one or two data points, but decision trees 

can be notoriously unstable. So, one way that people get around this problem 

of instability in decision trees is to make sure that you build many, many different decision 

trees with slightly different views of the data and then combine the predictions made 

by all the decision trees into a single tree. So, that way we can actually end make sure 

that the overall classifier that you build is reasonably stable and another point that 

we have to look at suppose you have fitting decision trees using regression. So, you can 

see that for each region we will be outputting a specific constant value at no point or we 

worried about how will the value change from R 1 to R 2. 

So, you are only worried about what is happening in R 1 and we are fitting a value in R 1 and 

therefore, there is no concern of smoothness in our faiths and therefore, the fits that 

given by decision tree can be pretty non smooth it is a smoothness is a criteria then we will 

have to either add some more regularizing factors into decision trees which makes it 

more complicated or will have to look at other forms of regressions. So, just keep in mind, 

so decision trees or very powerful classifiers, very powerful regressors they are wonderful 

in terms of interpretability, but they also come with their own caveats. See you later. 

English - NPTEL Official 

Bias Variance Dichotomy 

Hello and welcome to our lecture on Bias-Variance Dichotomy, this is a conceptual lecture. So, 

you must be right now going through lectures, where you are learning many of the machine 

learning tools and techniques and this is not one of them. So, we not using a new technique 

in this lecture, but we are introducing a very important concept that is machine learning 

and therefore, it applies to all the techniques I would say that you are learning and I would 

go in so for saying this is probably one of the most important concepts that you should 

understand in machine learning. 

So, let us derive into what the core concept, the concept can be said in very simple sentence. 

The idea here is that while adding complexity to a model, while if you want to keep adding 

like say complexity to a model, you might improve the fit of the model. So, you might 

find out that you are better describing the data, but that need not improve the predictive 

accuracy of this model when you compare it to new data that you get and this concept 

is to for whatever type of model that you look at. 

So, you can take an example on linear regression and you can add complexity to it by either 

adding more input variables or even with one input variable, you can add more complex transformations. 

So, for instance one way of just adding complexity to a simple problem, where you have one input 

variable, one output variable is that saying I am not only interested in looking it through 

the standard input variable, but I would like to look it as a polynomial. So, what is the 

model when you have x, x is the input variable, but you can also take x square, x cube and 

so on. So, you can have a more complicated fit between 

y and x, because in simple regression you always see y is equal to m x plus c, but what 

we have y is equal to m 1 x plus m 2 x square plus c. So, you can add complexity that way, 

you can add complexity by adding more variables. Now, this is again not confined to the regression 

any more. Almost any method that you take, you will typically find that there is some 

way of getting more and more complex. So, for instance you might have already covered 

trees, classification regression trees. You can add more complexity by, you know creating 

more and more and more branches to the point where you have such a complicated tree, you 

have such a large tree where each terminal node or leaf is a single data point that we 

are using in your training set. So, you have really the more complexity, but k need choices 

k could be assigned complexity. With neural networks something that you would learn the 

future, the number of layers in the neural networks can make the complexity. 

And this idea of complexity would become more clear as we, you know talk through some example. 

But, the idea is that you can make the model more and more and more complex, the model 

that you are going to use to create this relationship between input variables and the output variable, 

nothing become more and more and more complex and the more and more complex it becomes you 

will do a better job of fitting the data that you have. 

But, that does not mean you are creating a better model and the answer is might not, 

because while in might fit a data better that you have, tomorrow we need to predict using 

this model, you might not do a better job of predicting and we are going to see how 

that can happen that can possibly happen. Now, this core concept in machine learning 

is also sometimes referred to as Occam's razor that is more of mathematical concept and it 

is definitely used permanently in machine learning, And the idea there is not which 

is that if there are two models with equal predictive accuracy, then you prefer the model 

that is simpler that is less complex. So, that is not go back, but you see how these 

two highly related concepts, but the concept of bias variance dichotomy, you are essentially 

questions saying that I can add more complexity to the model and it will look like it is doing 

a better job of fitting a data, but am I getting better predictive. So, let us actually you 

know understand this through an example in this particular model in the two tables that 

have shown you here is model one which is your good old linear regression that you know 

and here is the data set. So, use the same data set which is the same 

x on the right hand side and the same y, but I say that I do not necessary believe this 

is the right model that is model one is the right model. What are the relationship between 

x and y were more complex. So, I added an x square term and x square is nothing but 

it is really simple, it just I take x and i square it and I created new column and like 

that I keep on adding columns still x to the power 9 and finally, I have all these as potential 

input variables as described by this model. So, y is some function of all these parameters 

and I do a multiple regression on that. So, how does it work up? 

So, here is my linear model, here are the data points that you saw before and here is 

the that line that you see is the fitted line that goes to these data standard and so this 

is what you get, how this what are you... So, the polynomial the one that I created 

before is actually an ninth order polynomial. So, how does that will work? 

Well it turns out that it does a really good job of fitting all the data points as we can 

see this ninth order polynomial described by this black curve goes through every single 

data point and that should not be surprised, you have ten data points and you using ninth 

order polynomial fit it there is enough flexibility in the model, in the coefficient is enough 

complexity in the model where you do not you can actual go through each data point, here 

in the first model it is a straight line, even if it wanted to it cannot even if I had 

the flexibility to put this line wherever you had I can move this line up and down I 

can rotate this line, but the best job that I can windup doing is actually the line that 

you see on this screen. So, I did a simple linear regression which 

does try to you know fit as many data points as possible and this is as best as it could 

do now with the ninth order polynomial, where it tries to do that it does really good job 

it fits all the data points. See you sitting there knowing like walk, so 

may be my system is ninth order polynomial, but it is not I let you learn a secret, this 

actual data between x and y was created by an actually a linear system with some amount 

of noise. So, you truly equation the true relationship which because I am the God out 

here, I am the one whose actually creating these data points. So, I have this oracle 

I am letting you on the secret that I actually created that this data points through some 

y is equal to beta naught to b naught plus b 1 x and not x square and x cube, but then 

was some error, some noise like any regular system. 

So, given this let us it looks like this the ninth order polynomial still developed better 

job of fitting, the fit is great. But, let us look at how these two models compare will 

they have to predict that another occurrence, here is the fitted model, the same graph has 

above in the linear model. But, I created a new data set I created new data set and 

I see how well my line does as a job predicting what is going to happen next's and the answer 

is it does not do too bad. So, this is the predicted line, the blue line is predicted 

line that I got from my training data. Now, I am go get new data it looks like I 

miss targeted couple of times and I probably should expect that given what I know right 

now that there is some amount of noise in the system that is irreducible. But, both 

this basically means is that tomorrow when you come to me with saying that hey 4.5 what 

you predict I say I am going predict this value and in reality I windup seeing this 

value that the line in red is what I windup seeing in the field when I makeup predicts 

in the prediction make is where I windup, what I windup using as the blue line. 

Now, what do you see in terms of predicting with the ninth order polynomial, you actual 

do quite terribly bad I mean look at this data point. So, this data point where it is 

1.5, my prediction at 1.5 would be something close to minus 40 minus 39 or whatever. So, 

add 1.5 if I were to use this ninth order polynomial as might fit it model I would be 

predicting minus 40, but look it what I actually got, I got plus 50. So, I was almost of by 

x 55 units, whereas in that much with the linear model and you are going to see the 

same kind of we are practitioner. In fact, while at something like 0.5 this model just 

goes through the roof I cannot even fit it inside the graphs. 

So, clearly the ninth order polynomial while is doing a great job of fitting does it very 

bad job of predicted and if my goal more often than I should say bad, but I did go more often 

than now is to really come up with good predicted accuracy. So, the most machinery from judges 

not trying to fit a model to the data that does not buy you the much, but you want to 

pay to model that can be generalized to other situations. 

So, tomorrow when you get a data set, because what you going to do with the model here either 

going to predict or either you going interpret the model in either case you need to acknowledge 

that what you have the data that you have is nothing but, a sample and if you take another 

sample I if it turns out that you would have told a completely different story they may 

be the way doing things is not really correct, my whole points is that for instance, if you 

had seen the reds star data in the linear regression model you might have created a 

slightly different line may be the line would have looked little bit like this. 

But, think about what you might done with the complex polynomial you might have created 

a you know completely different polynomial function that look than again will go through 

all the red data point, but if for one sample you create one story and for another sample 

you create completely different story can may be the way doing thing is not a really 

accurate. So, here is what we shown you the system where it was truly a linear system 

and clearly how using a linear model made more sense than having a more complex model. 

Now, what happens when there is more complexity to this system, let us for instance say that 

my true model was quadratic and that is what I going show here, I going take up quadratic 

model and then see what happen when I try a linear fit. So, here is quadratic model 

this is the truth that the machine learning algorithm does not know that in the world 

I will never know that true system is quadratic. The only thing that I have is data show you 

the data I am just setting you in a secret that for today's exercise I created this data. 

So, this is the data, this is the model that I created this data using this model and adding 

some amount of noise or uncertainty. So, the model that is creating is data has 

actually you know is more plus beta 1 x 1 that it because it only 1 x and beta 2 x square. 

So, I am using some model like this which some b naught, b 1, b 2 plus some amount of 

noise. Obviously, if you knew the this was the model then this is the model you going 

to try a fit, I mean if you knew this is the model then this is the model you should use 

with that truly known b 1, b 2. So, you do not even need to do any kind of machine learning 

statistics excise. But, sadly you are only given the data and 

you not told which model it is, now let us see what happens if you hand this data and 

then you try to fit a line this is one fit. But, the kind of give you a feel for what 

happens when you do this many times I generated another set of data points using this model. 

So, I have shown you only one set of blue dots, blue small mini circles, but effectively 

it will another set of blue mini circles and then fit a line, I fit another line. 

But, one thing you should note this is look all these lines are in general more or less 

they trying to the same job and in general they wind up feeling chronically in certain 

cases they always windup underestimating in this region, because their always under the 

truth, they always windup over estimating. So, each time I do this exercise it looks 

like chronically of in certain areas, but I am fairly consistent each time when we do 

this exercise I windup kind of creating the same line. 

Now, what happens when you have a ninth order problem, what happens is you still not doing 

to great and the reason you not doing to great is because this is a quadratic system and 

when you are trying to fit something so complex. So, you still over shooting a lot, but there 

are couple of things note this one in general as expected as shown in the previous slide 

you not always telling the same story, one time you telling one story the next time you 

know you predicting vastly different this kind of extreme variance from one kind of 

prediction to the other is not seen in the linear fit. 

But, take another look you are not; obviously, chronically off in certain areas, yes this 

is only five such fits, but imagine if you had five thousand such fits. Even if you had 

five thousand fits in the linear model, you will always be overestimating some regions, 

you always be underestimated some regions in that is called bias, whereas in ninth order 

polynomial the idea is that yes there so much variability, but if you were to do many, many, 

many fits that on average you might not be off from the blue line and that useful in 

concept it is not useful in practice, because in reality you are going to get only one data 

set and you are going to try one fit. So, if it is off, it is off whether it is 

because of whatever reason, but it really helps to understand why it is off, here in 

the linear fit it is off because you might be chronically always going to be off. Because, 

you are trying to fit very rudimentary model, very simplistic model for something that little 

more in reality more complex, the model the reality is more complex, so this is more complex 

because of it is quadratic and the model you are trying fit is too simplistic, because 

it can only here model can many be a line. So, it is a line which is you know simple 

whereas... So, here what you can to witness is a lot 

of a some region you are going to be always off whereas out here in the ninth order polynomial 

this is so much variability. Because, you are just getting fooled by the pure noise, 

the same thing that you saw in the previous equation, nothing is really in the previous 

line nothing is really change, you still trying to fit something that is a model that is overly 

complex to a system that is not that complex we went from a linear data source to quadratic 

data source, but that is still does not in ninth order polynomial, so this is a lot of 

variability. 

And this is the point that is get captured in what is offend, what is really described 

as a bias variance dichotomy. This graph is taken from the ESL book Hastie and Tibshirani, 

and it captured what we kind of try to illustrate in the last two slides, which is as model 

complexity goes increases it looks like you are doing a very good job of fitting the data 

this is nothing but, the fit. So, you take the data and look here error keeps you want 

very low error. So, as you keep on making a more and more 

and more and more complex model, you are going to go through more and more data points. But, 

at some point your ability to predict this is prediction on new data, the red line is 

prediction on new data, the blue line is the fit on the data that was given to you for 

training. So, it is call the training sample and the prediction is done on the test sample, 

you prediction keeps on getting low to a sweet spot for model complexity and then after that 

it goes up and what is the sweet spot, you that sweet spot would be at a, if you are 

able to match the exact complexity of the system. 

So, if you had a quadratic system for instance and you use a quadratic model that could might 

be an sweet spot. And so if have a complexity that is perfect you know and what we are going 

to do is, in the real world you don’t know what the true model is. So, how do you figure 

out what this complexity should be and that is going to be covered in or lectures on validation. 

How do you validate a model, how do you fine tune some parameters of a particular model, 

again it can be K-nearest neighbors, it can be trees, it can be neural networks, it can 

be support vector machine, it can be a simple regression. 

But, if there is a some kind of tuning parameters that there can increase or decrease complexity. 

How do you go about increasing and decreasing complexity is seen what works best and then 

choosing the appropriate one that we captured in validation, the lectures and invalidation 

when this lecture we want to create an appreciation that as model complexity increases, the fit 

becomes better, the prediction you need to find the sweet spot of model complexity and 

that is the core idea. The other idea here is that yes when model 

complexity is low, you do not do too well in terms of here prediction, but the reason 

for that is because there is high bias meaning in that linear regression if you remember, 

you always of when you trying to fit that linear regression to the quadratic function, 

you will always often certain regions. So, you had a high bias in a low variance, what 

we mean by that I go back to the slide is you are bias in certain regions. So, these 

are regions were you have bias, you are always going to be off, because of the nature of 

you try to fit a line through a curve, but you have a very low variance, if the variability 

between many such fits is low. So, on any given day you get any given data 

set it is not like you are going to come up with the completely new equation. So, that 

is what we call us high bias and low variance. Now, you step over to the ninth order polynomial 

here the bias is not that high, it is not like that can tell you that you are going 

to chronically be under predicting or over predicting in some regions. So, this has low 

bias, so I cannot tell you upfront that you going to be always off in one direction, but 

it is got high variance. What you mean by that is on any given day 

if I take any given data set I take the sample and I try to fit this polynomial I do not 

know which line I am going to get, I mean this line predicts some astronomical high 

value out here, whereas this curve predicts some astronomical low value out here. So, 

this like such high variance on a given data set I do not know how going to be predicting 

and therefore, if I can have such high variance, it just means that I am probably not going 

to do very good job of predicting. I do not even know what I am going to be predicting. 

So, that is the concept between the bias variance dichotomy, which is that when you go for lower 

model complexity you get high bias and low variance and we go for a higher model complexity 

you will get low bias and high variance and this is the very important concept to be internal 

analyst with respect to Machine learning and it this going to be extremely important even 

in terms of applying more advance techniques. So, I hope the bias variance dichotomy is 

clear. Thank you. 

English - NPTEL Official 

Model Assessment and Selection 

Hello and welcome to our lecture today on Model Assessment and Selection. So, this lecture 

is in many ways similar both in style and content to our lecture on bias variance dichotomy. 

It is similar in style and that, we are not going to be teaching you a new techniques, but we are teaching you a concept in machine learning that is really important. And so 

like the bias variance dichotomy, it is really applicable to almost every technique that 

you could be using in machine learning and should not just say, every technique in machine learning more narrowly in every technique in supervised learning. 

So, your regression, your neural networks, your cards, your decision trees, your SVM's 

you know, so whatever techniques you are learning right now and whatever you have learnt this is a very, very important concept. It is also very similar in content, because in the bias 

variance dichotomy lecture you really learnt about how it is really important to understand 

that you can have highly simplistic models, which would have high degree of bias and low 

variance and therefore, not really good. And at the same time you can have highly complex 

models, which tend to over fit the data and therefore, have a lot of variance, but very 

low bias. And therefore, you are being introduces the problem there, saying that well you it 

is not a good idea to go to either extreme. In today's lecture we will be answering coming 

up with solutions for that problem and not just that problem, for many other related 

problems, but somewhere the solution is in looking more carefully towards how do you access a particular model and therefore, how do you go about selecting between multiple 

models. So, jumping into the subject in terms of when you have data and you are trying to relate 

that, use the data towards models, there is one part which is actually using the data 

to train the model. There is another part, which is you could use some of the data to 

not just train the model, but to choose between models or to fine tune the model. 

So, using the data to train the model if model training, model selection the word selection 

is use kind of loosely here could mean selecting between multiple models or it could mean, you know it is a meaning you could use that select between completely different models 

or at the same time you could use that you can fix the model and you could just be like a parameters that define the model and you could go about figuring out what value to 

set it to by using data in the model selection. And finally, you have the concept of model 

assessment, which is once you fixed everything out, if you want to get an idea of how good 

this model is. So, that you know that when you take this model to the field, this is the performance you would expect that has to do with model 

assessment and corresponding to these three goals or objectives, you could break up your 

data and this is something that you might not always have the luxury to do. But, you 

can if you have a data rich situation we have enough data, then you could potentially break 

up the data and training, validation and test, where the training goes towards model training, the validation goes towards model selection and fine tuning and the test goes towards 

model assessment. And there is no formula exactly as to what 

this break up should be or what is enough data, that is a hard question to answer, it 

varies from case to case, but at the surface you know typically if you have enough data 

you tend to break it up as either 50 percent, 25 percent and finally, 25 percent out here 

50, 25, 25. But, you could also have you know 60, 20, 20 and again, even with this you cannot 

say one is right, one is wrong or something is perfectly right or wrong I am just giving you some values that are typically seen. You can also see in the picture that I have, 

the representation I have below that sometimes you might want to do training and validation 

together and we will talk about that more specifically. But, if you, this sometimes 

comes up when you do not have as much data, where you do not have the luxury of just taking 

one 25 percent and calling it validation. So, somewhere you know you just use the same 

data to perform training and validation and how you will do that is something that we are going to talk about in good detail in this lecture. 

But, you still try to keep some percentage for model assessment, then the final say on 

how good the model will be if you take it out to the field, if the model want to completely 

look at new data and try to predict how good will it work. So, what we are going to do 

next is, we are going to take each of these phases and talk in detail about what really 

we can do for each of these objectives. The first phase, which is model training or is one, where you take the data and you train. 

What we mean by training out here is really that you fixed everything, you fixed all your 

steps, you chosen a particular model and you chosen all the parameters associated with 

this model. So, at this phase you are not doing any model selection, in phase one we 

are not doing any model selection, no selection, you also not trying to decide on what values 

that some of the modeling parameter should be, what I mean by that is let us see you are doing a ridge regression, in ridge regression you have a complexity parameter. 

If you go back to our lecture on ridge regression, you are doing this optimization where it is 

look like the ordinary least square. But, you have another term, this is the term associated 

with regularization, where you penalize really large coefficients. So, for that you need to set a value, you need to set a particular lambda, you already decided that, in this 

phase when you are going for data for training, you fix the model and you fixed all the parameters 

associated with the model. If you for instance doing a neural network training, you are not using this data to decide how many intermediate node should be there, 

that is not what is happening. Once you finalized on the exact model and its details, you are 

just using this data to figure out what the model should be, meaning take the case of 

linier regression, you fixed everything, you fixed what the input variables are, you fixed 

what data you are going to process with respect to a simple linear regression or a multiple 

regression, there are no other parameters to fine tune. So, it is you just plug in the data and you get the betas, so here you might just be getting 

the betas. So, that you have created the model, you are essentially creating the model in 

this phase, if you are doing decision trees then let say you need to have fixed on the 

algorithm for the decision trees need to have fixed, what the input variables are, need to have fixed other parameters that you could probably play around within decision trees 

and we are going to talk about those parameter soon. But, once everything is fixed, you just 

plug in the data and how to get or is the tree structure or network, in the case of 

neural network. So, this is essentially the last leg, once 

you fixed everything you are not doing any model assessment, you are not doing any model selection, you already selected your model, you already set the parameters for your model. 

You just plugging the data into this finalized version and getting the actual model out of 

it. So, that is what you are doing in this data for training. What happens in the validation 

phase? In the validation phase, you are essentially using this data. So, this is separate data, 

you use some data for training. Now, think about this, you using separate data and you using this data to make some decisions. You could be using this data to 

select between multiple methods. So, you might say, hey I have this data I could do a linear 

regression on it, I could do a regression tree on it or a random forest or something. 

I could do a neural network, how do I know which to do, why not to do all of them and 

see, which one does better and how do you do, see which one does better. 

In the validation phase what you will do is, you will use the same training data that is you will use the same training data that you saw, we discussed in once and you will fit 

these three completely different approaches. You will create the data's for the regression, 

you will create a tree for the regression tree, you will create a network for the neural network. Now, you go apply these three to the validation data, what I mean by that is 

that you go take the input data in the validation and apply these three methods and you get 

some predicted outputs. Each of these methods are going to give you some completely different, I mean might not be completely different, but they are going 

to give you a different outputs, different predictions. Compare these predictions to 

the actual output that you have in the validation data, any. When we say data whether it is 

for training, validation or test, it means you would have input output pairs. So, if you have five input variables you will have values of each of those five input variables, 

one data point is nothing but, the vector of all values of the five input variables 

and the one output variables, so whatever value that is. So, in some sense when you have these input, output pairs what you can do in the validation 

phase is, you would have created the models from the training phase and what you are interested 

is in comparing multiple different models. So, you might have completely different methods and then you apply the input of the validation phase to get predictions from these methods 

and compare these predictions to the actual output and see how well they did. 

And you can do that by both in a regression context or in a classification context, meaning 

that if you predicted 13 and it is 13.5 I can say are you miss by 0.5, if you predicted 

that is going to be a class like I am predicting that this is 70 percent chance it is male 

and then it winds up being female you can say your you can use different kind of a metric to measure that performance like misclassification index or entropy or something. 

But, essentially the idea in the validation phase is to choose is make some decisions, 

we spoke about how you can use that to choose between methods, but more often what we see 

is that it is huge extensively to find tune parameters for a given algorithm. So, let 

say I go about and I am decided right at the stage. So, we have already talked about let 

us call this a, b and c, so we already talk about two a into b I am really talking about 

here is let say I have fixed I have decided that I am going to use a decision tree somebody 

use the cart algorithm. Now, the cart algorithm might have a lot of parameters that I need to set one potential parameter could be then minimum number of 

leaves in the terminal nodes what; that means, is the minimum number of data points in a 

terminal node of the tree. So, in a cart algorithm you can keep on splitting the tree into branches 

and branches still the terminal nodes just have a one data point, the idea is where do 

you stop and one way of choosing where to stop just to have a limit you put in the limit 

saying do not create new branches, if they are going to result in the final nodes having 

less than let us say 5000 data point or we calls them 5000 leaves. 

But, what should that be, should that be 5000, should that be 500, should that be 50,000 

what we can do is you can run all of them, you can create three completely different 

trees, one with each version and again validate that use the validation data set to see which 

does better and for pretty much any method, any complex enough method you will find that 

the method itself will have some parameters that you need to finalize. So, in the case I guess you guys have finished you might not have started neural networks here, but you 

must have finish. So, we finish ridge regression. So, again let us think about ridge regression, 

in ridge regression we know that what ridge regression it does this process of regularization 

by introducing a new parameter called lambda, lambda is what forces is kind lambda is kind 

of like the way in which penalty is applied towards that optimization problem and constraints 

the size of the betas. So, if we have a very high lambda that is like a very harsh penalty 

towards large sized data's or coefficient that linear equation. So, a ridge regression 

goes ahead and applies penalty. Now, if that lambda which is parameter is 

very low it is close to 0 then there is no penalty, the ridge regression becomes like 

the ordinary least square regression, it is actually identical if that lambda is exactly 

set to zero. But, then the question comes about what should I said lambda to. I understand the concept that I might want to apply some amount of penalty to really large betas and 

that was what we discuss and the course on regularization saying you know I do not if I have multicolinearity in my data I might have two data's that are off to opposing magnitudes 

that are becoming really large and so on and so forth. But, what should I said the lambda to be in the ridge regression, in the process of ridge 

regression I need to said the parameter lambda to some value on what do I said it too is there right answer to it. The answer is you might not know that apriori what to said lambda 

to, but you can use this validation phase, you can said lambda to 0.25 set lambda to 

0.5 set lambda to 0.75 and create three different ridge regression equations, three different 

predictors. Now, take the input data from the validation set enough and apply that and all three predictors, all these three predictors to come up with 

some predictions, three different sets of predictions of the validation data. Compare 

the predictions of each of these predicators, compare these three predicts sets of predictions 

to the actual output data of the validation data set and you might and you can then compare 

these three and say oh it is look like when I said penalty parameter lambda in ridge regression to 0.5 I do better than when I said that parameter to .25. 

So, again just you can go to the bank I know you finished a sub set of techniques and you 

are still going to learned for instance neural networks and some other methods. But, what you should take you know commit memory and some sense is that almost any machine learning 

technique that you adopt you will have to choose some parameters and this validation 

can help you choose the parameters. Another example I can think of is for instance, the 

neighbors what should be a value of K be? Am I suppose be a let me 5 nearest neighbors, 

6 nearest neighbors, 10 nearest neighbors, 1 nearest neighbor what should in K nearest 

neighbor algorithm that we discuss in our class, where we compare regression to K nearest neighbors. What should the value of K be? and you can use the validation to figure that 

out. The last use case and sense of the validation phase is one that can also be used to choose 

the correct number of input variables and to also say which ones those input variables 

should be such choosing the correct number and also choosing those which... So, I can say I need five input variables are not six, but which five should they be. 

So, in that sense and this kind of really links up with best some sets ridge regression where you are trying a whole combination of different inputs. Again the idea could be 

that I can have a fixed model, I could say I am going to use ridge regression or like 

I am going to use regression or I am going to use trees with this parameter, but which inputs should I will be using, should I use all my input variables, should I use input 

variable 4, 7 and 9 you know. So, that decision also sometimes like we spoke earlier about 

regression here R square will just keep on getting higher and higher as you keep on adding more inputs that does not mean you getting a better model. 

So, choosing those inputs says you can choose a model with inputs a, b, c, d another one 

where you use inputs d, e, f, g and you can really compare the performance of these two 

different models on the validation data set. Now, there are some techniques already there 

that we have discussed which helps you choose what the input should be and we spoken about 

that using metrics that are more complex and R square like for instance I adjusted R square which you know goes ahead and you know penalizes more complexity and there are a whole bunch 

of a blanket of approaches to penalize more important. So, we adjusted R square there something called the AIC information criteria AIC and then 

this Bayes Information Criteria (BIC). So, there are whole bunch of metrics that will just go way and say blanket I am going to penalize you for adding more input variables. 

But, we don’t need to do that by that might be computationally convenient and it is you 

could if you have the luxury of creating this validation data set. A simple thing that you could do is you can just try these different combinations of inputs 

for the chosen method of doing the training and so on and apply them and see which approach 

does better. So, you could just untimely wind up using prediction error instead of say something 

like a adjusted R square, you can use the prediction error. Because, you are not just blindly looking at how good the model fix the data that you train the model on you now 

taking the data that the model has never seen when I trained and you using that to decide 

which what your input variables should be or how many input variables you should have. So, the important thing is the validation itself and this whole process of model selection 

is not just one thing, you could use validation really it is choose between multiple methods, 

you can use at to find tune the parameters of the algorithm and you can also kind use that and some way to choose what your variables should be. Finally, you have the test phase, 

now we are at number 3, the test phase and we have three and now we are talking about 

three the test phase. The test phase, the purpose is different it is not to find tune the model, it is not to help you come up with the best model, you 

could come up with the good model, you could come up with the horrible model, the test phases I do not care, my job the job of the test phase is fairly simple. The job of the 

test phase is to give an accurate idea of what the performance of this algorithm is 

going to be and you can turn around and say why do not I just use you know something like 

my prediction error that I got in the validation phase. And the answer is, you cannot because you have or hell I mean you can go even one step 

for instance say why cannot I just look at the predicted the error, the residue will 

some of the squares and some sense. The error that is comes from the training, you cannot do that for the simple reason that you are explicitly find tuning the model, find tuning 

the co-efficient, find tuning the parameters irrespective and this I am loosely using these 

term, because it applies to any approach you take, but there is decision trees whether it is discriminant analysis, whether it is support vector machines, whether it is neural 

network, whatever approach you take, there is a whole bunch of optimization going on these approaches you have to sitting there and trying to make this model make sense out 

of this data that process itself is going to make you do very good on this data. 

Now, to get true picture of how well you do when you see a new data, you do not want data 

that was used to make this method good in the first place, you use some data and the 

training data to figure out the co-efficiency. So, use some data and the training data to construct the actual tree or that actual network, you use some data and the validation data 

said to find tune what your parameter should be why. So, that you do a good job of fitting. 

Now, you do not want to use the same data to tell me how good I will perform when I 

go when tomorrow someone comes to me and says here is input data can you come up with the 

prediction for me, to do that you want to really look at data that is never been looked 

at essentially this is you should think of this as data that is been kept in the vault 

and never looked at and you brining it out in the end after all the modeling has been 

finished, you use the training data to construct the tree, you use the select the validation data to fine tune it, you now have end product the end product regression could be a set 

of co efficient like the betas, then product in tree could be the tree structure, whatever 

it is. You basically have a finalized end product and you want to now see how good the end product works, now pull out this data, pull out this 

data provide the input data from the test data set ask the model to make predictions 

compare it to the actual output and you are just going to report that. You are not going 

to use this data to make any more decisions in terms of which method to use or you are 

not going to use data to fine tune anything about your model. Because, if you are doing that you are using the data to make the model better, but you 

are not giving any longer an accurate impression of how successful you would be if you were 

to completely see this data fresh and you were to make predictions, you are not giving 

an assessment of the model. So, in order to give the assessment of the model pull out 

this data that is never been seeing by model in the end and the only purpose of this data is to see how good the model is not to construct the model or not to fine tune the model. So, 

that is the test phase. Now, we are going to talk about in the last part and approach called cross validation 

and it is really ties to this case that I showed you in the first slide, where training 

and validation are kind of put together, this is an approach that really kicks and when if you feel like do not have enough that much data for separate validation set and you want 

to kind a squeeze more out of it. So, a simple approach there is just have the training and 

validation set as a one large data set and you can break it up into n chunks. So, out here of broken it up into five chunks going between five to ten is typical and the 

approach is called actually k fold cross validations. So, it is k fold where you have to choose 

a particular k and here I have chosen five and so basically broken the data set into 

five chunks; obviously, very important guys is that this break up is it is random. So, 

if you given the data in some order, you do not want to very you know conveniently just break it up into five chunks and because they might be some kind of you know implicit order 

may be the data was for instance chronologically order, you do not want to create training 

data set one of the early chronology data. So, you kind of want to shuffle the data up 

and then break it up into five bins and the idea is to use any four bins to train four 

to train and one to validate. So, you might sit there and say hay you know you just sounds 

like, you are breaking up the training and validation at the latest stage than at a earlier 

stage. But, the answer is no there is one more step, once you do that shuffle them around 

meaning, now in this particular graph I have shown you let us call them 1, 2, 3, 4 and 

5 in this graph you are using 1, 2, 3 and five to train and you are validating. So, 

this is train and you are validating with 4. Now, permute and the next step do 1, 2, 3, 4 and validate on 5 next step permute 2, 3, 

4, 5 and validate on 1 and so you like that you keep going till you done all the five 

combinations, where you would a validate on step and you take the cumulative validation 

results. So, you will take the cumulative validation results to see how well to make 

decisions in terms of validation and so this is called k fold cross validation. So, one question that comes up fairly frequently is what should the value of k be, in k fold 

cross validation, given you tentative idea there it is typically some are between five and ten, but again that is not something that is cast in stone and it is more important 

that you understand what it is means to choose high value of k verses low value of k. In 

the simplest sense our high value of k in a k fold cross validation leads to a model, 

where leads to an assessment I should say which is of very low bias, but of high variance 

and a very low k. So, k of like 2 or 3 or so. 

So, essentially could have low variance, but a higher bias and one extension of for instance 

the cross validation, where you have a very high k, where you like it to have an unbiased 

assessment, but assessment with lots of variance is the case of the leave one out cross validation. 

The idea here is that if you have n data points I am going to use n minus 1 data points to 

do the training and I am going to validate on that one data point that I left out and 

what I will do is I will keep just like in k fold cross validation, how you change the 

training sets in the validation sets, how you shuffle them, how you permute them, the 

same way here going to leave one data point out for validation train on the others and 

then predict on this and then keep doing that interactively. So; obviously, with that if it is essentially like k is equal to n then the cross validation 

is almost completely unbiased, but can have high variance, because the n training sets 

are so similar to one and other and some approach like that is also computationally fairly cumbersome. 

But, if you have that approach then leave one out the cross validation is also something 

that you might consider. So, I hope that gives you an idea of cross validation and more broadly the use of you know the idea behind validation as a means 

of model selection and this whole thing of breaking up training for creating models validation 

for assessing and kind of you know selecting models and test for pure assessment, but no 

for the selection and decision making. Thank you. 

English - NPTEL Official 

Support Vector Machines 

Hello and welcome to this module on Support Vector Machines. 

So, we have been looking at the variety of classifier so far and one of the things, let 

us look at the linear classifier. So, the one of the thing is, if I have data points 

that are even perfectly separable, here is a class and here is another class, you can 

see that they are very clearly separated. But, when I train a linear classifier it is 

not entirely clear, which of these many possible lines that could separate the data, would 

your classifier end up learning. There are many, many different lines that could separate 

the data and we are not sure, what your classifier would end up learning. 

So, support vector machines initially were born out of and need to answer this question. 

Among all of these different lines or all of these different decision surfaces that 

you could use for separating the data given to you, which of those is the best decision 

surface? Some of all those alternatives which you think should be the best decision surface. 

So, one answer to this question is to define an optimal separating, if an optimal separating 

hyper plane as the surface, such that the nearest data point to the surface is as far 

away as possible among all of it is surfaces. So, here is a separating line and the nearest 

data point to that is that or that or that. So, if you think about it, so the nearest 

data point cannot belong to just one class. So, I could draw a line like this, but then 

there would mean that I am reducing the distance of the data point to the separating surface 

or if I go this way, again I will be the reducing the distance of the data point to the surface 

in one class or the other. When I say that you are maximizing the distance of the closest 

data point to the separating hyper plane, that essentially means that the closest data 

point from either class is at the same distance away from the hyper plane. 

So, this distance and being same as this distance would be the same as this distance and this. 

So, this distance of the closest data point to the separating surface is known as the 

margin of the classifier, which we will denote by 

m. So, the goal of finding a separating optimal separating hyper plane is essentially to find 

the classifier, such that this margin m is as large as possible. So, let us step back 

and think about what such a line means. You know in all linear classifiers we have seen 

so far, so we know that we are going to say something like. 

So, y is beta naught plus beta transpose x, for convenience sake here I will write it 

as x transpose beta, since we are taking inner products that is fine. So, a line like this 

could essentially be obtained by setting this beta naught plus x transpose beta equal to 

0. So, all the data points on this line or those data points for which beta naught plus 

x transpose beta evaluates to 0, so that is the equation of the line here. So, if it is 

negative beta naught plus beta transpose x is less than 0, so we are going to say that 

x is of class minus 1 and if beta naught plus beta transpose x is greater than 0, we will 

say that next class plus 1. So, remember that, so we will, we using some 

kind of encoding for the class. The class could be does not buy a computer or buys a 

computer, he is sick, he is healthy. I mean the classes could be many different things, 

but numerically we are going to be assigning some encoding for the class and in this case, 

I choose to use minus 1 and plus 1 as the encoded. There is a reason for that as we 

will see shortly. So, if beta naught plus beta transpose x is less than 0 and I say, 

it is class minus 1. But, in this case what I really want, I do 

not want it to be this less than 0, but I want it to be at least m away from the hyper 

plane. I want it to be m away from the line beta naught plus beta transpose x equal to 

0. So, I might use x transpose and beta transpose x interchangeably at points, but as you know 

they are inner products, so that is fine. 

So, what I really want is, so y i is plus 1 I want beta naught plus x i transpose beta 

to be greater than m. What happens if y i is minus 1? I really want it to be at least 

m away in that case as well, but then we know that beta naught plus x i transpose beta would 

be negative, when the class is minus 1. So, what I do is I essentially just multiplied 

by the actual class variable and I want this whole distance. Because, if y i is plus 

1 I would like this also to be plus, the positive and I want it to be at least m away from the 

hyper plane and y i is minus 1, this is going to be negative. So, the product is going to 

be positive and I want that to be at least m away from the hyper plane. 

So, this is thick and strained that we want to satisfy and what is our goal. If we remember, 

our goal is to make sure that this m is as large as possible. So, what will do is, we 

will say maximize m beta naught beta subject toÉ So, I am going to maximize the margin 

m over beta naught and beta, subject to the constraints that y i times x i transpose beta 

plus beta naught is greater than or equal to m, for every data point in my training 

data. So, this kind be done assuming that all the 

data is nicely separated. So, and I can actually draw a linear surface that separates the data. 

So, if a kind of linear surface that separates data, then I can come up with at least one 

surface that satisfies this constraints for some value of m and essentially, I have to 

find value of m that is maximum here. But, one thing if you look at this equation or 

the constraint that we have written, so I can arbitrarily increase the value of beta 

and make this value as large as I want. So, I need to have some constraint on beta 

as well. So, what we will do is, we will constraint the norm of beta to be equal to 1. So, we 

will not look at all possible weights beta naught and beta, we will only look at those 

weights insist that the size of beta is constraint to be 1. So, the norm of beta is, you could 

take the Euclidean norm of beta, I am saying that the norm of beta should be 1. So, I hope 

the formulation of the optimization problem so far is clear. 

So, it is essentially saying that I want all my data points to be at least a distance m 

away from the hyper plane and subject to that constraint and subject to my beta being norm 

one, I want to maximize the margin. So, this is a pretty works and constraint, so we can 

try to get rid of it by changing the other inequality constraints to by normalizing them 

with the beta. So, this again allows me to achieve the same effect of not getting a high 

value for m just by increasing the size of beta, because I am dividing by the size of 

beta. So, that achieves the same constraint and 

you can essentially write it like that. So, one thing that we should note here is that, 

if a specific beta satisfies these constraints, any positively scale version of beta would 

also satisfies the constraints. I can just multiply by some positive number, if it is 

originally all, for all the exercise was giving me negative values larger than m or minus 

m or positive values larger than m, just multiplying it by a positive quantity will not change 

anything. It will still give me negative values that are lesser than minus m or positive values 

that are greater than m. Therefore I can essentially choose a specific value for beta, such that 

this evaluates to 1. 

So, I set, so accept norm beta equal to 1 by m, so that this constraint becomes y i 

x i transpose beta is greater than equal to 1 subject to the constraint that, you are 

finding the smallest such beta. So, this optimization problem then becomes, this is optimization 

problem of maximizing the margin, now essentially becomes the problem of finding the smallest 

beta, such that this conditions are satisfied. So, this is essentially means that my margin 

here is going to be 1 over now beta. So, to make it mathematically more convenient 

I am going to minimize the quadratic form of that. So, essentially I will be minimizing 

this square of beta, since it is norm any way. So, this would be positively to begin 

with, so I can minimize this square, that is not a problem and so that is my final optimization 

problem. So, this is the final optimization problems, where I am saying that, so together 

with these constraints a kind of define a slab around the separating hyper plane, I 

define a slab around these separating hyper plane of with 1 by beta. So, making sure that 

there are no data points with in this region, so I am trying to now maximize the width of 

this region, so that there are no data points in that region, that is essentially the idea 

behind this optimization problem. So, this defines the basic optimization problem 

in the case of support vector machines. So, in the next module we will look at, how do 

you go about setting up a solution for this optimization problem. 

 

 
