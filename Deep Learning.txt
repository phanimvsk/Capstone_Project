
ever wondered how google translates an entire web page to a different language in a matter of seconds or your phone
gallery groups images based on their location to achieve ai through algorithms trained with data and finally
deep learning is a type of machine learning inspired by the structure of the human brain in terms of deep
learning this structure is called an artificial neural network let's understand deep learning better and how
it's different from machine learning say we create a machine that could differentiate between tomatoes and
cherries if done using machine learning we'd have to tell the machine the features based on which the two can be
differentiated these features could be the size and the type of stem on them with deep learning on the other hand the
features are picked out by the neural network without human intervention of course that kind of independence comes
at the cost of having a much higher volume of data to train our machine now let's dive into the working of
neural networks here we have three students each of them write down the digit nine on a piece of
paper notably they don't all write it identically the human brain can easily
recognize the digits but what if a computer had to recognize them
that's where deep learning comes in here's a neural network trained to identify handwritten digits
each number is present as an image of 28 times 28 pixels now that amounts to a
total of 784 pixels neurons the core entity of a neural
network is where the information processing takes place each of the 784 pixels is fed to a
neuron in the first layer of our neural network this forms the input layer
on the other end we have the output layer with each neuron representing a digit with the hidden layers existing
between them the information is transferred from one layer to another over connecting channels each of these
has a value attached to it and hence is called a weighted channel all neurons have a unique number
associated with it called bias this bias is added to the weighted sum of inputs
reaching the neuron which is then applied to a function known as the activation function
the result of the activation function determines if the neuron gets activated every activated neuron passes on
information to the following layers this continues up till the second last layer
the one neuron activated in the output layer corresponds to the input digit the
weights and bias are continuously adjusted to produce a well-trained network so where is deep learning
applied in customer support when most people converse with customer support agents the conversation seems so real
they don't even realize that it's actually a bot on the other side in medical care neural networks detect
cancer cells and analyze mri images to give detailed results self-driving cars
what seem like science fiction is now a reality apple tesla and nissan are only
a few of the companies working on self-driving cars so deep learning has a vast scope but it too faces some
limitations the first as we discussed earlier is data while deep learning is the most
efficient way to deal with unstructured data a neural network requires a massive volume of data to train let's assume we
always have access to the necessary amount of data processing this is not within the capability of every machine
and that brings us to our second limitation computational power training
in neural network requires graphical processing units which have thousands of
cores as compared to cpus and gpus are of course more expensive and finally we
come down to training time deep neural networks take hours or even months to
train the time increases with the amount of data and number of layers in the network some of the popular deep
learning frameworks include tensorflow pytorch keras deep learning 4j cafe and
microsoft cognitive toolkit considering the future predictions for deep learning and ai we seem to have only scratched
the surface in fact horus technology is working on a device for the blind that uses deep learning with computer vision
to describe the world to the users replicating the human mind at the entirety may be not just an episode of
science fiction for too long the future is indeed full of surprises last summer
my family and i visited russia even though none of us could read russian we did not have any trouble in figuring our
way out all thanks to google's real-time translation of russian boards into english
this is just one of the several applications of neural networks neural networks form the base of deep
learning a subfield of machine learning where the algorithms are inspired by the structure of the human brain
neural networks take in data train themselves to recognize the patterns in this data and then predict the outputs
for a new set of similar data let's understand how this is done let's construct a neural network that
differentiates between a square circle and triangle neural networks are made up of layers of
neurons these neurons are the core processing units of the network
first we have the input layer which receives the input the output layer predicts our final output
in between exists the hidden layers which perform most of the computations required by our network here's an image
of a circle this image is composed of 28 by 28 pixels which make up for
pixels each pixel is fed as input to each neuron of the first layer
neurons of one layer are connected to neurons of the next layer through channels each of these channels is assigned a
numerical value known as weight the inputs are multiplied to the corresponding weights and their sum is
sent as input to the neurons in the hidden layer each of these neurons is associated with
a numerical value called the bias which is then added to the input sum
this value is then passed through a threshold function called the activation function
the result of the activation function determines if the particular neuron will get activated or not
an activated neuron transmits data to the neurons of the next layer over the channels
in this manner the data is propagated through the network this is called forward propagation
in the output layer the neuron with the highest value fires and determines the output the values are basically a
probability for example here our neuron associated with square has the highest
probability hence that's the output predicted by the neural network
of course just by a look at it we know our neural network has made a wrong prediction but how does the network
figure this out note that our network is yet to be trained during this training process along with
the input our network also has the output fed to it the predicted output is
compared against the actual output to realize the error in prediction the magnitude of the error indicates how
wrong we are and the sign suggests if our predicted values are higher or lower than expected
the arrows here give an indication of the direction and magnitude of change to reduce the error
this information is then transferred backward through our network this is known as back propagation
now based on this information the weights are adjusted this cycle of forward propagation and
back propagation is iteratively performed with multiple inputs this process continues until our weights
are assigned such that the network can predict the shapes correctly in most of the cases
this brings our training process to an end you might wonder how long this training process takes honestly neural networks
may take hours or even months to train but time is a reasonable trade-off when
compared to its scope let us look at some of the prime applications of neural networks facial
recognition cameras on smartphones these days can estimate the age of the person based on their facial features this is
neural networks at play first differentiating the face from the background and then correlating the
lines and spots on your face to a possible h forecasting neural networks
are trained to understand the patterns and detect the possibility of rainfall or rise in stock prices with high
accuracy music composition neural networks can even learn patterns in music and train itself enough to compose
a fresh tune with deep learning and neural networks we are still taking baby steps the growth in this field has been
foreseen by the big names companies such as google amazon and nvidia have
invested in developing products such as libraries predictive models and intuitive gpus that support the
implementation of
by simply learn my name is richard kirschner with the simply learn team and today we're going over the deep
learning tutorial what is deep learning deep learning is a type of machine learning that works on the basis of
functionalities of human brain it trains machines to work on vast volumes of structured and unstructured data
deep learning uses the concept of neural networks that is derived from the structure of a human brain and if you've
ever seen images of a human brain network you have dendrites inputs you have cell which is a nucleus as your
nodes you have synapses which create weights and an axon which create an output
now this is a very simplified version of the human brain and there are
certain sets of cells that are strung together very similar to how today's neural networks
perform but they still have a long ways to go and the human brain is still significantly more complex
with hundreds of different kinds of cells and different interactions we don't see yet
so what is deep learning artificial intelligence so you have your ai is a method of
building smart machines that are capable to think like human and mimic their actions
be careful with that definition we're a long way from a human cyborg coming in
and taking over when we talk about this we're usually talking about automating uh
single kind of instances or things going on how do we automate a new process how do we automate a small robot to do
something how do we get a drone to fly out when it loses contact to turn around and come back in the direction it came
from those are very simple processes and significantly lower than the scale of
like what human thinking does now a subcategory of artificial intelligence is machine learning
machine learning is an application of ai that allows machines to automatically learn and improve with experience
there's a lot of tools out there to use with machine learning and you'll see really regression models
and things like that they're much more common when dealing with straight numbers a linear regression model and
there's other models that just do basic math functions quite well
but then we get into one of the subcategories deep learning deep learning is a subfield of machine learning that is used to extract
patterns from data using neural networks and again we're talking about complicated patterns
we talk about image processing and things like that they're not as straightforward as the numbers in stock
exchange so you start looking at another way to solve these problems and figure out is that a raccoon in the picture or
something much more complicated if getting your learning started is half the battle what
if you could do that for free visit scale up by simply learn click on the link in the description to know more
deep learning performance so when you have we talk about performance of deep learning and the amount of data
the performance goes up the more data you have the higher the performance when we talk about a lot of machine
learning platforms they kind of peak out at a lower level so again you can think of this as having
thousands of pictures of raccoons like i said before versus the numbers in a stock exchange which are very uh rigid
and very clear there you have a close and an open in the stock exchange kind of thing where you have an image of a
raccoon the color shading all kinds of things go into trying to figure out is it a raccoon
so we look at what is a neural network we really look into kind of a nice image of it
today's neural networks usually have a layer of inputs and you'll see here we have input 1 2 and 3 with x1 x2 x3
so you have your input layer you have your hidden layers this might have multiple layers depending on what you're
working on then you have your output layer which in this case we have two outputs y1 and y2
and you can also see the connectivity here so everything in the first row connects with everything in the second row in the
hidden layer and if you had another hidden layer everything in the first hidden layer would connect to the second hidden layer and then everything in the
second hidden layer would connect to each of the outputs and then make calculations based on that and this
is interesting because there's so many different aspects of neural networks nowadays that are changing this basic
configuration they find that if you skip a hidden layer or two
with your input going through that that actually changes the results and
works better in some cases there's also convolutional neural networks which look at windows and adding up the numbers in
them there's a lot of complexity when we start getting into the different aspects of what you can do and how you build
neural networks and again it's very very much in an infant stage so this basic diagram does a great job of capturing
what it looks like now the input layer is responsible to accept the inputs in various formats
the hidden layers again there's that s could be multiple layers is responsible for extracting features and hidden
patterns from the data and you know a hidden patterns and features is important we want to look at
features you usually talk about features as your input you have input one is one feature input two is another feature if
you're looking at the iris data it might be the width and length of the pedal each one of those would be a feature
you have these nodes which generate a number that doesn't really have a specific representation
but it becomes a way of looking at it of the adding the features together and creating an importance value there
and the output layer produces the desired output after completing the entire processing of the data
what is a perceptron a perceptron is a binary classification algorithm proposed by frank rosenblatt
and you'll see here we have our constants come in and our inputs we have a constant 1 for the bias the bias is
important you can go back to euclidean geometry where you have a line
y equals mx plus b b being the y-intercept that's what that
constant is is there needs to be some kind of adjustment that basically is the y-intercept
and you have your inputs you have your weights you have your weighted sum your activation function and an output of 0
or 1. and so here we have we'll go ahead and walk through these we have x1 x2 x3 are
the inputs w naught w1 w2 w3 are the weights weights are values that determine the
strength of the connection between two neurons and if we go back
to the slide let me just flash back here to this slide you can see how each one of these nodes
has multiple inputs so when you look at the hidden layer of the outputs they have multiple inputs so each one of
these nodes has these inputs they might be the original features coming in they might
be the layer of nodes before so you have your x1 x2 and x3 are the input into your node your weights are the weighted
values that determine the strength of the connection between two neurons
the input neurons are multiplied with the weight and a bias is added there's that one which is weighted the
y-intercept in euclidean geometry to give the resulted weight sum
the activation function applies a step to check if the output of the weighting function is greater than 0 or not
there's a lot of ways to do an activation function but this is the most common or the most not common but the
most easy to see way of doing an activation on here so we look at here we go another go
walking back through this diagram and taking a closer look at it we have the predicted output is compared
with the actual output so once you go through the step and it adds everything together in there and
these are your weights are multiplied they're just multiple so you have feature of x1 times weight of 1
plus feature of x2 times weighted 2 so on plus the weight times the bias and we
go ahead and compute all those all the way through the node comes out and we have
the actual output and then we go ahead and have a predicted output what do we think it's going to be
and this is on each note so keep in mind we're zero in and on the node this isn't the whole process
because this actually goes through all the notes but there's a there's another step in here when we get to that part
the error in the network is estimated using the cost function and back propagation technique is used to improve
the performance so when we look at the in the back propagation algorithm the
cost function is minimalized by changing weights and biases in the network and you can see here we have our input layer
we have our hidden layers and we have our output layer and so you can think of this as we have our actual
output we predict what it's going to be in this case we have two outputs we might predict that it's either nothing
there or a raccoon so one of them comes up and says i don't see any kind of animal and the other one
says this is a raccoon so it's either yes or no and if it gets it wrong it says that's wrong it sends
that error back and that error goes through the first set of weights which then goes to the hidden layers and
their set of weights which goes back to b1 the other layer and their set of weights and it adjusts
those weights as it goes backwards and it says hey this is the error on the output
we kind of adjusted a little bit the part that we don't adjust in the first set of weights we say there's
still an error so we send it to the second set of weights and so forth what happens is as we adjust these
weights each one of these nodes starts creating kind of a category
or something to look for in whatever image or data we have going in
and so for making a better predictions a number of epochs are executed where the error is determined by the cost function
now epic is a important thing to keep track of epics is if you have a large
or any data set and let's say you split out your test date and your training data set you're
running your data through how many times do you have to run all of that data through
before you start getting something that is usable till the error goes down you can't adjust the error anymore it's the
lowest error you can get so each time you go through a full set of data
before you repeat and go through the same data that's called an epic the error is backward propagated until a
sufficiently small error is achieved and again that's what we're talking about we're trying to minimize the error
so we get to a point where that error really isn't changing anymore you just have the same error coming out
and there's other ways to weight that error too the cost function or loss function measures the accuracy of the network the
cost functions tries to penalize the network when it makes errors you can see here we have a formula for
the cost function c equals one-half of the y predicted minus the y the actual y
squared and of course the squared value removes the sign because we don't know
whether it's plus or minus when you look at an error and then the c this is your cost function how much is it going to cost how much of an error do we really
have that we're sending back and so we have with the cost function we can look at this we can look at it as a loss and
the epic again that's every time we go through the data that's an epic how many epic runs are we going to go and so we
have a nice graph here that shows like a very high learning rate low learning rate
different data is going to change depending on what you're working on they put the yellow as a good learning
rate because it has a nice slope to it and you think yeah the more epics i go in the lower the
loss so that means you're going in a good direction there and as you curves down he gets to the
the best answer has the lowest loss on there so uh looking at types of neural
networks uh we have the gan we have the dbn the rbfn the auto
encoder the rnn the lstm this is a flash
of some of the main ones that are out there there are now so many variations that there's variations on the
variations and so just quickly jumping into these as you can see we have a number of them here listed
these are just like a flash of some of the more common ones like the gan general adversarial network
where you have two different models competing against each other until they find which one can beat the other one
can i think of a chess game where you keep flipping who's in charge and then we might look at the
dbn which is a deep belief network
and they're used to recognize clusters and generate image video sequences generally and we have the rbf
network rbfn which is your radial basis function network which uses a different set of
activation functions to figure out which is the right weights
auto encoder the auto encoder is a little different
than the other ones in that it looks for an error but the error is
based in finding data that groups together uh so it's a way of sorting the data out without knowing the answer
that's what an autoencoder does rnn rnn has a couple different definitions uh
most the time you now see it as a recurrent neural network where the output
part of the output goes back into the input so they call it this was recurrent
there's also another rnn which deals with learning step by step as opposed to over
a set of data and weighting it as you go and there's the
ls tm or long short term memory recurrent neural network it's also an rnn
which deals with how do you sort something like a sentence out where the last word depends on what the
first word was and so it slowly waits things as it goes through to figure out what's being said
again these are just a quick flash and even as i was describing them you should have been like well why don't you hook
an auto encoder into again and then run that with a dbn well there are tools to go ahead and mix
and match these things so you will actually see when you start doing layers you might
have the layers of one type of neural network feed into the next neural network and that's actually pretty
common and we talk about deep learning libraries there's a lot of different
things out there but the big ones that they usually talk about and most of these are dealing with
larger data is like tensorflow cross cross and tincture flow play nice with
each other and cross kind of is is usually integrated with tensorflow cafe
theno pytorch dl 4j these are all different deep learning
libraries and there's many more out there there's a scikit has a deep learning library in it under python
scala has one that they've been building on there so these aren't the only ones tensorflow is probably the most
robust one at this time but that's not to say the other ones aren't catching up and there's not all kinds of other stuff out there now going
through and talking about these things is all fun but until you get to roll up your sleeves and take a look at the code and
see what's going on it's really hard to see what we're talking about so we're going to do a deep learning demo on text
classification now to do this there's a lot of different editors you can use for
python currently my favorite my favorites always one i'm currently using
which changes from month to month or year to year depending what projects we're doing we want to look at
anaconda navigator which is a nice job building your different packages and
environments and go into jupiter notebook and so we'll go and go into jupiter notebook and create our
our setup in there to run a neural network and so we open up our jupyter we'll go
ahead and create a new python three and let me just bring this out to the
top there we go uh so now we already have our jupiter three and i like to always give it a
setup on here so this is the text classification we're doing today
we'll go ahead and rename that and we were talking about if you remember from the beginning of the thing
i said some of the the more the most one of the most robust packages out there right now is
tensorflow along with keras which usually works nicely with tensorflow so
we're going to go ahead and be working with the tensorflow and the cross we want to go ahead and do
a number of imports in here this is just set up on this a lot of
this really isn't necessary for what we're going to do there's different things we can do so we're going to go
ahead and from future we're going to import absolute import division print function
these really don't do a lot other than they help us kind of display things later on
you don't really need them you can just do some basic print on this if you want to so i'm not going to dig too deeply into
that setup but we do want to take a look at iteration tools
because we're always iterating over things again you don't really need the iteration tools because a lot of
tensorflow will do this for you but it's a lot of times when we're building these packages we don't think
about it we just bring in all our tools that we might need the big ones though is our pandas db
and let me switch to a draw thing it's always nice to kind of give us arrows here uh
there we go uh so we're looking at you can see here here's our pandas panda sits on top of
uh numpy our number array pandas is our data frame just makes it really easy to
bring in the data view the data and kind of work with the file that's what these two
are for and of course our matplot library up here is for displaying it and then we have the sklearn
these are for doing metrics and pre-processing some of this you can actually pull off of tensorflow i tend to bounce back and
forth between the sk learn setup and by the way the sk learning has its own neural network a real simplified one
compared to what tensorflow does but we want to go ahead and do this in tensorflow where we're going to be using
tensorflow cross and cross is nice because it sits on top of tensorflow
but uh it's easier to use it's a little bit like you could think of tensorflow as being the back end which you can
program in and cross being a higher level language which makes it easier to do things in
and you can see here we went ahead and printed out tf version we're using
tensorflow as the back end and you have the tensorflow version 2.1.0
always important to double check your versions you never know what's not going to work with what version of
python and so forth so it's important to check those this is python36
i don't know if tensorflow is fully integrated with 37 or 38 yet
i'm sure if it isn't it soon will be and so we're going to go ahead and bring some data in this is
consumercomplaints.csv and if you want a copy of this file just for your own to play with you can
put a note send a note to simplylearn and they'll be happy to send you a copy of that
and then df stands for data frame that's very common and we're using the pandas
to go ahead and read this comma separated variable file in and we'll go ahead and print what they
called the head if you've never worked with data frames usually when you see head it means the first five rows of
that data frame so you can see what's going on in there and
give it just a moment to read that in there it goes
and you'll see in here that we have uh a date received a product mortgage
credit reporting consumer loan credit card sub product other mortgage nan nan
whatever that means whatever it is missing data right there there's nothing in there vehicle loan and so forth and
then of course different columns and this is a data frame data frame has rows and columns
and a lot of times when you're processing data frames you can think of mapping
map and reduce as a terminology we're mapping we're either mapping each of the rows like you might be running a process
on each row or you're running a process on each column and then one of the most common things
would be say to some parts of the row together and have a total value of cost or maybe some
the total uh you may come over here wherever i'm sure there's a value in here um complaint id yeah they don't actually
have a value but it'd be something you would process by column you might want to find out how
many different companies are listed here so that might be something you'd run on the column
so that gives you an idea we have our data frame df head on there and if you remember we're looking at
text classification so that makes sense that we don't have any dollar values in there and we're going to look at just a couple
of these columns certainly you can do this with a number of different objects on
here but we're going to take just the columns consumer complaint narrative and the product column
and in this case when we handle null values there's a lot of different ways to handle null values
but in this case we're just going to go ahead and do just the not null we only want those
that are not null in the consumer complaint narrative and let's go ahead and print that do the df head
in jupiter the last line if you just have a variable it automatically prints it out
um so you could put print and put brackets around df head and do the same thing
and you can see here we have our consumer complaint narrative i have outdated information on my credit
report and then you have your credit reporting and so forth on each one of these an
account on my credit card was mistaken the company refuses to provide me verification the complaint regards to
square to something so we're going to be looking at these complaint narratives and trying to understand them
and can we write a code to optimize that and then one of the things we always do
like up here you'll see where we already removed the null values
i would go ahead and just double check uh sum up the null values are there any
null values in our data frame and this is just a simple way of looking at every cell this includes consumer
complaint the product and so forth and you can see here we have
zero null values so that's good we're not we're not going to play with the null values today and then we can also
go ahead and take a look at uh our end
aspect we might be looking for which is our product we have credit reporting consumer loan credit reporting debt
collection debt collection uh let's go ahead and just take a look at this
and see how many accounts we have and you can see here that there is under
debt collection 47 000 entries under mortgage there's 36 000 entries under
credit card uh we have over 18 000 entries so this is a pretty big
database and you can see it going down all the way down here with the different
uh entries that go underneath of product so we have and one of the things we
don't spend a lot of time on in some of these demos is what is really defining what we're
looking for now if you're doing a data science project you usually start by exploring
the data and then you define what you're really looking for and so we're kind of skipping through that really that
particular process it does constitute a small amount of time
but as far as the importance is one of the most important things you can do in data science
is ask the right questions so even though you're spending 80 percent of the time cleaning data
building your models and everything that 20 percent of asking the right questions has a higher impact and so you really
should be making sure that you understand what the questions are that's domain knowledge so we're talking
in this case banking and so in this case it might be that as consumer complaint comes in
we can start looking at these consumer complaints and what product they're attached to what that means
we're not going to dig too much into the domain in asking the question you know what exactly we're looking for we really
want to look into the process as far as building a neural network
and so uh the first thing we do is go ahead and split our data uh we need a train set and a test set
the train size in this case we're going to do 80 of the data is going to be for training our neural network
um and then we'll go ahead and use the we'll switch that over for the test size
to be the 20 percent let me go and run that and you can see here train size has 15
159 000 entries and then we'll test that on we've held out roughly 40 000 of
those entries for our test size and then for this example we're just going to split it
the first part of the data will be for training our data and the second part will be for testing our data so we're
not really doing a random setup in here which is okay because this is an example
a lot of times you might split this one of the things i do with neural networks is i will split it into three
sections and i will test two as training and the third as um the
test this is obviously not big data you don't want to do this on something that might take days to process
um and then i flip it and then so i'll run it three times and those three times will give me a nice narrative of how
good my model is and then i'll actually run the the final product on all three sections to program it to train it
uh so it gives me a good basis of what kind of error i have versus you know on test models while having a very robust
model to publish in this case so we're just going to go ahead and split it based on
the first part goes to the train and the second part goes to the test so
one of the next things whenever we deal with text and this is such an important line
i want to go ahead and just highlight the tokenize you'll see the word tokenize tokenizer setting it up
we're taking the words and putting them into a format the computer can understand
there is a lot of ways to tokenize words in some cases you call them 0 1 2 3 4.
depending on what you're doing it might just be a zero or one um so when we talk about the encoder
we're usually talking about as far as tokenize we actually are looking at in this
particular case we'll be looking at each word as its own feature
so when we looked up here remember right up here let's see here we go money transfer let me let me go back up just a
note here turn that off so i can go back up if you remember up here we have this right here consumer complaint
narrative so i would be a feature have would be a feature outdated would be a
feature information would be a feature on feature my feature credit feature report
and you think wow that's a lot of features yes in running through bills
put out by the united states that are being voted on in this in the government
it comes out roughly 2.4 million different words are used in those bills
that's a lot of features and so we're going to go ahead and look at uh max words tokenize and tokenizing
words there's a lot that is happening here in the tokenized setup so we'll go
ahead and let me just put together some of this code here so the tokenize is an actual object in
here text tokenizer and it has number of words equals max words so we're going to limit it to a
thousand words character level equals false fit on text train narrative only fit on the training data and so this is
kind of interesting because it drops a lot of these words uh why would it drop a word well on on is probably used a
bunch and really doesn't have any value so that's would be one of the words they'll probably drop
and also there's other things it can do like you can also combine combinations a word it might be that this company this
complaint these might be always together so at some point it might actually bundle some of these
words as a single feature there's a lot of things that go into that we're not going to go into too much
detail because you really have to go through the api on these to understand all the different uh encoding and setups
you have this is just a really fast way to do this and it works
i like easy and i like things that work i don't know about you but what i'm running through and doing a lot of
different models i might start tweaking these once i have an answer
but until then we want to go ahead and run the encoder and do something simple
like this and so uh using sk learn utility to convert the label strings to number
index and so here's our encoder label encoder encoder fit
and this is sklearn of course everything's fit and then we do y train equals the encoder transform train product y test
equals encoder transform test product now what's going on here now up here we
did the x where we have let me view up here where we were looking at um
here we go uh i have outdated information on my credit card report now we're looking at this
credit reporting consumer loan credit reporting and if you remember this is our list of those debt collection here's
our list of them they're not a huge number and so we're going to encode these differently
this is just 0 1 2 3 4 and so on not necessarily in that order by order by
the way so be a little careful on order so we're going to convert the labels to one hot representation as our next step
and that's what this is doing so we have our number classes in p max y train plus one
our weight y train equals utilities to categorical so here's where we're taking our y value
in the training set and we do the same with the test set
and we go ahead and run that so now we've created a y train and a y test where we've numbered our categorical
we've created a categorical data so it's easy to read the answer and translate it back
and we've encoded up here our x and we used a tokenizer
so a little different encoder puts in 0 1 2 3 for the different listings an
encoder or token you can get tongue tied on these a
tokenizer takes and creates a huge in this case we've limited to 1 000 words
each word is its own token and to really see what we're looking at let me go ahead and we're going to inspect the
elements and see what we ended up with uh let me run this and so we have our y
train shape there's our 1000 what is that 1000 maximum words so we've tokenized the top
thousand words as each as its own feature
and the same thing with the x tests those are x train and x test we have our y train shape and our y train test shape
and um i thought the encoder depending on how you set it um either to zero one two three or in this
case it actually puts it out as 18. so it did the same thing as a tokenizer
as far as putting it as a zero one so you have 18 choices there and if we count these i'm guessing
there's 18 there now this is the part which is we look at the
encoders and the tokenizers these are the tools you need to process
text the computer doesn't see
the hello on you have to give it a zero or one in each one of those and so this is all
about the text classification part this is how we classify text
is we have to give it something that has a number representation so once we've sorted out our data and we've converted
it into something the computer can read for doing text we want to go ahead and create our model
and when we create our model one of the things to be aware of is this is what we call a black box model
now they've come a long ways in understanding how these different processes are created and so they're
starting to understand how you can put these together and go back and say why why does it pick this why does it pick
that how does it balance that but it's black boxing that it's really hard to do it's really hard to go in
there and figure out why it picks one over the other just by looking at the neural network itself
there are other machine learning tools like decision tree which make it much easier to see those
changes and how it branches down but they don't perform as well in many cases
and so we're going to go ahead and build a model we'll go ahead and run this just because it builds the model doesn't actually
start fitting it yet let's take a look at these different pieces so we have our model which is going to
be sequential that's a cross so we imported that at the beginning from the cross setup
this tells us what's going on um that we're going to be running this from top to bottom
and we're going to add a dense layer so each time you see add let me do
we look at these ad each one of these we're adding a row
so these are all rows and then of course our rows you're like what is a row
uh so we talk about row these are your layers we have you can see right there input
layer we're going to add an activation to this layer and we're going to use the relu activation then we're going to add
a dropout rate and what this does is they found that when they process a neural network
instead of processing every neuron each time you do the back propagation to train it
you only process some of them so only some of them are being trained by doing that it's able to create the
differentiation between different categories and it actually trains much better instead of trying to train them
all at once so that's what this is right here with the drop out point five and then we
added add dense number classes and the dense number classes is another
layer up here we have our add an activation relu layer so we have our input our relu layer with a dropout of
0.5 then we have a dense layer which uses a soft max
activation there is a lot going on with cross and these models you can get lost
in just the activation here's our two activation relu
is one type of activation softmax is another they work differently you can see when
we were talking about earlier we talked about the different kinds of neural networks in cross and tensorflow each of those
layers can be a different neural network layer and can function differently and
you can stack them on top of each other and feed them into each other and then the final thing of course is
your compile we have what we're using for loss remember we want to minimize loss so
category this is a type of way of minimizing that loss atom is an optimizer
you'll find there's a number of optimizer atom is used for larger data sets as you get to smaller data sets
you actually use a very different optimizer in here and we look at the accuracy that's just when do we stop compiling
this data how far do we go until it starts building what they call a bias it starts
it's over fitted we want to stop at the right moment and then finally we get to
actually training our model this is that black box we're going to fit it to the data we don't know what's actually going
on in there but we want to go ahead and run it and i'm going to go ahead and start it running because it takes a moment for it
to run through and you'll see the epic feed down here as it goes through epic 105
and so forth we'll freeze this just for a second there we go um so let's take a look at this we have
batch size um batch size oops i meant to do that in an arrow
there we go batch size is how many rows of data we're going to feed at a time
so you can feed larger rows there's there's different reasons to feed them at different sizes
um really a lot of times people just leave this as a default depending and let it choose four of them
i'm not sure why they picked 32 in this case there's probably when they were messing with this 32 probably was a good
batch size for fitting it the back end math deals with differential equations
and some calculus which we won't get into so being aware of that that this batch
size affects how it does that that back end differential equation in the reverse
propagation um tells us that this is actually a pretty important number if you get it too big
it's going to not it's not going to fit as well and if you get it too small it takes way too long to process
and a lot of times there's what they call a reinforced learning neural network where
it's batch size of one what does that mean well that means every action you take has a feedback you program the
neural network so you can guess what your next action is that's very common like in trying to beat video games with a
automated setup in a neural network the epic says we're going to go through
all the data in the training set five times so that's what this remember
we talked about epics that's what the epic is and we can see when i let it go out here
we're on epic two five so we'll go ahead and pause this i'm gonna go ahead and pause it for a second
let it finish running or while we're waiting we can actually take a look at some of this data here
and see what it's actually generating for us
and so we look at this accuracy loss
so we want to minimize loss the metrics is accuracy and you can see
that we're going to want the accuracy to go up and we want the loss to go down
and the loss is going to be what we want to minimize and this is just some of the metrics
and it talks about like value loss value accuracy and how they're changing
with each time we run it and so you get to a point where no longer gets better or worse and at that
point you really want to stop running it you can overfit it and over fit it means it's going to miss some of the
generalities that you need when solving some of these solutions and then i didn't talk too much about
what's actually going on here what are we doing in the domain of this particular one which it looks like it is
trying to figure out based on a consumer complaint narrative maybe they send in a
complaint what product is it connected to so maybe they get the complaints before the
product or something like that i'm not sure why you do this particular setup
that would be again a domain knowledge in here so what we're doing is we're using the consumer complaint narrative
to predict what product they're talking about someone comes sends in an email
and says i have outdated information on my credit report they're probably talking about a credit card reporting if you get a random email that says i
purchased a new car on blank the car something something probably alone that's what we're trying to do is if you
get a random input from the consumer complaint narrative you can point to the
product that they're referring to without having to call them up and ask them i guess that might be useful not sure
now that we've gone through all five epics let's go ahead and evaluate the accuracy
of our trained model and so we have um we're going to go ahead and do a score for the model.evaluate
a nice crass setup where we can do the x test and the y test the batch size verbose and then
we're going to it's going to generate a score and one of the things let's just go ahead and print
so you can see why they broke out the score let me just go ahead and print the score in here let's go ahead and run that
so it's testing the score it's going to take a moment to go through all the data and it gives us a nice score and it says
the test accuracy these can mean a lot of different things but we have a 0.5 to be honest without looking at the data
i would have to look and see exactly which things it missed on and how it how it scored on there but it's it's getting
about you know half of it it's able to pull in half of it and say hey if this is the complaint this is what
it's connected to keep in mind when we're talking about text that's really good can you imagine some
of the stuff i don't know if you've ever worked in tech support or i.t or at a counter in a in the mall or even in
taking orders someplace as a waiter or waitress or fast food or whatever
when you try to understand people it gets pretty crazy so even an accuracy
score like this is probably pretty good for understanding some of this text and there might be steps you could do to
improve that and so let's go ahead and go through here and look at actually using it
and we'll punch this and it says how to generate prediction on individual examples we have our text labels in our
encoder class and we'll just go through a prediction equals model predict np array of x test
of i predict label text labels prediction print test narrative
so forth this is just let's go ahead and run it because reading through print statements can be very painful sometimes
unless you actually see what's going on so when the president came out with the harp program dot dot dot the actual
label was mortgage the predicted label was mortgage so when this person sent this this complaint in maybe you don't
know what it's connected to yet you can guess it's probably a mortgage i filed a
dispute with capital one bank on dot and it says oh credit card well it turns
out it's actually debt collection it was what it predicted um so yeah missed a little bit uh if a lot
of times when you dispute something it probably is a debt collection
in this case it was specific to a credit card okay so we missed that one
i am disputing account number xxs with midland whatever actual test label debt collection was
debt collection i opened a barclay card on to help rebuild my credit credit card
credit card mortgage mortgage so forth we can go through all of these and you can see it does a pretty good
job it gets close or in fact most of these are pulled in correctly credit card reporting
this is a lot of what we talk about with text setup sentiment's a really big one
there's a whole sentimental libraries out there whether someone is positive or negative towards something if you're pulling off
twitter feeds uh you might want to look that up you might want to look at connection towards postings in what was it there was
running stock i actually do some some stock programming code so i end up coming back
to that a lot finding sentiment feeds on different stock values and different stocks out
there on out of national publications really helps trying to predict what the
stock's going to do what are people going to do in buying and selling stock same thing with this we can now predict
their complaints and try to figure out what it is they're complaining about now that wraps up our deep learning demo
on text classification and gives you a nice introduction to deep learning as opposed to shallow
learning bad joke on my part but you can see how a deep learning
model can go in and do a lot of things that you might not be able to do using basic linear regression or many of the
other machine learning models and text classification is one of those
where neural networks really shine because they can pick up things that you don't see you can't really measure in
other ways so what is a neural network so hi guys i heard you want to know what
a neural network is here we have looks like you just went shopping in a red tag cell my robot's back so as a matter of
fact you have been using neural network on a daily basis and today's world is just amazing how much we use our new
technology we're not even aware of it when you ask your mobile assistant to perform a search for you you know like
saying your google or siri or whoever you use amazon web self-driving cars so
that's the newest thing coming out they're just now trying to make those legal and different states in the u.s and around the world even in the uk they
now have a self-driving cars going up and down the streets pretty amazing these are all neural network driven computer games use it it's a lot of
computer games are driven by neural networks in the back end as part of the game system and how it adjusts to the
players and it's also used in processing the map images on your phone so every time you do a navigation someplace and
it opens it up they now use neural networks to help you find the quickest way to get there neural network a neural
network is a system or hardware that is designed to operate like a human brain
in today's development this is so important to understand because we don't have anything else to compare it to i'm
sure someday in the future the computer will redefine or the neural network or the ai artificial intelligence will
redefine what these mean but as far as we can today's world in today's commercial development we have to
compare it to what humans do so it's we want to compare and how it operates to a human brain and how it solves problems
like a human does what can a neural network do and really we're just going to dive in deeper to we just covered and
look at other examples so what can a neural network do well let's list out the things neural networks can do for
you translate text boy we got google translate and microsoft has their own
translate they have some really cool they actually have an earpiece it's supposed to start translating as you talk what a cool technology what a cool
time to live identify faces can you imagine all the uses for facial identification in the case of our
sample or our code that we're going to look at later we'll be identifying dogs and cats so not quite as detailed as uh
understanding whose face belongs to who i'm waiting for the google glasses to come out so i can see who's who and
identify faces as i'm walking around have a little name tag over them not out there yet but boy we are close we can
identify the faces and they have all kinds of technologies to bring that information back to us recognize speech
goes along with the translate text so now as you're talking into your assistant it can use that to do commands
turn lights on all kinds of things you can do with recognizing speech read had written text they're starting to
translate all these old text documents that they've had in storage instead of doing it individually where somebody's
going through each text by themselves in a room picture like an old raiders of the lost ark theme where he's in the
back you know archaeologists studying the text now it's fed into a computer they take a picture they even use neural
networks to take a scroll that is so messed up that they can't undo the scroll and they x-ray it and then they
use that x-ray to translate the text off of it without ever opening the scroll i
mean just way cool stuff they're starting to do with all this and of course control robots what would be a
neural network without bringing in the robots and we have our own favorite robot in the middle who goes to our red
tag cell and go shopping for us so you know these are just a few of the wonderful things that neural networks
are being applied to it's such an infant stage technology what a wonderful time to jump in and there are a lot of other
things it goes into i mean we could spend just forever talking about all the different applications from business to
whatever you can even imagine they're now applying neural networks to help us understand so now we talked a little bit
about all the cool things you can do with the neural network let's dive in and say how does a neural network work
so now we've come far enough to understand how neural network works let's go ahead and walk through this in
a nice graphical representation they usually describe a neural network as having different layers you'll see that
we've identified a green layer an orange layer and a red layer the green layer is the input so you have your data coming
in it picks up the input signals and passes them to the next layer the next layer does all kinds of calculations and
feature extraction it's called the hidden layer a lot of times there's more than one hidden layer we're only showing
one in this picture but we'll show you how it looks like in a more detail a little bit and then finally we have an
output layer this layer delivers the final result so the only two things we see is the input layer and the output
layer now let's make use of this neural network and see how it works wonder how traffic cameras identify vehicles
registration plate on the road to detect speeding vehicles and those breaking the law they got me going through a red
light the other day well last month that's like the horrible thing they sent you this picture of you and all your information because they pulled it up
off of your license plate and your picture they shouldn't have gone through the red light so here we are and we have
an image of a car you can see the license plates on there so let's consider the image of this vehicle and find out what's on the number plate the
picture itself is 28 by 28 pixels and the image is fed as an input to identify
the registration plate each neuron has a number called activation that represents
the grayscale value of the corresponding pixel range and we range it from zero to one one for a white pixel and zero for a
black pixel and you can see down here we have an example where one of the pixels is registered as like .82 meaning it's
probably pretty dark each neuron is lit up when its activation is close to one so as we get closer to black on white we
can really start seeing the details in there and you can see again the pixel shows this one up there it's like part of the car and so it lights up so pixels
in the form of arrays are fed to the input layer and so we see here the pixel of a car image fed as an input and
you're going to see that the input layer which is green is one dimension while our image is two dimension now when we
look at our setup that we're programming in python it has a cool feature that automatically does the work for us if
you're working with an older neural network pattern package you then convert each one of those rows so it's all one
array so you'd have like row one and then just tack row two on to the end you can almost feed the image directly into
some of these neural networks the key is though is that if you're using a 28 by 28 and you get a picture of this 30 by
30 shrink the 30 by 30 down to fit the 28 by 28 so you can't increase the
number of input in this case green dots it's very important to remember when you work on neural networks and let's name
the inputs x1 x2 x3 respectively so each one of those represents one of the pixels coming in and the input layer
passes it to the hidden layer and you can see here we now have two hidden layers in this image in the orange and
each one of those pixels connects to each one of those hidden layers and the interconnections are assigned weights at
random so they get these random weights that come through that if x1 lights up then it's going to be x1 times this
weight going into the hidden layer and we sum those weights the weights are multiplied with the input signal and a
bias is added to all of them so as you can see here we have x1 comes in and it actually goes to all the different
hidden layer nodes or in this case whatever you want to call them network setup the orange dots and so you take
the value of x1 you multiply it by the weight for the next hidden layer so x1
goes to hidden layer one x1 goes to hidden layer two x1 goes hidden layer one node two hidden layer one node three
and so on and the bias a lot of times they just put the bias in as like another green dot or another orange dot
and they give the bias a value one and then all the weights go in from the bias into the next node so the bias can
change we always just remember that you need to have that bias in there there's things that can be done with it
generally most the packages out there control that for you so you don't have to worry about figuring out what the
bias is but if you ever dive deep into neural networks you got to remember there's a bias or the answer won't come
out correctly the weighted sum of the input is fed as an input to the activation function to decide which
nodes to fire and for feature extraction as the signal flows within the hidden layers the weighted sum of inputs is
calculated and is fed to the activation function in each layer to decide which nodes to fire so here's our feature
extraction of the number plate and you can see these are still hidden nodes in the middle and this becomes important
we're going to take a little detour here and look at the activation function so we're going to dive just a little bit
into the math so you start to understand where some of the games go on when you're playing with neural networks in
your programming so let's look at the different activation functions before we move ahead here's our friendly red tag
shopping robot and so one is the sigmoid function and the sigmoid function which is 1 over 1 plus e to the minus x takes
the x value and you can see where it generates almost a 0 and almost a 1 with
a very small area in the middle where it crosses over and we can use that value to feed into another function so if it's
really uncertain it might have a 0.1 or 0.2 or 0.3 but for the most part it's going to be really close to 1 and really
close to this case 0 0 to 1. the threshold function so if you don't want to worry about the uncertainty in the
middle you just say oh if x is greater than or equal to zero if not then x is
zero so it's either zero or one really straightforward there's no in between in the middle and then you have the what
they call the relu relu function and you can see here where it puts out the value
but then it says well if it's over one it's going to be one and if it's less than zero it's zero so it kind of just
dead ends it on those two ends but allows all the values in the middle and again this like the sigmoid function
allows that information to go to the next level so it might be important to know if it's a 0.1 or a minus 0.1 the
next hidden layer might pick that up and say oh this piece of information is uncertain or this value has a very low
certainty to it and then the hyperbolic tangent function and you can see here it's a 1 minus e to the minus 2x over 1
plus e to the minus 2x and it's very much along the same theme a little bit different in here in that it goes
between -1 and one so you'll see some of these they go zero to one but this one goes minus one to one and if it's less
than zero it's you know it doesn't fire and if it's over zero it fires and it also still puts out a value so you still
have a value you can get off of that just like you can with the sigmoid function and the relu function very
similar in use and i believe the original used to be everything was done in the sigmoid function that was the most commonly used and now they just
kind of use more the relu function the reason is one it processes faster because you already have the value and
you don't have to add another compute the one over one plus e to the minus x for each hidden node and the data coming
off works pretty good as far as putting it into the next level if you want to know just how close it is to zero how
close is it not to functioning you know is it minus 0.1 minus 0.2 usually they're float values you get like minus
point minus 0.00138 or something so yeah important information but the relu is most commonly used these days as far as
the setup we're using but you'll also see the sigmoid function very commonly used also now that you know what an
activation function is let's get back to the neural network so finally the model
would predict the outcome of applying a suitable activation function to the output layer so we go in here we look at
this we have the optical character recognition ocr is used on the images to convert it into a text in order to
identify what's written on the plate and as it comes out you'll see the red node and the red node might actually
represent just a letter a so there's usually a lot of outputs when you're doing text identification we're not
going to show that on here but you might have it even in the order it might be what order the license plates in so you
might have a b c d e f g you know the alphabet plus the numbers and you might have the one two three
four five six seven eight nine ten places so it's a very large array that comes out it's not a small amount of you
know we show three dots coming in eight hidden layer nodes you know two sets of four we just saw one red coming out a
lot of times this is uh you know 28 times 28 if you did 30 times 30 that's 900 nodes so 28 is a little bit less
than that just on the input and so you can imagine the hidden layer is just as big each hidden layer is just as big if
not bigger and the output is going to be there's so many digits yeah it's a lot there's it's a huge amount of input and
output but we're only showing you just you know it would be hard to show in one picture and so it comes up and this is what it finally gets out in the output
as it identifies a number on the plate and in this case we have 0 8 d
0 3 8 5 8. error in the output is back propagated through the network and
weights are adjusted to minimize the error rate this is calculated by a cost function when we're training our data
this is what's used and we'll look at that in the code when we do the data training so we have stuff we know the
answer to and then we put the information through and it says yes that was correct or no because remember we
randomly set all the weights to begin with and if it's wrong we take that error how far off are you you know are
you off but is it if it was like minus one you're just a little bit off if it's like minus 300 was your output remember
when we're looking at those different options you know hyperbolic or whatever and we're looking at the rel the rel
doesn't have a limit on top or bottom it actually just generates a number so if it's way off you have to adjust those
weights a lot but if it's pretty close you might adjust the weights just a little bit and you keep adjusting the
weights until they fit all the different training models you put in so you might have 500 training models and those
weights will adjust using the back propagation it sends the error backward the output is compared with the original
result and multiple iterations are done to get the maximum accuracy so not only does it look at each one but it goes
through it and just keeps cycling through these the data making small changes in the network until it gets the
right answers with every iteration the weights at every interconnection are adjusted based on the error we're not
going to dive into that math because it is a differential equation and it gets a little complicated but i will talk a
little bit about some of the different options they have when we look at the code so we've explored a neural network
let's look at the different types of artificial neural networks and this is like the biggest area growing is how
these all come together let's see the different types of neural network and again we're comparing this to human
learning so here's a human brain i feel sorry for that poor guy so we have a feed for
forward neural network simplest form of a they call it a n a neural network data
travels only in one direction input to output this is what we just looked at so
as the data comes in all the weights are added it goes to the hidden layer all the weights are added it goes to the
next hidden layer all the weights are added and it goes to the output the only time you use the reverse propagation is
to train it so when you actually use it it's very fast when you're training it it takes a while because it has to
iterate through all your training data and you start getting into big data because you can train these with a huge
amount of data the more data you put in the better train they get the applications vision and speech
recognition actually they're pretty much everything we talked about a lot almost all of them use this form of neural
network at some level radio basis function neural network this model classifies the data point based on its
distance from a center point what that means is that you might not have training data so you want to group
things together and you create central points and it looks for all the things you know some of these things are just
like the other if you've ever watched a sesame street as a kid that dates me so it brings things together and this is a
great way if you don't have the right training model you can start finding things that are connected you might not have noticed before applications power
restoration systems they try to figure out what's connected and then based on that they can fix the problem if you
have a huge power system cajon and self-organizing neural network vectors of random dimensions are input to
discrete map comprised of neurons so they basically find a way to draw they call them they say dimensions or vectors
or planes because they actually chop the data in one dimension two dimension three dimension four five six they keep
adding dimensions and finding ways to separate the data and connect different data pieces together applications used
to recognize patterns and data like in medical analysis the hidden layer saves its output to be used for future
prediction recurrent neural networks so the hidden layers remember its output from last time and that becomes part of
its new input you might use that especially in robotics or flying a drone you want to know what your last change
was and how fast it was going to help predict what your next change you need to make is to get to where the drone
wants to go applications text-to-speech conversation model so i talked about
drones but you know just identifying on lexis or google assistant or any of
these they're starting to add in i'd like to play a song on my pandora and i'd like it to be at volume 90
so you now can add different things in there and it connects them together the input features are taken in batches like
a filter this allows a network to remember an image in parts convolution neural network today's world in photo
identification and taking apart photos and trying to you know you ever seen that on google where you have five
people together this is the kind of thing separates all those people so that it can do a face recognition on each
person applications used in signal and image processing in this case i use facial images or google picture images
as one of the options modular neural network it has a collection of different neural
networks working together to get the output so wow we just went through all these different types of neural networks
and the final one is to put multiple neural networks together i mentioned that a little bit when we separated
people in a larger photo in individuals in the photo and then do the facial recognition on each person so one
network is used to separate them and the next network is then used to figure out who they are and do the facial
recognition applications still undergoing research this is a cutting edge you hear the term pipeline and
there's actual in python code and in almost all the different neural network setups out there they now have a
pipeline feature usually and it just means you take the data from one neural network and maybe another neural network
or you put it into the next neural network and then you take three or four other neural networks and feed them into another one so how we connect the neural
networks is really just cutting edge and it's so experimental i mean it's almost creative in its nature there's not
really a science to it because each specific domain has different things it's looking at so if you're in the
banking domain it's going to be different than the medical domain than the automatic car domain and suddenly
figuring out how those all fit together is just a lot of fun and really cool so we have our types of artificial neural
network we have our feed forward neural network we have a radial basis function neural network we have our cohenin self
organizing neural network recurrent neural network convolution neural network and modular neural network where
it brings them all together and no the colors on the brain do not match what your brain actually does but they do
bring it out that most of these were developed by understanding how humans learn and as we understand more and more
of how humans learn we can build something in the computer industry to mimic that to reflect that and that's
how these were developed so exciting part use case problem statement so this is where we jump in this is my favorite
part let's use the system to identify between a cat and a dog if you remember correctly i said we're going to do some
python code and you can see over here my hair is kind of sticking up over the computer cup of coffee on one side and a
little bit of old school a pencil and a pen on the other side yeah most people now take notes
i love the stickies on the computer that's great that's that is my computer i have sticky notes on my computer in
different colors so not too far from today's programmer so the problem is is we want to classify photos of cats and
dogs using a neural network and you can see over here we have quite a variety of dogs in the pictures and cats and you
know just sorting out it is a cat it's pretty amazing and why would anybody want to even know the difference between a cat and a dog okay you know why well i
have a cat door it'd be kind of fun that instead of it identifying instead of having like a little collar with a
magnet on it which is what my cat has the door would be able to see oh that's the cat that's our cat coming in oh
that's the dog we have a dog too that's a dog i want to let in maybe i don't want to let this other animal in because
it's a raccoon so you can see where you could take this one step further and actually apply this you could actually
start a little startup company idea self-identifying door so this use case
will be implemented on python i am actually in python 3.6 it's always nice to tell people the
version of python because that does affect sometimes which modules you load and everything and we're going to start by importing the required packages i
told you we're going to do this in karass so we're going to import from karas models sequential from the cross
layers conversion 2d or conv 2d max pooling 2d flatten and dense and we'll
talk about what each one of these do in just a second but before we do that let's talk a little bit about the environment we're going to work in and
you know in fact let me go ahead and open a the website cross's website so we can learn a little bit more about cross so
here we are on the cross website and it's a k-e-r-a-s dot io that's the
official website for cross and the first thing you'll notice is that cross runs on top of either tensorflow
cntk and i think it's pronounced thanos or thiano what's important on here is that tensorflow and the same is true for
all these but tensorflow is probably one of the most widely used currently packages out there with the cross and of
course you know tomorrow this is all going to change it's all going to disappear and they'll have something new out there so make sure when you're
learning this code that you understand what's going on and also know the code i mean look when you look at the code it's
not as complicated once you understand what's going on the code itself is pretty straightforward and the reason we
like karas and the reason that people are jumping on it right now is such a big deal is if we come down here let me
just scroll down a little bit we talk about user friendliness modularity easy extensibility work with python python is
a big one because a lot of people in data science now use python although you can actually access cross other ways if
we continue down here is layers and this is where it gets really cool when we're working with cross you just add layers
on remember those hidden layers we were talking about and we talked about the relu activation you can see right here let me
just up that a little bit in size there we go that's big i can add in an r e l u
layer and then i can add in a soft max layer in the next instance we didn't talk about soft max so you could do each
layer separate now if i'm working in some of the other kits i use i take that and i have one setup and then i feed the
output into the next one this one i can just add hidden layer after hidden layer with the different information in it
which makes it very powerful and very fast to spin up and try different setups and see how they work with the data
you're working on and we'll dig a little bit deeper in here and a lot of this is very much the same so when we get to
that part i'll point that out to you also now just a quick side note i'm using anaconda with python in it and i
went ahead and created my own package and i called it the cross python36 because i'm in python36 anaconda is cool
that way you can create different environments real easily if you're doing a lot of different experimenting with these different packages probably want
to create your own environment in there and the first thing as you can see right here there's a lot of dependencies a lot
of these you should recognize by now if you've done any of these videos if not kudos for you for jumping in today pip
install numpy scipy the scikit-learn pillow and h5py are both needed for the
tensorflow and then putting the cross on there and then you'll see here and pip is just a standard installer that you
use with python you'll see here that we did pip install tensorflow since we're going to do cross on top of tensorflow
and then pip install and i went ahead and used the github so git plus get and you'll see here github.com this is one
of their releases one of the most current release on there that goes on top of tensorflow and you can look up these instructions pretty much anywhere
this is for doing it on anaconda certainly you'd want to install these if you're doing it in ubuntu server setup
you'd want to get i don't think you need the h5py and ubuntu but you do need the rest in there because they are
dependencies in there and it's pretty straightforward and that's actually in some of the instructions they have on their website so you don't have to
initially go through this just remember their website on there and then when i'm under my anaconda navigator which i like you'll
see where i have environments and on the bottom i created a new environment and i called it cross python 36 just to
separate everything you can say i have python 30.5 and python 36. i used to have a bunch of other ones but it kind
of cleaned house recently and of course once i go in here i can launch my jupyter notebook making sure i'm using
the right environment that i just set up this of course opens up my in this case i'm using google chrome and in here i
could go and just create a new document in here and this is all in your browser window when you use the anaconda do you
have to use anaconda and jupiter notebook no you can use any kind of python editor whatever setup you're
comfortable with and whatever you're doing in there so let's go ahead and go in here and paste the code in and we're
importing a number of different settings in here we have import sequential it's under the models because that's the
model we're going to use as far as our neural network and then we have layers and we have conversion 2d max pooling 2d
flatten dense and you can actually just kind of guess at what these do we're talking we're working in a 2d photograph
and if you remember correctly i talked about how the actual input layer is a single array it's not in two dimensions
it's one dimension all these do is these are tools to help flatten the image so it takes a two-dimensional image and
then it creates its own proper setup you don't have to worry about any of that you don't have to do anything special with the photograph you let the cross do
it we're going to run this and you'll see right here they have some stuff that is going to be depreciated and changed because that's what it does everything's
being changed as we go you don't have to worry about that too much if you have warnings if you run it a second time the warning will disappear and this has just
imported these packages for us to use jupiter is nice about this that you can do each thing step by step and i'll go
ahead and also zoom in there a little control plus that's one of the nice things about being in a browser
environment so here we are back another sip of coffee if you're familiar with my other videos
you notice i'm always sipping coffee i always have a in my case latte next to me an espresso so the next step is to go
ahead and initialize we're going to call it the cnn or classifier neural network
and the reason we call it a classifier is because it's going to classify it between two things it's going to be cat
or dog so when you're doing classification you're picking specific objects you're specific it's a true or
false yes no it is something or it's not so first thing we're going to create our classifier and it's going to equal
sequential so their sequential setup is the classifier that's the actual model we're using that's the neural network so
we call it a classifier and the next step is to add in our convolution and
let me just do a let me shrink that down in size you can see the whole line and let's talk a little bit about what's
going on here i have my classifier and i add something what am i adding well i'm
adding my first layer this first layer we're adding in is probably the one that takes the most work to make sure you
have it set correct and the reason i say that this is your actual input and we're going to jump here to the part that says
input shape equals 64 by 64 by 3. what does that mean well that means that our
pictures coming in and there's these pictures remember we had like the picture of the car was 128 by 128 pixels
well this one is 64 by 64 pixels and each pixel has three values that's where
these numbers come from and it is so important that this matches i mentioned a little bit that if you have like a
larger picture you have to reformat it to fit this shape if it comes in as something larger there's no input notes
there's no input neural network there that will handle that extra space so you have to reshape your data to fit in here
now the first layer is the most important because after that keras knows what your shape is coming in here and it
knows what's coming out and so that really sets the stage most important thing is that input shape matches your
data coming in and you'll get a lot of errors if it doesn't you'll go through there and picture number 55 doesn't
match it correctly and guess what it does it usually gives you an error and then the activation if you remember we
talked about the different activations on here we're using the relu model like i said that is the most commonly used
now because one it's fast doesn't have the added calculations in it it just says here's a value coming out based on
the weights and the value going in and from there you know it's uh if it's over
one then it's good or over zero it's good if it's under zero then it's considered not active and then we have
this conversion 2d what the heck is conversion 2d i'm not going to go into
too much detail in this because this has a couple of things it's doing in here a little bit more in depth than we're
ready to cover in this tutorial but this is used to convert from the photo because we have 64 by 64 by three and
we're just converting it to two dimensional kind of setup so it's very aware that this is a photograph and that
different pieces are next to each other and then we're going to add in a second convolutional layer that's what the conv
stands for 2d so these are hidden layers so we have our input layer and our two
hidden layers and they are two-dimensional because we're doing with a two-dimensional photograph and you'll see down here that on the last one we
add a max pooling 2d and we put a pool size equals 2 2. and so what this is is
that as you get to the end of these layers one of the things you always want to think of is what they call mapping
and then reducing wonderful terminology from the big data we're mapping this data through all these layers and now we
want to reduce it to only two sets in this case it's already in two sets because it's a 2d photograph but we had
you know two dimensions by we actually have 64 by 64 by three so now we're just getting it down to a two by two just the
two dimension two dimensional instead of having the third dimension of colors and we'll go ahead and run these we're not
really seeing anything in our run script because we're just setting up this is all set up and this is where you start
playing because maybe you'll add a different layer in here to do something else to see how it works and see what your output is that's what makes cross
so nice is i can with just a couple flips of code put in a whole new layer that does a whole new processing and see
whether that improves my run or makes it worse and finally we're going to do the final setup which is to flatten
classifier add a flattened setup and then we're going to also add a layer a dense layer and then we're going to add
in another dense layer and then we're going to build it we're going to compile this whole thing together so let's flip
over and see what that looks like and we've even numbered them for you so we're going to do the flattening and
flatten is exactly what it sounds like we've been working in a two-dimensional array of picture which actually is in
three dimensions because of the pixels the pixels have a whole another dimension to it of three different values and we've kind of resized those
down to two by two but now we're just going to flatten it i don't want to have multiple dimensions being worked on by
tensor and by keras i want just a single array so it's flattened out and then step four full connection so we add in
our final two layers and you could actually do all kinds of things with this you could actually leave out this
some of these layers and play with them you do need to flatten it that's very important then we want to use the dents
again we're taking this and we're taking whatever came into it so once we take all those different the two dimensions
or three dimensions as they are and we flatten it to one dimension we want to take that and we're going to pull it
into units of 128. they got that you just say where did they get 128 from you could actually play with that number and
get all kinds of weird results but in this case we took the 64 plus 64 is 128.
you could probably even do this with 64 or 32. usually you want to keep it in the same multiple of whatever the data
shape you're already using is in and we're using the activation the relu just like we did before and then we finally
filter all that into a single output and it has how many units one why because we want to know
whether true or false it's either a dog or a cat you could say one is dog zero
is cat or maybe you're a cat lover and it's one is cat and zero is dog if you love both dogs and cats
you're gonna have to choose and then we use the sigmoid activation if you remember from before we had the relu and
there's also the sigmoid sigmoid just makes it clear it's yes or no we don't want any kind of in-between number
coming out and we'll go ahead and run this and you'll see it's still all in setup and then finally we want to go
ahead and compile and let's put the compiling our classifier neural network and we're going to use the optimizer
atom and i hinted at this just a little bit before where does adam come in where
does an optimizer come in well the optimizer is the reverse propagation when we're training it it goes all the
way through and says error and then how does it readjust those weights there are a number of them atom is the most
commonly used and it works best on large data most people stick with the atom
because when they're testing on smaller data see if their model is going to go through and get all their errors out before they run it on larger data sets
they're going to run it on atom anyway so they just leave it on atom most commonly used but there are some other
ones out there you should be aware of that that you might try them if you're stuck in a bind or you might floor that
in the future but usually atom is just fine on there and then you have two more settings you have loss and metrics we're
not going to dig too much into loss or metrics these are things you really have to explore cross because there are so
many choices this is how it computes the error there's so many different ways to on your back propagation and your
training so we're using the atom model but you can compute the error by standard deviation standard deviation
squared they use binary cross entropy i'd have to look that up to even know what that is there's so many of these a
lot of times you just start with the ones that look correct that are most commonly used and then you have to go read the cross site and actually see
what these different losses and metrics and what different options they have so we're not going to get too much into
them other than to reference you over to the cross website to explore them deeper but we are going to go ahead and run them and now we've set up our classifier
so we have an object classifier and if you go back up here you'll see that we've added in step one we added in our
layer for the input we added a layer that comes in there and uses the relu
for activation and then it pulls the data so this is even though these are two layers the actual neural network
layer is up here and then it uses this to pull the data into a two by two so into two dimensional array from a
three-dimensional array with the colors then we flatten it so there's our outer flattened and then we add another dense
what they call dense layer this dense layer goes in there and it downsizes it to 128 it reduces it so you can look at
this as we're mapping all this data down the two-dimensional setup and then we flatten it so we map it to a flattened
map and then we take it and reduce it down to 128 and we use the relu again
and then finally we reduce that down to just a single output and we use the sigmoid to do that to figure out whether
it's yes no true false in this case cat or dog and then finally once we put all these layers together we compile them
that's what we've done here and we've compiled them as far as how it trains to use these settings for the training back
propagation so if you remember we talked about training our setup and when we go
into this you'll see that we have two data sets we have one called the training set and the testing set and
that's very standard in any data processing is you need to have that's pretty common in any data processing is
you need to have a certain amount of data to train it and then you got to know whether it works or not is it any good and that's why you have a separate
set of data for testing it where you already know the answer but you don't want to use that as part of the training set so in here we jump into part two
fitting the classifier neural network to the images and then from cross let me
just zoom in there i always love that about working with jupiter notebooks you can really see we're going to come in here we do the cross pre-processing and
image and we import image data generator so nice of course it's such a high-end
product right now going out and since images are so common they already have all this stuff to help us process the
data which is great and so we come in here we do train data gin and we're going to create our object for helping
us train it for reshaping the data so that it's going to work with our setup and we use an image data generator and
we're going to rescale it and you'll see here we have one point which tells us it's a float value on the rescale over
255. where does 255 come from well that's the scale in the colors of the pictures we're using their value from 0
to 255. so we want to divide it by 255 and it will generate a number between 0
and 1. they have shear range and zoom range horizontal flip equals true and
this of course has to do with if the photos are different shapes and sizes like i said it's a wonderful package you
really need to dig in deep to see all the different options you have for setting up your images for right now
though we're going to stick with some basic stuff here and let me go ahead and run this code and again it doesn't really do anything because we're still
setting up the pre-processing let's take a look at this next set of code and this one is just huge we're creating the
training set so the training set is going to go in here and it's going to use our train data gen we just created
dot flow from directory it's going to access in this case the path data set
training set that's a folder so it's going to pull all the images out of that folder now i'm actually running this in
the folder that the data sets in so if you're doing the same setup and you load your data in there and you're doing this
make sure wherever your jupyter notebook is saving things to that you create this path or you can do the complete path if
you need to you know c colon slash etc and the target size the batch size and
class mode is binary so the class is we're switching everything to a binary value batch size what the heck is batch
size well that's how many pictures we're going to batch through the training each time and the target size 64 by 64.
little confusing but you can see right here that this is just a general training and you can go in there and look at all the different settings for
your training set and of course with different data we're doing pictures there's all kinds of different settings
depending on what you're working with let's go ahead and run that and see what happens and you'll see that it found 800
images belonging to one classes so we have 800 images in the training set and
if we're going to do this with the training set we also have to format the
pictures in the test set now we're not actually doing any predictions we're not actually programming the model yet all
we're doing is preparing the data so we're going to prepare a training set and the test set so any changes we make
to the training set at this point also have to be made to the test set so we've done this thing we've done a train data
generator we've done our training set and then we also have remember our test set of data so i'm going to do the same
thing with that i'm going to create a test data gen and we're going to do this image data generator we're going to
rescale 1 over 255 we don't need the other settings just the single setting for the test data gen and we're going to
create our test set we're going to do the same thing we did with the test set except that we're pulling it from the test set folder and we'll run that and
you'll see in our test set we found 2 000 images that's about right we're using twenty percent of the images as
test and eighty percent to train it and then finally we've set up all our data we've set up all our layers which is
where all the work is is uh cleaning up that data and making sure it's going in there correctly and we're actually going
to fit it we're going to train our data set and let's see what that looks like and here we go let's put the information
in here and let's just take a quick look at what we're looking at with our fit generator we have our
dot fit generator that's our back propagation so the information goes through forward with a picture and it
says oh you're either right or you're wrong and then the air goes backward and reprograms all those weights so we're
training our neural network and of course we're using the training set remember we created the training set up
here and then we're going steps per epic so it's 8 000 steps epic means that
that's how many times we go through all the pictures so we're going to rerun each of the pictures and we're going to go through the whole data set 25 times
but we're going to look at each picture during each epic 8 000 times so we're really programming the heck out of this
and going back over it and then they have validation data equals test set so
we have our training set and then we're gonna have our test set to validate it so we're gonna do this all in one shot
and we're gonna look at that and they're gonna do 200 steps for each validation and we'll see what that looks like in just a minute let's go ahead and run our
training here and we're going to fit our data and as it goes it says epic one of 25 you start realizing that this is
going to take a while on my older computer it takes about 45 minutes i
have a dual processor and we're processing uh 10 000 photos that's not a
small amount of photographs to process so if you're on your laptop in which i am it's going to take a while so let's
go ahead and go get our cup of coffee and a sip and come back and see what this looks like so i'm back you didn't
know i was gone that was actually a lengthy pause there i made a couple changes and let's discuss those changes
real quick and why i made them so the first thing i'm going to do is i'm going to go up here and insert a cell above
and let's paste the original code back in there and you'll see that the original thing was steps per epic 8 000
25 epics and validation steps 2 000 and i changed these to 4 000 epics or 4 000
steps per epic 10 epics and just 10 validation steps
and this will cause problems if you're doing this as a commercial release but for demo purposes this should work and
if you remember our steps per epic that's how many photos we're going to process in fact let me go ahead and get
my drawing pin out and let's just highlight that right here well we have 8 000 pictures we're going through so for
each epic i'm going to change this to 4 000 i'm going to cut that in half so it's going to randomly pick 4 000
pictures each time it goes through an epic and the epic is how many processes so this is 25 and i'm just going to cut
that to 10. so instead of doing 25 runs through 8 000 photos each which you can
do the math of 25 times 8 000. i'm only going to do 10 through 4 000 so i'm going to run this 40 000 times through
the processes and the next thing i know that you'll you'll want to notice is that i also changed the validation step
and this would cause some major problems in releasing because i dropped it all the way down to 10. with the validation
step does is it says we have 2 000 photos in our trainings or in our testing set and we're going to use that
for validation well i'm only going to use a random 10 of those to validate so not really the best settings but let me
show you why we did that let's scroll down here just a little bit and let's look at the output here and see what
that what's going on there so i've got my drawing tool back on and you'll see here it lists a run so each time it goes
through an epic it's going to do 4 000 steps and this is where the 4 000 comes in so that's where we have we have epic
one of 10 4 000 steps it's randomly picking half the pictures in the file and going through them and then we're
going to look at this number right here that is for the whole epic and that's 241
seconds and if you remember correctly you divide that by 60 you get minutes if you divide that by 60 you get hours or
you can just divide the whole thing by 60 times 60 which is 3600. if 3600 is an
hour this is roughly 45 minutes right here and that's 45 minutes to process half the pictures so if i was doing all
the pictures we're talking an hour and a half per epic times 36 or no 25 they had
25 up above 25 so that's roughly a couple days a couple days of processing well for this demo we
don't want to do that i don't want to come back the next day plus my computer did a reboot in the middle of the night
so we look at this and we say okay let's we're just testing this out my computer that i'm running this on is a dual core
processor runs point nine gigahertz per second for a laptop you know it's good about four years ago but for running
something like this is probably a little slow so we cut the times down and the last one was validation we're only
validating it on a random 10 photos and this comes into effect because you're going to see down here where we have
accuracy value loss value accuracy and loss those are very
important numbers to look at so the 10 means i'm only validating across 10 pictures that is where here we have
value this is acc is for accuracy value loss we're not going to worry about that too much and accuracy now accuracy is
while it's running it's putting these two numbers together that's what accuracy is and value accuracy is at the
end of the epic what's our accuracy into the epic what is it looking at in this tutorial we're not going to go so deep
but these numbers are really important when you start talking about these two numbers reflect bias that is really
important let me just put that up there and bias is a little bit beyond this tutorial but the short of it is is if
this accuracy which is being our validation per step is going down
and the value accuracy continues to go up that means there's a bias that means
i'm memorizing the photos i'm looking at i'm not actually looking for what makes
a dog a dog what makes a cat a cat i'm just memorizing them and so the more this discrepancy grows the bigger the
bias is and that is really the beauty of the cross neural network is a lot of built-in
features like this that make that really easy to track so let's go ahead and take a look at the next set of code so here
we are into part three we're going to make a new prediction and so we're going to bring in a couple tools for that and
then we have to process the image coming in and find out whether it's an actual dog or cat we can actually use this to
identify it and of course the final step of part three is to print prediction we'll go ahead and combine these and of
course you can see me there adding more sticky notes to my computer screen hidden behind the screen and
last one was don't forget to feed the cat and the dog so let's go and take a look at that and
see what that looks like in code and put that in our jupiter notebook all right and let's paste that in here and we'll
start by importing numpy as np numpy is a very common package i pretty much
import it on any python project i'm working on another one i use regularly is pandas they're just ways of
organizing the data and then np is usually the standard in most machine learning tools as a return for the data
array although you know you can use the standard data array from python and we have cross pre-processing import image
fish that all look familiar because we're going to take a test image and we're going to set that equal to in this
case cat or dog one as you can see over here and you know let me get my drawing tool back on so let's take a look at
this we have our test image we're loading and in here we have test image one and this one hasn't data hasn't seen
this one at all so this is all new oh let me shrink the screen down let me start that over so here we have my test
image and we went ahead and the cross processing has this nice image set up so we're going to load the image and we're
going to alter it to a 64 by 64 print so right off the bat we're going to cross
as nice that way it automatically sets it up for us so we don't have to redo all our images and find a way to reset
those and then we use also to set the image to an array so again we're all in pre-processing the data just like we
pre-processed before with our test information and our training data and then we use the numpy here's our numpy
that's uh from our right up here important numpy as np expand the dimensions test image axes equal zero so
it puts it into a single array and then finally all that work all that pre-processing and all we do is we run
the result we click on here we go result equals classifier predict test image and then we find out well what is the test
image and let's just take a quick look and just see what that is and you can see when i ran it it comes up dog and if
we look at those images there it is cat or dog image number one that looks like a nice floppy eared lab
friendly with his tongue hanging out it's either that or a very floppy eared cat i'm not sure which but according to
our software says it's a dog and uh we have a second picture over here let's just see what happens when we run the second picture we can go up here and
change this uh from dog image one to two we'll run that and it comes down here and says cat you can see me highlighting
it down there as cat so our process works we are able to label a dog a dog and a cat a cat just from the pictures
there we go cleared my drawing tool and the last thing i want you to notice when we come back up here to when i ran it
you'll see it has an accuracy of 1 and the value accuracy of 1. well the value
accuracy is the important one because the value accuracy is what it actually runs on the test data remember i'm only
testing it on i'm only validating it on a random 10 photos and those 10 folders just happened to come up one now when
they ran this on the server it actually came up about 86 percent this is why cutting these numbers down so far for a
commercial release is bad so you want to make sure you're a little careful of that when you're testing your stuff that you change these numbers back when you
run it on a more enterprise computer other than your old laptop that you're just practicing on or messing with and
we come down here and again you know we have the validation of cat and so we have successfully built a neural network
that could distinguish between photos of a cat and a dog imagine all the other things you could distinguish imagine all
the different industries you could dive into with that just being able to understand those two difference of pictures what about mosquitoes could you
find the mosquitoes that bite versus mosquitoes that are friendly it turns out the mosquitoes that bite us are only
four percent of the mosquito population if even that maybe two percent there's all kinds of industries that use this
and there's so many industries that are just now realizing how powerful these tools are just in the photos alone there
is a myriad of industries sprouting up and i said it before i'll say it again what an exciting time to live in with
these tools and that we get to play with what is deep learning again this video is not about deep learning but there are
other videos we have created in detail about what is deep learning in this video we'll just touch upon the basics
so that that's like a nice segue into tensorflow so deep learning is
in a way a subset of machine learning and we use primarily neural networks in deep
learning and the underlying technology behind artificial intelligence is deep
learning and here we teach them how to recognize let's say images or voice and
so on and so forth so it is a learning mechanism but here unlike traditional
machine learning the data is far more complicated and far more unstructured
like it could be primarily in the form of images or audio files or text files
and one of the core components of deep learning is neural network and a neural network somewhat looks like this there
is something known as an input layer and then there is an output layer and in between there are a bunch of hidden
layers so typically there would be at least one hidden layer and anything more than one hidden layer is known as a deep
neural network so any neural network with more than three layers altogether
right is known as a deep neural network all right so what are the functions of
the various layers let's take a quick look so the input layer accepts the input so this could be in the form of
let's say if it is an image it could be the pixel values of the images so that's what the input layer does and then it
passes on to the hidden layers and the hidden layers in turn perform certain computations and they have what is known
as as a part of the training they have these weights and biases that they keep
updating till the training process is complete and each neuron has multiple
weights and there will be one bias and these are like variables and we will see
when we go into the tensorflow code what we actually mean by that and so that's what the hidden layer does it does a
bunch of computation and passes its values to the output layer and then the output layer in turn gives the output it
could be in the form of a class so for example if we are doing classification
it tells us which class a particular image maybe belongs to for example let's
say if this is a image classification application then the input could be a bunch of images of maybe cats and dogs
and the output will be like it will say okay if this is activated this gives a zero and this gives a one that means it
is a cat if this gives a one and this gives us zero that means it is a dot so that is a kind of a binary classification and that can be extended
with multiple neurons on the output side to have many more classes for example or
it can also be used for regression as well not necessarily only classification again since this video is not about deep
learning or machine learning or neural network we will probably not go into a lot of details but you can check other
videos where we have given a lot more details about neural networks and deep learning and so on so in order to
develop a deep learning application how do you go about primarily there are two or three components that are required in
order to develop deep learning application you need obviously a programming language so typically python
is used and that's what we are going to use in this particular video but you can also use other languages like java or c
plus plus and so on and there are some libraries that are readily available and for primarily for doing um machine
learning and deep learning programming so these are a list of libraries these are by no means the exhaustive list but
some of the most common ones like keras theano tensorflow and so on and so forth
tensorflow has nowadays become very very popular this is developed by google and it is an open
source library and keras was there before now keras has actually now become a part of tensorflow as well so it is
one player about tensorflow so in that sense they're well integrated so combination of keras and uh tensorflow
is uh pretty good then of course you have a torch and tl4j and so on and so forth so there are multiple libraries
but this video is about tensorflow and we will be focusing on tensorflow what are the benefits of tensorflow and what
are its components and our towards the end we will show you a code in python we've written a code in python by the
way tensorflow can be used with multiple languages it supports multiple languages though python is by far the most popular
language so let's take a look at what exactly is tensorflow and why we are so excited about tensorflow so tensorflow
offers apis now we can earlier without when these libraries none of these libraries
were there even then we were doing people were doing machine learning and deep learning and so on but the coding
mechanism was much more complicated what these libraries like tensorflow offer is
they provide kind of a high level api so that we don't have to go really deep into writing all the stuff that is
required let's say to prepare a neural network and to even configure or even to
program a neuron and so on right so these are done by the library so all you
need to do is they offer a higher level api you need to use that api and call that api and maybe pass the data and
that would pretty much it's much easier rather than actually going down and writing everything by yourself so
tensorflow that way it offers apis for to write your code in python or even c
plus plus and so on other languages java as well it has an integration with r as well apparently okay and it supports
cpus as well as gpus now deep learning applications are very compute intensive
especially the training process needs a lot of computation it takes very long as you can imagine because the the data
size is large and there are so many iterative processes there are so much of mathematical calculations matrix
multiplication and so on and so forth so for that if you perform these activities on a normal cpu typically it would take
much longer but gpus are graphical processing units you must have heard
gpus in the context of games and so on because where you need the screen needs
to be of high resolution and the images need to be of high resolution and so on so gpus that as the name such as
graphical processing unit were originally designed for that but since they are very good at handling this kind
of iterative calculations and so on now they are kind of they are being used or leveraged rather for doing or developing
deep learning applications and tensorflow supports gpus as well as cpus
so i think that's one of the major advantages of tensorflow as well now again what is exactly tensorflow it's a
open source library developed by google and open source and primarily for deep
learning development but tensorflow also supports traditional machine learning by the way so if you want to do some
traditional machine learning we can do it however it is probably a bit of a overhead to use tensorflow for doing
traditional machine learning and this is really good for performing deep learning activities and again if you want to get
into more details about what's the difference between machine learning and deep learning there is another video about it you can probably take a look at
that video what else it is developed originally for large numerical computation so originally when
tensorflow was developed they never thought of it as a keeping deep learning in mind but ultimately it so happened
that it's really very good for deep learning development and therefore google has open sourced it and
tensorflow as the name suggests the data is in the form of what is known as tensors these are like multi-dimensional
arrays and they are very handy in handling large amounts of data and we will see that as well as we move forward
and then of course the execution mechanism is in the form of graphs so that makes it much easier to execute
this code in a distributed manner across a cluster of computers and and also
using gpus and so on and so forth right so that's a quick overview about what is
uh tensorflow we will see a little bit more detail the two major components that is basically the tensors and the
graphs let's take a look at what they are so what are tensors tensor is as i
mentioned earlier it is like a multi-dimensional array in which the data is stored now when we are doing
deep learning especially the training process you will have large amounts of data and the data is in typically in a
very complicated format and it really helps when you are able to put this use
this or store it in a compact way and so tensors actually offer a very nice and
compact way of storing the data handling the data during computation this is not really for storing on your hard disk or
things like that but in memory when you are doing the computation tenses are really really very handy in terms of
keeping the data compact because they are like multi-dimensional arrays so the data is stored in tensors and then it is
fed into the neural network and then you get the output all right so there are some terms associated with tensors let's
get ourselves familiarized one is uh the dimension and another is the rank so
what is dimension typically dimension is like the number of elements in a way so
for example this is a five by four dimension tensor and then you can have again the multi dimensions right so you
can this can be like three by three by three so that is uh the dimension and then you have ranks so what are tensor
at ranks ranks are basically traditionally we would have thought of as dimensions that is actually in this
case it is called rank so it becomes easier when we see examples so a tensor's rank is supposed to be zero
when there is only one element we also call this as scalar so just one element
and this is not really a vector it's just an element like 200 it's also known as a scalar so such a tensor is supposed
to be having a rank of zero then you have let's say one dimensional array
this is a vector with a row of elements this has rank of one now if you have
traditionally what we called as a two dimensional like a matrix for example then the rank is two and in this case
the rank is 3 and it can have more ranks as well as i mentioned it is like a
multi-dimensional array so you can have rank 5 6 and so on okay so those are the terminologies in tensorflow terms
dimensions and ranks so this is just to make sure that we are picking the same language so whenever we talk about the
rank of a tensor you understand what exactly is meant by that now in addition to tensors in which the data is actually
stored all right so the data is stored in the tensors and then once you have the data there is a computation that
needs to be done now the computation happens in the form of graphs so what we
typically in a tensorflow program what we do is it's not like traditional programming where you just write a bunch
of lines and then everything gets executed in sequence here we prepare graphs various nodes and then these are
executed in the form of a session and they use the data from these tensors now i know this is a slightly new concept
for a lot of you so it may be a little difficult to probably understand in the first cut but when we look at the code
when we go into the tutorial the code walkthrough i think that time it will become much clearer as well but to start
with just uh we need to keep in mind that we have to first prepare a graph
and when you're preparing the graph no none of the code is actually getting executed you write the code to prepare
the graph and then you execute that graph so that's the way by creating a session that's the way tensorflow
program works so and each of these computation is represented as what is
known as a data flow graph and we will also see that whenever you start tensorflow when you create an object
tensorflow object there will be what is known as the default graph and then if required i know probably in the
beginning it may not be required but in more advanced programming you can actually have multiple graphs instead of
the default graph you can create your own graph and have multiple graphs and use it as well but there is always
whenever you create a tensorflow object there will be a default graph and this will be very nicely illustrated in the
example code that we will take to explain this so there it becomes much clearer than in these slides so the
graph gets executed and it processes all the data that we are feeding all the external data will be fed in the form of
what is known as placeholders and then you have variables and constants again this will also become clear when we take
a look at the code and once you have the graph then the execution can be enabled
either on regular cpus or on gpus and also in a distributed mode so that the
processing becomes much faster as i mentioned the training of the models in deep learning takes extremely long
because of the large amount of data and therefore using tensorflow actually makes it much easier to write the code
for gpus or cpus and then execute it in a distributed manner so this is how the
tensorflow program looks so there is uh you need to build a computational graph
that's the first step and then you execute that graph so the first step is to write the code for preparing the
graph and then you create what is known as a session and then in that session you ask the session to execute this
graph so this will again become much clearer when we look at the code as some of you may be aware it's not that easy
to set up the tensorflow environment there are several components there are several possibilities for example
you can set up on windows you can set up on ubuntu now ubuntu has multiple
versions which version to use and then you have python which release of python to use whether to do a pip install
whether to do install using anaconda and how do you then link it up with jupiter
notebook these are multiple possibilities and it takes up a lot of
time to try all of these so today what i'm going to do is show you a tried and
tested method of setting up the tensorflow environment and this will
help primarily those who are starting with tensorflow so that they don't have
to waste so much time on setting up the environment in experimenting with the installation and setting up of the
environment now what we are going to do is i will show you a method by which you
you know it is a tried and tested method and of course tensorflow home page has a
install page and it shows you some ways to install but again the challenge is the same there are multiple versions
multiple methods shown there so it's highly confusing for somebody who is new as to decide which one which path to
take so in today's session what we are going to do is we will set up tensorflow
on ubuntu and i'm going to show you in a virtual box but then if you are using a
laptop with ubuntu installed you can straight away use the same method
however we need to keep one thing in mind that the various releases and
versions of ubuntu and python and tensorflow not all of them
are compatible with each other so these versions and releases need to be very specific so i will tell you which is the
version and releases what combination is best suited for you to get started and
later on of course you can then experiment with other possibilities and other releases and so on once you get
familiar with tensorflow to start with i would also like to mention that it is a good idea to install or start with
ubuntu environment or linux any other linux also but here we will focus on ubuntu rather than windows so for those
who are already let's say using a windows system the question may arise
what do we do but there is a easy option as you can see i'm actually using a
virtual box so you need to install virtualbox let me just show you so this
is the virtual box oracle vm virtual box and there are tons of videos on youtube
how to install a virtual box and how to create ubuntu image i think we will not
spend time on that but if you're using windows and my preference would be to
set up a virtual box and set up your environment in ubuntu image so we will
start by assuming that you have an ubuntu environment especially release
14.04 lts there are multiple ubuntu versions and again we will not try to
get the latest and the greatest versions or latest and greatest releases but the focus here is to take the releases and
versions which are working and where you will not waste time so the setup process
will be smooth if you stick to these releases you can of course experiment later on with other versions and try out
but here we will be working with ubuntu 1404 lts and we will use python 3.4 and
we will use tensorflow 1.5 this is a tried and tested combination and i would
also recommend that you use the same if you want a smooth start and so let's get
started with that let me login to my ubuntu system okay so we have the ubuntu
system running here now if you go to tensorflow.org there is a page which
mentions how to install tensorflow and as you can see there are multiple
possibilities you have ubuntu you have windows and mac os and so on and so forth and if you
go to ubuntu for example further you will get multiple options whether you want cpu or
gpu and whether you want to do a pip install or using a virtual native pip virtual env
anaconda and so on and so forth so all these options are very complicated or rather very
confusing i would say not complicated depending on whether you're expert or of course i am talking about beginners here
but if you click on some of these options they may look very easy so for example if you go back and
if you select for example native pip it may appear like oh this is just you know one
single or two steps and that's about it everything gets installed you see here it should be just one step install it
you say three install tensorflow and everything gets installed unfortunately
it doesn't work that way so it's not as easy as so don't get kind of fooled by
the simplicity of the of the documentation here again it's not their fault because of the multiple
combinations of releases and so on and so forth it is not that easy so what we will do is we will take a slightly
roundabout method which is using anaconda which has a few more steps but
you're sure that this is going to work so that is what we are going to do and that's what i am going to show you so
what are the steps involved of course i will not go exactly by what they have mentioned here as i said i will show you
the steps which are again sure shot to work whereas here again if you follow just this
document there will be some variations which they have kind of not documented so that's the reason i will show you the
steps separately all right so these are the main four steps you need to download and install anaconda and then create a
virtual environment with python and the release as i mentioned is python 3.4 and
we will install tensorflow version 1.5 and then we will install and configure
jupiter which will be our development environment all right so let's get started this is our ubuntu and let's get
a terminal started here and okay yes so
all right so we started the terminal and then what we'll also do is we'll go to the anaconda website because we need to
install anaconda we need to download and install anaconda so for that you need to
go to the anaconda website so you can just open a browser and google and you will find anaconda
website so this is the site anaconda anaconda.org
click on this link and it will take you to the anaconda website
you don't have to sign up or anything like that just look for the download anaconda
so just click on that it will take you to the download pages and it automatically recognizes that you are on
ubuntu so it will show you the links to download all right so as you can see it
is recognized that you are on a linux operating system so
you can just click on this download and it will start the download there are two
versions of course python with python 3.6 and 2.7 i recommend you start with
python 3.6 now just want to clarify that we will be actually using python 3.4
this is just for the initial installation but subsequently when we set up the virtual environment
you will see there is one more step where we set up the virtual environment there we will actually be using python
3.4 so just that you are not confused all right so it has started the download we just say
no thanks for this and it is downloading while it is downloading you can
click on this link saying how to install anaconda so
there are a couple of steps mentioned there that we will be using so let me just in the meanwhile click on this for
some reason the network is a little slow so it's taking time all right so
once the download is done we will be using the options that are mentioned here now the first step of course is we
we are already doing the first step which is downloading the installer for linux the second step is not mandatory
so it's an optional step most often you can actually skip that and third step is
what we are going to do and since we are using at this point we are using python 3.6 we should use this command so
basically the terminal we have opened here is is to use or to run this command
that's the reason i opened this terminal however make sure that the download is complete before you run this command so
we'll just wait for a couple of minutes and we'll come back once the download is done all right so as you can see the
download is done this is a fairly large file so if you are on a slow internet or
low internet bandwidth then it might take quite a while it's about uh 578 mb and so this is the file now what
you need to do is you need to go back to this installation steps and let me minimize this and
yeah so this is the command that you need to run so you can just directly copy and paste
this command from here okay so i do copy right mouse click
now it will ask you a bunch of questions as documented here so
most of them you need to just say enter or yes and that's about it except for the last step i will just show you what
i mean and here you need to do press enter multiple times just to make sure
you agree to all these agreement the license agreement and so on and so
forth so once you do multiple enters it will
bring you to the next step
all right so now here again they say do you accept the terms you just say yes
and then press enter and here it will ask you a couple of questions and pretty
much you need to just go for the default version press enter to confirm you just say press enter and
this is primarily when it will pretty much start the installation process of anaconda this might take a
little while depending again on your internet speed so you need to have some patience we will also probably come back
once this installation is done or if it asks for any further questions
all right so here you'll get again one more question do you used to wish the installer to prepend so you
just say yes for this question and keep going now the last question
i think we are pretty much at the end and that is about microsoft
vss i guess yeah vs code so for this you can just say no because we will not be
using this and that's it you're done so this is a completion of installation of anaconda
so it's always a good idea to exit and start a fresh terminal okay so
let's start again terminal so we are done with the
installation of anaconda the next step is to create a virtual environment with python 3.4 so for that this is the
command conda create dash n and this is the name of your virtual environment you
can give any name but for easy reference i have given us tensorflow and we have
to specify the python version as i mentioned earlier we will be using the combination of python 3.4
and tensorflow 1.5 now this is not the latest version python has probably 3.6
at this point at the time of creation of this video and even tensorflow probably has the latest version as 1.7 but then
these combinations sometimes may not work and i found that after several
trials and errors 3.4 with 1.5 seems to be most reliable and that's the reason i
have chosen this i would recommend you to try with this first if you are a beginner and later on maybe you can try
out the other permutations and combinations all right one step i just wanted to show you is before we do the
installation or creation of the virtual environment if you want to just clarify or confirm
whether anaconda has been installed or not you can run what is known as anaconda navigator so just say anaconda
[Music] navigator okay we need to edit this part
hold on one second i guess it is
all right so you see here when you start anaconda navigator this
anaconda navigator will open up that is the indication that anaconda has been installed
properly so that's just a quick check and you don't have to do anything just you can go back and close it
once this comes up and then in the meanwhile let me just open one more terminal
for the creation of the virtual environment and
yeah so this is your anaconda navigator and you can just say okay don't show me
just say okay and then you can close this
you can say file you can do a file exit or you can click on close button
whichever all right so we will exit from here
we have a terminal open here we will use this for our next steps all right so what we have
to do now is type in this piece of
code or this command for some reason copy paste is not
working so it's a small command so it shouldn't be that big of an issue so we say conda
[Music] and we say create
so this is the command for creating a new environment slash dash n
and then we give the name of the environment so we will we can give any name for uh
convenience i'm just calling it tensorflow but you can actually name it anything
and then we need to specify with the which version of python so we say pip
[Music] python and then that is equal 3.4 okay so it will create an
environment this will take a little while it will ask you a question just say yes and then it will be done
so this is just a warning you can simply ignore that and for the question that it asks you just
say yes and i think you should be good all right so it says do you want to
proceed you say yes then enter
this will again take a little little time so we will probably move forward and then come back once this is done
all right so the environment has been created now it's always a good idea to whenever some
of these steps get over or each step gets over you just exit the terminal and
start afresh with the new terminal for some reason sometimes it causes problem if you continue so i found that it is
a safe practice to each time exit the terminal and start afresh now that we
have an environment created we can by the name tensorflow
you can enter that environment by calling the command source
activate tensorflow to name whatever name you
have given so you'll see here once you do that you will get this the name of the environment will be shown here so it
is like your own again a special environment within within the system and
uh from here onwards you can do other stuff like actually installing
tensorflow and so on and you will see that it has installed python 3 here so
3.4 right so it is installed that's what we during creation of this environment
we wanted python 3.4 so that is what it is showing but we are not yet done so we
need to still install tensorflow so in this environment you need to
run a command which will actually install the correct combination of
tensorflow now this is as you can see it's a slightly complicated command with
install dash dash ignore install so what we can do is uh from within here we
can go and go to tensorflow install page and there
is a sample code there you can pick up from there so that you don't have to type in
so much so we say tensorflow tensor flow
dark and i'll also open up probably a notepad here so that we can construct
that command and go here
you say ubuntu if you come further down there will be
by the way we are doing cpu installation i think that is clear we are not there is a
possibility to do gpu as well we will probably create a separate video for that and
if we go here yeah so again you probably if you follow the
entire document out here it may still not work so i do not recommend that for now at least for beginners but just to
copy this command because this is a fairly complicated command so this is much easier to copy it from here that's
the reason i'm on this page and we will
then modify it according to yes okay let's say
copy and don't do it directly here because that is probably not yet the right
command let us go and create a notepad
page empty document i'll just say
tf.txt okay this is just a temporary file what
we need to do here is we need to adjust this command to suit the
versions that we are installing here so let me just explain what it is what you need to do
so first thing you can get rid of this one yes okay now here these parts you can
simply ignore here you need to go and change it to version 1.5
and i think we are good with that okay so the cp34 indicates your python 3.4
so later on when you're trying to experiment and if you want to do other combinations of tensorflow and
python version then you can change this for example you can change this to 3 6 both these places you need to change
this to 3 6 if you are using with python 3.6 and similarly if you want the latest
version which is of tensorflow which is 1.7 you need to change this to 1.7 and so on but at this point to start with if
you're a beginner i would recommend that you stick to this tensorflow 1.5 with
python 3.4 this is tried and tested and it works okay just a word of caution
that just by changing these versions the whole thing will not work there may be other places
where you may have to some of the steps may change so just a word of caution that just by
changing this if you want to try out with a different version some of the steps may also change so don't think
that just by changing this just one particular command you will be fine okay so
that's been my experience so to start with i would recommend use this particular combination all right so it
looks good uh tensorflow 1.5 and let me
just reconfirm with my notes and i think it looks good so this is install ignore
1.5 cp34 cp34 and so on okay
okay so let's hit enter so it started the process again this might take a little while so we will
pause the recalling and then get back when the installation is done okay so
looks like this installation is done and as always let us
exit and then come back now you are currently in the environment virtual environment um and it's a good
practice before you exit out of the terminal to come out doctor out of the virtual environment so just like you did
source activate you need to do source deactivate
and the name tensor flow okay you will come back to the
original command prompt and then from here you can do an exit now we need to validate
whether the tensorflow installation has gone through properly or not so
the way to validate is to first start python and then import
tensorflow library see if it imports without error so that's the best way to validate so we'll open a terminal
and whenever you want to get into the environment this is how you have to do come to environment and then say source
and obviously you can also use existing so source activate
tensorflow okay so we do this and you need to run python from here remember you should say
python3 because you have installed python 3.4 now here when you say import
tensorflow as tf when you get back a prompt like this
without any errors that means tensorflow installation has been successful if
tensorflow was not installed properly you will get an error saying tensorflow is not available or module doesn't exist
or something like that so tensorflow installation is done but now we will go through the process of installing
jupyter notebook so that you can do your development so let me just exit and in order to install your jupiter
notebook which is your which is going to be your development environment we will first
install ipython using conda so conda install ipython and then pip install
jupiter so let's first do the ipython conda
install hide python again it will ask you a question saying
these are the components do you want to install you just say yes
all right you say yes [Music] once again it will take a little while so we might pause the recording and come
back once it is done great so that did not take much
time anyways so we will as usual we will exit which is
source deactivate tensorflow exit and then we will start a fresh
terminal in order to install jupiter
now in some places uh they may say install jupiter using pip3 but in my
experience it did not work with p3 and that's the reason we will use pip however just keep in mind you need to
first go to the environment so source activate tensorflow
and you need to use this command which is pip install jupiter usually when
you're on python 3 you use pip3 but as i said it did not work so
in my experience it works with pip install and not pick 3 so pip install
[Music] i hope you already noticed that it is jupy per
and not jpi so again it's probably taking a little
while we will pause the recording and come back once it is done okay so jupiter installation is also
done once again you need to it's a good idea not you need to but it's a good idea to
come out of this terminal session and then start afresh so we exit and we have a fresh terminal
already available here and we will once again
go to tensorflow and we say jupiter
notebook that's how you start your jupyter notebook
so this looks good now only thing is since we are running uh for the first time what you need to do is if you come
back to your command prompt it will show you that if you are running
it for the first time it will say that you need to copy paste this link so
you you can just for the first time only once you need to do this copy this
and paste it in your browser and remember this is only the first time
when you're running jupiter you need to do this subsequently you don't have to it will
automatically open up all right so now that we have jupiter installed and
it starts up we need to test whether tensorflow is working fine or not and in order to
do that you can create a new notebook and you say
import tensorflow as tf and shift enter if everything is
fine tensorflow is installed properly you don't get any errors and then it works fine this is
an indication that tensorflow got installed successfully and that's about it so
that's all about installing tensorflow with python 3.4 so remember this is
installation of tensorflow 1.5 with python 3.4 on ubuntu and uh once again in case you
have a windows system in fact i have also done it on a windows system you can use virtual blocks to create a virtual
environment ubuntu environment and then follow these steps and there are tons of
videos to create virtual box and that's the reason we have not included those steps in this
and if there are any comments there are better ways to do this please mention it
in the comment section below or if you need any further help just mention it and with your email we will respond to
you now what are the various elements of a tensorflow program as i mentioned tensorflow program is slightly different
from regular programming that we do so even if you're familiar with python this may still be new for you the way you
write a tensorflow program is different from the regular python programming that you would have done or even machine
learning program you would have written some machine learning program using scikit-learn or regular python libraries
this is different from even that so let us see what are the various elements so first of all the way we handle data
inside of a program itself is a little different from how we normally do in a normal programming language a variable
is a variable in your program right so you have anything that can keep changing you just create as a variable or even
constants in fact are actually created as variables but in tensorflow the storage in the program consists of three
types one is constants another is variable and the third is a placeholder so and they are there is a lot of
difference between these types and we will see how they vary and how they are used and so on so constants are like
variables which cannot be changed so for example if you define a constant like this this is how you define a constant
by the way the simplest format is like this like for example b is equal to tf.constant and then you give a value
here a slightly more advanced version is you also specify the type so you say tf.constant 2.0 tf load 32. so the type
is of type float now in case of constants you cannot during the computation you cannot really change
these values so for example if you want to change the value of b from 3 to 5 or any other number it is not possible so
that is the meaning of constant all right so then we have variables so
variables we are all familiar with what are variables whenever we use programming we use variables so this is
pretty much the same this is the way you define variables df dot variable now one thing you need to note is this is the
only type in which we have v capital okay constant has c the small c and placeholder as small p but variable as
capital v and tf.variable and then you give the a value and then you can
specify what is the type and then you can use the variable change the variable
at any point in time with a different value and so on you can update the variable and so on so we will see all of
this in the code i will illustrate how a variable is defined and how it can be changed whereas the constant cannot be
changed and so on and so forth and then we have placeholders placeholders are really a special type and this may
be something completely new for many of us who have been doing programming but in tensorflow this is a completely new
concept so this is very important to understand this placeholders are like variables but only thing is that they
are used for feeding the data from outside so typically when you are performing some computations you need to
load data from a file or from an image file or from a csv file or whatever so
there is a provision with the special kind of variables which can be fed in a regular basis because the reason one of
the reasons for having this kind of provision is that if you get the entire input in one shot typically it may be
very difficult to handle memory and so on so i think that was the reason they came up with this mechanism where you
can feed in patches and there is a certain way of populating the placeholder we call this feed dig feed
underscore dick and this is a parameter name by the way and you feed the placeholders right that is the meaning
here so there is a certain way of feeding the placeholders and we will again see this in the example code as we
move forward okay so there are three types one is the constant which cannot be changed once you assign a value then
you have variables which are like normal variables we are all familiar with and then you have placeholders this is
primarily for feeding data from usually from outside but of course you can also for temporary testing purpose you can
feed within the program as well but primarily the purpose of a placeholder is to get data from outside so all right
so those were the constants variables and placeholders that is how you
handle data within the tensorflow program and then you create a graph and
once you create a graph then you have what is known as a session and then you
create a session object and you create a session and then you run a particular computation or a node or an operation
and so typically what you need to do is every variable or a computation that you
perform is like an operation or a node within a graph so initially the graph
will be what is known as the default graph the moment you create a tensorflow object or tf this tf here you see this
is the tensorflow object and again in the code when we go into the code it will become much easier to understand so
when you create a tensorflow object there is like a default graph which doesn't have any operations no nodes or
anything like that so it's like a clean slate the moment you assign variables or
constants or placeholders each of them is in tensorflow terms it is known as an
operation again you need to get familiar with these terms and they are not very intuitive this is not really an
operation you are just creating a constant or a variable but and this c is
equal to a into b would traditionally or would intuitively be an operation here
you're actually performing an operation but in tensorflow terms each of these everything is an operation so if you're
creating a constant that is an operation another constant or variable that's an operation so you can actually run each
of these and they are also known as referred to as nodes and when you are in your session you will actually run each
of these you can potentially run each of these nodes okay so a typical example
would look like this so you have two constants created a is equal to tf.constant its value is 5 b is equal to
this and then you say c is equal to a into b and then you create a session now remember all this you are just creating
a graph at this point no execution has happened all right so only at this point
once you create a session and then you say session or says dot run c is when
actually this whole thing will get executed all right so that is a different way of programming compared to
our traditional way of writing programs so you need to get used to this new format and when we look at the code as
we move forward and when we look in the jupyter notebook it will become much easier probably to understand this
rather than in the slide so these are the slides showing the code but what we can do is go straight into the
lab and take a look at the various examples that are there and starting from the very basic one how you create
variables and so these are some of the slides that are showing so this is about how to create variables how to create
constants and the variables or constants can also be strings so this is like our
first hello world program and we will talk about placeholders how to define a placeholder and how to execute and
populate the placeholder values into placeholder we will see these examples in the lab actually i will run the code
and we will also perform a small computation of adding and multiplying these
variables and we will in the end we will take up a use case implementation using tensorflow so
let's first go and see those examples and then come back and i'll explain this use case and then we will execute the
use case in the lab so let's go and check our lab okay so i'm in the jupiter
notebook environment and this is one of the development environments we can use
this is regular python anytime you do python programming we use jupiter notebook or there are rather of
course there are other ways of using other tools like pycharm and so on but for this particular tutorial i'm
comfortable using jupyter notebook so i will show you in jupyter notebook so this is the very basic example to
demonstrate how to create variables and constants and placeholders and what is
the difference between them how they behave and so on and as i mentioned the assumption here is that you know at
least some basic python programming or some programming language so at least you understand the code here and first
two pieces of code you will not need machine learning background but when we
do the use case of the case study there it is expected that you know at least some basic machine learning concepts so
in case you need to brush up the machine learning part you may have to do that before you go to the third one but here
just at least some idea of programming will be sufficient so what are we doing here in this particular line or in this
particular cell we are importing tensorflow it is uh as i mentioned it is a library so we are importing tensorflow
and we are calling it tf so this is this just a name you can give it any name but it is very common and everywhere
wherever tensorflow programming is done it is always named as tf but you can name anything actually okay so this will
import the tensorflow into my session now this is the way to create a variable
so let's start by creating a variable and this is the name of my variable i am
starting by giving a name called zero and i say zero is equal to tf dot
variable and then i'm giving the value of the variable here so this is the very basic way and the simplest way to create
a variable we will see a little later there are other formats as well but the bare minimum way of creating a variable
is this so i'm creating a variable by the name zero and as i mentioned you need to pay attention to the capital v
here in case of variable it is a capital v uppercase constant and placeholders
are lowercase now i'm creating a constant then the constant i'm naming it as one
and the way to create a constant is tf dot constant and then you give the
value of the constant so the value of the variable here is 0 and the value of constant here is 1. and again constant
you can also have additional parameters like a name and so on probably you must have seen in other tutorials or in other
places where the code is written but the basic format construct to create a constant is this this is sufficient to
create a constant so i created one variable and one constant now what else can we do again here there is no real
execution happening we are just building a graph we have not yet executed any tensorflow code we are building a graph
and i will show you how the graph looks as well first let us understand the constants and variables and so on and
then i will show you how the graph is generated and so on so what i want to do here is i want to add
0 and 1 and put it into a new variable called new value so what i do here
tensorflow offers these methods like add assign multiply or
you have matrix multiplication matmul and so on right so i will use one of those methods which is tf.add and then
pass these two as parameters so we can add a variable and a constant there are no restrictions on that so this is a
variable and this is a constant so if i do this it will be okay i did not
execute this code so it is giving an error now we are good okay so what it has done is new value will be equal to
tf dot add 0 1 so then we have one variable and one constant now let's say
i want to change the value of the variable because that is possible right so we want to change the value of the
variable to something some new value so we have in this case new value has one because we added a zero and a one so new
value has one now i want to assign this to this originally what we called as
zero so i will call that as update is equal to tf dot assign so assign is
basically changing the value so that's what we are going to do here now it has not complained we'll just say fine right
there are no issues now let us try this something similar with this 1 as well so 0 is a variable
so we were able to change the value and we will in a little while we will see what exactly those values are but before
doing that now let's say i will uncomment this part and i want to do something similar for my constant right
1 is a constant now i want to let's say change the value of the constant and make that also something different okay
so if i execute this piece of code it will give an error now again the error message may not be very intuitive this
doesn't say that you cannot do this for a constant it will just say tensor object has no attribute assign now that
sometimes may be confusing especially when you're starting but the meaning here is that one is a constant and
you're trying to modify the constant so it will not allow that's the reason it is complaining so let's put that back in
the comment and okay so that is done so only variables you can modify now what you have to do
let's skip this piece of code i'll come back to this in a bit but let's say we start by creating you
remember i told you we need to create a session so the way to create a session there are a couple of ways of creating a
session but this is for beginners this is the easiest way all you need to do is assign a variable called says or you can
give any name and that is equal to tf dot session that is the session method here and you create a session object by
the name says now what i'll do is i'll skip this as well i there is a purpose behind that now let's go ahead and run
this piece of particular operation remember i mentioned that everything you need to run so as of now the code has
not really got executed tensorflow code has not got executed you only created the graph so only when you
run through the session is when that actually the program gets executed so when you do this now you see observe
that it is giving an error okay the reason behind that is remember we skip these two lines of code for variables
okay this and then this now this is something very very important to observe
if you have variables in your code what i mean by that is let's say you are not
using variables but you have only constants and placeholders then this
will not complain and you will not get this error but in our case we also have variables so whenever you have variables
in that case you need to do an initialization and this is just a standard code there is nothing that we
need to add or modify or anything like that this is a standard piece of code you create this name of course can be
anything you can give us any name but this global variables initializer is
what you need to call tf.global variables initializer this will kind of initialize all the variables that you
may be using and then again remember this doesn't execute anything right so all you're doing here is you're creating
an operation but in order to run that operation you need to also run this known as
run init underscore op so this operation you need to run after creating the session all right so we executed this
now let us execute this and now when i run this piece of code it will run
successfully okay i hope you observed that so it is whenever you use variables
these two lines of code one is the operation you need to create an operation for saying global variables
initializer and then you need to run before running anything else you need to do a session.run this operation okay
again only if you're using variables of course you invariably in
all your programs you will use variables you cannot just write a program with constants and placeholders so you can
pretty much assume that this has to be there in pretty much all the programs now again why
this has not been taken care of in the library that's a different question but you need to keep in mind and always
remember if you don't do this for whatever reason if you're forgotten you will get an error and the error won't be
intuitive so you need to remember this that this could be because of the variables okay good so we have seen how
to create a constant and we have seen how to create a variable and we have
seen that you cannot modify or update a constant and we have also seen if you have variables that you need to execute
or have these two lines of code to initialize the variables and then we have seen that after creating a graph
how to run the graph in a session and i will show you a little bit more in detail how exactly the graph gets
created and how it gets executed but this was the first very quick code on
creating variables and constants now we will keep going and
we will also show you or i will show you the placeholders but before that one more small piece of code so this is how
you write a for loop okay so we are saying five times you run this piece of code now like any python program this is
nothing different so you have a for loop and this is indented that's the reason and therefore you say session.run update
so this will run five times and it will get printed that's all so update will
run for five times each time it will come and do this particular operation that's all we are we are asking the
system to do so let me just run that and show you let me yeah so it will okay so
it has done five times one that's for it is basically adding if you recall it starts from 0 it adds 1 to
0 then 1 to 1 1 to 2 and so on so it's nothing but it is generating five numbers one two three four five that's
it okay so then you can also have your constants as
strings for example so you can also work in a similar way you can work with string so most of the computation
happens mainly on numerical values so we normally hand handle numbers but
there will be situations where you may have to handle strings as well or text as well so this is an example of text
operation so you can similar to numbers you just create strings or store
in constants instead of a number you say hello is equal to tf.constant and then you assign the string that you want it
to be assigned and then the next one is one more string which is for world and
then you say add these two so it is nothing but concatenation of these two words hello and
world so we will do that and remember this is only creation of the graph right so in
order to actually execute this graph you need to run the session now you need to
keep in mind we don't have to create the session once again because once you create a session till you close the
session it remains valid okay so we have created this session here now till you
say says dot close it will remain for you so that's why we did not create one more so we are just reusing that session
says dot run hello world so in this hello world what is the operation that we are doing here the operation is add
these two concatenate these two so if i run this it will print hello world okay
now that's string operation now we will take a look at placeholders so this is
slightly more complicated and something new even for people who have been writing programs so but i'll just
explain it with uh with a quick example first of all how do you declare or create a placeholder you just do
tf.placeholder and the p is lowercase only in case of variables the v is
uppercase but otherwise constant and placeholders it is lowercase and you just say because as i mentioned this is
a placeholder it doesn't have any value so you unlike a constant or your variable you can't usually you will not
specify any value you just say what type of placeholder you want and very often
most of the cases it is a floating point so we just say placeholder of type
float32 so this tf.float32 tells the system that it is a floating point okay and then so let me just run
this and then you can do some or define some computation like for example b is
equal to a into 2. now remember here right now a has no value but what we are
saying here is at any point later on when a gets some value then b should be
equal to twice the value of a that's all we are saying here okay and also as i
mentioned earlier we are not yet executing anything we are just creating the graph here okay so now you have that
now how do you feed the value or populate the placeholder there is a certain syntax in which you can populate
the placeholder and you do that using what is known as a dictionary you're all
probably familiar those with python especially must be familiar with the dictionaries so you need to create a
dictionary and then pass that in order to feed the placeholder so in
this particular example if i want to run something if i want to calculate or compute b obviously i need to feed the
value of a so the value of a i'm feeding using this dictionary and i'm saying that a is equal to 3. now there are
multiple ways of populating the value of a because it typically this won't be just a single a scalar value like in
this case right remember scalar so it's just one value one number typically it would be a vector or or a tensor very
often it is a tensor so we will see step by step so we can start with the very basic example where a can be a scalar so
in this case a is equal to 3 that's what we are saying feed underscore dict in this case feed underscore dict is the
name of a variable by the way so you can't give anything else here so if you just say feed is equal to this it will give you an error all right so keep that
in mind either you specify feed underscore dick is equal to and give the dictionary pass the dictionary or you
straight away like in this case this is another format so let's see first let me execute this it has given 6 okay i think
let me see if i can clear out this particular cell so it becomes easier to understand okay so it is giving you 6.
let me do that once again clear out this and then okay and let me change the
format so this is one way of doing it another way is you don't even have to specify the name of this parameter feed
underscore dict you just pass the dictionary and it will execute okay but typically this makes
the code readable so that's the reason most of the places whenever you find tensorflow code you will see that they
explicitly mention feed underscore date so that the code is readable otherwise it can get confusing so i will also
comment this and retain the other format okay so i hope it's clear so what we are doing here we are running the operation
b and what exactly is bb is twice a a into two so therefore what we are doing
and a therefore b gets a value of 6 and that's what is being fed into result and that's what is getting printed here okay
now as i mentioned a need not be just a scalar value it can be a vector with a
multi-dimension array and so on and so forth so let's start showing you examples of that again
let me just clear out the outputs let me clear this and let me also clear this
for now okay you can also do before running up this you could we could also do clear all output all output clear it
will clear everything but that will we'll have to start all over again so i'm just doing individually all right so
in this example we are taking a one dimension array where rank is equal to 1
and i am feeding 3 4 5. now let us see what happens if i run this right as you
can imagine it gets multiplied each number gets multiplied so the result is 6 8 10 okay now as i said it can get
more complicated now let's say this is a multi-dimensional vector so if you what
happens if you feed this okay so as you can see this is pretty complicated now but if you feed this again all you're
doing is there is okay there is a slight change here again you can also
create your dictionary outside and feed that here you don't have to directly do
it here like in this case you see there are variations of the exact syntax in
the way you write it right so there is a slight variation so in this case you are creating the dictionary straight away
here kind of inline but here you created the dictionary separately because it is more complicated and then you you're
passing that dictionary here so i said dictionary is equal to a and then i'm saying this is what should be the value
of a this is and i'm feeding that and then i'm using directly that dictionary here
okay so this is equivalent to putting this code here right so if we execute this what happens same so you get
because a was a let's say this is a 3 by
4 by 2 right so that is one yeah 3 by four by two right so you see here this
is a multi-dimensional array so you feed that and the values you see here there's
one two three it became two four six four five six became eight ten twelve seven eight nine became 14 16 18 and so
on and so forth okay i hope now you got an idea about placeholders and in real
life what happens is you typically won't create these dictionaries manually and
since this is a quick demo we did this you will actually read the data into
this dictionary and then you feed that when you're doing the computation okay so it could be a csv file or it could be
a image file so the input is basically read and fed in
usually it is also done in patches so you don't read the entire thing because they can be large amount of data so that
is the idea behind having these placeholders and the way we feed these placeholders there is a provision
there's a deliberately that has been made a provision for this kind of getting data into the
program in chunks okay now in this particular example we will close the
session here and then i will show you what is the other way of creating a session right so now if i close the
session here now if i try to do anything with the session it will give us an error so i will probably not do that
right now now there is another way or another format and this is a very common way of using the session but i didn't
want to start off with this because this could get a little confusing the a simpler way was to create session saying
s is equal to tf.session but typically you will do it in what is known as a
width block this is a very common way of creating session so once you have the graph you would execute the graph in a
session using a width block and this is the syntax so you say with df dot session as says all right and then you
put all your code here so what will happen is you don't have to explicitly
close the session the moment this width block gets completed the session gets closed okay
so let's just take a quick look at this example now if i uh let me clear this
current output be clear okay so this is the hello world example a little bit
earlier we did that so i'm doing with tf.session assessment result is
equal to says dot run hello plus world and then i say print result okay so within this with block i'm doing all my
computation and this gets executed now let's say i try to run remember i
created a session here now if i want to let's say do something like run sesh dot
run a it is giving an error because the session is no longer valid
right so you cannot either you have to run this if you want to run this you have to run this in this with block
or you shouldn't have closed the session here right you remember this we said session dot close so since we have
closed the session there is no active session therefore it is giving an error okay now there is a third way
of creating a session but for now i will skip it because that is very very specific to the notebooks and and so on
so we will just avoid it you start with for beginners i think the best way is to start with creating a separate session
and then writing your code and then closing your session now of course remember if you don't do this part says
dot close it will not give any error or anything like that only thing is that it's a good practice and your resources
will get released otherwise if you do multiple of these programs if they are running then your resources
will get blocked that's the only thing and to start with it is it is simpler to do it this way so initially when you're
doing you start with this but as you move forward as you become familiar with tensorflow programming i would recommend
all of you to get into this model most of the programming tensorflow programming is done like this with df
dot session as ss okay all right so that was one book or one page of a code that
we have done explaining about variables constants and placeholders now let me explain how the graph works so in this
example we are going to see how the graph is created how the graph gets
executed and so on okay now that we got a understanding of constants and variables and placeholders okay so we
will start as usual by importing our library and as i said whenever you
create a tf or tensorflow object there will be what is known as a default graph
and you can get a handle of that by saying get default graph so let's do
that and you can display the operations remember every node in the graph is
considered as an operation right so as of now as you can see we have not done anything we have not
created a constant or variable or nothing right so there was there is no operation so by the way let's do one
thing let's start with the all output clear so that i don't have to again do individually so
we i'll just run these two lines of code okay now so yeah so we have the default
graph and as of now there are no operations so there is a method called get underscore operations which will
kind of display or show what are the various operations that have been performed on the graph so
as i said to start with they have no operations being performed so when i try
to display it's empty there are no nothing is there no operations okay now let's see what happens when we slowly
step by step start writing or building the graph so my first step may be to
create a constant okay remember in the previous example i have shown you how to create a constant in the most simple or
the simplest way which is like constant and then the number right now a small
extension of that is you can actually give a name to your constant now again this need not be the same i can say a is
equal to name and xyz i can give anything here okay this is just for your
reference but here for simplicity because there is a purpose why i am using the same name as the name of the
variable and there are other reasons as well why you would give a name again to
a constant or a variable this is again useful when we are using what is known as tensorboard otherwise it doesn't have
much value this additional name doesn't have much value in this our case also we will be seeing the graph and that's why
i'm using this but otherwise this having additionally a name usually doesn't add much value all right so let's create a
constant i created a constant now let us see what's going on in the graph so i
will just say operations is equal to get operations and if you see here now
you remember here it was blank now you see the first operation is showing up and this a that's the reason i put here
so it is showing as a and it says that okay you have performed an operation and there is a constant you have created
a constant okay all right so let's move on let's say we want to create a second
constant and this is b so if i see what does what are the operations list of
operations that have been performed you see here there are two of them so you have operation a which is a constant
then you have an operation b another constant you recall we in the term operation here is not very intuitive
right so we are just creating constants or numbers or variables and it is saying it's an operation but that's that's a
terminology in tensorflow everything is an operation here okay all right so then what else let's say i want to create a
third operation and that is basically adding which is this is a real operation in normal sense
as well so let's say i'm creating c which is um adding a and b and the c
value obviously will be a plus b okay now if we want to see what is in c
and we say c you see here it is not showing 30 it just says it is a tensor
and it is of type in 32 right because we have not executed the graph we are
still only building the graph i hope this makes it easy to understand okay so we are just building the graph we have
till we do remember till we do a session and run a particular operation you're
not actually executing anything okay you're just building the graph all right so now let us see what are the
operations we have done a b and now c so you will see there are three of them a b and c okay now let's do one more
operation which is d and here i am doing multiplication remember we did a tf dot add now i will
do tf.multiply and i'm multiplying a and b and i'm calling this operation as d so
a has i think 10 and b has 20 so d will be actually 200 when we execute
it right now it is just a tensor and it is of shaping 32 okay and once again we
can see what are the operations abcd and let's do one last operation here which
is e which is once again multiplication of c and d now that c has 30 which is a
plus b and d has 200 which is a into b so now c into d will be 30 into 200
which is which is equal to 6 000 but as we have seen earlier that multiplication
will not happen right now so now you have built a fairly simple but operation graph which
consists of a b c d e 1 2 3 for five operations okay
so you built a small it's not a very complicated one but a simple graph now you need to execute these operations so
what you need to do you need to create a session okay so says dot tf is equal to
session and then you can run these session print says dot run e now am i missing
something do i have to initialize the variables that's a question for you thing can tell me do i have to
initialize the variables that remember i ran one piece of code i think you must have got the answer i don't have to
because i am just using all constants here i have not used any variable here
right so i don't have to write execute that piece of code to initialize the variables so i can just create a session
and run any of the operations that i want to run so in this case i am running session e operation e right this
operation e now i can actually run individually instead of doing directly e
i could have run only a or i could have run b and so on and so forth but in this case i have run e which in turn will do
all the required computations which is for example assigning the value to a and
b and c doing the multiplication and so on all those operations will be performed which are required to do the
computation of e okay now there is a small piece of code that you can use to
find out what are the various operations that are there in this particular graph in this case of course it was very easy
very straightforward i have created abcde but in normal programming in real
life this can sometimes help to see how what is there in your graph what kind of
operations are going on in your graph so you can write this piece of code and it may sometimes help debugging as well
okay all right so i finished my session what do we have to do we need to close the session right because i did not use
the width block so i need to close the session as a good practice so i close the session good so that's how the graph
works i hope it was helpful in understanding how variables and constants and
placeholders are created in the previous example we saw and how exactly you create a graph and then you actually
execute that graph that's the way tensorflow program works that's the structure of tensorflow program okay so
now that we understood the structure of tensorflow programming let's take an example and a classification example and
take some real data from from outside external data csv file and
try to solve a classification problem for that let's first go back to the slides and understand what is the
problem statement and then we will come back okay so what is the case here we
have some census data and which has all these features or variables like um the
age of a person what class work class what is his education what is his marital status
gender and so on and so forth so there are a bunch of a bunch of features that are available and
we have to basically classify or not classify we have to build a model for
classifying whether the income of this person is above 50k or below 50k okay so
there is actually a label data available we will then try to build a model and
then see what is the accuracy so that this is a typical machine learning problem actually but as i said
tensorflow can be used for doing machine learning as well so we will as a simple example we will take machine learning so
this is how the high level the code looks and in tensorflow programming we also take a help of regular python
libraries like for example it could be numpy or it could be scikit-learn or it could be pandas in
this case and because before you actually develop or before you train
your model create your model you need to prepare the data into a certain format
and sometimes if you're doing machine learning activity you all are familiar those familiar with machine learning
will know you need to split your data into training and test data sets so all
that can be done using the regular non-tensorflow libraries right so these are like regular python libraries and
scikit-learn and so on so we use that and then prepare the data and then use tensorflow for performing either the
machine learning or deep learning activity now so what is the advantage of using tensorflow the main advantage is
that tensorflow offers high level apis right we talked about that so in this
case we will be using i'll explain what is in the slide but before that i wanted to just show you what is the api that we
will be using so we will be using the estimator api we will be using the
estimator api to create a classifier what is known as a linear classifier now
in order to do that you need to prepare the data and you need to prepare the data into a certain structure or format
and the api itself needs to be fed in a certain way so that is exactly what we are doing before we get into the regular
tensorflow code okay so these slides will quickly show you what exactly is happening but i will take you into
jupiter notebook and run that code and show you so here we are importing the data the data is the name of the file is
census underscore data.csv it's a csv file and then there is uh
income bracket is one of the columns which basically is our target that is what is used that is the label rather
but this doesn't have numeric values so it has the income so we need to convert that into binary
values either 0 or 1. so that's what we are doing here and then we are splitting
using scikit learn we are doing splitting of the data into test and
training right so this is a standard scikit-learn people some of you who are familiar with machine traditional
machine learning and you have done machine learning in python using scikit-learn will immediately recognize
this this is a sk learner's psychic learn and there is a readily available method to split the data into training
and test that's what we are doing here and then you need to create what is
known as feature columns and input functions before you can call the api estimator
api that's what we are doing here and data and also there is there are
different ways of creating the feature columns for numeric values and for categorical
values or rather for continuous values and categorical values okay so that's what has been
two types we are doing here we will see again in the code as well and
then you create your model so this is basically model is equal to df.estimator
linear classifier and then you're feeding the feature columns and also subsequently you will call the training
by passing the input function and you specify how many iterations to be done for the training process and again this
is all typical machine learning process so there is nothing specific for tensorflow here and then you evaluate
your model because this is classification model so you can again take help of the scikit-learn to test
the accuracy and get the reports evaluation perform the evaluation and get the report like this one right so
this is the classification report and you will see what is the prediction recall f1 score and so on again this is
a standard machine learning process all right so that's pretty much as far as the code here is concerned once again
let me take you into our lab and
we will get started with this particular code now let me clear out the outputs so
that you can step by step you can see the output so this is our data and
we will actually be this is just a display here the code starts from here so before when we start with our
tensorflow part of it as i mentioned we now remember in the previous examples the data was kind of cooked up
internally we created some data and we were using it right the constants variables and so on here is a real life
example so you are actually getting data from outside so you know before we get into tensorflow this is actually regular
python code so we are using pandas for example to create our data frame so we
will import pandas and then we will basically read our file into a pandas
data frame and this is how the data looks right and if you do a head it will
give you about six readings or observations five or six observations and then you can take a
look at how your data is looking so for example you have age work class education and so on and so forth
and you see here this last column income underscore bracket it says whether the income is greater
than 50k or less than 50k so this cannot be understood or this cannot be fed into
our model or you know so that's the reason we need to convert this this will not be understood
right so this needs to be converted saying okay maybe less than 50 k is 0 and greater
than 50 k is 1 so that conversion needs to be done before we feed it into our
model so that's what we are going to do as we move forward so this is the code for doing that okay so we take this and
then we define a function and then we run that function so now we
have wherever it is less than 50k it will have 0 and greater than 50k it
will have 1. so we have now updated the data then the next part is to split our
data into training and test data set so how are we going to do that we again
will take the standard python library which is scikit-learn sk-learn library and we use an existing
function there which is a train test split and what we are doing here is basically splitting the data into i'm
sorry i need to run this and then this okay sorry about that so
once i run this what is what are we doing here again those are familiar with scikit learn will immediately recognize
i am splitting this into test and train i'm saying test size is 30 so 30 percent
of the data should go into test and training should have 70 okay so that's all we are doing 0.3 indicates 30
so you can again this can be individual preferences some people do it like 50 50
some people 20 80 and in our case we are doing 70 30. so training is 70 and test is 30.
okay all right so we have so far we've been doing regular
non-tensorflow stuff so preparing the data so that we can now use for
tensorflow and in tensorflow what we are doing is we will be using the api called
estimator now estimator in order to call estimator you need to prepare what is
known as feature columns so feature columns have to be in a certain format basically it is nothing but the columns
we have to put them in a certain format and then when we are calling the the training we need to pass what is known
as an input function again a certain way in which you need to pass that function so that is what we need to do before we
create the model and run the model for training so the next few lines will be doing that quick look
at before we even get in once again into the tensorflow code so this is just showing us what are the various columns
names of the columns of this data okay so that is what is shown here and this
is more for when i was writing the code i could copy paste from here that's the reason i this line of code is there
instead of typing in these values okay good so now is where the actual tensorflow action starts so i
import tensorflow and now i will use this feature column functionality to
create my feature columns and there is a certain construct how you create the feature columns again
probably the details of that is out of scope here but just that you need to know that we need to create feature
columns and the way you create feature columns for continuous values and for categorical values is slightly different
and that's the reason you have two different blocks okay so for categorical values you need to use what is known as
tf dot feature column dot categorical column width vocabulary list and again
within this again there are two ways in which you can create for categorical values one is a vocabulary list where
you know how many types are there so for example gender column can have only male and
female so in such a case if there are let's say a finite number of values for a given column then you use a column
with vocabulary list okay so and then you say what is the name of the column and what are the possible values so in
this case male and female now there will be situations where the the values that this column can have
okay first of all it is a categorical column that means it can have names or some non-numerical values but the number
of values is probably unknown or undefined so for example occupation now
occupation can have any value it can be self-employed it can be a software
professional bank a teacher doctor so so many possibilities are there and we don't
know in the given data how many occupations have been listed right so that is where you use what is
known as categorical column with hash bucket and you specify the hash bucket
size so what it what this says is there can be a maximum of thousand such values
so typically if you give a safe number safely large number so that you don't run out of these number of occupations
so i think thousand is a safe number i don't think there will be more than thousand occupations in a given data so
that's the idea behind it but that's a judgment call we need to take what should be the size of this bucket size
right hash bucket size so these are two different types for categorical values for creating the feature column so we
will execute this for all the categorical columns obviously you need to know the type of the columns and that
is where if you see right at the beginning this information was provided what is the data type and you see here
the column name and what whether it is continuous or categorical this information is provided in the census
data wherever we pick this census data from by the way this was picked from
online from a government website and we stored locally so on that website it is
mentioned the details of this data were mentioned and they have mentioned whether each of these columns is
categorical or continuous so that information we have to use and we have to create feature columns using this
categorical method for the categorical type of columns and then for numeric or
continuous values you just use what is known as feature column dot
numeric column okay so these are all continuous or numeric values like age education number capital gain capital
loss hours per word these are all numerical values or continuous values right so you just say tf.feature
underscore column.numeric underscore column and then give the name of the column so once you do that i think i
have executed the previous one yeah this one now so your feature columns are ready and uh you need to basically
create a vector of all these columns and we call that as feet underscore calls
which is which is like a short form for feature columns now as you have seen i've mentioned earlier also there are
two things that are needed one is the feature columns and another is the input function so feature columns to first
create the model and then to train the model you need to create an input function so now we have the feature
columns ready next is to create an input function and there is a certain way in which you need to
this construct is pretty much very common construct so we will use that so the input function takes these x and y
values for training and tests you remember these are the x strain is the x
values of the training data set and white rain is the labels of the training
data set so that's what we are using here extreme y train and then we specify
what is the batch size batch size you remember i mentioned this is what will say how many records need to be read
each time right so typically in when we are doing the training process we don't
get the entire data in one shot we do it in batches so what size you specify here
number of epochs in this case we just say none but number of epochs is again uh probably the definition is not
required here but at a very high level how many times the entire data has to be passed through the model right so if you
have thousand records and you pass all the thousand records three times for
training purpose then you call that as three epochs right so there is a difference between the batch size and
epochs so when let's say we have thousand records and you're saying batch size is 100 that means there will be 10
batches right so if you take 10 batches then that is one epoch gets completed
okay then you the training obviously doesn't get completed in one round okay
so you need to pass this data again maybe a second time or a third time till
you get the right accuracy that is known as epochs okay again if you need more details about this you may have to go
through the machine learning tutorial i think it becomes much clearer okay so you have your input function as well
let's execute that okay and now we are ready to create the use the api right so
what will we do we will create the linear classifier using tf.estimator.linearclassifier
and this guy needs feature columns right so that's why we created we did all that
so much of manipulations to create this feature columns so we have feature columns so we
pass that and we create our model there may be a few warnings don't worry about that so we will just ignore that and now
we use that model and we actually what we did is we created an instance of the
model we have not really run anything we just created a instance of the model now
we will create the node for training so model.train and this needs the input
function remember we we created the input function so we need to pass the input function and then we say
the steps is now telling how many iterations this training has to be run so we are saying 5000 and
this may be it will probably take a little long so we will okay but that's fine i think we will leave it 5000.
sometimes if you have probably a less powerful machine you can cut it down to
maybe thousand or something like that but in this case i have a fairly powerful system so i think it shouldn't
take much long let's uh go ahead and give couple of minutes for it to get
over okay so let us go back yeah i think this is done you see here the hourglass
has disappeared that means this computation is done yes okay this is done right saving checkpoint for 5000 that means it
is done so training is done so now what we have to do we need to do the evaluation so
again this is a standard machine learning method so methodology rather so training has been
done with the training data set now you need to evaluate using your test data set so that's what we are doing here for
example you see here x underscore test is your the test data set remember we
used scikit-learn to split the data into training and test data set so x
underscore test and a quick question why are we not using y underscore test here like we did in case of uh training
obviously we are expecting the model to predict the values y values right so that's why we don't pass the y values
here all right so let's execute this for doing the evaluation all right and then
we put them in a proper format because the output doesn't come out in a proper format so we put it in a list format and
then we can just check yeah so this is done and we can check what is the first element
in this what is the probabilities and so on so it's just a quick way to see if the values are
there or not you can take any value here right any element here all right now that the testing is done what we need to
do again we will probably put them in some kind of this is more of a formatting thing we will just uh
run through quickly and then we will use once again scikit-learn for finding out
the accuracy and so on because scikit-learn offers what is known as a classification report functionality so
we will use that and find out how well our model has performed so you see here
again people with machine learning background will be immediately able to recognize this so it
gives us what is the precision what is the recall and f1 score this is pretty much like you're getting
like almost 85 percent accuracy which is pretty okay and there are other ways to increase the
accuracy for example you can run the training for more iterations that is one way get more data and so on and so forth
or use this is a linear classifier we could you have used a non-linear classifier and so
on so again multiple ways of doing it to increase the accuracy but the idea here was to quickly show you a piece of
tensorflow code and that's what we have done here we had the original tensorflow
release of 1.0 and then they came out with the 2.0 version and the 2.0
addressed so many things out there that the 1.0 really needed so we start talking about tensorflow 1.0 versus 2.0
i guess you would need to know this for a legacy programming job if you're pulling apart somebody else's code the
first thing is that tensorflow 2.0 supports eager execution by default it allows you to build your models and run
them instantly and you can see here from tensorflow 1 to tensorflow 2 we have
almost double the code to do the same thing so if i want to do with tf.session
or tensorflow session as a session the session run you have your variables your
session run you have your tables initializer and then you do your model fit
x train y train and then your validation data your x value y value and your epics and your batch size all that goes into
the fit and you can see here where that was all just compressed to make it run easier you can just create a model and
do a fit on it and you only have like that last set of code on there so it's automatic that's
what they mean by the eager so if you see the first part you're like what the heck is all the session thing going on that's tensorflow 1.0 and then when you
get into 2.0 it's just nice and clean if you remember from the beginning i
said cross on our list up there and across is the high level api in
tensorflow 2.0 cross is the official high level api of tensorflow 2.0 it has incorporated cross
as tf.caras cross provides a number of model building apis such as sequential
functional and subclassing so you can choose the right level of abstraction for your project and
we'll hopefully touch base a little bit more on this sequential being the most common uh form that is your your layers
are going from one side to the other so everything's going in a sequential order
functional is where you can split the layer so you might have your input coming in one side
it splits into two completely mod different models and then they come back together
and one of them might be doing classification the other one might be doing just linear regression kind of stuff or a neural basic
reverse propagation neural network and then those all come together into another layer which is your
neural network reverse propagation setup subclassing is the most complicated as you're
building your own models and you can subclass your own models into cross so very powerful tools here this is all the
stuff that's been coming out currently in the tensorflow cross setup a third big change we're going to look at is it
in tensorflow 1.0 in order to use tf layers as variables
you would have to write tf variable block so you'd have to pre-define that in tensorflow 2 you just add your layers
in under the sequential and it automatically defines them as long as they're flat layers of course this changes a little bit as a
more complicated tensor you have coming in but all of it's very easy to do and that's what 2.0 does a really good job
of and here we have a little bit more on the scope of this and you can see how tensorflow 1 asks
you to do these different layers and values if you look at the scope and the default name you start looking at all
the different code in there to create the variable scope that's not even necessary in tensorf 2.0 so you'd have
to do one before you do do what you see the code in 2.0 in 2.0 you just create
your model it's a sequential model and then you can add all your layers in you don't have to pre-create the
variable scope so if you ever see the variable scope you know that came from an older version and then we have the
last two which is our api cleanup and the autograph in the api cleanup tensorflow 1 you
could build models using tf gans tf app tf contrib tf flags etc in tensorflow 2
a lot of apis have been removed and this is just they just cleaned them up because people weren't using them and
they've simplified them and that's your tf app your tf flags your tf logging are all gone
so there's those are three legacy features that are not in 2.0 and then we
have our tf function and autograph feature in the old version tensorflow 1
0 the python functions were limited and could not be compiled or exported
re-imported so you were continually having to redo your code you couldn't very easily just
put a pointer to it and say hey let's reuse this in tensorflow 2 you can write a python
function using the tf function to mark it for the jit compilation for the python jit so that tensorflow runs it as
a single graph autograph feature of tf function helps to write graph code using
natural python syntax now we just threw in a new word in you graph a graph is not a picture of a
person you'll hear graph x and some other things graph is what are all those lines that
are connecting different objects so if you remember from before where we had
the different layers going through sequentially each one of those white lined arrows would be a graph x that's
where that computation is taken care of and that's what they're talking about and so if you had your own special code
or python way that you're sending that information forward you can now put your own function in there instead of using
whatever function they're using in neural networks this would be your activation function although it could be
almost anything out there depending on what you're doing next let's go for hierarchy and architecture and then we'll cover three
basic tools in tensorflow before we roll up our sleeves and dive into the example
so let's just take a quick look at tensorflow toolkits in their hierarchy at the high level we have our object
oriented api so this is what you're working with you have your tf cross you have your estimators this sits on top of
your tf layers tf losses tf metrics so you have your reusable libraries for
model building this is really where tensorflow shines is between the cross
running your estimators and then being able to swap in different layers you can your losses your metrics all of that is
so built into tensorflow makes it really easy to use and then you can get down to your low level tf api
you have extensive control over this you can put your own formulas in there your own procedures or models in there
you could have it split we talked about that earlier so with the 2.0 you can now have it split one direction where you do
a linear regression model and then go to the other where it does a neural network and maybe each neural
network has a different activation set on it and then it comes together into another layer which is another neural
network so you can build these really complicated models and at the low level you can put in your own apis you can
move that stuff around and most recently we have the tf code can run on multiple platforms
and so you have your cpu which is basically like in the computer i'm
running on i have eight cores and 16 dedicated threads i hear they now have one out there that
has over 100 cores so you have your cpu running and then you have your gpu which is your graphics
card and most recently they also include the tpu setup which is specifically for
tensorflow models neural network kind of setup so now you can export the tf code and it
can run on all kinds of different platforms for the most diverse setup out there and moving on
from the hierarchy to the architecture in the tensorflow 2.0 architecture
we have you can see on the left this is usually where you start out with and 80 percent of your time in data science is spent
pre-processing data making sure it's loaded correctly and everything looks right so the first level in tensorflow is
going to be your read and pre-processed data your tf data feature columns this is going to feed into your tf cross
or your premade estimators and kind of you have your tensorflow hub that sits on top of there so you can see
what's going on uh once you have all that set up you have your distribution strategy where are you going to run it
you're going to be running it on just your regular cpu are you going to be running it with the gpu added in
like i have a pretty high end graphics card so it actually grabs that gpu processor and uses it or do you have a
specialized tpu setup in there that you paid extra money for it could be if you're
in later on when you're distributing the package you might need to run this on some really high processors because
you're processing at a server level for uh let's say net you might be processing this at a
distribute you're distributing it not the distribution strategy but you're distributing it into a server where that
server might be analyzing thousands and thousands of purchases done every minute
and so you need that higher speed to give them a um to give them a recommendation or a suggestion so they
can buy more stuff off your website or maybe you're looking for data fraud analysis working with the
banks you want to be able to run this at a high speed so that when you have hundreds of people
sending their transactions in it says hey this doesn't look right someone's scamming this person and probably has
their credit card so when we're talking about all those fun things we're talking about saved model this is we were
talking about that earlier where it used to be when you did one of these models it wouldn't truncate the float numbers
the same and so a model going from one you build the model on your machine in the office and then you need
to distribute it and so we have our tensorflow serving cloud on premium that's what i was talking about if
you're like a banking or something like that now they have tensorflow lite so you can actually run a tensorflow on an
android or an ios or raspberry pi a little breakout board there in fact they just came out with a new one that has a
built-in there's this little mini tpu with the camera on it so it can pre-process a video so you can load your
tensorflow model onto that um talking about an affordable way to beta test a new product uh you have the
tensorflow js which is for browser and node server so you can get that out on the browser for some simple computations
that don't require a lot of heavy lifting but you want to distribute to a lot of end points and now they also have
other language bindings so you can now create your tensorflow backend save it and have it accessed from c java go
c sharp rust r or from whatever package you're working on so we kind of have an overview of the architecture and what's
going on behind the scenes and in this case what's going on as far as distributing it let's go ahead and take
a look at three specific pieces of tensorflow and those are going to be constants
variables and sessions so very basic things you need to know and understand when you're working with
the tensorflow setup so constants in tensorflow in tensorflow constants are created using
the function constant in other words they're going to stay static the whole time whatever you're working with the
syntax for constant value d type 9 shape equals none name
constant verify shape equals false that's kind of the syntax you're looking at and we'll explore this with our hands
on a little more in depth and you can see here we do z equals tf.constant 5.2 name equals x
d type is a float that means that we're never going to change that 5.2 it's going to be a constant value and then we
have our variables in tensorflow variables in tensorflow are in memory buffers that store tensors and so we can
declare a two by three tensor populated by ones you could also do constants this way by the way you can create a an array
of ones for your constants i'm not sure why you'd do that but you know you might need that for some reason
in here we have v equals tf.variables and then in tensorflow you have tf.ones
and you have the shape which is 2 3 which is then going to create a nice 2x3 array that's filled with ones and then
of course you can go in there and they're variables so you can change them it's a tensor so you have full control over that
and then you of course have sessions in tensorflow a session in tensorflow is
used to run a computational graph to evaluate the nodes
and remember when we're talking a graph or graph x we're talking about all that information then goes through all those
arrows and whatever computations they have that take it to the next node and you can see down here where we have
import tensorflow as tf if we do x equals a tf.constant of 10
we do y equals a tf constant of 2.0 or 20.0 and then you can do z equals
tf.variable and it's a tf.ad x comma y
and then once you have that set up in there you go ahead and knit your tf global variables initializer with tf
session as session you can do a session run init and then you print the session run y
uh and so when you run this you're going to end up with of course the 10 plus 20 is 30. and we'll be looking at this a
lot more closely as we actually roll up our sleeves and put some code together
so let's go ahead and take a look at that and for my coding today i'm going to go ahead and go through anaconda and
then i'll use specifically the jupiter notebook on there and of course this code is going to work uh whatever
platform you choose whether you're in a notebook the jupiter lab which is just a jupiter
notebook but with tabs for larger projects we're going to stick with jupiter notebook pycharm uh whatever it is you're gonna
use in here uh you have your spider and your qt console for different programming environments the thing to
note um it's kind of hard to see but i have my main pi 36
right now when i was writing this tensorflow works in python version 3 6.
if you have python version 3 7 or 3 8 you're probably going to get some errors
in there might be that they've already updated and i don't know it now you have an older version but you want to make sure
you're in python version 3 6 in your environment and of course in anaconda i can easily set that environment up make
sure you go ahead and pip in your tensorflow or if you're in anaconda you
can do a conda install tensorflow to make sure it's in your package so let's just go ahead and dive in and
bring that up this will open up a nice browser window i just love the fact i can zoom in and
zoom out depending on what i'm working on making it really easy to just um demo for the right size go under new and
let's go ahead and create a new python and once we're in our new python window this is going to leave it untitled
let's go ahead and import import tensorflow as tf at this point we'll go ahead and just
run it real quick no errors yay no errors i
i do that whenever i do my imports because i unbearably will have opened up a new environment and forgotten to
install tensorflow into that environment or something along those lines so it's
always good to double check and if we're going to double check that we also it's also good to know
what version we're working with and we can do that simply by
using the version command in tensorflow which you should know is is probably
intuitively the tf dot underscore underscore version
underscore underscore and you know it always confuses me because sometimes you do tf.version for one
thing you do tf dot underscore version underscore for another thing this is a double underscore in
tensorflow for pulling your version out and it's good to know what you're working with we're going to be working
in tensorflow version 2.1.0 and i did tell you the the we were going to dig a
little deeper into our constants and you can do an array of constants and we'll just create this nice array
a equals tf.constant and we're just going to put the array right in there 4361.
we can run this and now that is what a is equal to and if we want to just double check that remember we're in
jupiter notebook where i can just put the letter a and it knows that that's going to be print
otherwise you round you surround it in print and you can see it's a tf tensor it has the shape the type and the and the array
on here it's a two by two array and just like we can create a constant we can go and create a variable and this is also
going to be a two by two array and if we go ahead and print the v out we'll run that
and sure enough there's our tf variable in here then we can also let's just go back up
here and add this in here i could create another tensor and we'll make it a constant this time
and we're going to put that in over here we'll have b tf constant and if we go and print out v and b
i'm going to run that and this is an interesting thing that always that happens in here you'll see
right here when i print them both out what happens it only prints the last one unless you use print commands so
important to remember that in jupyter notebooks we can easily fix that by go ahead and print and surround v with
brackets and now we can see with the two different variables we have we have the three one five two which is
a variable and this is just a flat constant so it comes up as a tf tensor
shape two kind of two and that's interesting to note that this label is a tf.tensor and this
is a tf variable so that's how it's looking in the back end when you're talking about the difference between a
variable and a constant the other thing i want you to notice is in variable we capitalize the v and with
the constant we have a lowercase c little things like that can lose you
when you're programming and you're trying to find out hey why doesn't this work so those are a couple of things to note
in here and just like any other array in math we can do like a concatenate or
concatenate the different values here and you can see we can take a b
concatenated you just do a t f dot concat values and there's our a b axis on one
hopefully you're familiar with axes and how that works when you're dealing with matrixes and if we go ahead and print
this out uh you'll see right here we end up with a tensor so let's put it in as a
constant not as a variable and you have your array four three seven eight and six one four five it's
concatenated the two together and again i wanna highlight a couple things on this our axes equals one this means
we're doing the columns um so if you had a longer array like right now we have an array that is like you know has a shape
one whatever it is two comma two axes zero
is going to be your first one and axes one is going to be your second one and it translates as columns and rows if we
had a shape let me just put the word shape here um so you know what i'm talking about is
very clear and this is i'll tell you what i spent a lot of time looking at these shapes and trying to
figure out which direction i'm going in and whether to flip it or whatever so you can get lost in which way your
matrix is going which is column which is rows are you dealing with the third axes or the second axes
axis 1 you know zero one two that's gonna be our columns uh and if you can do columns then we also can do rows and
that is simply just changing the concatenate uh we'll just grab this one here and copy it we'll do the whole
thing over ctrl copy ctrl v and changes from axis one
to axis zero and if we run that you'll see that now we concatenate by a
row as opposed to column and you have four three six one seven eight four seven so it just brings it
right down and turns it into rows versus columns you can see the difference there your output this really you want to look
at the output sometimes just to make sure your eyes are looking at it correctly and it's in the format i find
visually looking at it is almost more important than understanding what's going on because conceptually
your mind just just too many dimensions sometimes the second thing i want you to notice is this says a numpy array so tensorflow is
utilizing numpy as part of their format as far as python is concerned
and so you can treat you can treat this output like a numpy array because it is just that it's going to be a numpy array
another thing that comes up uh more than you would think is filling um one of
these with zeros or ones and so you can see here we just create a tensor tf.zeros
and we give it a shape we tell it what kind of data type it is in this case we're doing an integer
and then if we print out our tensor again we're in jupiter so i just type out tensor and i
run this you can see i have a nice array of the shape three comma four of zeros one
of the things i want to highlight here is integer 32. if i go to the
tensorflow data types i want you to notice how we have float 16 float 32 float 64
complex if we scroll down you'll see the integer down here of 32. the reason for this is that we want to control how many
bits are used in the precision this is for exporting it to another platform
so what would happen is i might run it on this computer where python goes does a float to indefinite however long it
wants to um and then we can take it but we want to actually say hey we don't want that
high precision we want to be able to run this on any computer and so we need to control
whether it's a tf float 16 in this case we did an integer 32
we could also do this as a float so if i run this as a float 32 that means this has a 32 bit precision
you'll see zero point whatever and then to go with uh zeros we have ones if we're going from the
opposite side and so we can easily just create a tensorflow with ones and you might ask yourself why would i
want zeros and ones and your first thought might be to initiate a new tensor usually we initiate a lot of this stuff
with random numbers because it does a better job solving it if you start with a uniform
set of ones or zeros you're dealing with a lot of bias so be very careful about
starting a neural network for one of your rows or something like that with ones and zeros
on the other hand i use this for masking you can do a lot of work with masking you can also have
it might be that one tensor row is massed you know zero is is false one is true or
whatever you want to do it um and so in that case you do want to use the zeros and ones and there are cases
where you do want to initialize it with all zeros or all ones and then swap in different numbers as
the tensor learns so it's another form of control but in general
you see zeros and ones you usually are talking about a mask over another array and just like in numpy you can also
do reshapes so if we take our remember this is shaped three comma four maybe we wanna swap that to four comma
three and if we print this out you will see let me just go and do that
ctrl v let me run that and you'll see that the the order of these is now switched instead of four
across now we have three across and four down and just for fun let's go back up here
where we did the ones and i'm going to change the ones to tf.random
uniform uh and we'll go ahead and just take off well we'll go and leave that we're going to run this
and you'll see now we have 0.0441 and this way you can actually see how the reshape looks a lot different
0.041.15.71 and then instead of having this one it rolls down here to the 0.14
and this is what i was talking about sometimes you fill you a lot of times you fill these with random numbers and
so this is the random dot uniform is one of the ways to do that now i just talked
a little bit about this float 32 and all these data types one of the things that comes up of
course is recasting your data so if we have a d type float 32 we might
want to convert these two integers because of the project we're working on i know one of the projects i've worked
on ended up wanting to do a lot of round off so that it would take a dollar amount or a float value and then have to
round it off to a dollar amount so we only wanted two decimal points in which case you have a lot of different options you can multiply by
100 and then round it off or whatever you want to do there's a lot or then convert it to an integer was one way to
round it off uh kind of a cheap and dirty trick
so we can take this and we can take the same tensor and we'll go ahead and create a as an integer and so we're going to take
this tensor we're going to tf.cast it and if we print
tensor and then we're going to go ahead and print our
tensor let me just do a quick copy and paste and when i'm
actually programming i usually type out a lot of my stuff just to double check it
in doing a demo copy and paste works fine but sometimes be aware that copy and paste can copy the wrong code over
personal choice depends on what i'm working on and you can see here we took a float 32 4.6 4.2 and so on and it just
converts it right down to a integer value sorry integer 32 set up and
remember we talked about a little bit about reshape as far as flipping it and i just did
four comma three on the reshape up here and we talked about axes zero axis one
uh one of the things that is important to be able to do is to take one of these variables we'll just take this last one
tensor as integer and i want to go ahead and transpose it
and so i can do we'll do a equals tf.transpose
and we'll do our tensor integer in there and then if i print the a out and we run this
you'll see that's the same array but we've flipped it so that our columns and rows are flipped this is
the same as reshaping so when you transpose you're just doing a reshape what's nice about this is if you look at
the numbers the columns when we went up here and we did the reshape they kind of rolled down to the
next row so you're not maintaining the structure of your matrix so when we do a reshape up here they're similar but
they're not quite the same and you can actually go in here and there's settings in the reshape that would allow you to
turn it into a transform so we come down here it's all done for
you and so there are so many times you have to transpose your digits that this is important to
know that you can just do that you can flip your rows and columns rather quickly here and just like numpy you can
also do multiple your different math functions we'll look at multiplication and so we're going to take matrix
multiplication of tensors we'll go ahead and create a as a constant 5839
and we'll put in a vector v 4 comma 2. and we could have done this where they
matched where this was a two by two array but instead we're going to do just a two by one
array and the code for that is your tf.mat mole so matrix multiplier and we have a times
v and if we go ahead and run this up let's make sure we print out our av on there
and if we go ahead and run this you'll see that we end up with 36 by 30.
and if it's been a while since you've seen the matrix math this is
5 times 4 plus 8 times 2 3 times 4 plus 9 times 2. and that's
where we get the 36 and 30. now i know we're covering a lot really quickly as
far as the basic functionality so the matrix or your matrix multiplier
is a very commonly used back-end tool as far as computing
different models or linear regression stuff like that one of the things
is to note is that just like in numpy you have all of your different math so
we have our tf math and if we go in here we have functions we have our cosines absolute
angle all of that's in here so all of these are available for you to use in the
tensorflow model and if we go back to our example and let's go ahead and pull
oh let's do some multiplication that's always good we'll stick with our av
our constant a and our vector v
and we'll go ahead and do some bit wise multiplication and we'll create an av which is a times b let's go and print
that out and you can see coming across here we have the 4 2 and the 5 8 3 9 and it
produces 20 32 6 18. and that's pretty straightforward if you
look at it you have 4 times 5 is 20 4 times 8 is
32 that's where those numbers come from
now we can also quickly create an identity matrix which is basically
your main values on the diagonal being ones and zeros across the other side
let's go ahead and take a look and see what that looks like and we can do let's do this
so we're going to get the shape this is a simple way very similar to your numpy you can do a dot shape and it's going to
return a tuple in this case our rows and columns and so we can do a quick
print we'll do rows oops
and we'll do columns
and if we run this you can see we have three rows uh two columns
and then if we go ahead and create an identity matrix
the scripts the script for that got a wrong button there the script for that looks like
this where we have the number of rows equals rows the number of columns equals
columns and d type is a 32 and then if we go ahead and just print out our identity
you can see we have a nice identity column with our ones going across here now clearly we're not going to go
through every math module available but we do want to start looking at this as a prediction model
and seeing how it functions so we're going to move on to a more of a
direct setup where you can actually see the full tensorflow in use for that let's go back and create a
new setup and we'll go in here new python 3 module
there we go bring this out so it takes up the whole window because i like to do that
hopefully you made it through that first part and you have a basic understanding of tensorflow as far as being
a series of numpy arrays you've got your math equations and different things that go into them
we're going to start building a full um setup as far as the numpy so you can see
how uh cara sits on top of it and the different aspects of how it works the first thing we want to do is we're
going to go ahead and do a lot of imports date times warning sci pi scipy is your
math so the backend scientific math warnings because
whenever we do a lot of this you have older versions newer versions
and so sometimes when you get warnings you want to go ahead and just suppress them we'll talk about that if it comes up on
this particular setup and of course date time pandas again is your data frame think
rows and columns we import it as pd numpy is your numbers array which of
course tensorflow is integrated heavily with seaborn for our graphics and the
seaborne as sns is going to be set on top of our matplot library which we import as mpl and then of course we're
going to import our matplot library pi plot as plt and right off the bat we're
going to set some graphic colors um patch force edge color equals true
the style we're going to use the 538 style you can look this all up there's
when you get into matplot library into seaborne there are so many options in
here it's just kind of nice to make it look pretty when we start the um when we start up that way we don't have to think about it later on
uh and then we're going to take we have our mplrc we're going to put a patch ed color dim gray line width again this is
all part of our graphics here in our setup we'll go ahead and do an interactive shell
node interactivity equals last expression here we are pd for pandas options
display max columns so we don't want to display more than 50. and then our matplot library is going to
be inline this is a jupiter notebook thing the matplot library inline then
warnings we're going to filter our warnings and we're just going to ignore warnings that way when they come up we don't have to worry about them
not really what you want to do when you're working on a major project you want to make sure you know those warnings and then
filter them out and ignore them later on and if we run this it's just going to be loading all that into the background
so that's a little back end kind of stuff then what we want to go ahead and do is we want to go ahead and import
our specific packages that we're going to be working with which is under cross now remember cross
kind of sits on tensorflow so when we're importing cross and the sequential model we are in effect importing
um tensorflow underneath of it uh we just brought in the math probably should have put that up above
and then we have our cross models we're going to import sequential now if you remember from our
uh slide there was three different options let me just flip back over there so we can have a quick uh recall on that
and so in karass cross we have sequential functional and
subclassing so remember those three different setups in here we talked about earlier and if you remember from here we
have a sequential where it's going one tensorflow layer at a time you go
kind of like think of it as going from left to right or top to bottom or whatever direction it's going in but it
goes in one direction all the time where functional can have a very complicated graph of directions you can have the
data split into two separate tensors and then it comes back together into another tensor
all those kinds of things and then subclassing is really the really complicated one where now you're adding your own subclasses into the tensor to
do external computations right in the middle of like a huge flow of data
but we're going to stick with sequential it's not a big jump to go from sequential to functional
but we're running a sequential tensorflow and that's what this first import is here we want to bring in our
sequential and then we have our layers and let's talk a little bit about these layers
this is where cross and tensorflow really are happening this is what makes
them so nice to work with is all these layers are pre-built so from cross we have layers import
dents from cross layers import lstm
when we talk about these layers cross has so many built-in layers you can do your own layers
the dense layer is your standard neural network by default it uses relu for its
activation and then the lstm is a long short term
memory layer since we're going to be looking probably at sequential data we want to go ahead and do the lstm and
if we go into cross and we look at their layers this
is across website you can see as we scroll down for the cross layers that are built in
we can get down here and we can look at let's see here we have our layer activation our base layers
activation layer weight layer so there's a lot of stuff in here we had the relu which is the basic activation that was
listed up here for layer activations you can change those and here we have our core layers
and our dense layers we have an input layer a dense layer and then we've added a more customized
one with the long-term short-term memory layer and of course you can even do your own custom layers in cross there's a
whole functionality in there if you're doing your own thing what's really nice about this is it's all built in even the convolutional
layers this is for processing graphics there's a lot of cool things in here you can do
this is white cross is so popular it's open source and you have all these tools right at your fingertips so from cross
we're just going to import a couple layers the dense layer and the long short term memory layer
and then of course from sk learn our scikit we want to go ahead and do our min max
scalar standard scalar for pre editing our data and then metrics just so we can
take a look at the errors and compute those let me go ahead and run this and that just loads it up we're not
expecting anything from the output and our file coming in is going to be air
quality.csv let's go ahead and take a quick look at that this is in open office it's just a
standard you know might do excel whatever you're using for your spreadsheet and you can see here we have
a number of columns a number of rows it actually goes down to like 8 000.
the first thing we want to notice is that the first row is kind of just a random number put in
going down probably not something we're going to work with the second row
is bandung i'm guessing that's a reference for the profile if we scroll to the bottom which i'm not
going to do because it takes forever to get back up they're all the same uh the same thing with the status the status is
the same we have a date so we have a sequential order here
um here is the jam which i'm going to guess is the time stamp on there so we have a date and time
we have our o3 co no2 reading so2 no co2
voc and then some other numbers here pm1 pm 2.5 pm 4 pm 10
10 without actually looking through the data i mean some of this i can guess is like temperature
humidity i'm not sure what the pms are but we have a whole slew of data here uh so we're looking at air quality as far
as an area in a region and what's going on with our date time stamps on there and so code wise we're going to read
this into a pandas data frame so our data frame df is a nice abbreviation
commonly used for data frames equals pd.read csv and then our the path to it just happens to be on my d drive
separated by spaces and so if we go ahead and run this we'll print out the head of our data and
again this looks very similar to what we were just looking at being in jupiter i can take this and go
the other way make it real small so you can see all the columns going across and we get a full view of it
or we can bring it back up in size that's pretty small on there overshot
but you can see it's the same data we were just looking at we're looking at the number we're looking at the profile which is the bandung
the date we have a timestamp our o3 count co and so forth on here
and this is just your basic pandas printing out the top five rows we could easily have done
three rows five rows ten whatever you want to put in there by default that's five for
pandas now i talk about this all the time so i know i've already said it at least once or twice during this video
most of our work is in pre-formatting data what are we looking at how do we bring it together
so we want to go ahead and start with our date time it's come in in two columns
we have our date here and we have our time and we want to go ahead and combine that
and then we have this is just a simple script in there that says combined date time that's our formula we're building
our we're going to submit our pandas data frame and the tab name when we go ahead and do
this that's all of our information that we want to go ahead and create and then goes for i and range df
shape 0. so we're going to go through the whole setup and we're going to list tab append df location i
and here is our date going in there and then return the numpy array list tab d
types date time 64. that's all we're doing we're just switching this to a date time stamp and if we go ahead and
do df date time equals combine date time and then i always like to uh print we'll
do df head just to see what that looks like and so when we come out of this uh we
now have our setup on here and of course it's edited on to the far right here's our date time you can see the format's
changed uh so there's our we've added in the date time column and we've brought
the date over and we've taken this format here and it's an actual variable with a 0 0 0
on here well that doesn't look good so we need to also include the time part of this and we want to convert it into
hourly data so let's go ahead and do that to do that
to finish combining our date time let's go ahead and create a a little script here to combine the time in there
same thing we just did we're just creating a numpy array returning a numpy array and cr forcing this into a date
time format and we can actually spend hours just going through uh these
conversions how do you pull it from the panda's data frame how
do you set it up so i'm kind of skipping through it a little fast because i want to stay focused on tensorflow and cross
keep in mind this is like 80 of your coding when you're doing a lot of this stuff is going to be reformatting these
things resetting them back up uh so that it looks right on here and you know it just takes time to get
through all that but that is usually what the companies are paying you for that's what the big
bucks are for and we want to go ahead and a couple
things going on here is we're going to go ahead and do our date time we're going to reorganize some of our setup in here convert into hourly data we just
put a pause in there now remember we can select from df are
different columns we're going to be working with and you're going to see that we actually dropped a couple of the columns those ones i
showed you earlier they're just repetitive data so there's nothing in there that exciting and then we want to go ahead and we'll
create a second data frame here let me just get rid of the df head and df2 is we're going to
group by date time and we're looking at the mean value and then we'll print that out so you can
see what we're talking about uh we have now reorganized this so we put in date time 03 co
so now this is in the same order as it was before and you'll see the date time
now has our 0 0 same date 1 2 3 and so on so let's group
the data together so there's a lot more manageable and in the format we want and in the right sequential order
and if we go back to um there we go our air quality you can see right here we're looking at
um these columns going across we really don't need since we're going to create
our own date time column we can get rid of those these are the different columns of information we want and that should
reflect right here in the columns we picked coming across so this is all the same columns on there that's all we've
done is reformatted our data grouped it together by date and then you can see the different data coming out
set up on there and then as a data scientist first thing i want to do is get a
description what am i looking at uh and so we can go ahead and do the df2 describe and this gives us our you know
describe gives us our basic data analytics information we might be looking for like what is the mean
standard deviation uh minimum amount maximum amount we have our first quarter second quarter and
third quarter um numbers also in there so you can get a quick look at a glance
describing the data or descriptive analysis and even though we have our quantile
information in here we're going to dig a little deeper into that we're going to calculate the quantile
for each variable we're going to look at a number of things for each variable and we'll see right here q1
we can simply do the quantile 0.25 percent which should match um our 25 percent up
here and we'll be looking at the min the max and we're just going to do this is basically we're breaking this down for
each different variable in there one of the things that's kind of fun to
do we're going to look at that in just a second let me get put the next piece of code in here
clean out some of our um we're going to drop a couple things our last rows and first row because those
have usually have a lot of null values and the first row is just our titles so that's important it's important to drop
those rows in here and so this right here as we look at our different quantiles
again it's it's the same you know we're still looking at the 25 quantile here
we're going to do a little bit more with this so now that we've cleared off our first and last rows
we're going to go ahead and go through all of our columns and this way we can look at each
column individually and so we'll just create a q1 q3 min max min iqr max
iqr and calculate the quantile of i of df2
we're basically doing the math that they did up here but we're splitting it apart that's all this is
and this happens a lot because you might want to look at individual if this was my own project i would probably spend
days and days going through what these different values mean
one of the biggest data science things we can look at that's important
is uh use your use your common sense you know if you're looking at this data and it doesn't make sense and you go
back in there and you're like wait a minute what the heck did i just do at that point you probably should go
back and double check what you have going on now we're looking at this and you can see
right here here's our attribute for our o3 so we've broken it down we have our q1 5.88 q3 10.37 if we go
back up here here's our 5 8 we've rounded it off 10.37 is in there
so we've basically done the same math just split it up we have our minimum and our max iqr and that's computed let's
see where is it here we go uh q1 minus 1.5 times iqr and the iqr is your q3
minus q1 so that's the difference between our two different quarters and this is all
data science as far as the hard math we're really not we're actually trying to focus on cross
and tensorflow you still got to go through all this stuff i told you 80 of your programming is going through and
understanding what the heck uh happened here what's going on what does this data mean
and so when we're looking in that we're going to go ahead and say hey
we've computed these numbers and the reason we've computed these numbers is if you take the minimum value and it's
less than your minimum iqr that means something's going wrong there
and it usually in this case is going to show us an outlier so we want to go ahead and find the minimum value if it's less than the min
minimum iqr it's an outlier and if the max value is greater than the max iqr we have an outlier and that's
all this is doing low outliers found minimum value high outlier is found
really important actually outliers are almost everything in data sometimes sometimes you do this project just to
find the outliers because you want to know uh crime detection what are we looking for we're looking for the outliers what doesn't fit a normal
business deal and then we'll go ahead and throw in just threw in a lot of code oh my goodness uh so we have if your max is
greater than iqr print outlier is found what we want to do is we want to start cleaning up these outliers and so we
want to convert we'll do create a convert nand x max iqr equals max
underscore iqr min iqr equals mini qr so this is just saying this is the data we're going to send that's all that is
in python and if x is greater than the max iqr and x is less than the min iqr x
equals null we're going to set it to null why because we want to clear
these outliers out of the data now again if you're doing fraud detection you would do the opposite you would be
cleaning everything else that's not in that series so that you can look at just the outlier and then we're going to
convert the nand hum again we have x max iqr is 100
min iq r is min iqr if x is greater than max iqr and x is
less than min iqr again we're going to return a null value otherwise it's going to remain the same value x x equals x
and you can see as we go through the code if i equals our hum
then we go ahead and do this that's a column specific to humidity
that's your hum column uh then we're going to go ahead and convert do the
run a map on there and convert the none hum you can see here it's this is just
cleanup uh we run we found out that humidity probably has some weird values in it
we have our outliers that's all this is and so when we go ahead and finish this
and we take a look at our outliers and we run this code here
we have a low outlier 2.04 we have a high outlier 99.06
outliers have been interpolated
that means we've given them a new value chances are these days when you're looking at something like um these
sensors coming in they probably have a failed sensor in there something went wrong
that's the kind of thing that you really don't want to do your data analysis on so that's what we're doing is we're pulling that out and then uh converting
it over and setting it up uh method linear so we interpolate left
linears it's going to fill that data in based on a linear regression model of similar data
same thing with this up here with the df2i interpolate that's what we're doing
again this is all data prep we're not actually talking about tensorflow we're just trying to get all our data
set up correctly so that when we run it it's not going to cause problems or have a huge bias
so we've dealt with outliers specifically in humidity and again this is one of these things
where when we start running we run through this you can see down
here that we have our outliers found high low outliers
migrated them in we also know there's other issues going on with this data
how do we know that some of it's just looking at the data playing with it until you start understanding what's going on let's take
the temp value and we're going to go ahead and use a logarithmic function on the temp
value and it's interesting because it's like how do you how do you heck do you even know
to use logarithmic on the temp value that's domain specific
we're talking about being an expert in air care i'm not an expert in air care um you know it's not what i go look at i
don't look at air care data in fact this is probably the first air care data set up i've looked at but the experts come in there and they come to you and say
hey in data science this is a exponentially variable variable on here so we need to go ahead and do
transform it and use a logarithmic scale on that
so at that point that would be coming from your data here we go data science programmer
overview does a lot of stuff connecting the database and connecting in with the experts
data analytics a lot of times you're talking about somebody who is a data analysis might be all the way usually a
phd level data science programming level interfaces database manager that's going
to be the person who's your admin working on it so when we're looking at this we're looking at something they've sent to me
and they said hey domain air care this needs to be this is a skew because
the data just goes up exponentially and affects everything else and we'll go ahead and take that data let me just go
ahead and run this just for another quick look at it we have our
uh we'll do a distribution df we'll create another data frame from the temp values and then from a data set
from the log temp so we can put them side by side and we'll just go ahead and do a quick
histogram and this is kind of nice plot of figure figure size here's our plt from matplot library uh and then we'll
just do a distribution underscore df there's our data frames this is nice because it just integrates
the histogram right into pandas love pandas and this is a chart you would send back to your data analysis and say
hey is this what you wanted this is how the data is converting on here as a data science scientist the first thing i note
is we've gone from a 10 20 30 scales to 2.5 3.0 3.5 scale
and the data itself has kind of been adjusted a little bit based on some kind
of a skew on there so let's jump into we're getting a little closer to actually doing our
cross on here we'll go ahead and split our data up and this of course is any good data
scientists you want to have a training set and a test set and we'll go ahead and do the train size
we're going to use 0.75 percent of the data make sure it's an integer we don't want to take a slice as a float value
give you a nice error and we'll have our train size of 75 percent and the test size is going to be
of course the train size minus the length of the data set and then we can simply do train comma test
here's our data set which is going to be the train size the test size uh and then if we go and print this let
me just go ahead and run this we can see how these values split it's a nice split of 1298 and then
433 points of value that are going to be for our setup on here and if you remember we're
specifically looking at the data set where did we create that data set from that was from up here that's what we
called the logarithmic value of the temp that's where the data set came from so
we're looking at just that column with this train size and the test with the train and test data set here and let's
go ahead and do uh convert an array of values into a data set matrix we're going to create a little
set up in here we create our data set our data set's going to come in we're going to do a look back of one so we're going to look back one piece of data
going backward and we have our data x and our data y for i and range length of data set look
back minus one uh this is creating let me just go ahead and run this actually the best way to do
this is to go ahead and create this data and take a look at the shape of it let
me go ahead and just put that code in here so we're going to do the look back one here's our train x our train y and it's
going to be adding the data on there and then when we come up here and we take a look at the shape
there we go when we run this piece of code here we look at the shape on this and we have
a new slightly different change on here but we have a shape of x 1296 comma 1
shape of y train y test x text y and so what we're looking at is that
the x comes in and we're only having a single value at out
we want to predict what the next one is that's what this little piece of code is here for what are we looking for we want
to look back one that's the what we're going to train the data with is yesterday's data yesterday says hey
the humidity was at 97 what should today's humidity be at if
it's 97 yesterday is it going to go up or is it going to go down today of 97
does it go up to 100 what's going on there uh and so our we're looking forward to the next piece of data which
says hey tomorrow's is going to you know today's humidity is this this is what tomorrow's humidity is going to be
that's all that is all that is is stacking our data so that our y is basically
x plus 1 or x could be y minus 1.
and then a couple things to note is our x data we're only dealing with the one column
but you need to have it in a shape that has it by the columns so you have the two different numbers and since we're
doing just a single point of data we have and you'll see with the train y
we don't need to have the extra shape on here now this is going to run into a problem
and the reason is is that we have what they call a time step
and the time step is that long-term short-term memory layer so we're going to add another reshape on
here let me just go down here and put it into the next cell and so we want to reshape the input
array in the form of sample time step features we're only looking at one feature
and i mean this is one of those things when you're playing with this you're like why am i getting an error in the numpy array why is this giving me
something weird going on uh so we're going to do is we're going to add one more level on here instead of being 12.99 one
we want to go one more and when they put the code together in the back you can see we kept the same
shape the 1299 we added the one dimension and then we have our train x shape one
and this could have depends again on how far back in the long short term memory you want to go that is what that piece of code is for
and that reshape is and you can see the new shape is now one 1299 one one versus the 1299 one and
then the other part of the shape 432 one one again this is our tr our x in and of
course our test x and then our y is just a single column because we're just doing one output that we're looking for
so now we've done our 80 percent um you know that's all the the writing all the code reformatting our
data bringing it in now we want to go ahead and do the fun part which is we're going
to go ahead and create and fit the lstm neural network and if we're going to do that the first
thing we need is we're going to need to go ahead and create a model and we'll do this sequential model and if you remember sequential means it
just goes in order that means we have if you have two layers the layers go from
layer one to layer two or layer zero to layer one this is different than functional
functional allows you to split the data and run two completely separate models and then bring them back together
we're doing just sequential on here and then we decided to do the long short term memory uh and we have our input
shape uh which it comes in again this is what all this switching was we could have easily made this one two three or
four going back as far as the end number on there we just stuck to going back one
and it's always a good idea when you get to this point where the heck is this model coming from
what kind of models do we have available and there's let me go and put the next
model in there because we're going to do two models and the next model is going to go ahead and we're going to do dent so we have model
equal sequential and then we're going to add the lstm model and then we're going to add a
dense model and if you remember from the very top of our code where we did the imports oops there we
go our cross this is it right here here's our importing a dense model and here's our importing an lstm now just about every
tensorflow model uses dents your dense model is your basic
forward propagation reverse propagation error where it does
reverse propagation to program the model so any of your neural networks you've already looked at that
luxon says here's the error and sends the error backwards that's what this is the long short term memory is a little
different the real question that we want to look at right now is where do you find these
models what kind of models do you have available and so for that let's go to the cross website
which is the cross dot io if you go under api layers and i always have to do a search
just search for karass api layers it'll open up and you can see we have
your base layers right here class trainable weights all kinds of stuff like there your activation
so a lot of your layers you can switch how it activates uh relu which is like your smaller arrays or if you're doing
convolutional neural networks the convolution usually uses a relu um your sigmoid all the way up to soft mac soft
plus all these different choices as far as how those are set up and what we want to do is we want to go ahead and if you
scroll down here you'll see your core layers and here is your dense layer so you have an input object your dense
layer your activation layer embedding layer this is your your kind of your one set
up on there that's most common uh convolutional neural networks or convolutional layers these are like for
doing uh image categorizing uh so trying to find objects in a picture that kind
of thing we have pooling layers so as you have the layers come together usually bring them down into
a single layer although you can still do like global max pulling 3d and there is just i mean this list just goes on and
on there's all kinds of different things hidden in here as far as what you can do and it changes you know you go in here
and you just have to do a search for what you're looking for and figure out what's going to work best
for you as far as which project you're working on uh long short term memory is a big
one because this is when we start talking about text uh what if someone says the what comes
after the uh the cat in the hat little kid's book there
it starts programming it and so you really want to know not only what's going on but it's going to be
something that has a history the history behind it tells you what the next one coming up is now once we've built all
our different you know we built our model we've added our different layers we went in there um play with it remember if you're in
functional you can actually link these layers together and they branch out and come back together if you do a
the sub setup then you can create your own different model you can embed a model in
there that might become linear regression you can embed a linear regression model as part of your functional split and
then have that come back together with other things so we're going to go ahead and compile your model this brings everything
together we're going to put in what the loss is which we'll use the mean squared error and we'll go ahead and use the atom
optimizer clearly there's a lot of choices on here depending on what you're doing and just like any of these
different prediction models if you've been doing any uh scikit from python uh you'll recognize
that we have to then fit the model uh so what are we doing in here we're gonna send in our train x our train y
we're going to decide how many epics we're going to run it through 500 is probably a lot for this
i'm guessing it'd probably be about two or three hundred probably do just fine our batch size
so how many different when you process it this is the math behind it if you're in data analytics
you might try to know what this number is as a data scientist where i haven't had the phd level math that says this is
why you want to use this particular batch size you kind of play with this number a little bit you can dig deeper into the math
see how it affects the results depending what you're doing and there's a number of other settings on here we did verbose 2. i'd have to
actually look that up to tell you what verbose means i think that's actually the default on there if i remember correctly
there's a lot of different settings when you go to fit it the big ones are your epic and your batch size those are what we're looking
for and so we're going to go ahead and run this and this is going to take a few minutes
to run because it's going through um 500 times
through all the data so if you have a huge data set this is the point where
you're kind of wondering oh my gosh is this going to finish tomorrow if i'm running this on a single machine
and i have a tera terabyte of data going into it
if this is my personal computer and i'm running a terabyte of data into this um you know this is running rather quickly
through all 500 iterations uh but yeah a terabyte of data we're talking something
closer to days week you know even with a
3.5 gigahertz machine and in eight cores it's still going to take a long time to
go through a full terabyte of data and then we want to start looking at putting it into some other framework
like spark or something that will print the process on there more across multiple processors and multiple computers
and if we scroll all the way down to the bottom you're going to see here's our square mean error of .0088
if we scroll way up you'll see it kind of oscillates between 0.088 and 0.08089
it's right around 2 to 250 where you start seeing that oscillation where it's really not going anywhere so we really
didn't need to go through a full 500 epics uh you know if you're retraining the stuff over and over again it's kind of
good to know where that error zone is so you don't have to do all the extra processing of course if
you're going to build a model we want to go ahead and run a prediction on it so let's go ahead and make our
prediction remember we have our training test set and our test set or the
we have the train x and the train y for training it or train predict and then we have our
test x and our test y going in there so we can test to see how good it did
when we come in here we have you'll see right here we go ahead and do our train predict equals
model predict train x and test predict model predict test x
why would we want to run the prediction on train x well it's not 100 on its prediction we know it has a certain
amount of error and we want to compare the error we have on what we programmed it with with the error we get when we
run it on new data that's never seen the model has never seen before and one of the things we can do we go ahead and
invert the predictions this helps us level it off a little bit more
get rid of some of our bias we have train predict equals and np exponential m1 the train predict and
then train y equals the exponential m1 for train y and then we do the again that with train test predict and test y
again reformatting the data so that we can it all matches and then we want to go ahead and calculate the root mean
square error so we have our train score which is your math square root times the
mean square root error train y and train predict and again we're just
this is just feeding the data through so we can compare it and the same thing with the test and let's take a look at that because
really the code makes sense if you're going through it line by line you can see we're doing but the answer really
helps to zoom in so we have a train score which is 2.40
of our root mean square error and we have a test score of 3.16 of the
root mean square error if these were reversed if our test score
is better than our training score then we've over trained something's really wrong at that point you got to go
back and figure out what you did wrong because you should never have a better result on your test data than you do
when you're training data and that's why we put them both through that's why we look at the error for both the training and the testing
when you're going out and quoting you're publishing this you're saying hey how good is my model it's the test score
that you're showing people this is what it did on my test data that the model had never seen before this is how good
my model is and a lot of times you actually want to put together like a little formal code
where we actually want to print that out and if we print that out you can see down here
test prediction standard deviation of data set 3.16 is less than 4.40
i'd have to go back and we're up here if you remember we did the square means error this is standard
deviation that's why these numbers are different it's saying the same thing that we just talked about
3.16 is less than 4.40 model is good enough we're saying hey this is this model is valid we have a valid model
here so we can go ahead and go with that uh and along with putting a formal print out of there
we want to go ahead and plot what's going on uh and this we just want to pretty graft
here so that people can see what's going on when i walk into a meeting and i'm dealing with a number of people
they really don't want these numbers they don't want to say hey what's i mean standard deviation unless you know what statistics are
um you might be dealing with a number of different departments head of cells might not work with standard deviation
or you have any idea what that really means number wise and so at this point we really want to put it in a graph so
we have something to display and with displaying you remember that we're looking at the data
today going into it and what's going to happen tomorrow so let's take a quick look at this we're
going to go ahead and shift the train predictions for plotting we have our train predict plot
np empty like data set train predict plot set it up with null values
you know it's just kind of it's kind of a weird thing where we're creating the um the
data groups as we like them and then putting the data in there is what's going on here so we have our
train predict plot which is going to be our look back our length plus look back
we're just is going to equal train train predict so we're creating this basically we're
taking this and we're dumping the train predict into it so now we have our nice train predict plot
and then we have the shift test predictions for the plotting uh we're going to continue more of that oops looks like i put it in here double no
it's just uh yeah they put it in here double um didn't mean to do that
we really only need to do it once oh here we go um this is where the problem
was is because this is the test predict so we have our training prediction we're
doing the shift on here and then the test predict we're going to look at that same thing we're just creating those two
data sets uh test predict plot length prediction setup on
there and then we're going to go through the plotting the original data set and the predictions so we have a time axis
always nice to have your time set on there set that to the time array
time axes lap all this is setting up the time variable for the bottom and then we have a lot of
stuff going on here as far as setting up our figure let's go ahead and run that
and then we'll break it down we have on here our main plot we have two different
plots going on here the ispu going up and the data and the ispu here with all these different
settings on it and so we look at this we have our ax1
that's the main plot i mean our ax that's the main plot and we have our ax1
which is the secondary plot over here so we're doing a figure plt or plt.figure
and we're going to dump those two graphs on there and so we take and if you
go through the code piece by piece which we're not going to do we're going to do the um the
data set here exponential reverse exponential so it looks correctly we're going to label it
the original data set we're going to plot the train predict plot that's what we just created
we're going to make that orange and we'll label it train prediction uh test predicts plot we're going to
make that red and label it test prediction and so forth set our ticks up this actually just put
ticks time axes gets its ticks the little little marks there going along the axes that kind of thing and
let's take a look and see what these graphs look like and these are just kind of fun you know when you show up into a meeting and this
is the final output you say hey this is what we're looking at here's our original data in blue
here's our training prediction you can see that it trains pretty close to what the data is up there
i would also probably put a like a little little time stamp and do
just right before and right after where we go from train to test prediction
and you can see with the test prediction the data comes in in red and then you can also see what the
original data set looked like behind it and how it differs and then we can just isolate that here
on the right that's all this is is just the test prediction on the right uh and it's
you know there's you'll see with the original data set there's a lot of peaks were missing and a lot of lows were missing
but as far as the actual test prediction it's pretty does pretty good it's pretty
right on you can get a good idea what to expect for your ispu and so from this we would probably
publish it and say hey this is what you expect and this is our area of this is a range of error that's the kind
of thing i put out on a daily basis maybe we predict the cells are going to be this or maybe weekly so you kind of get a nice you
kind of flatten the data coming out and you say hey this is what we're looking at the big takeaway
from this is that we're working with let me go back up here oops oh too far
there we go is this model here this is what this is all about we worked through all of those
pieces all the tensorflows and that is to build this sequential model and we're only putting in the two
layers this can get pretty complicated if you get too complicated it never
it never verges into a usable model so if you have like 30 different layers
in here there's a good chance you might crash it kind of thing so don't go too haywire on that and that
you kind of learn as you go again it's domain knowledge um and also starting to understand all
these different layers and what they mean the data
analytics behind those layers is something that your data analysis
professional will come in and say this is what we want to try but i tell you as a data scientist
a lot of these basic setups are common and i don't know how many times
working with somebody and they're like oh my gosh if i only did a tangent h instead of a relu activation
i worked for two weeks to figure that out well the data science i can run it through the model in you know five
minutes instead of spending two weeks doing the the math behind it so that's one of the advantages of data
scientist is we do it from programming side and a data analytics is going to look for it how does it work in math
and this is really the core right here of tensorflow and cross is being able to
build your data model quickly and efficiently and of course uh with any data science
putting out a pretty graph so that your shareholders again we want to take and
reduce the information down to something people can look at and say oh that's what's going on they can see stuff
what's going on as far as the dates and the change in the ispu now let's talk a little bit about
recurrent neural networks so neural networks are of different types we have cnn we have rnn and so on now rnn is one
type of neural network rnn stands for recurrent neural network the networks like cnn and a n are feed forward
network which means that the information pretty much only goes from left to right or from front to back whichever way you
call it whereas in case of recurrent neural network there will be some information traveling backwards as well
so that is why it is known as a recurrent neural network and each of these types have a specific application
so for example convolutional neural networks or cnn are very good for doing image processing and
object detection using for video and images whereas recurrent neural networks are pretty good for doing nlp or speech
recognition and so on okay so for the next few minutes we will kind of focus on recurrent neural networks and we will
see an example of how we can use rnn to do a time series analysis so in a
typical neural network this is how it looks right so where you have a neuron
and then the inputs are coming to the neuron and then you have an output which goes to other neurons in the other
layers in case of recurrent neural network what happens is you have the inputs let's say at a given point in
time but then a part of the previous output also gets fed in along with the
inputs for the given time now this can be little confusing so let's see if we
can take a little expanded view of this so this is another view of one single
neuron which is what is known as in an unfolded manner okay so if we are
getting inputs or data over a period of time then this is how the neuron will
look remember these are not three neurons this is one neuron and it is what is known as it is shown in a
unfolded way okay we are on we have unfolded this single neuron over a period of time so at a given time let's
say t minus 1 an input of x t minus 1 is fed to the neuron and it gets a output
of y t minus one then the next instant which is x t at a time t right there is
an input of x t and then there is an output of y t so this is a single neuron
but is displayed in an unfolded weight so this is like expanding it over a period of time so let's start with this
part here this particular neuron gets an input at at an instant t minus 1 let's
say time is equal to t minus 1 it gets an input of x t minus 1 and it gives an
output of y t minus then when we go to the instant t here it gets an input of x
t and it also additionally gets an input from this previous time frame t minus 1.
so this y t minus 1 gets fed here and that results in y t all right then when
we go to the time t plus 1 there is an input of x t minus 1 at that given time
plus the input from the previous time frame which is a time frame of t that also gets fed in here and then we get an
output of y t plus one okay so let me explain once again this is a single neuron so these are not three different
neurons a single neuron seen over a period of time from t minus one to t plus one and unlike a regular feed
forward neuron which only gets x in this case the input is x and also another
input which is coming from the previous time frame and that is what this loop is in this on the left hand side diagram
this loop is indicating that okay so on the right hand side it is represented in an unfolded way okay so the input if we
take a time frame t for example is not only x t which is the input which is the
normal input at the time frame t but it is also getting an input from the
previous time frame which is this yt minus 1 which is also being fed as an
input that is the key differentiator here okay similarly at the instant t plus 1 it is getting a normal input of x
t plus 1 plus this y t is actually being fed here and then the output comes as y
t plus 1 okay so this is the construct of a recurrent neural network now recurrent neural networks again can be
of different subtypes it can be one to one one too many many to one and many
too many depending on what kind of application we want to use one of the examples is like the stock price so
you're feeding so there is only one input only thing is it is spread over a period of time so you're feeding the stock price input that is what comes
here as an input and you get an output which is again the stock price which is probably predicted over the next two
days or three days or 10 days or whatever so the number of outputs and inputs is the same there is only one
input there is only one output only thing is it is spread over a time so variables are not many then you have one
too many so there is one input but you are expecting multiple outputs what is an example of this let's say you want to
caption an image so how do you want to do that what is the input that will go here is just an image which is one input
but what is output you are looking for you are looking for a phrase maybe right not just a word but a phrase so it's
like cat is sitting on a map right so the images there is only one image which consists of the cat sitting on a mat but
the output are like maybe three or four words which is saying the cat is sitting on a mat so this is one too many right
then you can have many to one examples of this are like you're feeding some text and you want to know the output
whether the text is what kind of sentiment is expressed by the text it could be positive it could be negative
so that's the output you're looking for so you feed a large number of words maybe the text may consist of words or
lines or whatever so that is what is the multiple inputs but the output all it says is the sentiment is positive so it
is just one output or the sentiment is negative just one output right so this is many to one then we have many too
many so what is an example of this let's say you want to do some translation so how do you do a translation you feed in
a sentence maybe in a particular language english and then you want another a sentence actually in another
language so that is like there are multiple inputs one sentence can consist of multiple words and the output also is
a sentence consisting of multiple words all right so how do we implement rnn
this is an example of implementing an rnn for a particular use case so in our
particular example we have the data about milk production over a few months
and using rnn we want to predict because this is time series analysis so rnn is
good to do time series analysis so using rnn we want to predict what will be the
milk production in the future so let us see how we can do that i will first take you through quickly through the slides
and then i will actually run this in in a jupiter notebook the code i will run it in the jupyter notebook so this is
how it looks the code looks so while this is tensorflow you still use the standard python libraries like
numpy and pandas and even matplotlib to do some initial processing getting the data processing it cleaning it whatever
all that can be still done in within the same program so that's what we're doing here we're importing some libraries like
numpy and pandas and then we read the data file and if we plot the data we see here it is the data for for the years
1962 to 75 and we can see that there is a certain trend right so this is how a
typical time series data would look again some of you if you are not familiar with time series and time
series analysis and so on i would recommend you to go through some videos around that which will make it easy to
understand this so this is our typical time series data would look in this case it is nothing but there is only one
variable which is milk production and it is spread across several years so this
is going from 1962 to 75 and that value has been plotted so if you see there is
a certain kind of a trend here which is basically going upwards and there will be some seasonality so time series data
has these three components right so it will have a trend it will have a seasonality it will have seasonality and
then it will have some randomness so that's what this graph is showing and
now if we want to perform analysis on this time series analysis on this first
thing we need to do is split the data into train and test and in this case we will just use a straightforward method
which is uh taking the data for the first 13 years so the idea here is we need to train our model right this is
time series data so what and we want to predict for the future so the way we need to use this is we have to take the
data for a certain period of time and train our model and then we use a part
of the known data so that we can then ask our model to predict for that duration and compare it with the known
information so what do we mean by that so here if you see this data i will show you in the notebook as well jupiter
notebook as well this has 13 years of data so what we are doing is we are taking the first 12 years and using that
as our training set right so 12 years we are doing and this has about i think 14 years of data so we are taking 13 years
of data for training purpose and then we are using the last one year remaining
one year of data for testing purpose because here what we can do is this one year data we know the values right
because if we want to compare the accuracy or anything like that we need to know the value so in this case we
know the values of this one particular year so we will use that but at the same time we train the model for 13 years of
data and for the 14th year we will ask our model to predict and then compare it with this known value so that we know
how accurate our model is i hope that makes sense okay so that's what we are doing here 156 is nothing but 12 into 13
on a monthly basis we get the data the next step is to scale the data this is all regular data manipulation data
munching activity and then you split it you are basically assigning the train and test data to the to the scaled
variables and then you need to read the data in batches so
it is very important as i mentioned earlier also that instead of reading the entire data in one shot you feed the
data in batches so in order to do that we are writing a function for that that's all we are doing here so that is
still here what we have done is regular python programming there is no tensorflow as of now here okay so so far
what we have done is preparation data preparation data munching now what we are doing is actually training our model
so this is where tensorflow now comes into picture so we are importing first step is to import the tensorflow library
so this is how you do it import tensorflow sdf and then you can define some variables or constants whichever
way now here of course there are a couple of ways of doing it you can create them as tensorflow nodes but that
is a little bit of an overhead we will just use the regular python variables are constant so here i am
creating regular python variables and i am saying number of inputs is one so instead of hard coding i'm just storing
them as variables so this is number of inputs is one number of time steps is 12
number of neurons is hundred and so on right and then learning rate is 0.03 and number of iterations we are saying is
4000. then we create placeholders now we will be storing the independent variables in x and the dependent
variables in y and this we will read from an external file remember i told you placeholders are used for getting
data from outside and then feeding it to our model so that's the reason we have two placeholders one for reading the x
values and another for reading the y values in this step we are only just creating the nodes right so this is just
creating the graph and similarly we are mentioning what is the loss function and what is optimizer and how to run the
optimizer and once that is done you are initializing the variables and then you are creating a saver variable or a saver
instance so saver is basically nothing but in machine learning you can train your model and you can save it for later
use so that's where the saving comes into play we will see how to use that as well and then the last step is to create
your session and run this graph right so we are initializing the variables remember this we run init so which is
nothing but this one so we are initializing the variables whatever are there instead of hard coding remember in
earlier case we were hardcoding how many iterations so here we are saying for the given number of iterations which are
stored in this maybe how many iterations we said is what is the value of iterations this is training iterations
is 4000. so we specify that based on the number of training iterations next batch
will basically fetch the data and then we run the session we are basically saying train is the node which we will
run in the session and this will basically train our model and this is
more for printing every hundred times you print it that's all this there's nothing more to it so for example the
output would look somewhat like this this is for 0 this 400 next for 200 and so okay and then you save the model you
remember saver we created so you save the model for later use so this is our test data remember we are doing this on
training data right so once the training data training is done we then try to
create the inference on the test data so that we can compare how accurate this is
right so this is how the test set would look and then we will basically restore this
model okay because in the previous slide we created the model and we stored it so now we have to restore the model and
then run our test data against it and see what are the values that are predicted what are the y values and then
compare with x to see the accuracy so train seed is what we are seeing here so
that is what will have the predicted values and these predicted values what we want to
do is we want to create an additional column because remember we need to compare because we want to find out the
accuracy of this model so we need to compare the predicted values with the actual value so what we are doing here
is adding another column called generator and assign a value to that all right so this
is the result of the prediction and then if we need to reshape because we have to
show this in the form of a monthly results so that's what we're doing here and
once we generate the results and then display it we can actually see it
month-wise here and the actual and the generated values okay so we create a data frame which is a
combination of the actual values and the predicted values from the test set and then we can plot to see how the trend is
as you can see pretty much the actual value is in blue color and the generated or predicted value the curve is in
yellow color the trend is maintained so it will probably it's not a hundred percent accurate but the trend is
maintained all right so let's do one thing let's go into the jupiter notebook and take a look at how this works in
tensorflow environment actually the code i will walk you through the code this is my jupyter notebook and
the data is taken from this particular link in case you're interested you can
download it from there and this is the data for the years 1962-75 the first thing we do is import
these libraries because before we start with the actual tensorflow activity we need to prepare the data and so on so
for that you can use your regular python libraries which are like numpy pandas and so on so that's what we're doing
here and then we read the data using pandas into a data frame so this is a
data frame milk is a data frame and we will do some quick exploratory analysis
just to see how our data is looking initials five records that we'll get one two three four five that is ahead so it
goes from 1960 to january to 1960 to may once we re-change or basically we need
to split this into month specifically separately so that's what we are doing here date to time so we kind of do a
little bit of re-indexing and then if we plot this this is how the data look as you can see there is a clear upward
trend and there is some seasonality and so on but anyway we will not break that up into
those components we will just use rnn to predict and test okay so the next thing
is to check some if we can do run like info it will tell us what is the
information about this data set so let us just run that okay so it is just telling us how many total columns and
what is the size of the file and so on and so forth again in case you are interested in doing some exploratory
analysis so what we'll do next is we will take the 13 years of data for our
training data set so what are we going to do now let me step back so what have we seen here we have seen that there are
168 records so which is nothing but 14 years of data we now have to split this
into our training and test data set so how do we do that i think in case of time series we cannot do it like 80 20
like we do in normal splitting in normal machine learning process here it is time series data so what we are going to do
is we will take out of this 14 years we will take the first 13 years of data and we will use that for training and then
we will test it with the last which is the 14th year data we will use it for testing okay so that's what we are going
to do here so let's uh split that so training set will consist of my 156
records which is nothing but 12 13 into 12 right my 13 years of data i'm taking
for training and then my test set will consist of the bottom 12 observations
which is last one year of data which is the 14th year okay so that's now done training and test splitting is done the
next step is to do some normalizing which is basically we'll use our scaling rather we will use min max scala and we
will just scale the data and now we do it for both test as well as train now we are ready to create our rnn model but
before that one quick thing in order to fetch the data we have to create a function so that's what we are doing
here we create a function called next batch and how you want to fetch the steps they are all defined as our
constants if you remember and that's what this function is all about so we define that function we will be calling
that in our training method all right so from here onwards up to there we are done with the preparation of the data
and whatever functions or whatever are required from here onwards is where the tensorflow part starts so the first
thing we do is import the tensorflow library typically this is a very standard way of importing the library we
say import tensorflow as tf now tf is nothing but a name so you can change it to tf1 or abc or xyz whatever so this
part you can change but by and large everybody uses this so i would rather recommend you also use the same syntax
so you say import tensorflow stf so it's very easy for others to understand as well and then you declare or define
a bunch of constants uh that's what we are doing here like for example the number of uh time steps in this case it
is 12 number of neurons there are 100 number of outputs it is only one and so on right and then the learning rate and
all that we are declaring or defining those variables in this particular block next is to create placeholders you
remember the placeholders are used for feeding the data so we have x and y x is for the input which is the independent
variable and y is the output which is the dependent variable and in our case these are not different characteristics
or features but it is the same one feature or one variable but it is over a period of time so that's the only
difference so that's what we are doing here and now we need to create our network right the neural network so in
our case we said we will create a rnn layer now there are different ways or different formats of rnn which is
for creating our layer so each cell will be with an output projection wrapper
there is a need for doing that and again the details of this we will probably do in another video where we talk in detail
about rnn so for now we are creating a gre cell with the wrapper and the the
syntax for creating the gru cell is like this the number of units which we we
said there should be 100 neurons what will be the activation function in this case it is relu and then number of
outputs in our case it is only one okay so we create the cell here okay then we
got the gru cell and then we say what is our output which is nothing but we get
the outputs and the states and their states and which is uh the way we get that is tf.nnn
dot dynamic underscore rnn so if we call this and we pass the cell and the data
which is basically in placeholder again remember all we are doing here is we are creating just the nodes for the graph so
nothing is actually getting executed from a tensorflow perspective okay all right so once that is done now we need
to pass this calculate the outputs and the states using the dynamic underscore rna method and we pass the cell as a
parameter and then the x values as a parameter and if we run that
we will have the outputs and the states and this is where we actually run the
training method so or create not really run the training method but we create the nodes for the
training and optimizer then we have the initialization of the variables and we
save this model we create a saver object just to save the model because we will then restore it and run it to do our
predictions so this is what we will do here so that is another node and this is where we actually create a session and
run the training okay so let's go ahead and do that that will take a little while because we said 4000 iterations so
we will allow it to finish we will probably come back once the training is
completed all right so the training is done now let's go and
run this on the test data so let's just quickly take a look at the test data set so this is our one year data for the
year of 1975 so if we see this is from january february and so on and we will pass this to our
model so what are we doing here we are restoring the model here for example right this is the path that we have
given when we were saving the model let's go back once and show you let me show you where we did the saving of the
model yes so we save the model here so that again we will restore that and we will use that to run it on our test data
and see how well it predicts so let's go ahead and run this and this is just 12
records so this will not take time remember i told you training is what takes a lot of time the regular
inference this is called inference doesn't take much time right so it just depends on how much data there were only
12 records here so it was very quick but in training what we do we pass this multiple times there are iterations 4
000 iterations we did for example so that takes longer and in general in machine learning deep learning training
is what takes the maximum amount of time all right so let's go ahead and see the
results we will in order to plot it we will have to kind of adjust the format
of the results otherwise we will not be able to see it in a proper way and then we will so this is our so what we have
done is we created a data frame which consists of the predicted value and and the actual value so this is the data
frame so this is the actual value this 8 834 782 and so on this is what our model
has predicted so it may not be so easy to see in a tabler format so let's go ahead and plot it so that we can compare
these two so when we plot it you will see that the trend is more or less
maintained right so we go from the blue color is the actual values and the
yellow color or the orange color whatever is it is the one which is generated by our model so it pretty much
is following the actual trend so not bad for such a quick iteration and
training this tutorial is about object detection we will walk you through a tensorflow code using which we will do
object detection in images we will tell you what are the libraries that are required a little bit about the cocoa
data set and then we will show you the implementation code itself a demo of the code all right so let's get started so
what is the tensorflow object detection api this is an open source framework
which is actually provided by the tensorflow team and there are trained models available and the sample code is
also available which we can use in order to easily detect objects in images and
videos this is pretty robust and can detect objects fairly quickly and this
is very easy for people to use as well people with very less or no knowledge of
machine learning or deep learning can also with a little bit of python programming knowledge can actually use
this api this library to build object detection applications this is a list of
libraries that are required and they have been shown in the code as well the
exact purpose of each of this library why it is required is out of the scope of this tutorial but we
will see in the code as we walk through some of these libraries how and why they are used the cocoa data set coco stands
for common objects in context so this data set consists of 300 000
images of 90 most commonly found objects like
chairs and tables and so on and so forth so this model has been trained or in fact a
set of models have been trained on this data set and this is pretty good to
detect the most common objects in the images and videos so with that let's get
into the code all right so the first part is to import all these libraries and this we have shown you in the slides
as well again a large part of this will be for doing some helper functions and
maybe for visualizing the images and so on and so forth so that's the reason they are required as i said the exact
details of each and every library probably is out of scope but these are needed so as a first step maybe you just
go ahead and include these libraries and run the code and maybe a later point we
can discuss what each of these libraries does now this will work with tensorflow
version higher than 1.4 so if you are having tensorflow version below 1.4 you
may have to upgrade to a higher version so let me go ahead and execute the cell
and we also need this line of code to make sure that once we
run this object detection the labeled images are displayed within this
notebook and many of you by now must be familiar with this and we will import a few utility
libraries and you will see that we will be using some of these for visualization purpose so once the
objects are detected we need to display the information what that object is and then what percentage of confidence the
model has so all these details that's the utility functions that stored here
and then the next part is to prepare the model as i mentioned we will be using an
existing trained model the tensorflow team has actually provided these models the one that we
will be using is ssd with the mobile net but you can actually use any one of the
ones that are listed in this url let me just quickly take you through this url
these are a bunch of models trained models that are readily available for
anybody to use it is open source and let me scroll down the only thing is that if
there are some of them with the accuracy is much higher but they take longer and
there are some where the accuracy is not so high and they are much faster so they
are faster but the accuracy may not be that very high so you can play around with some of these and we in this
particular exercise or in this particular tutorial we are using this ssd model which is ssd mobile at
question one so that's the model that we'll be using so in this cell we are primarily creating a bunch of variables
with the various for example the name of the model the path and so on and so forth so that we will be using these
names in the next step which is to download this model and install it
locally these are also referred to as a frozen model so once they are trained
and then you kind of extract or you freeze the model so that's the reason they are called frozen model so that
others can just use this without any further training so this is where we download and extract our model locally
so this will take a little while let me see if i can wait or maybe pause the
video and come back once it is done might take a little while let's see if it
completes i have a pretty high speed network but even then it takes some time so that's good but this part is over now
let us see this part and yes both are done so once that is done we need to
load some label mapping our basically what this will do is your model as you
may be aware by now if you do some classification the model will actually
not give any output text it will give some numbers so if there are five classes it will say okay
this belongs to modern class one or two or three or five and so on the numbers now each of these will obviously the
numbers will not make any sense to the outside world so we need to do some small mapping so in this case let's say
one maybe a chair two maybe a table three maybe a balloon and so on so that kind of number to text mapping we need
to do and that is what is being done in this particular cell and then we have a helper code which will load the image
and convert it into a numpy array so that the number array is what gets
processed and used by the model to do the detection part of it so that is what this
method is all about so we will be the later on we will be calling that function and
next is preparation for detection so here we are basically telling where the
images are stored and how many images or what is the naming convention or format
of the images now if you want you can modify this code for example currently i
have test underscore images as my folder so let me go and show this to you so this is under my object detection folder
i have another subfolder where i am storing my images which is text underscore images now you can rename
this folder and give some other name and then in your code you can probably give
that particular name for the subfolder similarly the format of these files what
is the name and format of this files here it is in a very simple format which is the names of the files are like beach
one beach two b3 and so on so i have taken beach as the theme so i have
images which are related to beaches so this is beach one beach two and then beach three a few others but we will use
these three for our demo and so that's what i'm saying here the name of the
images will be beach something dot jpeg which is jpeg format and in this curly
braces basically we will will be filled with either one two or three based on in this
particular for loop okay so that is what this is doing all right so the next step is to run inference on these images in a
loop and what we are basically doing here is getting these images one by one and then
running through the model to find out what are the objects that can be detected and then against each of the
object a box will be drawn and it will be labeled with the name and the
percentage of accuracy or confidence that the model detects these objects
okay so that is the function here and so let me just run that piece of code and here is basically
where we are calling this function so we are loading this images and then we are calling this function for each image and
then we are displaying this using the matplotlib library so let's run this it
will take one image at a time and then detect the images now the beauty is that
the same format of the code can be used for doing object detection in a video so
we have another video for doing object detection in a video so most of the code
out there will be reused from here and the only thing is that instead of reading the images from the local
storage we read the frames from the video and there is a neat little video reader that
is available and it will be shown in the other video and frame by frame we read
the video and then pass on to this function and it will act as if each of
these frames is an image and then it will do the same object detection on the
entire video so that's in a separate video just uh look out for that and actually the information about that is
uh provided in the cards the eye symbol so that's the the video object detection
in video that's the separate uh tutorial all right so now that we have all the pieces together this the last cell is
where the whole action takes place so let's run this and see how it looks so
it will take probably a little while and there are about three images let's see
what it detects there we go so good so the first one it has detected a person
and that too with 97 percent accuracy which is i think pretty good
okay and then the next image it detects umbrella and chair there are a few other
objects but it's not able to detect it has detected umbrella with 63 percent
accuracy or confidence rather and the chair with 58 percent again not bad
then let's see the next image so here these are actually balloons hot air
balloons but the model thinks it is a kite which is uh probably not that bad
it sees there is something in the sky and therefore probably it thinks it is a kite and it detects that with 65 percent
confidence okay so that was pretty much all i wanted to show you in this
particular tutorial about object detection in images and in this
video i will walk you through the tensorflow code to perform object detection in a video so let's get
started this part is basically we are importing all the libraries we need a
lot of these libraries for example numpy we need image io datetime and pill and
so on and so forth and of course matplotlib so we import all these libraries and then there are a bunch of
variables which have some paths for the files and folders so this is regular
stuff let's keep moving then we import the matplotlib and make it inline and
a few more imports all right and then these are some warnings we can just ignore them so if i run this code once
again we could go away right and then here onwards we do the model preparation
what we are going to do is we're going to use an existing neural network model so we are not going to chain a new one
because that really will uh take a long time and uh it needs a lot of computation resources and so on and it
is really not required there are already modules that have been trained and in this case it is the ssd with mobilenet
that's the model that we are going to use and this model is trained to detect objects and it is readily available as
open source so we can actually use this and if you want to use other models
there are a few more models available so you can click on this link here and
let me just take you there there are a few more models but we have chosen this particular one because this is faster it
may not be very accurate but that is one of the faster models but on this link you will see a lot of other models that
are readily available these are trained models some of them would take a little longer but they may be more accurate and
so on so you can probably play around with these other models okay so we will
be using that model so this piece of code this line is basically importing
that model and this is also known as frozen model the term we use is frozen
model so we import download and import that and then we will actually use that
model in our code all right so these two cells we have downloaded and import the model
and then once it is available locally we will then load this into our program all
right so we are loading this into memory and
you need to perform a couple of additional steps which is basically we need to map the numbers to
text as you may be aware when we actually build the model and when we run predictions the model will not give a
text the output of the model is usually a number so we need to map that to a
text so for example if the network predicts that the output is 5 we know
that 5 means it is an airplane things like that so this mapping is done in
this next cell all right so let's keep moving and then we have a helper code which will basically load the data or
load the images and transform into numpy arrays this is also used in doing object
detection in images so we are actually going to reuse because video is nothing but it consists of frames which in turn
are images so we are going to pretty much use reuse the same code which we
use for doing object detection in images so this is where the actual detection
starts so here this is the path for where the images are stored so this is
here once again we are reusing the code which we wrote for detecting objects in
an image so this is the path where the images were stored and this is the extension and this was done for about
two or three images so we will continue to use this and we go down i'll skip this section so
this is the cell where we are actually loading the video and converting it into frames and then using frame by frame we
are detecting the objects in the image so in this code what we are doing basically is there are a few lines of
code what they do is basically once they find an object a box will be drawn around those
each of those objects and the input file the name of the input video file is
traffic it is the extension is mp4 and we have this video reader it's a
excellent object which is basically part of this class called image i o so we can
read and write videos using that and the video that we are going to use is
traffic.mp4 you can use any mp4 file but in our case i picked up video which has
like car so let me just show you so this is in this object detection folder i
have this mp4 file i'll just quickly play this video it's a little slow yeah okay so here we go this is the video
it's a short one relatively small video so that for this particular demo and
what it will do is once we run our code it will detect each of these cars and it
will annotate them as cars so in this particular video we only have cars we
can later on see with another video i think i have cat here so we can also try
with that but let's first check with this traffic video so let me go back so we will be reading this frame by frame
and you know actually we will be reading the video file but then we will be analyzing it frame by frame and we
will be reading them at 10 frames per second that is the rate we are mentioning here and analyzing it and
then annotating and then writing it back so you will see that we will have a
video file named something like this traffic underscore annotated and
we will see the annotated video so let's go back and run through this piece of code and then we will come back and see
the annotated video this might take a little while so i will pause the video
after running this particular cell and then come back to show you the results all
right so let's go ahead and run it so it is running now and it is also important
that at the end you close the video writer so that it is similar to a file
pointer when you open a file you should also make sure you close it so that it doesn't hog the resources so it's very
similar at the end of it the last piece or last line of code should be video underscore writer dot close all right so
i'll pause and then i'll come back okay so i will see you in a little bit all
right so now as you can see here the processing is done the hourglass has disappeared that means the video has
been processed so let's go back and check the annotated video we'll go back to my file manager so this was the
original traffic.mp4 and now you have here traffic underscore annotated mp4 so
let's go and run this and see how it looks
you see here just got each of these cars are getting detected let me pause and
show you so we pause here it says car 70 let us allow it to go a little further
it detects something on top what is that truck okay so i think because of the board on top it somehow thinks there is
a truck let's play it some more and see if it detects anything else so this is
again a car looks like so let us yeah so this is a car and
it has confidence level of 69 percent okay this is again a car all right so
basically till the end it goes and detects each and every car that is passing by now we can quickly repeat
this process for another video let me just show you the other video which is a cat again there is uh this cat is not
really moving or anything but it is just standing there staring and moving a little slowly and
our application will our network will detect that this is a cat and
even when the cat moves a little bit in the other direction it will continue to detect and show that it is a cat okay so
yeah so this is how the original video is let's go ahead and change our code to
analyze this one and see if it detects our network detects this cat close this
here we go and i'll go back to my code all we need to do is change this traffic
to cat the extension it will automatically pick up because it is given here and
then it will run through so very quickly once again what it is doing is this video reader video underscore reader has
a neat little feature or interface whereby you can say for frame in video
underscore reader so it will basically provide frame by frame so you're in a loop frame by frame and then you take
that each frame that is given to you you take it and analyze it as if it is an image individual image so that's the way
it works so it is very easy to handle this all right so now let's once again
run just this cell the rest of the stuff remains the same so i will run this cell
again it will take a little while so the r classes come back i will pause and then come back in a little while all
right so the processing is done let's go and check the annotated video go here so
we have get annotated dot mp4 let's play this all right so you can see here it is
detecting the cat and in the beginning you also saw it detected something else here there looks like it detected one
more object so let's just go back and see what it has detected here let's see
yes so what is it trying to show here it's too small not able to see but
it is trying to detect something i think it is saying it is a car i don't know all right okay so in this video there's
only pretty much only one object which is the cat and uh let's wait for some time and see if it continues to detect
it when the cat turns around and moves as well just in a little bit that's going to happen
and we will see there we go and in spite of turning the other way i think our
network is able to detect that it is a cat so let me freeze and then show here
it is actually still continues to detect it as a cat all right so that's pretty
much it i think that's the only object that it detects in this particular video okay so close this so that's pretty much
it thank you very much for watching this video in this tutorial we will take the
use case of recognizing handwritten digits this is like a hello
world of deep learning and this is a nice little mnist database is a nice little
database that has images of handwritten digits nicely formatted because very
often in deep learning and neural networks we end up spending a lot of time in
preparing the data for training and with mnist database we can avoid that
you already have the data in the right format which can be directly used for training and mnist also offers a bunch
of built-in utility functions that we can straight away use and call those
functions without worrying about writing our own functions and that's one of the reasons why mnist database is very
popular for training purposes initially when people want to learn about deep learning and tensorflow this is the
database that is used and it has a collection of 70 000 handwritten digits
and a large part of them are for training then you have test just
like in any machine learning process and then you have validation and all of them are labeled so you have the images and
their label and these images they look somewhat like this so they are hand written images collected from a lot of
individuals people have these are samples written by human beings they
have hand written these numbers these numbers going from zero to nine so people have written these numbers and
then the images of those have been taken and formatted in such a way that it is
very easy to handle so that is mnis database and the way we are going to
implement this in our tensorflow is we will feed this data
especially the training data along with the label information and the data is basically these images are
stored in the form of the pixel information as we have seen in one of the previous slides all the images are
nothing but these are pixels so an image is nothing but an arrangement of pixels and the value of
the pixel either it is lit up or it is not or in somewhere in between that's how the images are stored and that is
how they are fed into the neural network and for training once the network is
trained when you provide a new image it will be able to identify within a
certain error of course and for this we will use one of the simpler neural
network configurations called softmax and for simplicity what we will do is we
will flatten these pixels so instead of taking them in a two-dimensional arrangement we just
flatten them out so for example it starts from here it is a 28 by 28 so there are 784 pixels so pixel number 1
starts here it goes all the way up to 28 then 29 starts here and goes up to 56 and so on and the pixel number 784 is
here so we take all these pixels flatten them out and feed them like one single
line into our neural network and this is a what is
known as a soft max layer what it does is once it is trained it will be able to
identify what digit this is so there are in this output layer there are 10 neurons each
signifying a digit and at any given point of time when you feed an image
only one of these 10 neurons gets activated so for example if this is strained
properly and if you feed a number 9 like this then this particular neuron gets
activated so you get an output from this neuron let me just use a pen or a laser to show you here okay
so you're feeding the number nine let's say this has been trained and now if you're feeding a number nine this
will get activated now let's say you feed one to the trained
network then this neuron will get activated if you feed two this neuron will get activated and so on i hope you
get the idea so this is one type of a neural network or an activation function
known as soft max layer so that's what we will be using here this is one of the simpler ones for
quick and easy understanding so this is how the code would look we will go into
our lab environment in the cloud and we will show you there directly but very
quickly this is how the code looks and let me run you through briefly here and then we will go into
the jupiter notebook where the actual code is and we will run that as well so
as a first step first of all we are using python here and that's why the syntax of the language is python and the
first step is to import the tensorflow library so and we do this by using this line of
code saying import tensorflow as tf df is just for convenience so you can name give any name and
once you do this tf is tensorflow is available as an object in the name of tf and then you can run its methods and
accesses its attributes and so on and so forth and mnist database is actually an
integral part of tensorflow and that's again another reason why we as a first step we always use this example mnist
database example so you just simply import mnist database as well using this
line of code and you slightly modify this so that the labels
are in this format what is known as one hot true which means that the label
information is stored like an array and let me just use pen to show what exactly
it is so when you do this one hot true what happens is each label is stored in
the form of an array of 10 digits and let's say the number is
8 okay so in this case all the remaining values there will be a bunch of zeros so
this is like array at position 0 this is at position 1 position 2 and so on and
so forth let's say this is position 7 then this is position 8 that will be 1
because our input is 8 and again position 9 will be 0 okay so one hot
encoding this one hot encoding true will kind of load the data in such a way that the
labels are in such a way that only one of the digits has a value of one and
that indicates so based on which digit is one we know what is the label
so in this case the eighth position is one therefore we know this sample data the value is eight similarly if you have
a two here let's say then the labeled information will be somewhat like this so you have your labels so you have this
as zero the zeroth position the first position is also zero the second
position is one because this indicates number two and then you have third as zero and so on okay so that is the
significance of this one hot true all right and then we can check how the data
is uh looking by displaying the the data and as i mentioned earlier this is
pretty much in the form of digital form like numbers so all these are like pixel
values so you will not really see an image in this format but there is a way to visualize that image i will show you
in a bit and this tells you how many images are there in each set so the training there are 55
000 images in training and in the test set there are 10 000 and then validation
there are 5 000 so all together there are 70 000 images all right so let's
move on and we can view the actual image by
using the matplotlib library and this is how you can view this is the code for viewing the images and you can view them
in color or you can view them in grayscale so the c map is what tells in
what way we want to view it and what are the maximum values and the minimum values of the pixel values so these are
the max and minimum values so of the pixel values so maximum is 1 because
this is a scaled value so 1 means it is white and 0 means it is black and in
between is it can be anywhere in between black and white and the way to train the model there is
a certain way in which you write your tensorflow code and the first step is to create some
placeholders and then you create a model in this case we will use the softmax model one of the simplest ones and
placeholders are primarily to get the data from outside into the neural network so this is a very common
mechanism that is used and then of course you will have variables which are your remember these are your weights and
biases so for in our case there are 10 neurons and each neuron actually has 784
because each neuron takes all the inputs if we go back to our slide here actually
every neuron takes all the 784 inputs right this is the first neuron it has it
receives all the 784 this is the second neuron this also receives all the 700 so each of these inputs needs to be
multiplied with the weight and that's what we are talking about here so these are this is a a matrix
of 784 values for each of the neurons and so it
is like a 10 by 784 matrix because there are 10 neurons
and similarly there are biases now remember i mentioned
biases only one per neuron so it is not one per input unlike the weights so
therefore there are only 10 biases because there are only 10 neurons in this case so that is what we are
creating a variable for biases so this is something little new in tensorflow
you will see unlike our regular programming languages where everything is a variable here the variables can be
of three different types you have placeholders which are primarily used for feeding data you have variables
which can change during the course of computation and then a third type which is not shown here are constants so these
are like fixed numbers all right so in a regular programming language you may have everything as variables are at the
most variables and constants but in tensorflow you have three different types placeholders variables and
constants and then you create what is known as a graph so tensorflow programming consists of graphs and
tensors as i mentioned earlier so this can be considered ultimately as a tensor
and then the graph tells how to execute the whole implementation so that the
execution is stored in the form of a graph and in this case what we are doing is we are doing a multiplication tf you
remember this tf was created as a tensorflow object here one more level one more so tf is available here now
tensorflow has what is known as a matrix multiplication or matmal function so
that is what is being used here in this case so we are using the matrix multiplication of tensorflow so that you
multiply your input values x with w right this is what we were doing x w
plus b you're just adding b and this is in very similar to one of the earlier
slides where we saw sigma x i wi so that's what we are doing here matrix
multiplication is multiplying all the input values with the corresponding weights and then adding the bias so that
is the graph we created and then we need to define what is our loss function and
what is our optimizer so in this case we again use the tensorflow's apis so tf
dot nn soft max cross entropy with logits is the api that we will use and reduce mean is
what is like the mechanism whereby which says that you reduce
the error and optimizer for doing deduction of the error what optimizer are we using
so we are using gradient descent optimizer we discussed about this in a couple of slides
earlier and for that you need to specify the learning rate you remember we saw that there was a slide somewhat like
this and then you define what should be the learning rate how fast you need to come down that is the learning rate and
this again needs to be tested and tried and to find out the optimum level of
this learning rate it shouldn't be very high in which case it will not converge or shouldn't be very low because it will
in that case it will take very long so you define the optimizer and then you call the method
minimize for that optimizer and that will kick-start the training process and
so far we've been creating the graph and in order to actually execute that graph
we create what is known as a session and then we run that session and once the training is completed we specify how
many times how many iterations we wanted to run so for example in this case we are saying thousand steps so that is a
exit strategy in a way so you specify the exit condition so training will run for thousand iterations and once that is
done we can then evaluate the model using some of the techniques
shown here so let us get into the code quickly and see how it
works so this is our cloud environment now you can install tensorflow on your
local machine as well i'm showing this demo on our existing cloud but you can
also install tensorflow on your local machine and there is a separate video on
how to set up your tensorflow environment you can watch that if you want to install your local environment
or you can go for other any cloud service like for example google cloud amazon or cloud labs any of
these you can use and run and try the code okay so it has got
started we will login
all right so this is our deep learning tutorial uh code
and this is our tensorflow environment and so let's get started
the first we have seen a little bit of a code walkthrough uh in the slides as
well now you will see the actual code in action so the first thing we need to do is import tensorflow and then we will
import the data and we need to adjust the data in such a way that the
one hot is encoding is set to true one hot encoding right as i explained earlier so in this case the label values
will be shown appropriately and if we just check what is the type of the data
so you can see that this is a data sets python data sets and if we
check the number of images the way it looks so this is how it looks it is an array of type float 32
similarly the number if you want to see what is the number of
training images there are 55 000 then there are test images 10 000 and then validation images 5000 now
let's take a quick look at the data itself visualization so we will use
matplotlib for this and if we take a look at the shape now
shape gives us like the dimension of the tensors or or the arrays if you will so in this
case the training data set if we see the size of the training data set using the method shape it says there are 55 000
and 55 000 by 784 so remember the 784 is
nothing but the 28 by 28 28 into 28 so that is equal to 784 so that's what it
is uh showing now we can take just uh one image and just see what is the
the first image and see what is the shape so again size obviously it is only 784 similarly you can
look at the image itself the data of the first image itself so this is how it
shows so large part of it will probably be zeros because as you can imagine
in the image only certain areas are written rest is blank so that's why you
will mostly see zero say that it is black or white but then there are these
values are so the values are actually they are scaled so the values are between zero and one okay so this is
what you're seeing so certain locations there are some values and then other locations there are zeros
so that is how the data is stored and loaded
if we want to actually see what is the value of the handwritten image
if you want to view it this is how you view it so you create like do this reshape and
matplotlib has this feature to show you these images so we
will actually use the function called i am show and then if you pass this
parameters appropriately you will be able to see the different images now i
can change the values in this position so which image we are looking at right so we can say
if i want to see what is there in maybe 5000 right so
5000 has 3 similarly you can just say 5 what is in 5
5 as 8 what is in 50
again 8 so basically by the way if you're wondering how i am executing this code shift enter
in case you are not familiar with jupiter notebooks shift enter is how you execute each cell individual cell and if
you want to execute the entire program you can go here and say run all
so that is how this code gets executed and
here again we can check what is the maximum value and what is the minimum value of this pixel values as i
mentioned this is it is scaled so therefore it is between the values lie between one and zero now this is
where we create our model the first thing is to create the required placeholders and variables and
that's what we are doing here as we have seen in the slides so we create one placeholder and we create two
variables which is for the weights and biases these two variables are actually
matrices so each variable has 784 by 10 actual values okay so
one for this 10 is for each neuron there are 10 neurons and 784 is for the pixel
values inputs that are given which is 28 into 28 and the biases as i mentioned one for
each neuron so there will be 10 biases they are stored in a variable by the name b
and this is the graph which is basically the multiplication of these matrix multiplication of x into w and then the
bias is added for each of the neurons and the whole idea is to minimize
the error so let me just execute i think this code is executed then we define
what is our the y value is basically the label value so this is another placeholder we had x
as one placeholder and white underscore true as a second placeholder and this
will have values in the form of 10 digit 10 digit arrays
and since we set one hot encoded the position which has a one value indicates
what is the label for that particular number all right then we have cross entropy which
is nothing but the loss loss function and we have the optimizer we have chosen
gradient descent as our optimizer then the training process itself so the training process is nothing but to
minimize the cross entropy which is again nothing but the loss function
so we define all of this in the form of a graph so up to here remember what we
have done is we have not exactly executed any tensorflow code till now we
are just preparing the graph the execution plan that's how the tensorflow
code works so the whole structure and format of this code will
be completely different from how we normally do programming so even with
people with programming experience may find this a little difficult to
understand it and it needs quite a bit of practice so you may want to view this
video also maybe a couple of times to understand this flow because the way tensorflow programming
is done is slightly different from the normal programming some of you who let's say
have done maybe spark programming to some extent will be able to easily understand this
but even in spark the the programming the code itself is pretty straightforward behind the scenes the
execution happens slightly differently but in tensorflow even the code has to
be written in a completely different way so the code doesn't get executed in the same way as you have written so
that that's something you need to understand and a little bit of practice is needed for this so so far what we have done up to here
is creating the variables and feeding the variables and rather not feeding but setting up the
variables and the graph that's all defining maybe the
what kind of a network you want to use for example we want to use a softmax and so on so you have created the variables
how to load the data loaded the data viewed the data and prepared everything but you have not
yet executed anything in tensorflow now the next step is the execution intensive
law so the first step for doing any execution in tensorflow is to
initialize the variables so anytime you have any variables defined in your code
you have to run this piece of code always so you need to basically create
what is known as a node for initializing so this is a node you still are not yet executing anything here you just created
a node for the initialization so let us go ahead and create that
and here onwards is where you will actually execute your code
in tensorflow and in order to execute the code what you will need is a session
tensorflow session so tf.session will give you a session and there are a
couple of different ways in which you can do this but one of the most common methods of doing this is with what is
known as a width loop so you have a width tf dot session as says and with the
colon here and this is like a block starting of the block and these indentations tell how far this block
goes and this session is valid till this block gets executed so that is the
purpose of creating this width block this is known as a width block so with tf dot session as says you say says
dot run init now says dot run will execute a node that is
specified here so for example here we are saying says dot run cess is basically an instance of the session
right so here we are saying tf.session so an instance of the session gets created and we are calling that
says and then we run a node within that one of the nodes in
the graph so one of the nodes here is init so we say run that particular node and
that is when the initialization of the variables happens now what this does is
if you have any variables in your code in our case we have w
is a variable and b is a variable so any variables that we created you have to
run this code you have to run the initialization of these variables otherwise you will get an error okay so
that is the that's what this is doing then we within this with block we specify a
for loop and we are saying we want the system to iterate for a thousand steps and perform
the training that's what this for loop does run training for thousand iterations
and what it is doing basically is it is fetching the data or these images remember there
are about 50 000 images but it cannot get all the images in one shot because
it will take up a lot of memory and performance issues will be there so this is a very common way of performing deep
learning training you always do in batches so we have maybe 50 000 images
but you always do it in batches of 100 or maybe 500 depending on the size of your
system and so on and so forth so in this case we are saying okay get me 100
images at a time and get me only the training images remember we use only the
training data for training purpose and then we use test data for test purpose you must be familiar with machine
learning so you must be aware of this but in case you are not in machine learning also not this is not specific
to deep learning but in machine learning in general you have what is known as training data set and test data set your
available data typically you will be splitting into two parts and using the
training data set for training purpose and then to see how well the model has been trained you use the test data set
to check or test the validity or the accuracy of the model so that's what we are doing here and you observe here that
we are actually calling an mnist function here so we are saying mnist train dot next batch
right so this is the advantage of using mnist database because they have provided some very nice helper functions
which are readily available otherwise this activity itself we would have had to write a piece of code to fetch this
data in batches that itself is a lengthy exercise so we can avoid all that if we are using mnist database and that's why
we use this for the initial learning phase okay so when we say fetch what it
will do is it will fetch the images into x and the labels into y and then you
use this batch of hundred images and you run the training so says dot run basically what
we are doing here is we are running the training mechanism which is nothing but it passes this through the neural
network passes the images through the neural network finds out what is the output and if the output obviously they
initially it will be wrong so all that feedback is given back to the neural
network and thereby all the w's and b's get updated
till it reaches thousand iterations in this case the exit criteria is thousand but you
can also specify probably accuracy rate or something like that for the as an exit criteria so here it is it just says
that okay this particular image was wrongly predicted so you need to update your weights and biases that's the
feedback given to each neuron and that is run for 1000 iterations and typically
by the end of this thousand iterations the model would have learned to recognize these handwritten images
obviously it will not be 100 accurate okay so once that is done
after so this happens for 1000 iterations once that is done you then test the accuracy of these
models by using the test data set right so this is what we are trying to
do here the code may appear a little complicated because if you're seeing this for the first time you need to understand uh the various methods of
tensorflow and so on but it is basically comparing the output with
what has been what is actually there that's all it is doing so you have your test data and you're trying to find out
what is the actual value and what is the predicted value and seeing whether they are equal or not tf dot equal right and
how many of them are correct and so on and so forth and based on that the
accuracy is calculated as well so this is the accuracy and that is what we are trying
to see how accurate the model is in predicting these numbers or these
digits okay so let us run this this entire thing is in one cell so we will
have to just run it in one shot it may take a little while let us see and not bad so it has finished the
thousand iterations and what we see here as an output is the accuracy so we see
that the accuracy of this model is around 91 percent okay now which is
pretty good for such a short exercise within such a short time we got 90
percent accuracy however in real life this is probably not sufficient so there
are other ways in to increase the accuracy we will see probably in some of
the later tutorials how to improve this accuracy how to change maybe the hyper
parameters like number of neurons or number of layers and so on and so forth and
so that this accuracy can be increased beyond 90 percent we're going to dive right into what is keras we'll also go
all the way through this into a couple of tutorials because that's where you really learn a lot is when you roll up your sleeves so we talk about what is
cross cross is a high level deep learning api written in python for easy
implement implementation of neural networks it uses deep learning frameworks such as tensorflow pytorch
etc is back in to make computation faster and this is really nice because as a
programmer there is so much stuff out there and it's evolving so fast it can get confusing and having some
kind of high level order in there we can actually view it and easily program these different neural networks uh is
really powerful it's really powerful to have something out really quick
and also be able to start testing your models and seeing where you're going so crossworks by using complex deep
learning frameworks such as tensorflow pytorch ml played etc as a backend for fast
computation while providing a user friendly and easy to learn front-end and you can see here
we have the cross api uh specifications and under that you'd have like tf cross
for tensorflow thano cross and so on and then you have your tensorflow workflow
that this is all sitting on top of and this is like i said it organizes everything the heavy lifting is still
done by tensorflow or whatever you know underlying package you put in there and
this is really nice because you don't have to dig as deeply into the heavy end stuff
while still having a very robust package you can get up and running rather quickly and it doesn't distract from the
processing time because all the heavy lifting is done by packages like tensorflow this is the organization on
top of it so the working principle of cross the working principle of keras is cross
uses computational graphs to express and evaluate mathematical expressions
you can see here we put in blue they have the expression expressing complex problems as a
combination of simple mathematical operators where we have like the percentage or in this case in python that's usually your
left your remainder or multiplication uh you might have the operator of x to the power of 0.3 and it
uses useful for calculating derivatives by using back propagation so if we're doing with
neural networks we send the error back up to figure out how to change it uh this makes it really easy to do that
without really having not banging your head and having to hand write everything it's easier to implement distributed
computation and for solving complex problems specify input and outputs and make sure
all nodes are connected and so this is really nice as you come in through is that as your layers are
going in there you can get some very complicated different setups nowadays which we'll
look at in just a second and this just makes it really easy to start spinning this stuff up and trying
out the different models so we look across models across model we
have a sequential model sequential model is a linear stack of layers where the previous layer leads
into the next layer and this if you've done anything else even like the sk learn with their neural
networks and propagation in any of these setups this should look familiar you should have your input layer it goes
into your layer 1 layer 2 and then to the output layer and it's useful for simple classifier
decoder models and you can see down here we have the model equals across sequential this is
the actual code as you can see how easy it is uh we have a layer that's dense your
layer one has an activation now they're using the relu in this particular example and then
you have your name layer one layer dense relu name layer 2 and so forth
and they just feed right into each other so it's really easy just to stack them as you can see here and it automatically takes care of everything else for you
and then there's a functional model and this is really where things are at this
is new make sure you update your cross or you'll find yourself running this doing the functional model you'll run
into an error code because this is a fairly new release and he uses multi input and multi
model the complex model which forks into two or more branches and you can see here we have our image
inputs equals your cross input shape equals 32 by 32 by 3. you have your dense layers dan 64
activation relu this should look similar to what you already saw before but if you look at the graph on the
right it's going to be a lot easier to see what's going on you have two different inputs
and one way you could think of this is maybe one of those is a small image and one of those is a full-sized image
and that feedback goes into you might feed both of them into one note because it's
looking for one thing and then only into one node for the other one and so you can start to get kind of an idea that
there's a lot of use for this kind of split and this kind of setup we have multiple information coming in
but the information is very different even though it overlaps and you don't want it to send it through the same
neural network and they're finding that this trains faster and is also has a better result
depending on how you split the data and how you fork the models coming down
and so in here we do have the two complex uh models coming in uh we have our image inputs which is a 32 by 32 by
three or three channels or four if you're having an alpha channel uh you have your dense your layers dense is
64 activation using the relu very common x equals dense inputs x layers dense x64
activation equals relu x outputs equals layers dense 10 x model equals cross model inputs equals
inputs outputs equals outputs name equals minced model so we add a little name on there and
again this is this kind of split here this is setting us up to have the input go into different areas
so if you're already looking at cross you probably already have this answer what are neural networks uh but it's
always good to get on the same page and for those people who don't fully understand neural networks to dive into them a little bit or do a quick overview
neural networks are deep learning algorithms modeled after the human brain they use multiple neurons which are
mathematical operations to break down and solve complex mathical problems
and so just like the neuron one neuron fires in and it fires out to all these other neurons or nodes as we call them
and eventually they all come down to your output layer and you can see here we have the really standard graph input layer a hidden
layer and an output layer one of the biggest parts of any data
processing is your data pre-processing so we always have to touch base on that
with a neural network like many of these models they're kind of uh when you first start
using them they're like a black box you put your data in you train it and you test it and see how
good it was and you have to pre-process that data because bad data in is
bad outputs so in data pre-processing we will create our own data examples set
with cross the data consists of a clinical trial conducted on 2100
patients ranging from ages 13 to 100 with a the patients under 65 and the
other half over 65 years of age we want to find the possibility of a patient experiencing side effects due to
their age and you can think of this in today's world with covid uh what's going to happen on there and
we're going to go ahead and do an example of that in our live hands-on like i said most of this
you really need to have hands-on to understand so let's go ahead and bring up our anaconda and open that up and
open up a jupiter notebook for doing the python code in now if you're not familiar with those you can use pretty
much any of your setups i just like those for doing demos and
showing people especially shareholders it really helps because it's a nice visual so let me go and flip over to our
anaconda and the anaconda has a lot of cool two tools they just added data lore
and ibm watson studio cloud into the anaconda framework but we'll be in the jupiter lab or
jupiter notebook i'm gonna do jupiter notebook for this because i use the lab for like large projects with multiple
pieces because multiple tabs where the notebook will work fine for what we're doing
and this opens up in our browser window because that's how jupiter helps our jupiter
notebook is set to run and we'll go under new create a new python 3 and it creates an
untitled python we'll go ahead and give this a title and we'll just call this cross
tutorial and let's change that to capital there
we go i'm going to just rename that and the first thing we want to go ahead and do is uh
get some pre-processing tools involved and so we need to go ahead and import some stuff for that like our numpy do
some random number generation um i mentioned sklearn or your scikit if
you're installing sklearn the sk learn stuff it's a scikit you want to look up
that should be a tool of anybody who is doing data science if you're not if
you're not familiar with the sklearn toolkit it's huge but there's so many things in
there that we always go back to and we want to go ahead and create some train labels and train samples for
training our data and then
just a note of what we're we're actually doing in here let me go ahead and change this this is kind of a fun thing you can
do we can change the code to markdown and then markdown code is nice for doing
examples once you've already built this our example data we're going to do experimental
there we go experimental drug was tested on 2 100 individuals between 13 to 100 years of
age half the participants are under 65 and 95 of participants are under 65
experience no side effects well 95 percent of participants over 65 experience side effects
so that's kind of where we're starting at and this is just a real quick example because we're going to do another one
with a little bit more complicated information and so we want to go ahead and generate
our setup so we want to do for i and range and we want to go ahead and create if you look
here we have random integers trading the labels append so we're just creating some random data
let me go ahead and just run that and so once we've created our random data
and if you if i mean you can certainly ask for a copy of the code from simplylearn they'll send you a copy of
this or you can zoom in on the video and see how we went ahead and did our train samples of pin
and we're just using this i do this kind of stuff all the time i was running a thing on uh that had to do with errors
following a bell shaped curve on a standard distribution error and so
what do i do i generate the data on a standard distribution error to see what it looks like and how my code processes
it since that was the baseline i was looking for in this we're just doing generating random data for our setup on
here and we could actually go in print some of the data up let's just do
this print we'll do train
samples and we'll just do the first five pieces of data in there to see what
that looks like and you can see the first five pieces of data in our train samples is 49 85 41 68
19 just random numbers generated in there that's all that is and we generated
significantly more than that um let's see 50 up here 1000. yeah so there's
1000 here 1000 numbers we generated and we could also if we wanted to find that out we can do a quick print the length
of it and
so or you could do a shape kind of thing and if you're using numpy although the link for this is just fine
and there we go it's actually 2100 like we said in the demo setup in there
and then we want to go ahead and take our labels oh that was our train labels
we also did samples didn't we uh so we could also print
do the same thing labels
let's change this to labels
and labels and run that just to double check and
sure enough we have 2100 and they're labeled one zero one zero one zero
i guess that's if they have symptoms or not one symptoms zero none and so we want to go ahead and
take our train labels and we'll convert it into a numpy array and the same thing with our samples
and let's go ahead and run that and we also shuffle this is just a neat feature you can do in numpy right here
put my drawing thing on which i didn't have on earlier i can take the data and i can shuffle it
so we have our so it just randomizes it that's all that's doing
we've already randomized it so it's kind of an overkill it's not really necessary
but if you're doing a larger package where the data is coming in and a lot of times it's organized somehow and you
want to randomize it just to make sure that that you know the input doesn't follow a certain pattern
that might create a bias in your model and we go ahead and create a scalar the scalar range minimum max scalar
feature range zero to one uh and then we go ahead and scale the uh scaled train samples so we're gonna go
ahead and fit and transform the data uh so it's nice and scaled and that is the age
so you can see up here we have 49 85 41. we're just moving that so it's going to be between 0 and 1. and so this is true
with any of your neural networks you really want to convert the data
to 0 and 1. otherwise you create a bias so if you have like a hundred creates a
bias versus the math behind it gets really complicated um
if you actually start multiplying stuff there's a lot of multiplication addition going on in there
that higher end value will eventually multiply down and it will have a huge bias as to how the model
fits it and then it will not fit as well and then one of the fun things we can do in jupiter notebook is that if you have
a variable you're not doing anything with it it's the last one on the line it will automatically print
and we're just going to look at the first five samples on here it's just going to print the first five samples
and you can see here we go 0.919
so everything's between 0 and 1. and that just shows us that we scaled it
properly and it looks good it really helps a lot to do these kind of print ups halfway through
you never know what's going to go on there i don't know how many times i've gotten down and found out that the data is sent
to me that i thought was scaled was not and then i have to go back and track it down and figure it out on there
so let's go ahead and create our artificial neural network and for doing that this is where we
start diving into tensorflow and cross
tensorflow if you don't know the history of tensorflow it helps to jump into we'll just use
wikipedia be careful don't quote wikipedia on these things because you get in trouble
but it's a good place to start uh back in 2011 google brain built disbelief as a proprietary machine learning setup
tensorflow became the open source for it so tensorflow was a google product
and then it became uh open sourced and now it's just become
probably one of the defactos when it comes for neural networks as far as where we're at
so when you see the tensorflow setup it it's got like a huge following there
are some other setups like um the side kit under the sk learn has their own little neural network
but the tensorflow is the most robust one out there right now and cross sitting on top of it makes it
a very powerful tool so we can leverage both the cross easiness in which we can build a
sequential setup on top of tensorflow and so in here we're going to go ahead
and do our input of tensorflow uh and then we have the rest of this is all cross here from number
two down uh we're gonna import from tensorflow the cross connection
and then you have your tensorflow cross models import sequential it's a specific kind of model we'll look at that in just
a second if you remember from the files that means it goes from one layer to the next layer to the next
layer there's no funky splits or anything like that uh and then we have from tensorflow
cross layers we're going to import our activation and our dense layer
and we have our optimizer atom this is a big thing to be aware of how
you optimize your data when you first do it atoms as good as any atom is usually there's a
number of optimizer out there there's about there's a couple of main ones but atom is usually assigned to bigger data
it works fine usually the lower data does it just fine but atom is probably the mostly used but there are some more out there and depending on what you're
doing with your layers your different layers might have different activations on them and then finally down here
you'll see our setup where we want to go ahead and use the metrics
and we're going to use the tensorflow cross metrics for categorical cross entropy
so we can see how everything performs when we're done that's all that is a lot of times you'll see us
go back and forth between tensorflow and then scikit has a lot of really good metrics also for measuring these things
again it's the end of the day at the end of the story how good does your model do and we'll go ahead and load all that and
then comes the fun part um i actually like to spend hours messing with these
things and four lines of code you're like oh you're gonna spend hours on four lines of code
um no we don't spend hours on four lines of code that's not what we're talking about when i say spend hours on four lines of code
uh what we have here i'm gonna explain that in just a second we have a model and it's a sequential model if you
remember correctly we mentioned the sequential up here where it goes from one layer to the next
and our first layer is going to be our input
it's going to be what they call dense which is usually just dense and then you have your input and your activation
how many units are coming in we have 16 what's the shape what's the activation and this is where
it gets interesting because we have in here relu
on two of these and soft max activation on one of these
there are so many different options for what these mean
and how they function how does the relu how does the soft max function
and they do a lot of different things um we're not going to go into the activations in here that is what really
you spend hours doing is looking at these different activations and just
some of it is just almost like you're playing with it like an artist you start getting a fill
for like a inverse tangent activation or the 10 h activation
takes up a huge processing amount so you don't see it a lot yet
it comes up with a better solution especially when you're doing uh when you're analyzing word documents and
you're tokenizing the words and so you'll see this shift from one to the other because you're both trying to
build a better model and if you're working on a huge data set
you'll crash the system it'll just take too long to process and then you see things like soft max
softmax generates an interesting setup
where a lot of these when you talk about rayleigh it let me do this rayleigh there we go relu has
a setup where if it's less than zero it's zero and then it goes up
and then you might have what they call lazy setup where it has a slight negative to it so that the errors can translate
better same thing with softmax it has a slight laziness to it so that errors translate better all these little
details make a huge difference on your model so one of the really cool things about
data science that i like is you build your uh what they call you build to fail
and it's an interesting uh design setup oops i forgot the end of my
code here the concept to build a file is you want the model as a whole to work so you can
test your model out so that you can do
you can get to the end and you can do your let's see where was it over shot down here
you can test your test out that the quality of your setup on there and
see where did i do my tensorflow oh here we go it was right above me there we go we start doing your cross entropy and
stuff like that is you need a full functional set of code so that when you run it
you can then test your model out and say hey it's either this model works better than this model and this is why
and then you can start swapping in these models and so when i say spend a huge amount of time on pre-processing data is
probably 80 percent of your programming time well between those two it's like 80 20
you'll spend a lot of time on the models once you get the model down once you get the whole code and the flow down
set depending on your data your models get more and more robust as you start
experimenting with different inputs different data streams and all kinds of things and we can do a simple model summary
here here's our sequential here's a layer output a parameter
this is one of the nice things about cross is you just you can see right here here's our sequential one model boom
boom boom boom everything's set and clear and easy to read so once we have our model built
the next thing we're going to want to do is we're going to go ahead and train that model
and so the next step is of course model training and when we come in here
this a lot of times it's just paired with the model because it's so straightforward it's nice to
print out the model setup so you can have a tracking but here's our model
the key word in cross is compile optimizer atom learning rate another
term right there that we're just skipping right over that really becomes the meat of
the setup is your learning rate so whoops i forgot that i had an arrow but i'll just underline it
a lot of times the learning rate set to 0.00 set to 0.01
depending on what you're doing this learning rate can overfit and underfit
so you'd want to look up i know we have a number of tutorials out on overfitting and underfitting that are
really worth reading once you get to that point and understanding and we have our loss
sparse categorical cross entropy so this is going to tell keras how far
to go until it stops and then we're looking for metrics of accuracy so we'll go ahead and run that
and now that we've compiled our model we want to go ahead and
run it fit it so here's our model fit we have our scaled train samples
our train labels our validation split in this case we're going to use 10
percent of the data for validation batch size another number you kind of
play with not a huge difference as far as how it works but it does affect how long it takes to
run and it can also affect the bias a little bit most of the time though a batch size is
between 10 to 100 depending on just how much data you're processing in there we want to go ahead
and shuffle it we're going to go through 30 epics and put the verb rose of two let me just go
ahead and run this and you can see right here here's our epic here's our training
here's our loss now if you remember correctly up here we set the loss let's see where was it
compiled our data there we go loss so it's looking at the
sparse categorical cross entropy this tells us that as it goes how how
how much how how much does the error go down
is the best way to look at that and you can see here the lower the number the better it just keeps going down
and vice versa accuracy we want let's see where's my accuracy
value accuracy at the end and you can see 0.619 0.69 0.74 it's
going up we want the accuracy would be ideal if it made it all the way to one but we also the loss is
more important because it's a balance you can have 100 accuracy and your model
doesn't work because it's over fitted again you won't look up over fitting and
under fitting models and we went ahead and went through 30 epics it's always fun to kind of
watch your code going to be honest i usually uh uh the first time i run it
i'm like oh that's cool i get to see what it does and after the second time of running it i'm like i'd like to just
not see that and you can repress those of course in your code uh repress the warnings in the printing
and so the next step is going to be building a test set and predicting it now so here we go we want to go ahead and
build our test set and we have just like we did our training set
a lot of times you just split your your initial set up but we'll go ahead and do a separate set on here
and this is just what we did above there's no difference as far as
the randomness that we're using to build this set on here the only difference is that
we already did our scalar up here well it doesn't matter because the the data is going to
be across the same thing but this should just be just transformed down here instead of fit transform
because you don't want to refit your data on your testing data
there we go now we're just transforming it because you never want to transform the test data
easy to mistake to make especially on an example like this where we're not doing
you know we're randomizing the data anyway so it doesn't matter too much because we're not expecting something weird
and then we went ahead and do our predictions the whole reason we built the model is we take our model we predict
and we're going to do here's our x scale data batch size 10 verbose and now we
have our predictions in here and we could go ahead and do a oh we'll print
predictions and then i guess i could just put down predictions in five so we can look at
the first five of the predictions and what we have here is we have our
age and the prediction on this age versus what is what we think it's going to be but
what we think is going to they're going to have symptoms or not and the first thing we notice is that's
hard to read because we really want to yes no answer so we'll go ahead and just
round off the predictions using the arg max the numpy arg max
for prediction so it just goes to 001 and if you remember this is a jupiter
notebook so i don't have to put the print i can just put in rounded predictions and we'll just do
the first five and you can see here zero one zero zero zero so that's what the predictions are
that we have coming out of this is no symptoms symptoms no symptoms symptoms no symptoms
and just as we were talking about at the beginning we want to go ahead and take a look at this there we go
confusion matrix is for accuracy check most important part
when you get down to the end of the story how accurate is your model before you go and play with the model and see
if you can get a better accuracy out of it and for this we'll go ahead and use the site kit
the sk learn metrics site kit being where that comes from
import confusion matrix some iteration tools and of course a nice matplot library
that makes a big difference so it's always nice to have a nice graph to look at a
picture is worth a thousand words um and then we'll go ahead and call it cm for confusion matrix why true equals
test labels why predict rounded predictions and we'll go ahead and load in our cm
and i'm not going to spend too much time on the plotting going over the different plotting code
you can spend like whole we have whole tutorials on how to do your different plotting on
there but we do have here is we're gonna do a plot confusion matrix there's our cm our
classes normalize false title confusion matrix c map is going to be in blues
and you can see here we have uh to the nearest cmap titles all the different pieces whether you put tick marks or not
the marks the classes the color bar so a lot of different information on here as far as how we're doing the
printing of the of the confusion matrix you can also just dump the confusion matrix into a seaborn and real quick get
an output it's worth knowing how to do all this uh when you're doing a presentation to the
shareholders you don't want to do this on the fly you want to take the time to make it look really nice like our guys
in the back did and let's go ahead and do this forgot to put together our cm
plot labels we'll go ahead and run that and then we'll go ahead and call the
little the definition for our mapping
and you can see here plot confusion matrix that's our the little script we just wrote and we're going to dump our
data into it so our confusion matrix our classes title confusion matrix and let's just go
ahead and run that and you can see here we have our basic setup
no side effects 195. had side effects 200
no side effects that had side effects so we predicted that 10 of them who actually had side effects and that's
pretty good i mean i i don't know about you but you know that's five percent error on this and this is because
there's 200 here that's where i get five percent is uh divide these both by by two and you get five out of a hundred uh
you can do the same kind of math up here not as quick on the fly because it's 15 and 195 not an easily rounded number but
you can see here where they have 15 people who predicted to have no
with the no side effects but had side effects kind of set up on there and these
confusion matrix are so important at the end of the day this is really where where you show uh
whatever you're working on comes up and you can actually show them hey this is how good we are or not how messed up it
is so this was a uh i spent a lot of time
on some of the parts uh but you can see here is really simple uh we did the random generation of data but when we
actually built the model coming up here here's our model summary
and we just have the layers on here that we built with our model on this and then we went ahead and trained it and ran the
prediction now we get a lot more complicated let me flip back on over here because we're going to do another uh demo
so that was our basic introduction to it we talked about the uh oops there we go
okay so implementing a neural network with cross after creating our samples and labels we need to create our cross
neural network model we will be working with a sequential model which has three layers and this is what we did we had
our input layer our hidden layers and our output layers and you can see the input layer coming in
was the age factor we had our hidden layer and then we had the output are you going to have symptoms or not
so we're going to go ahead and go with something a little bit more complicated training our model is a two-step process
we first compile our model and then we train it in our training data set so we have compiling compiling converts
the code into a form of understandable by machine we use the atom in the last example a
gradient descent algorithm to optimize a model and then we trained our model which means it let it
learn on training data and i actually had a little backwards
there but this is what we just did is we if you remember from our code we just had let me go back here
[Music] here's our model that we created summarized
we come down here and we compile it so it tells it hey we're ready to build
this model and use it and then we train it this is the part where we go ahead and fit our model and
and put that information in here and it goes through the training on there and of course we scaled the data which
was really important to do and then you saw we did the creating a confusion matrix with keras
as we are performing classifications on our data we need a confusion matrix to check the results a confusion matrix
breaks down the various misclassifications as well as correct classifications to get the accuracy
and so you can see here this is what we did with the true positive false positive true negative false negative
and that is what we went over let me just scroll down here on the end we printed it out and you can
see we have a nice printout of our confusion matrix with the true positive false positive
false negative true negative and so the blue ones uh we want those to be the biggest numbers because those are the
better side and then we have our false predictions on here
as far as this one so i had no side effects but we predicted let's see no side effects predicting
side effects and vice versa if getting your learning started is half the battle what if you could do that for free
visit skill up by simply learn click on the link in the description to know more
now uh saving and loading models with carrass we're going to dive into a more
complicated demo and you can say oh that was a lot of complication before well if you broke it
down we randomized some data we created the um cross setup we compiled it we
trained it we predicted and we ran our matrix so we're going to dive into something a a little bit more fun is we're going to
do a face mask detection with cross so we're going to build a cross model to check if a person is wearing a mask or
not in real time and this might be important if you're at the front of a store this is something today which is
might be very useful as far as some of our you know making sure people are safe
and so we're going to look at mask and no mask and let's start with a little bit on the data
and so in my data i have with a mask you can see they just have a number of images showing the people in masks and
again if you want some of this information contact simply learn and they can send
you some of the information as far as people with and without masks so you can try it on your own and this is just such a wonderful
example of this setup on here so before i dive into the mass detection
uh talk about being in the current with a covid and seeing if people are wearing masks this particular example i had to
go ahead and update to a python 3.8 version uh it might run in a three seven
i'm not sure i kind of skipped three seven and installed three eight uh so i'll be running in a 3 python 38
and then you also want to make sure your tensorflow is up to date because the um they call functional
layers with that's where they split if you remember correctly from back uh let's take a look at this
remember from here the functional model and a functional layer allows us to feed
in the different layers into different you know different nodes into different layers and split them a very powerful
tool very popular right now in the edge of where things are with neural networks
and creating a better model so i've upgraded to python 3.8 and let's go ahead and open that up and
go through uh our next example which includes uh multiple layers
programming it to recognize whether someone wears a mask or not and then saving that model so we can use it in
real time so we're actually almost a full end-to-end development of a product here
of course this is a very simplified version and it'd be a lot more to it you'd also have to do like recognizing
whether it's someone's face or not all kinds of other things go into this so let's go ahead and jump into that
code and we'll open up a new python three oops python three
it's working on it there we go and then we want to go ahead and train
our mask we'll just call this train mask
and we want to go ahead and train mask and save it so it's it's
safe mask train mask detection not to be confused with masking data a
little bit different we're actually talking about a physical mask on your face
and then from the cross stand point we got a lot of imports to do here
and i'm not going to dig too deep on the imports we're just going to go ahead and notice a few of them
so we have in here we go alt d there we go have something to draw with a little bit
here we have our image processing and the image processing right here the
underline that uh deals with how do we bring images in because most images
are like a square grid and then each value in there has three values for the three different
colors cross and tensorflow do a really good job of working with that so you don't
have to do all the heavy listening and figuring out what's going to go on and we have the mobile net average
pooling 2d this again is how do we deal with the images and
pulling them dropout's a cool thing worth looking up if you haven't when as you get more and
more into cross and tensorflow it will auto drop out certain nodes that
way you'll get a better the notes just kind of die they find that they actually create more
of a bias and help and they also add processing time so they remove them
and then we have our flatten that's where you take that huge array with the three different colors and you find a
way to flatten it so it's just a one dimensional array instead of a two by two by three
dense input we did that in the other one so that should look a little familiar oops there we go our input our model
again these are things we had on the last one here's our optimizer with our atom
we have some pre-processing on the input that goes along with bringing in the data in
uh more pre-processing with image to array loading the image
this stuff is so nice it looks like a lot of works you have to import all these different modules in here but the
truth is is it does everything for you you're not doing a lot of pre-processing you're letting the software do the
pre-processing and we're going to be working with the setting something to categorical
again that's just a conversion from a number to a category zero one doesn't really mean anything it's like true
false label binarizer the same thing we're changing our labels around and
then there's our train test split classification report our
im utilities let me just go ahead and scroll down here a notch for these
this is something a little different going on down here this is not part of the tensorflow or the sklearn this is the
site kit setup and tensorflow above the path this is part of
opencv and we'll actually have another tutorial going out with the opencv so if you want to know more about opencv you'll get a
glance on it in this software especially the second piece when we reload up the data
and hook it up to a video camera we're going to do that on this round but this is part of the opencv thing
you'll see cv2 is usually how that's referenced but the im utilities has to do with how
do you rotate pictures around and stuff like that uh and resize them and then the matplot
library for plotting because it's nice to have a graph tells us how good we're doing and then of course our numpy numbers array and just the straight os
access wow so that was a lot of imports uh like i said i'm not going to spend i spent a little time going through them
but we didn't want to go too much into them and then i'm going to create
some variables that we need to go ahead and initialize we have the learning rate number of epics to train for and the batch size
and if you remember correctly we talked about the learning rate to the negative 4.0001
a lot of times it's 0.001 or 0.001 usually it's in that
variation depending on what you're doing and how many epochs and they kind of play with the epics the epics is how
many times we're going to go through all the data now i have it as two
the actual setup is for 20 and 20 works great the reason i have it for two is it
takes a long time to process one of the downsides of jupiter
is that jupiter isolates it to a single kernel so even though i'm on an eight core processor
with 16 dedicated threads only one thread is running on this no matter what so it doesn't matter
so it takes a lot longer to run even though tensorflow really scales up nicely and
the batch size is how many pictures do we load at once in process again those are numbers you have to
learn to play with depending on your data and what's coming in and the last thing we want to go ahead and do is there's a directory with a data set
we're going to run and this just has images of mass and not
masks and if we go in here you'll see data set
um and then you have pictures with mass they're just images of people with mass on their face
and then we have the opposite let me go back up here without masks so it's pretty straightforward they look kind of a skew
because they tried to format them into very similar uh setup on there so they're they're mostly squares you'll see some that are
slightly different on here and that's kind of important thing to do on a lot of these data sets
get them as close as you can to each other and we'll we actually will run in the in this processing of images up here
and the cross layers and importing and dealing with images it does such a wonderful job of converting these that a
lot of it we don't have to do a whole lot with so you have a couple things going on there
and so we're now going to be this is now loading the images and let me see
and we'll go ahead and create data and labels here's our
here's the features going in which is going to be our pictures and our labels going out
and then for categories in our list directory directory and if you remember
i just flashed that at you it had a face mask or or no face mask those are
the two options and we're just going to load into that we're going to append the image itself and the labels so we'll just create a
huge array and you can see right now this could be an issue if you had more data at some
point thankfully i have a 32 gig hard drive or
ram even that you could do with a lot less of that probably under 16 or even 8 gigs would
easily load all this stuff and there's a conversion going on in here i told you about how we are going
to convert the size of the image so it resizes all the images that way our data
is all identical the way it comes in and you can see here with our labels we
have without mask without mask without mask the other one would be with mask those
are the two that we have going in there uh and then we need to change it to the
one not hot encoding and this is going to take our um
up here we had was it labels and data we want the labels to be categorical so
we're going to take labels and change it to categorical and our labels then equal a categorical list
we'll run that and again if we do labels and we just do the last or the
first 10 let's do the last 10 just because minus 10 to the end there we go
just so we can see where the other side looks like we now have one that means they have a mask one zero one zero so on
one being they have a mask and zero no mask and if we did this in reverse
i just realized that this might not make sense if you've never done this before let me run this
zero one so zero is uh do they have a mask on zero do
they not have a mask on one so this is the same as what we saw up here without
mask one equals the second value is without mass so with mass without mask
and that's just a with any of your data processing we can't really zero if you have a 0 1
output it causes issues as far as training and
setting it up so we always want to use a one hot encoder if the values are not actual uh linear value or regression
values or not actual numbers if they represent a thing and so now we
need to go ahead and do our train x test x train y test y train split test data
and we'll go ahead and make sure it's going to be random and we'll take 20 percent of it for testing and the rest
for setting it up as far as training their model this is something that's become so cool
when they're training these set they realize we can augment the data what does augment mean
well if i rotate the data around and i zoom in i zoom out i rotate it
share it a little bit flip it horizontally fill mode as i do all these different
things to the data it is able to it's kind of like increasing the number of samples i have
so if i have all these perfect samples what happens when we only have part of the face or the face is tilted sideways
or all those little shifts cause a problem if you're doing just a standard set of
data so we're going to create an augment in our image data generator which is going to rotate zoom and do all
kinds of cool thing and this is worth looking up this image data generator and all the different features it has
a lot of times i'll the first time through my models i'll leave that out because i want to make sure there's a
thing we call build to fail which is just cool to know you build the whole process and then you
start adding these different things in so that you can better train your model and so we go and run this and then we're
going to load and then we need to go ahead and you probably would have got an error if you hadn't put this piece in right here
i haven't run it myself because the guys in the back did this we take our base model and one of the
things we want to do is we want to do a mobile net v2 and this what we this is a big thing
right here include the top equals false a lot of data comes in with a label on
the top row so we want to make sure that that is not the case
and then the construction of the head of the model that will be placed on the top of the base model
we want to go ahead and set that up
and you'll see a warning here i'm kind of ignoring the warning because it has to do with the
size of the pictures and the weights for input shape so they'll switch things to defaults
just saying hey we're going to auto shape some of this stuff for you you should be aware that with this kind of imagery we're already augmenting it by
moving it around and flipping it and doing all kinds of things to it so that's not a bad thing in this but
another data it might be if you're working in a different domain and so we're going to go back here and
we're going to have we have our base model we're going to do our head model equals our base model output
and what we got here is we have an average pooling 2d pool size 77 head model
head model flatten so we're flattening the data so this is all
processing and flattening the images and the pooling has to do
with some of the ways it can process some of the data we'll look at that a little bit when we get down to the lower levels processing it
and then we have our dents we've already talked a little bit about a dents just what you think about and then the head model has a drop out
of 0.5 what we can do with a dropout
the dropout says that we're going to drop out a certain amount of nodes while training
so when you actually use the model it will use all the nodes but this drops certain ones out and it helps
stop biases from forming so it's really a cool feature in here they discovered this a while back
we have another dense mode this time we're using soft max activation lots of different activation options
here softmax is a real popular one for a lot of things and so is a relu
and you know there's we could do a whole talk on activation formulas uh and why
what their different uses are and how they work when you first start out you'll you'll use mostly the relu and the softmax for
a lot of them uh just because they're some of the basic setups it's a good place to start
uh and then we have our model equals model inputs equals base model dot input outputs equals head model so again we're
still building our model here we'll go ahead and run that and then we're going to loop over all
the layers in the base model and freeze them so they will not be updated during the first training process
so for layer and base model layers layers dot trainable equals false a lot of times when you go through your
data you want to kind of jump in part way through i
i'm not sure why in the back they did this for this particular example but i do this a lot when i'm working
with series and specifically in stock data i wanted to iterate through the first set of 30 data
before it does anything i would have to look deeper to see why they froze it on this particular one
and then we're going to compile our model so compiling the model atom
init layer decay initial learning rate over epics
and we go ahead and compile our loss is going to be the binary cross entropy which will have that print out
optimizer for opt metrics is accuracy same thing we had before not a huge jump
as far as the previous code
and then we go ahead and we've gone through all this and now we need to go ahead and fit our model uh so train the
head of the network print info training head run now i skipped a little time because it
you'll see the run time here is um at 80 seconds per epic takes a couple minutes
for it to get through on a single kernel one of the things i want you to notice
on here while we're while it's finishing the processing is that we have up here our augment
going on so anytime the train x and trading y go in there's some randomness going on there
and is jiggling it around what's going into our setup of course we're batch sizing it so it's
going through whatever we set for the batch values how many we process at a time and then we have the steps per epic
the train x the batch size validation data here's our test x and test y where
we're sending that in and this again it's validation
one of the important things to know about validation is our when both our training data and our test
data have about the same accuracy that's when you want to stop that means that
our model isn't biased if you have a higher accuracy on your
testing you've trained it and your accuracy is higher on your actual test data then
something in there is probably uh has a bias and it's overfitted so that's what this is really about
right here with the validation data and validation steps
so it looks like it's let me go ahead and see if it's done processing looks like we've gone ahead and gone through two epics again you could run this
through about 20 with this amount of data and it would give you a nice refined model at the end
we're going to stop at 2 because i really don't want to sit around all afternoon and i'm running this on a single thread
so now that we've done this we're going to need to evaluate our model and see how good it is and to
do that we need to go ahead and make our predictions these are predictions on our test x
to see what it thinks are going to be so now it's going to be evaluating the network and then we go ahead and go down
here and we will need to turn the index in because remember it's
it's either zero or one it's a zero one zero one so you have two outputs uh not wearing uh wearing a mask
not wearing a mask and so we need to go ahead and take that argument at the end and change those
predictions to a 0 or 1 coming out and then
to finish that off we want to go ahead and let me just put this right in here and do it all in one shot we want to
show a nicely formatted classification report so we can see what that looks like on here
and there we have it we have our precision it's 97 with the mask
there's our f1 score support without a mask 97 percent
so that's pretty high set up on there you know you three people are going to sneak into the store who
are without a mask and thinks they have a mask and there's going to be three people with a mask
that's going to flag the person at the front to go oh hey look at this person you might not have a mask that's if i guess it's just set up in
front of a store um so there you have it and of course one of the other cool things about this
is if someone's walking in to the store and you take multiple pictures of them
you know this is just an it would be a way of flagging and then you can take that average of those pictures and make
sure they match or don't match if you're on the back end and this is an important step because we're gonna this is just
cool i love doing this stuff so we're going to go ahead and take our model and we're going to save it
so model save massdetector.model we're going to give it a name we're going to save the format
in this case we're going to use the h5 format and so this model we just programmed has
just been saved so now i can load it up into say another program what's cool about this is let's
say i want to have somebody work on the other part of the program well i just save the model they upload the model now
they can use it for whatever and then if i get more information and we start working with that at some
point i might want to update this model and make a better model and this is true of
so many things where i take this model and maybe i'm running a prediction on uh
making money for a company and as my model gets better i want to keep updating it and then it's
really easy just to push that out to the actual end user uh here we have a nice graph you can see
the training loss and accuracy as we go through the epics we only did the you know only shows just
the one epic coming in here but you can see right here as the value loss train accuracy and value
accuracy starts switching and they start converging and you'll hear converging
this is a convergence they're talking about when they say you're you're i know when i work in the
scikit with sk learn neural networks this is what they're talking about a convergence is our loss and our accuracy
come together and also up here and this is why i'd run it more than just two epics as you can see they still haven't
converged all the way so that would be a cue for me to keep going
but what we want to do is we want to go ahead and create a new python 3
program and we just did our train mask so now we're going to go ahead and import that
and use it and show you in a live action get a view of both myself in the
afternoon along with my background of an office which is in the middle still of
reconstruction for another month and we'll call this a mask
detector and then we're going to grab a bunch of
a few items coming in we have our
mobile net v2 import pre-processing input so we're still going to need that
we still have our tensorflow image to array we have our load model that's where most of this stuff is going on
this is our cv2 or opencv again i'm not going to dig too deep into that we're
going to flash a little open cv code at you and we actually have a tutorial on that coming out
our numpy array our im utilities which is part of the opencv or cv2 setup
and then we have of course time and just our operating system so those are the things we're going to go ahead and set up on here
and then we're going to create this takes just a moment
our module here which is going to do all the heavy lifting uh so we're going to detect and predict
the mask we have frame face net mass net these are going to be generated by our
open cv we have a frame coming in and then we want to go ahead and create a mask around the face it's going to try
to detect the face and then set that up so we know what we're going to be processing through our model
and then there's a frame shape here this is just our height versus width that's all hw stands for
they've called it blob which is a cv2 dnn blob form image frame so this is reformatting this frame
that's going to be coming in literally from my camera and we'll show you that in a minute that little piece of code
that shoots that in here and we're going to pass the blob through the network and obtain the face
detections so face net dot set import blob detections face net forward
print detections shape uh so these this is what's going on here
this is that model we just created we're going to send that in there and i'll show you in a second where that is but
it's going to be under face net and then we go ahead and initialize our list of faces their corresponding
locations and the list of predictions from our face mass network
we're going to loop over the detections and this is a little bit more work than you think
as far as looking for different faces what happens if you have a crowd of faces
so we're looping through the detections and the shapes going through here and probability associated with the
detection here's our confidence of detections we're going to filter out weak detection
by ensuring the confidence is greater than the minimum confidence so we've set it remember zero
to one so 0.5 would be our minimum confidence probably is pretty good
[Music] and then we're going to put in compute bounding boxes for the object if i'm
zipping through this it's because we're going to do an open cv and i really want to stick to just the cross part
and so i'm just kind of jumping through all this code you can get a copy of this code from simply learn and take it apart
or look for the opencv coming out and we'll create a box uh the box sets
it around the image ensure the bounding boxes fall within the dimensions of the frame
so we create a box around what's going to be hope is going to be the face extract the face roi convert it from bgr
to rgb channel again this is an open cv issue not
really an issue but it has to do with the order i don't know how many times i forgot to check the order colors when working with opencv
because there's all kinds of fun things when red becomes blue and blue becomes red uh and we're gonna
go ahead and resize it process it frame it face frame setup again the face the cbt color we're
going to convert it we're going to resize it image to array pre-process the input
pin the face locate face start x dart y and x boy that was just a huge amount and i
skipped over a ton of it but the bottom line is we're building a box around the face
and that box because the open cv does a decent job of finding the face and that box is going to go in there and see hey
does this person have a mask on it uh and so that's what that's what all this
is doing on here and then finally we get down to this where it says predictions equals mass net dot predict faces batch
size 32. uh so these different images of where we're guessing where the face is are
then going to go through and generate an array of faces if you will and we're going to look through and say
does this face have a mask on it and that's what's going right here is our prediction that's the big thing that we're working
for and then we return the locations and the predictions the location just tells
where on the picture it is and then the prediction tells us what it is is it a
mask or is it not a mask all right so we've loaded that all up
so we're going to load our serialized face detector model from disk
and we have our the path that it was saved in obviously going to put it in a different path depending on where you have it or however you want to do it and
how you saved it on the last one where we trained it and then we have our weights path
and so finally our face net here it is equal dot read cb2.dnn.readnet
prototext path weights path and we're going to load that up on here so let me go ahead and run that
and then we also need to i'll just put it right down here i always hate separating these things in there
and then we're going to load the actual mass detector model from disk this is the the model that we saved so let's go
ahead and run that on there also so this is pulling on all the different pieces we need for our model
and then the next part is we're going to create open up our video
and this is just kind of fun because it's all part of the opencv video set up
and we just put this all in as one there we go uh so we're going to go ahead and open
up our video we're going to start it and we're going to run it until we're done
and this is where we get some real like kind of live action stuff which is fun this is what i like working about with
images and videos is that when you start working with images and videos it's all like right there in front of you it's
visual and you can see what's going on so we're going to start our video streaming this is grabbing our video
stream source zero start that means it's k grabbing my main camera i have hooked
up and then you know starting video you're going to print it out here's our video source equals 0 start
loop over the frames from the video stream oops a little redundancy there
let me close i'll just leave it that's how they headed in the code so
so while true we're going to grab the frame from the threaded video stream and resize it to have the maximum width of
400 pixels so here's our frame we're going to read it from our visual
stream we're going to resize it and then we have we're returning
remember we returned from the our procedure the location and the prediction so detect and predict mask
we're sending it the frame we're sending it the face net and the mast net so we're sending all the
different pieces that say this is what's going through on here
and then it returns our location and predictions and then for our box and predictions in the location and
predictions and the boxes is again this is an open cv set that says hey this is a box
coming in from the location because you have the two different points on there
and then we're going to unpack the box in predictions and we're going to go ahead and do mask without a mask equals
prediction we're going to create our label no mask and create color if the label
equals mask l0 225 and you know it's going to make a lot more sense when i hit the run button here
but we have the probability of the label we're going to display the label and bounding box rectangle on the output
frame and then we're going to go ahead and show the output from the frame cv to im
show frame frame and then the key equals cp2 weight key one we're just going to wait till the next one comes through
from our feed and we're going to do this until we hit the stop button pretty much
so you're ready for this to see if it works we've distributed our our model we've loaded it up into our
distributed code here we've got a hooked into our camera and we're going to go ahead and
run it and there it goes it's going to be running and we see the data coming down here and we're waiting for the pop-up
and there i am in my office with my funky head set on uh
and you can see in the background my unfinished wall and it says up here no mask oh no i don't have a mask on
i wonder if i cover my mouth what would happen uh you can see my no
mask goes down a little bit i wish i brought a mask into my office it's up at the
house but you can see here that this says you know there's a 95 98
chance that i don't have a mask on and it's true i don't have a mask on right now and this could be distributed this
is actually an excellent little piece of script that you could start you know you install somewhere on a video feed on a
on a security camera or something and then you have this really neat setup saying hey do you have a mask on when
you enter a store or public transportation or whatever it is where they're required to wear a mask
let me go ahead and stop that now if you want a copy of this
code definitely give us a hauler we will be going into opencv in another one so i skipped a lot of the opencv
code in here as far as going into detail really focusing on the karass
saving the model uploading the model and then processing a streaming video through it so you can see that the model
works we actually have this working model that hooks into the video camera
which is just pretty cool a lot of fun so i told you we're going to dive in and
really roll up our sleeve and do a lot of coding today we did the basic uh demo up above for just pulling in across
and then we went into a cross model where we pulled in data to see whether someone was wearing a mask or not so
very useful in today's world as far as a fully running application
what is in it for you today we're going to cover what is keras computational graphs
what are neural networks sequential models and then we'll do a hands-on demo so you can see what's going on with
sequential models and keras so what is keras cross is a high-level python deep
learning api which is used for easy implementation of neural networks it has multiple
low-level back-ends like tensorflow fano pi torch etc which are used for fast
computation so you can think of this as cross being almost its own little programming
language and then it sits on neural networks in this case uh the ones listed were
tensorflow thano and pytorch which can all integrate with the cross model this makes it very diverse and also
makes it very easy to use and switch around with different things cross is very
user friendly as far as neural network software goes as a high level api
computational graphs so computational graphs are really the heart and soul of neural networks
we talk about a computational graph they are a visual representation of expressing and evaluating mathematical
equations the nodes and data flow in a graph correspond to mathematical operations
and variables you'll hear a lot some of the terms you might hear are
node and edge the edge being the data flow in this case it could also represent an actual value
they have oh i think in spark they have a graph x which works just on computing edges
there's all kinds of stuff that has evolved from computational graphs we're focusing just on keras and on neural
networks so we're not going to go into great detail on everything a computational graph does
it is a core component of a neural network is what's important to know on this so cross offers a python user-friendly
front-end while maintaining a strong computation power by using a low-level api
like tensorflow pi torch etc which use computational graphs as a back end
so one this allows for abstraction of complex problems while specifying control flow
if you've ever looked at some of the backend or the original versions of tensorflow it's really a nightmare you have all
these different settings you have to put in there and create it's a lot of a lot of back in programming this is like the old
computers when you had to tell it how to dispose of a variable and how to properly re-allocate the memory
for use all that is covered nowadays in our higher level programming well this is
the same thing with keras is it covers a lot of this stuff and does things for you that you could spend hours on just
trying to figure out it's useful for calculating derivatives by using back propagation
we're definitely not going to teach a class on derivatives uh in this little video but understanding uh a derivative is the
rate of change so if you have a particular function you're using in your neural network
a lot of them is just simple y equals mx plus b
your euclidean geometry where you just have a simple slope times the intercept and they get very complicated they have
the inverse tangent function for activation as opposed to just a linear euclidean model and you can think about
this as you have your data coming in and you have to alter it somehow well you alter it going down to get an
answer you end up with an error and that error goes back up and you have to have that back propagation with the
derivative you want to know how it changed so that you can figure out how to adjust it for the error
a lot of that's hidden so you don't even have to worry about it with cross and in today's cross you'll even if you create
your own formula for computing an answer it will automatically compute the back prop the
the derivative for you in a lot of cases it's easier to implement distributed computation
so cross is really nice way to package it and get it off on different computers and share it and it allows parallelism
which means that two operations can run simultaneously so as we start developing these back
ends it can do all kinds of cool things and utilize multiple cores gpus on a
computer to get that parallel processing up
what are neural networks well like i said there are already we've talked about in computational edges you
have a node and you have a connection or your edge so neural networks are algorithms fashioned after the human
brain which contain multiple layers each layer contains a node called a neuron
which performs a mathematical operation they break down complex problems into simple operations
so one an input layer takes in our data and pre-processes it when we talk about pre-processing when
you're dealing with neural networks uh you usually have to pre-process your data so that it's between minus one and one or zero and one
um into some kind of value that's usable that occurs before it gets to the neural network in fact 80 of data science is
usually in prepping that data and getting it ready for your different models
two you have hidden layer performs a non-linear transformation of input
now it can do a hidden a linear transformation it can use just a basic um euclidean geometry and you could
think of a node adding all the different connections coming in uh so each connection would have a weight
and then it would add to that weight plus an intercept in the note itself so you can actually use euclidean geometry
but a lot of these get really complicated they have all these different formulas and they're really cool to look at but when you start
looking at them look at how they work you really don't need to know the high
math behind it to figure them out and figure out what they're doing which is really cool that means a lot of
people can use this without having to go get a phd in mathematics number three the output layer takes the
results from hidden layer transform them and gives a final output
so sequential models so what makes this a sequential model sequential models are linear stacks of
layers where one layer leads to the next it is simple and easy to implement and you just have to make sure that the
previous layer is the input to the next layer so you have used for plain stack of
layers where each layer has one input and one output tensor and this is what tensorflow is
named after is each one of these layers is like a tensor each node is a tensor and the layer is
also considered a tensor of values and it's used for simple classifier
declassifier models you can it's also used for regression models too so it's not just about uh this is something this
is a teapot this is a cat this is a dog it's also used for generating um
regret the actual values you know this is worth ten dollars that's worth thirty dollars uh the weather is going to be 90
degrees out or whatever it is so you can use it for both classifier and declassifier models
and one more note when we talk about sequential models the term sequential is
used a lot and it's used in different areas and different notations when you're in data science so when we talk
about time series we'll talk about sequential that is something very different sequential in this case means
it goes from the input to layer 1 to layer 2 to the output so it's very directional
it's important to note this because if you have a sequential model can you have a non-sequential model and the answer is
yes if you master the basics of a sequential model you can just as easily have
another model that shares layers you can have another model where you have an input coming in and it splits
and then you have one set that's doing one set of nodes maybe they're doing a
yes no kind of node where it's either putting out a zero or a one a classifier and the other one might be regression
it's just processing numbers and then you recombine them for the output that's what they call across the cross api
so there's a lot of different availabilities in here and all kinds of cool things you can do as far as
encoding and decoding and all kinds of things and you can share layers and things like that we're just focusing on
the basic cross model with the sequential model
so let's dive into the meat of the matter let's do and do a demo on here today's demo in this demo we'll be
performing flower classification using sequential model and cross and we'll use our model to classify between five
different types of flowers
now for this demo and you can do this demo on whatever platform you want or whatever
user interface for developing python i'm actually using anaconda and then i'm
using jupyter notebooks to develop in and if you're not familiar with this you
can go under environment once you've created environment you can come in here to open a terminal window
and if you don't have the different modules in here you can do your conda install whatever module it is
just happened that this particular setup didn't have a seaborn in it which i already installed
so here's our anaconda and then i'm going to go back and start up my jupiter notebook
where i already created a new python project python3 i'm in python 3.8
on this particular one sequential model for flowers so lots of
fun there so we're going to jump right into this the first thing is to make sure you have
all your modules installed so if you don't have numpy pandas
matplot library and seaborn and the cross an sk learn or site kit it's not
actually sklearn you'll need to go ahead and install all of those now having done this for years and
having switched environments and doing different things i get all my imports done and then we
just run it and if we get an error we know we have to go back and install something right off the bat though we have numpy
pandas matplot library seaborn these are built on top of each other panda's the data frame and built on top of numpy the
data array and then we bring in our sklearn or scikit this is the scikit setup sci
kit even though you use sklearn to bring it in it's a scikit and then our cross we
have our pre-processing the images image data generator our model this is our basic model our
sequential model uh and then we bring in from cross layers import dents
optimizers these optimizers a lot of them already come in these are your different
optimizers and it's almost a lot of this is so automatic now um adam
is the a lot of times the default because you're dealing with a large data
and then we get our sgd which is uh smaller data does better on smaller pieces of data
and i'm not going to go into all of these different optimizers we didn't even use these in the actual demo you just have
to be aware that they are different optimizers and the digger the more you dig into these models
you'll hit a point where you do need to play with these a little bit but for the most part leave it at the default when you're first starting out
and we're doing just the sequential you'll see here layers dense and then if we come down a little bit
more when they put this together and they're running the dense layers you'll also see they have dropout they have
flatten they have activation they have the convolutional
layer 2d max pooling 2d batch normalization
what are all these layers and when we get to the model we're going to talk about them a lot of times when you're
just starting you can just import cross dot layers and then you have your drop out your flattened
your convolutional neural network 2d and we'll we'll cover what these do in
the actual example when we get down there what i want you to take from here though is you need to run your imports
and load your different aspects of this and of course your tensorflow tf because
this is all built on tensorflow and then finally import random is rn
just for random generation and then we get down here we have our cv2
that is your open image or your opencv they call it for processing images that's what the
cvd2 is we have our tqdm
the tqdm is for um it's a progress bar just a fancy way
of adding when you're running a process you can view the bar going across in the jupiter
setup not really necessary but it's kind of fun to have we want to shuffle some files uh again
these are all different things pill is another um image processor it goes with the cv2 a
lot of times you'll see both of those and so we run those we've got to bring them all in and the next thing is to set up our
directories and so when we come into the directories there's an important thing to note on
here other than we're looking at a lot of flowers which is fun as we get down here we have our
directory archive flowers that just happens to be where the different files for different flowers are put in
we're denoting an x and a z and the x is the date of the image and
the z is the tag for it what kind of flower is this and the image size is really important
because we have to resize everything if you have a neural network and if you remember from our neural networks let me
flip back to that slide we look at this slide we have two input nodes here with an image you have an
input node depending on how you set it up for each pixel and that pixel has three different
color schemes usually in it sometimes four so if you have a picture that's 150 by 150
you multiply 150 times 150 times three that's how many nodes input layers
coming in i mean so this is a massive input a lot of times you think oh yeah it's just a small amount of data or
something like that no it's a full image coming in and then you have your hidden layers a lot of times they match what the image
size is coming in so each one of those is also just as big and then we get down to just a single output
so that's kind of a thing to note in here what's going on behind the scenes and of course each one of these layers
has a lot of processes and stuff going on and then we have our different uh
directories on here let me go and run that so i'm just setting the directories that's all this is
archive flowers daisy sunflower tulip dandelion rose just our different directories that
we're going to be looking at
uh and then we want to go ahead and we need to assign labels remember we defined x and z
so we're just going to create a definition here
and the first thing is uh return flower type okay it just returns it what kind of flower
it is i guess a sign label to it but we're going to go ahead and make our train data and when you look at this there's a
couple things to take away from here the first one is we're just appending right
onto our numpy array the image we're going to let numpy handle all that
different aspects as far as 150 by 150 by three
we just dump it right into the numpy which makes it really easy we don't have to do anything funky on the processing
and we want to leave it like that and i'm going to talk about that in a minute and then of course we have to have the
string append the label on there and i want you to notice right here we're going to read the image in
and then we're going to size it and this is important because we're just changing this to 150 by 150 we're resizing the
image so it's uniform every image comes in identical to the other ones
this is something that's so important is when you're resizing or reformatting your data you really have to be aware of
what's going on with images it's not a big deal because with an image you just resize it so it looks
squishy or spread out or stretched the neural network picks up on that and
it doesn't really change how it processes it so let's go ahead and run that
and now we've got our definition set up on there and then we want to go ahead and make
our training data so make the train data
daisy flower daisy directory uh print length of x so here we go let's go and run that
and we're just loading up the flower daisy so this is going all in there and it's setting
it's adding it in to the our setup on there to our x and z setup
and we see we have 769 and then of course you can see this nice bar here this is the bar going across is
that little added uh code in there that just makes it really cool for doing demos uh not necessarily when you're
building your own model or something like that but if you're going to display this to other people adding that little what was it called
tqdm i can never remember that but the tqdm module in there is really nice and
we'll go ahead and do sunflowers and of course you could have just created an array of these
but this has an interesting problem that's going to come up and i want to show you something it doesn't matter how good the people in
the back are or how good you are programming errors are going to come up and you got to figure out how to handle them uh and
so when we get all the way down to the where is it dandelion here's our
dandelion directory we're going to build jupiter has some cool things it does
which makes this really easy to deal with but at the same time you want to go back
in there depending on how many times you rerun this how many times you pull this so when you're finding errors
going in here there's a couple things you can do and we're just going to um oh it wasn't there it is there's our error
i knew there was an error this processed 1062 out of 1065.
now i can do a couple things one i could go back into our definition and i can just put in here try and so if
it has a bad conversion this is where the error is coming from uh just skip it that's one way to do it
when you're doing a lot of work in data science and you look at something like this where you're losing three points of
data at the end you just say okay i lost three points who cares or you can go in there and try to delete
it it really doesn't matter for this particular demo and so we're just going to leave that
error right alone and skip over because it's already added all the other files in there and this is a wonderful thing
about jupiter notebook is that i can just continue on there and the x and z which we're creating
is still running and we'll just go right into the next flower rose so all these flowers
are in there that's just a cool thing about jupiter notebook
and then we can go ahead and just take a quick look and see what we're dealing with and this is of
course really when you're dealing with the other people and showing them stuff this is just kind of fun where we can
display it on the plot library here and we're just going to go through
let's see what we got here uh looks like we're gonna do like five of each of them
i think is that how they set this up um plot library five by two okay oh i see
how they did it okay so two each so we have five by two set up on our axes and we're just going to go in and look at a
couple of these flowers it's always a good thing to look at some of your data
no matter what you're doing we've reformatted this to 150 by 150
you can see how it really blurs this one up here on the tulip that is that resize to 150 by 150
and these are what's actually going in these are all 150 by 150 images you can check the dimensions on the side
and you can see just a quick sampling of the flowers we're actually going to process on here
and again like i said at the beginning most of your work in data science is reprocessing
this different uh information so we need to go ahead and take our labels uh and run a label encoder on there and
then we're just gonna le is a label encoder one of the things we imported
and then we always use the fit um to categorical y comma five x here's
our array um x so if you look at this here's our fit we're gonna transform z
that's our z array we created and then we have y which equals that and
then we go ahead and do to categorical we want five different categories and then we create our x
in p array of x x equals x over 255 so what's going on here there's two
different transformations one we've turned our categories into 0 1 2 3 4 5 as the output
and we have taken our x array and remember the x array is three values
of your different colors this is so important to understand when we do this across a numpy array this
takes every one of those three colors so we have 150 by 150 pixels out of those 150 by 150 pixels they each
have three um color arrays and those color arrays range from zero to 250. so when we take
the x equals x over 255 i'm sorry range from zero to 255
this converts all those pixels to a number between 0 and 1.
and you really want to do that when you're working with neural networks now if you do a linear regression model
it doesn't affect it as much so you don't have to do that conversion if you're doing straight numbers but when you're running neural networks if you
don't do this you're going to create a huge bias and that means it'll do really good on predicting one or two things and they'll
just totally die on a lot of other predictions so now we have our
x and y values x being the data in y being our known output
and with any good setup we want to divide this data into our training so we have x train
we have our x test this is the data we're not going to program the model with and of course your y train corresponds
to your x train and your y test corresponds to your x test the outputs and this is when we do the train test
split this was from the site kit sklearn we imported train test split and we're
just going to go ahead and do the test size at about a quarter of the data 0.25 and of course random's always good this
is such a good tool i mean certainly you can do your own division um you know you could just take the first
you know 0.25 of the data or whatever do the length of the data not real hard to do but this is randomized so if you're
running this test a few times you can kind of get an idea whether it's going to work or not sometimes what i will do
is i'll just split the data into three parts and then i'll test it on two with one
being the or trained on two of those parts with one being the test and i rotate it so i come up with three different answers
which is a good way of finding out just how good your model is but for setting up let's stick with the x train x test and the sk learn package
and then we're going to go ahead and do a random seed now a lot of times the cross actually
does this automatically but we're going to go ahead and set it up on here and you can see we did an np random seed
from 42 and we get a nice rn number and then we do tf random we set the seed
so you can set your randomness at the beginning of your tensorflow and that's what the
tf.random.set is so that's a lot of prep
all this prep and then we finally get to the exciting part um this is where you probably spend once
you have the data prepped and you have your pipeline going and you have everything set up on there
this is the part that's exciting is building these models and so we look at this model one we're
going to designate a sequential they have the api which is across the cross tensorflow api versus sequential
sequential means we're going one layer to the next so we're not going to split the layer and bring it back together
it looks almost the same with the exception of bringing it back together so it's not a
huge step to go from this to an api and the first thing we're going to look at is
our convolutional neural network in 2d so what's going on here there's a lot of
stuff that's going on here the default for well let's start with the beginning what is a convolutional 2d
network well convolutional 2d network creates a number of small windows and those small
windows float over the picture and each one of them is our own neural network and this basically
becomes like a a categorization and then it looks at that and it says oh if we add these
numbers up a certain way we can find out whether this is the right flower based on this this little
window floating around which looks at different things and we have filters 32 so this is
actually creating 32 windows is what that's doing
and the kernel size is five by five so we're looking at a five by five square
remember it's 150 by 150 so this narrows it down to a five by five it's a two d
so it has your x y coordinates um and we look at this five by five remember each one of these is is
actually looking at five by five by three uh so we're actually looking at fifteen by fifteen different um pixels
and padding is just um usually just ignore that activation by default is relu we went
ahead and put the relu in there there's a lot of different activations
relu is for your smaller when you remember i mentioned atom when you have a lot of datum data use an atom
kind of activation or using atom processing we're using the relu here
it kind of gives you a yes or no but it it doesn't give you a full yes or no it
has a zero and then it kind of shoots off at an angle very common it's the most common one and
then of course here's our input shape 150 by 150 by three pixels
and then we have to pool it so whenever you have a two convolutional 2d layer we have to bring this back
together and pull this into a neural network and then we're going to go ahead and repeat this
so we're going to add another network here one of the cool things if you look at this is that it as it comes in it just
kind of automatically assumes you're going down to the next layer and so we have another convolutional
null network 2d here's our max pooling again we're going to do that again max pooling
and we're just going to filter on down now one of the things they did on this one is they changed the kernel size they
changed the number of filters and so each one of these steps kind of looks at the data a little bit
differently and that's kind of cool because then you get a little added filtering on there this is where you start playing with the
model you might be looking at a convolutional neural network which is great for image classifications
we get down to here one of the things we see is flattened so we added we just flatten it remember this is 150 by 150
by three well and actually the pool size changes so it's actually smaller than that flattened just puts
that into a 1d array so instead of being you know a tensor of this really
complexity with the the pixels and everything it's just flat and then the dense
is just another activation on there by default it is probably relu
as far as its activation and then oh yeah here we go in sequential they actually added the
activation as relu so this just because this is sequential this activation is attached to the dents
and there's a lot of different activations but rayleigh is the most common one and then we also see a soft max
soft max is similar but it has its own kind of variation and one of the cool things you know what
let me bring this up because if we if you don't know about these activations this doesn't make sense
and i just did a quick google search on images of tensorflow activations
um i should probably look at which website this is but this is the output of the values uh
so as your x as it adds in all those uh weighted x values going into the node
it's going to activate it a certain way that's a sigmoid activation and you can see it goes between zero and one and has
a nice curve there this also shows the derivatives and if we come down the seven popular
activation functions nonlinear activations there's a lot of different options on this let me see if i can find
the let's make sure we can find the specific to relu
so this is a leaky rayloo and you can see instead of it just being zero and then a value between uh going
up it has a little leaky there otherwise your rayleigh loses some nodes they just become inactive
but you can see there's a lot of different options here here's a good one right here with the rayleigh you can see the rayleigh function on the upper on
the upper left here and then the leaky railway over here on the right which is very commonly used also
one of the things i use with processing language is the sig as the exponential
one or the tangent h the hyperbolic tangent because they have that nice
funky curve that comes in that has a whole different meaning and captures word use better
again these are very specific to domain and you can spend a lot of time playing with different models
for our basic model we'll stick to the relu and the softmax on here and we'll go ahead and run and build this model
so now that we've had fun playing with all these different models that we can add in there we need to go ahead and have a batch
size on here 128 epix10
this means that we're going to send 128 rows of data or flowers at a time to be
processed and the epics 10 that's how many times we're going to loop through all the data
reduce the values and verbose verbose equals 1 means that
we're going to show what's going on value monitor what we're monitoring
we'll see that as we actually train the model this is what's what's going to come out of there if you set the verbose
equal to 0 you don't have to watch it train the model although it is kind of nice to actually know
what's going on sometimes and since we're still working on
bringing the data in here's our batch side here's our epics we need to go ahead and create a data generator
this is our image data generator and it has all the different settings in
here almost all of these are defaults so if you're looking at this going oh my gosh this is confusing
most the time you can actually just ignore most of this vertical flip so you can randomly flip pictures you can
randomly horizontally flick them you can shift the picture around this kind of helps gives you multiple data
off of them zooming rotation there's all kinds of different things you can do with images
most of these we're just going to leave as false we don't really need to do all that
setup because we already have a huge amount of data if you're short data you can start flipping like a horizontal picture and
it will generate it's like doubling your data almost so the upside is you double your data
the downside is that if you already have a bias in your data you already have
5 000 sunflowers and only two roses that's a huge bias it's also going to
double that bias that is the downside of that
and so we have our model comp model compile and this you're going to see in all the cross we're going to take this
model here we're going to take all this information as far as how we want it to go and we're going to compile it
this actually builds the model and so we're going to run that and i want you to notice uh
learning rate very important this is the default zero zero one
there's there you really don't this is how slowly it adjusts to find the right answer
and the more data you have you might actually make this a smaller number with larger with you have a very small
sample of data you might go even larger than that and then we're going to look at the loss categorically categorical cross entropy
most commonly used and this is how how much it improves the model is improving is what this number
means or yeah that's that's important on there and then the accuracy we want to know just how good our model is on the
accuracy and then one of the cool things to do is if
you're in a group of people who are studying the model if you're in shareholders you don't want to do this
is you can run the model summary i do this by default and you can see the different layers that you built into
this model just a quick summary on there so we went ahead and we're going to go ahead and create a
we'll call it history but we want to do a model fit generator
and so what this history is doing is this is tracking what's going on as while it fits the model
now there's a lot of new setups in here where they just use fit and then you put the generator in here
we're going to leave it like this even though the new default is a little different on that
it doesn't really matter it does the same thing and we'll go ahead and just run that
and you can see while it's running right here we're going through the epics we have one of ten now we're going through
six to 25. here's our loss we're printing that out so you can see how it's improving and
our accuracy the accuracy gets better and better and this is six out of 25. this is going to take a couple minutes
to process because we are training 150 by 150 by 3 pixels across
six layers or eight layers whatever it was that is a huge amount of processing so
this will take a few minutes to process this is when we talk about the hardware and the problems that come up in data
science and why it's only now just exploding being able to do neural networks this is why this process takes
a long time now you should have seen a jump on the screen here because i did
pause the recorder to let this go ahead and run all the way through its epics let's go ahead and take a look and see
what these epics are and if you set the verbose to zero instead of one it won't show what's
going on in the behind the scenes as it's training it so we look at this epic 10 epic so we
went through all the data 10 times if i remember correctly there's roughly a gig of data there so that's a lot of
data the first thing you're going to notice is the 270 seconds
that's how much each of those epics took to run and so if you divide 60 in there you
roughly get about five minutes worth of each epic so if i have 10 epics that's
50 minutes almost an hour of run time that's a big deal we talk about
processing uh in on this particular computer i actually have what is it uh
eight cores with 16 dedicated threads so it runs like a 16 core computer it
alternates the threads going in and it still takes it five minutes for each one of these epics so you start to
see that if you have a lot of data this is going to be a problem if you have a number of models you want to find out
how good the models are doing what model to use and so each of those models could take all night to run in fact i have a model
i'm running now that takes over takes about a day and a half to test each model
it takes four days to do with the whole data so what i do is i actually take a small piece of the data
test it out to find out get an idea of how the different setups are going to do
and then i increase that size of the data and then increase it again and i can just take that that curve and kind
of say okay if the data is doing this then i need to add in more dense layers
or whatever so you can do a small chunks of data then figure out what it costs to do a large set of data and what kind of model
you want the loss as we see here continues to go
down this is the error this is how much error is in there it really isn't a
user-friendly number other than the more it trends down the better and so if you continue to see the loss going
down eventually get to the point where it stops going down and it goes up and down and kind of wavers a little bit at
that point you know you've run too many epics you're starting to get a bias in there and it's not going to give you a
good model fit the accuracy just turns us into something that
we can use and so the accuracy is what percentage of guesses in this case is categorical
so this is a percentage of guesses are correct um value loss is similar you know it's a
minus a value loss and then you have the value accuracy and you'll see the value accuracy is pretty similar to the accuracy um just rounds
it off basically and so a lot of times you come down here and you go okay we're doing 0.5.6
0.7 and that is 70 accuracy or in this case 68.4
accuracy that's a very usable number and it's very important to have if you're identifying flowers that's
probably good enough if you can get within a close distance and knowing what flower you're identifying if you're trying to figure out whether
someone's going to die from a heart attack or not you might want to rethink it a little bit or re-key how you're
building your model so if i'm working with a a group of clients
shareholders in a company or something like that you don't really want to show them this
you don't want to show them hey you know this is what's going on with the accuracy these are just numbers and so
we want to go and put the finishing touches just like when you are building a house and you put in the frame and the
trim on the house it's nice to have something a nice view of what's going on and so we'll go ahead and do a pie plot
and we'll just plot the history of the loss the history of the value loss
over here epics train and test and so we're just going to compute these this is really important and what i want
you to notice right here is when we get to about oh five epics a little more than five six
epics you see a cross over here and it starts crossing as far as the
value loss and what's going on here is you have the loss in your actual model and your actual data and you have the
value loss where it's testing it against the the test data the data wasn't used
to program your model wasn't used to train your model on and so when we see this crossing over
this is where the bias is coming in this is becoming over fitted and so when you put these two together
uh right around five and six you start to see how it does this this switch over
here and that's really where you need to stop right around five yeah six um
it's always hard to guess because at this point the model is kind of a black box uh see but you know that right around
here if you're saving your model after each run you want to use the one that's right around five epics because that's
the one that's going to have the least amount of bias so this is really important as far as guessing what's going on with your model and its
accuracy and when to stop it also is you know i don't show people this mess up here
i show somebody this kind of model i say this is where the training and the testing comes in on this model
it just makes it easier to see and people can understand what's going on
so that completes our demo and you can see we did what we were set out to do we took our flowers and we were able to
classify them uh within about you know 68 70 accuracy whether it's going to be a
dahlia sunflower cherry blossom rose a lot of other things you can do with your output as far as a
different tables to see where the errors are coming from and what problems are coming up
and we're going to take a look at image classification using cross and the basic setup and we'll actually look at two
different demos on here what's in it for you today
what is image classification intel image classification data
creating neural networks with keras and the vgg16 model
what is image classification the process of image classification
refers to assigning classes to an entire image images can be classified based on different categories like weather
it is a night time or daytime shot what the image represents etc you can see here we have mountains looking for
mountains well i should be doing some pictures of scenery and stuff like that
in deep learning we perform image classification by using neural networks to extract features from images and
classifies them based on these features and you can see here where it says like
what computer sees and says oh yeah we see mostly forests maybe a little bit of mountains because the way the image is
and this is really where one of the areas that neural networks really shines
if you try to run this stuff through more like a linear regression model
you'll still get results but the results kind of miss a lot of things as the
neural networks get better and better at what they do with different tools we have out there
so intel image classification data the data being used is the intel image
classification data set which consists of images of six types of land areas and so we have forest building glaciers
and mountains sea and street and you can see here there's a couple of the images out of there as a setup in
the intel image classification data that they use
and then we're going to go into creating a neural networks with cross
the convolutional neural network that we are creating from scratch looks uh as shown below
you'll see here we have our input layer um they haven't listed max pooling so you
have as you're coming in with the input layer and this the input layer is actually
before this but the first layer that it's going to go into is going to be a convolutional neural network
then you have a max pooling that pulls those the the convolutional neural networks returns
in this case they have two of those that is very standard with convolutional neural networks
one of the ones that i was looking at earlier that was standard being used by um i want to one of the larger companies i
can't remember which one for doing a large amount of identification had two convolutional neural networks each with
their max pooling and then about 17 dense layers after it we're not going to do that heavy duty of
a of a code but we'll get you head in the right direction and that gives you an idea of what you're actually going to
be looking at when you look at the flattened part and then the dents we're talking like 17 dense layers afterwards
i find that a lot of the stuff i've been working on i end up maxing it out right around nine dense layers it really
depends on what you have going in and what you're working with and the vgg16 model
vgg16 is a pre-trained cnn model which is used for image classification it is
trained on a large varied data set and fine-tuned to fit image classification data sets with ease
and you can see down here we have the input coming in the convolutional neural network one to
one one to two and then pooling and then we do two to one two to two convolutional network than pooling three
to two and you can see there's just this huge layering of convolutional neural networks and in this case they have five
such layers going in and then three dents going out or uh more
now when they took this setup this actually won an award back in 2019 for this particular
setup and it does it does really good except that again
we only show the three dense layers here and as you find out depending on your data going in what you
have set up that really isn't enough on one of these setups and i'm going to show you why we
restricted it because it does take up a lot of processing power in some of these things so let's go ahead and roll up our
sleeves and we're going to look at both the setups we're going to start with the the first classification
and then we'll go into the vgg16 and show you how that's set up now i'm going to be using anaconda and
let me flip over to my anaconda so you can see what that looks like now i'm running in the anaconda here
you'll see that i've set up a main python 3 8. i always put that in there this is
where i'm doing like most of my kind of playing around uh this is done in python version 3.8
we're not going to dig too much into versions at this point you should already have cross installed on there usually cross
takes a number of extra steps and then our usual
setup is the numpy the pandas your sk your scikit which is going to be
the sk learn your seaborn and i'll show you those in just a minute and then i'm just going to be in the
jupiter lab where i've created a new notebook in here
and let's flip on over there to my blank notebook
now there's a couple cool things to note in here is that um one i use the the
anaconda jupiter notebook setup because it keeps everything separate except for cross
cross is actually running separately in the back i believe it's a c program
uh what's nice about that is that it utilizes the multiprocessors on the computer and i'll mention that just in a
little bit when we actually get down to running the code and when we look in here a couple things
to note is here's our oops i thought i'd grab the other drawing
thing but here's our numpy and our pandas right here and our operating system
this is our psi kit you always import it as sklearn for the classification report
uh we're going to be using well usually import like seaborn brings in all of your pie plot library also
kind of nice to throw that in there i can't remember if we're actually using seaborne if they just the people in the back just threw that together
and then we have the sklearn shuffle for shuffling data here's our matplot library that the seaborn is pretty much
built on um cv2 if you're not familiar with that that is
our image module for importing the image and then of course we have our
tensorflow down here which is what we're really working with and then the last thing is just for
visual effect while we're running this if you're doing a demo and you're working with the partners or the
shareholders this tqdm is really kind of cool it's an extensible progress bar for python and
i'll show you that too remember data science is not i mean you know muscle's code when i'm looking
through this code i'm not going to show half of this stuff to the shareholders or anybody i'm working with they don't
really care about pandas and all that we do because we want to understand how it works
so we need to go ahead and import those different setup on there and then the next thing
is we're going to go ahead and set up our classes uh now we remember if we had mountain
street glacier building c and forest those were the different images that we have coming in
and we're going to go ahead and just do class name labels and we're going to kind of match that class name of ifri
class name equals the class names so our labels are going to match the names up here
and then we have the number of classes and the print the class names and labels
and we'll go ahead and set the image size this is important that we resize everything because if you remember with neural networks
they take one size data coming in and so when you're working with images you
really want to make sure they're all resized to the same setup it might squish them it might stretch them that generally does not
cause a problem in these and some of the other tricks you can do with if you if you need more data
and this is one that's used regularly we're not going to do it in here is you can also take these images and not only
resize them but you can tilt them one way or the other crop parts of them
so they process slightly differently and they'll actually increase your accuracy of some of these predictions
and so you can see here we have mountain equals zero that's what this class name label is street equals one glacier
equals two buildings equals three c four four s equals 5.
now we did this as an enumerator so each one is 0 through 5.
a lot of times we do this instead as
0 1 0 1 0 1 so you have five outputs and each one's a zero or a one coming out
so the next thing we really want to do is we want to go ahead and load the data up and just put a label in there loading
data just just so you know what we're doing i'm going to put in the loading data down here
make sure it's well labeled uh and we'll create a definition for this and this is all part of your
pre-processing at this point you could replace this with all kinds of different things
depending on what you're working on and if you once you download you can go download this data set uh send a note to
the simply learn team here in youtube and they'll be happy to direct you in the right direction and make sure you
get this path here um so you have to write whatever wherever you saved it a lot of times i'll just abbreviate the
path or put it as a sub thing and just get rid of the directory but again double check your paths we're
going to separate this into a segment for training and a segment for testing and that's
actually how it is in the folder let me just show you what that looks like
so when i have my lengthy path here where i keep all my programming simply learn this particular setup we're
working on image classification and image classification clearly you probably wouldn't have that
lengthy of a list and when we go in here you'll see sequence train sequence test
they've already split this up this is what we're going to train the data in and again you can see buildings forest glacier mountain sea street
and if we double click let's go under forest you can see all these different forest images and there's a lot of variety here
i mean we have winter time we have summer time so it's kind of interesting you know here's like a fallen tree
versus a road going down the middle that's really hard to train and if you
look at the buildings a lot of these buildings you're looking up a skyscraper we're looking down the
setup here's some trees with one and i want to highlight this one it has trees in it ah
let me just open that up so you can see it a little closer
the reason i want to highlight this is i want you to think about this we have trees growing is this the city
or a forest so this kind of imagery makes it really hard for a classifier and if you start
looking at these you'll see a lot of these images do have trees and other things in the foreground
weird angles really a hard thing for a computer to sort out and figure out whether it's
going to be a forest or a city
and so in our loading of data one we have to have the path the directory
we're going to come in here we have our images and our labels so we're going to load the images in one
section the labels in another and if you look through here it just
goes through the different folders in fact let me do this let me
there we go uh as we look at this we're just going to loop through the three to the six different folders that have the
different landscapes and then we're going to go through and pull each file out
and each label so we set the label we set the folder for file and list
here's our image path join the paths this is all kind of not general stuff
so i'm kind of skipping through it really quick and here's our image setup if you
remember we're talking about the images we have our cv2 reader so it reads the
image in it's going to go ahead and take the image and convert it to
from blue green red to red green green blue this is a cv2 thing almost all the time
it imports it and instead of importing it as a standard that's used just about everywhere it imports it with the bgr
versus rgb rgb is pretty much a standard in here you have to remember that with cv2
and then we're going to go ahead and resize it this is the important part right here we've said that we've decided
what the size is and we want to make sure all the images have the same size on them
and then we just take our images and we're just going to pin the image pin the label and then the images it's going to turn
into a numpy array this just makes it easier to process and manipulate
and then the labels is also a numpy array and then we just return the output append images and labels and we return
the output down here
so we've loaded these all into memory we haven't talked too much there'd be a
different setup in there because there is ways to feed the files directly into your cross model
but we want to go ahead and just load them all because really
for today's processing and what our computers can handle that's not a big deal
and then we go ahead and set the train images train labels test images test labels and that's going to be
returned in our output append and you can see here we did images and labels set up in there and it
just loads them in there so we'll have these four different categories let me just go ahead and run that
so now we've gone ahead and loaded everything on there
and then if you remember from before uh we imported just go back up there
shuffle here we go here's our sk learn utilities import shuffle and so we want to take these labels and
shuffle them around a little bit um just mix them up so it's not having the same if you run the same process
over and over uh then you might run into some problems on there
and just real quick let's go ahead and do uh um a plot so we can just you know we've
looked at them as far as from outside of our code we pulled up the files and i showed you what that was going on we can
go and just display them here too and i tell you when you're working with different people
this should be highlighted right here this thing is like when i'm working on code and i'm looking at this data and
i'm trying to figure out what i'm doing i skip this process the second i get into a meeting and i'm
showing what's going on to other people i skip everything we just did so
and go right to here where we want to go ahead and display some images and take a look at it
and in this display i've taken them and i've resized the images to 20 by 20.
[Music] that's pretty small so we're going to lose just a massive amount of detail
and you can see here these nice pixelated images i might even just stick with the folder
showing them what images were processing uh again this is you gotta be a little careful this
maybe resizing it was a bad idea in fact let me try it without resizing and see what happens oops so i took out
the image size and then we put this straight in here one of the things again this is um
but the d there we go one of the things again that we want to know whenever we're working on these things
uh is the cv2 there are so many different uh image
classification setups it's really a powerful package when you're doing images but you do need to switch it around so
that it works with the pie plot and so make sure you take your numpy array and change it to a u integer eight format uh
because it comes in as a float otherwise you'll get some weird images down there um and so this is just basically we've
split up our we've created a plot we went ahead and did the plot 20 by 20
or the plot figure size is 20 by 20 and then we're doing 25 so a 5 by 5
subplot nothing really going on here too exciting but you can see here where we get the images
and really when you're showing people what's going on this is what they want to see
so you skip over all the code and you have your meeting you say okay here's our images of the building
don't get caught up in how much work you do get caught up in what they want to see so if you want to work in data
science that's really important to know and this is where we're going to start
having fun here's our model this is where it gets exciting when you're digging into these models
and you have here let me get there we go
when you have here if you look here here's our convolutional neural network 2d
and 2d is an image you have two different dimensions x y and even though there's three colors it's still considered 2d
if you're running a video you'd be convolutional neural network 3d if you're doing a series going across
a time series it might be 1d and on these you need to go ahead and
have your convolutional neural network if you look here there's a lot of really cool settings going on
to dig into we have our input shape so everything's been set to 150 by 150
and it has of course three different color schemes in it that's important to notice activation
default is relu this is small amounts of data are being
processed on a bunch of little neural networks
and right here is the 32 that's how many of these convolutional null networks are
being strung up on here and then the three by three uh when it's doing its steps it's
actually looking at a little three by three square on each image and so that's what's going on here and
with convolutional neural networks the window floats across and adds up all these numbers going
across on this data and then eventually it comes up with 30 in this case 32 different
feature options that it's looking for and of course you can change that 32 you
can change the 3x3 so you might have a larger setup you know if you're going across 150
by 150 that's a lot of steps so we might run this as 15 by 15.
there's all kinds of different things you can do here we're just putting this together again that would be something you would play
with to find out which ones are going to work better on this setup and there's a lot of play involved
that's really where it becomes an art form is guessing at what that's going to be the second part i mentioned earlier and
i can only begin to highlight this when you get to these dense layers
one is the activation is a relu they use a relu and a softmax here
it's a whole a whole setup just explaining why these are different
and how they're different because there's also an exponential there's a tangent in fact there's just a ton of these and you can
build your own custom activations depending on what you're doing a lot of different things go into these
activations uh there are two or three major thoughts on these activations and
relu and softmax are well relu you're really looking at just the number
you're adding all the numbers together and you're looking at euclidean geometry ax plus b
x 2 plus c x 3 plus a bias
with soft max this belongs to the party of um it's activated or it's not except it's
they call it soft max because when you get the the to zero instead of it just being zero
uh it's actually slightly a little bit less than zero so that when it trains it doesn't get lost
um there's a whole series of these activations another activation is the tangent
um where it just drops off and you have like a very narrow area where you have
from minus one to one or exponential which is zero to one so there's a lot of different ways to do
the activation again we can do that would be a whole separate lesson on here
we're looking at the convolutional neural network and we're doing the two pools this is so common you'll see two two
convolutional neural networks stacked on top of each other each with its own max pool underneath let's go ahead and run
that so we built our model there and then we need to go ahead and
compile the model so let's go ahead and do that
we're going to use the atom optimizer the bigger the data the atom
fits better on there just some other optimizer but i think atom's a default um i don't really play with the
optimizer too much that's like the if once you get a model that works really good you might try some different
optimizers atoms usually the most and then we're looking at loss
pretty standard we want to minimize our law we want to maximize the loss of error
and then we're going to look at accuracy everybody likes the accuracy i'm going to tell you right now
i i start talking to people and i'm like okay what's what's the loss on this and that and as a data science yeah i want
to know how the lot what's going on with that we'll show you why in a minute but everybody wants to see accuracy we
want to know how accurate this is uh and then we're going to run the fit and i wanted to do this just so i can
show you even though we're in a python
setup in here where jupiter notebook is using only a single processor i'm going to bring over my little cpu
tool uh this is eight cores on 16 dedicated threats so it shows up as 16
processors and actually i got to run this and then move it over so we're going to run this
and hopefully it doesn't destroy my mic uh and as it comes in you can see it's starting to go through the epics and we
said i set it for five epics and then this is really nice because cross uses all the different uh threads
available so it does a really good job of doing that uh this is going to take a while if you
look at here it's uh eta two minutes and 25 seconds 24 seconds so
this is roughly two and a half minutes per epic and we're doing five epics
so this is going to be done in roughly 15 minutes
i don't know about you but i don't think you want to sit here for 15 minutes watching the green bars go across so
we'll go ahead and let that run and there we go there was our 15 minutes it's actually
less than that because i did when i went in here realized that uh where was it
here we go here's our model compile here's our model flip fit and here's our epics so i did four epics
so a little bit better a little more like 10 to 11 minutes instead of uh doing the full
uh 15. and when we look at this here's our model we did we talked about the compiler uh
here's our history we're going to click history equals the model fit we'll go into that in just a minute
and we're looking at is we have our epics here's our validation split
so as we train it we're weighing the accuracy versus you kind of pull some data off to the
side uh while you're training it and the reason we do that is that
you don't want to overfit and we'll look at that chart in just a minute
here's batch size this is just how many images you're sending through at a time
the larger the batch it actually increases the processing speed and there's reasons to go up or down on
the batch size because of the uh the the smaller of the batch there's a certain point where
you get too large of a batch and it's trying to fit everything at once uh so i yeah 128 is kind of big
depends on the computer you're on what it can handle and then of course we have our train images and our train labels going in
telling it what we're going to train
on then we look at our four epochs here uh here's our accuracy we want the
accuracy to go up and we get all the way up to 0.83 or 83 percent now this is actual
percentage based pretty much and we can see over here our loss we want our loss
to go down really fluctuates 55 1.2 0.77
0.48 so we have a lot of things going on there let's go ahead and graph those
turn that up and our team in the back did a wonderful job of putting together
this basic plot set up here's our subplot coming in we're going to be looking at
from the history we're going to send it the accuracy and the value accuracy
labels and set up on there and we're going to also look at loss and value loss
so you can see what this looks like what's really interesting about this setup and let me let me just go
ahead and show you because without actually seeing the plots it doesn't make a whole lot of sense
it's just basic plotting of uh of the data using the pi plot library
and i want you to look at this this is really interesting when i ran this the first time i had
very different results um and they vary greatly and you can see
here our accuracy continues to climb um
and there's a crossover here put it in here right here's our crossover
and i point that out because as we get to the right of that crossover where our
accuracy we're like oh yeah i got point eight percent we're starting to get an overfit here
that's what this this switch over means as our value
as a training set versus the value accuracy stays the same and so that this
is the one we actually really want to be aware of and where it crosses
is kind of where you want to stop at um and we can see that also with the train loss versus the value loss right here we
did one epic and look how it just flatlines right there with our loss
so really one epic is probably enough
and you're gonna say wow okay point eight percent um certainly if i was working with the shareholders um
telling them that it has an eighty percent accuracy isn't quite true and and we'll look at that a little deeper
it really comes out here that the accuracy of our actual values is closer to 0.41
right here even after running it this number of times and so you really want to stop right
here at that crossover one epic would have been enough so the date is a little over fitted on
this when we do four epics
and uh whoops there we are okay my drawing won't go away um let's see if
i can get there we go for some reason i've killed my drawing
ability and my recorder all right took a couple extra clicks
uh so let's go ahead and take a look at our actual test loss so you see where it crosses over that's
where i'm looking at that's where we start overfitting the model and this is where
if we were going to go back and continually upgrade the model we would start taking a look at the
images and start rotating them we might start playing with the convolutional neural network instead of
doing the three by three window um we might expand that or you know find different things that might make a big
difference as far as the way it processes these things um so let's go ahead and take a look at our our test loss now remember we had
our training data now we're going to look at our test images and our test labels
for our test loss here and this is just model evaluate just like we did fit up here
where was it one more model fit with our training data going in now we're going to evaluate it on the
and and this data has not been touched yet so this model has never seen this data this
is on completely new information as far as the model is concerned of course we
already know what it is from the labels we have
and this is what i was talking about here's the actual accuracy right here 0.48
or four eight four seven so this forty nine percent of the time guesses what the image is
uh and i mean really that's the bottom dollar uh does this work for what you're
needing does 49 work do we need to upgrade the model more
in some cases this might be oh what was i doing i was working on
stock evaluations and we were looking at what stocks were the
top performers well if i get that 50 correct on top performers
uh i'm good with that um that's actually pretty good for stock evaluation
fact the number i had for stock was more like 30 something percent as far as being a
top performer stock much harder to predict but at that point you're like well
you'll make money off of that so again this number right here depends a lot on the domain you're
working on
and then we want to go ahead and bring this home a little bit more as far as looking at the different setup
in here and one of the from sk learn if you remember actually
let's go back to the top uh we had the classification report and this came in from our sklearn or scikit
setup and that's right here you can see it right here on the um see there we go
classification report right here uh sklearn metrics import classification report this we're going to look at next
a lot of this stuff uh depends on who you're working with so when we start looking at um precision
you know this is like for each value i can't remember what one one one was probably mountains so if 44
is not good enough if you're doing like um you're in the medical department and you're doing cancer is it is this
cancerous or not i'm only 44 accurate not a good deal you know i would not go with that um
so it depends on what you're working with on the different labels and what they're used for facebook
you know 44 i'm guessing the right person i hope it does a little bit better than
that um but here's our main accuracy this is what almost everybody looks at they say oh 48 that's what's important
again it depends on what domain you're in and what you're working with
and now we're going to do the same model oops somehow i got my there it goes i thought i was going to get stuck in
there again uh well this time we're going to be using the vgg16 and remember this one
uh all those layers going into it so it's basically a bunch of convolutional neural networks getting
smaller and smaller on here uh and so we need to go ahead and
import all our different stuff from cross we're importing the main one is the v g 16 set up on there let me just aim
that there we go there's kind of a pre-processing images
applications pre-process input this is all part of the vg g16 set up on there
and once we have all those we need to go ahead and create our model
and we're just going to create a vgg16 model in here inputs model inputs outputs model inputs
i'm not going to spend as much time as they did on the other one we're going to go through it really quickly one of the first things i would do is if
you remember in cross you can treat treat a model like you would a layer
and so at this point i would probably add a lot of dense layers on after the
vgg16 model and create a new model with all those things in there and we'll go
ahead and run this because here's our model coming in and our setup
and it'll take it just a moment to compile that what's funny about this is i'm waiting
for it to download the package since i pre-ran this it takes it a couple minutes to download
the vgg16 model into here and so we want to go ahead and train
features for the model we're going to predict that we're going to predict the train images and we're going to test
features on the predict test images on here and then i told you it's going to create
another model too and the people in the back did not disappoint me they went ahead and did just that
and this is really an important part um this is worth stopping for i told you i
was going to go through this really quick so here's our uh
we we have our model 2 um coming in and we've created a model
up here with the vgg16 model equals model inputs model inputs and so we have
our vgg16 this has already been pre-programmed uh and then we come down here i want you
to notice on this right here layer model 2 layers minus 4 to 1 x
layer x we're basically taking this model and
we're adding stuff onto it and so we've taken we've just basically
duplicated this model we could have done the same thing by using
model up here as a layer we could have the input go to this model
and then have that go down here so we've added on this whole setup this whole block of code from 13 to 17
has been added on to our vgg16 model and we have a new model uh with the
layer m input and x down here let's go ahead and run that and compile
it and that was a lot to go through right there when you're building these models this is the part that gets so
complicated did you get stuck playing in and yet it's so fun uh it's like a puzzle how
can i loop these models together and in this case you can see right here
that the layers we're just copying layers over and adding each layer in
this is one way to build a new model and we'll go ahead and run that
like i said the other way is you can actually use the model as a layer i've had a little trouble playing with it
sometimes when you're using the straight model over you run into issues
it seems like it's going to work and then you mess up on the input and the output layers there's all kinds of things that come up
let's go ahead and do the new model we're going to compile it again here's our metrics accuracy sparse categorical
loss pretty straightforward just like we did before we got to compile the model
and just like before we're going to take our create a history the history is going to be
a new model fit train 128 and just like before if you remember
when we started running this stuff we're going to have to go ahead and it's going to light up our setup on here and
this is going to take a little bit to get us all set up it's not going to just happen in a couple minutes so let me go
ahead and pause the video and run it and then we'll talk about what happened okay now when i ran that these actually
took about six minutes each um so it's a good thing i put it on whole
we did four epics i actually had to stop it says at 10 and switch it to forks i didn't want to wait an hour
and you can see here our accuracy and our loss numbers going down
and just at a glance it actually performed if you look at the
accuracy 0.2658 so our accuracy is going down or you
know 26 percent um 34 35 and you can see here at some point it
just kind of kicks the bucket again this is overfitting that's always an issue when you're
running on programming these different neural networks
and then we're going to go ahead and plot the accuracy history we built that nice little subroutine up above so we might as well
use it and you can see it right here
there's that crossover again and if you look at this look how the how the
the red shifts up how the our loss functions and everything crosses over we're overfitting after one
epic um we're clearly not helping the problem or doing better
um we're just gonna it's just gonna baseline this one actually shows with the training versus the loss
value loss maybe second epic so on here we're now talking more between the first
and the second epic and that also shows kind of here so somewhere in here it starts overfitting
and right about now you should be saying oh something went wrong there i thought
that when we went up here and ran this look at this we have the accuracy up
here it's hitting that 48 and we're down here um
you look at the score down here that looks closer to 20 percent not nearly
anywhere in the ballpark of what we're looking for and we'll go ahead and run it through
the uh the actual test features here and there it is um we actually run this on
the unseen data and everything point uh one eight or eighteen percent
um i don't know about you but i wouldn't want you know at 18 this did a lot worse than the other one
i thought this was supposed to be the super model the model that beats all models the vgg-16
that won the awards and everything well the reason is is that
one we're not pre-processing the data so it needs to be more there needs to be more
as far as like rotating the data at you know 45 degree angle taking partials of it
so you can create a lot more data to go through here um and that would actually greatly change the outcome on here and
then we went up here we only added a couple dense layers we added a couple convolutional neural
networks this huge pre-trained setup is looking for a lot more information coming in
as far as how it's going to train and so this is one of those things where i thought it would have done better and i
had to go back and research it and look at it and say why didn't this work why am i getting only uh 18 here instead of
44 or better and that would be wise it doesn't have enough training data coming in
and again you can make your own training data so it's not that we have a shortage of data it's that that some of that has
to be switched around and moved around a little bit and this is interesting right here too
if you look at the precision we're getting it on number two and yet we had zero on everything else
so for some reason it's not seeing uh the different variables in here so
it'd be something else to look in and try to track down and that probably has to do with the
input but you can see right here we have a really good solid 0.48 up here and that's where i'd really go with is
starting with this model and then we look at this model and find out why are these numbers not coming up better is it
the data coming in where's the setup on there and that is the art of data science
right there is finding out which models work better and why deep learning interview questions and we're going to
go from the very basics of neural networks and deep learning into some of the more commonly used models so you can
have understanding of what kind of questions are going to come up and what you need to know in interview questions we'll start with a very general concept
of what is deep learning this is where we take large volumes of data in this case on cats and dogs or whatever a lot
of times you use a training setup to train your model remember it's kind of like a magic black
box going on there then we use that to extract features or extract information and in this case classify the image of a
cat and a dog so the primary takeaway we're talking about deep learning is it learns from large volumes of structured
and even unstructured data and uses complex algorithms to train neural network it also performs complex
operations to extract hidden patterns and features and if we're going to discuss deep learning in this very
simplified overview and we also have to go over what is a neural network this is a common image you'll see of a drawing
of a forward propagation neural network and it's it's a human brain inspired system which replicates the way humans
learn so this is inspired how our own neurons in our brain fire but at a much simplified level obviously it's not
ready to take over the human uh population and be our leader yet not for many years it's very much in its infant
stage but it's inspired by how our brains work and they use a lot of other inspirations you can study brains of
moths and other animals that they've used to figure out how to improve these neural networks the most common one
consists of three layers of network and this is generally how you view these networks is you have an input you have a
hidden layer and an output and the neural network is broken up into many pieces but we focus just on the neural
network it's always on the hidden layers that we're making all the adjustments and figuring out how to best set up
those hidden layers for their functions to both train faster and to function better when we look at this of course we
have our input hidden and output each layer contains neurons called as nodes perform various operations and you can
see here we have the list of the nodes we have both our input nodes and our output nodes and then our hidden layer
nodes
with what is a multi-layer perceptron or mlp a lot of times they're referred to
and you'll see these abbreviations i'll be honest i have to write them down on a piece of paper and go through them
because i never remember what they all mean even though i play with them all the time what is a multi-layer precipitant well if you look at the
image on the right it's very similar to what we just looked at you have your input layer your hidden layer and your
output layer and that's exactly what this is it has the same structure of a single layer perceptron with one or more
hidden layers except the input layer each node in the other layers uses a non-linear activation function what that
means is your input layer is your data coming in and then your activation function is based upon all those nodes
and weights being added together and then it has the output mlp uses supervised learning method called back
propagation for training the model very key word there is back propagation single layer perceptron can classify
only linear separable classes with binary output 0 1 but the mlp can
classify non-linear classes so let's break this down just a little bit the multi-layer perceptron with an input
layer and a hidden layer and an output layer as you see that it comes in there it has adds up all the numbers and
weights depending on how your setup is that then goes to the next layer that then goes to the next hidden layer if
you have multiple hidden layers and finally to the output layer the back propagation takes the error that it sees
so whatever the output is it says hey this has an error to it it's wrong and then sends that error backwards from
where it came from and there's a lot of different functions used to uh train this based on that error and how that
error goes backwards in the notes so forward is you get your answers backward
is for training you see this every day even my uh google pixel phone has this
it they train the neural network which takes a lot more data to train than it does to use and then they load up that
neural network into in this case i have a pixel 2 which actually has a built-in neural network for processing pictures
and so it's just the forward propagation i use when it processes my photos but when they were training it use the back
propagation to train it with the errors they had we'll be coming back to different models that are used for right
now though multi-layer perceptron mlp but then down is your vocabulary word
and of course back propagation what is data normalization and why do we need it this is so important we spend so
much time in normalizing our data and getting our data clean and setting it up so we talk about data there's a
pre-processing step to standardize the data so whatever we have coming in we don't want it to be a you know one
gigabyte file here a two gigabyte picture here and a three kilobyte text there even as a human i can't process
those all in the same group i have to reformat them in some way that loops them together so the standardized format
we use this uh data normalization in pre-processing to reduce it and
eliminate data redundancy a lot of times the data comes in and you end up with two of the same images or
the same information in different formats then we want to rescale values to fit into a particular range for
achieving better convergence what this means is with most neural networks they
form a bias we've seen this in recently in attacks on neural networks where they light up one pixel or one piece of the
view and it skews the whole answer so suddenly because one pixel is really bright it
doesn't know what to do but when we start rescaling it we put all the values between say minus one and one and we
change them and refit them to those values it helps get rid of that bias helps fix for some of those problems and
then finally we restructure the data and improve the integrity we want to make sure that we're not missing values or we
don't have partial data coming in one way to look at this is uh bad data in bad data out
so you want clean data in and you want good answers coming out one of the most basic models used is a boltzmann machine
so let's address what is a boltzmann machine and if you know we just did the mlp multi-layer perceptron so now we're
going to come into almost a simplified version of that and in this we have our visible input layer and we have our
hidden layer the boltzmann machines are almost always shallow they're usually just two layer neural nets that make
stochastic decisions whether a neuron should be on or off true false yes no
first layer is a visible layer and second layer is the hidden layer nodes are connected to each other across
layers but no two nodes of the same layer are connected hence it is also known as restricted boltzmann machine
now that we've covered a basic mlp or multi-layer perceptron and we've gone over the boltzmann machine also known as
the restricted boltzmann machine let's talk a little bit about activation formulas and this is a huge topic that
can get really complicated but it also is automated so it's very simple so you have both a complicated and a simple at
the same time so what is the role of activation functions in a neural network activation function decides whether a
neuron should be fired or not that's the most basic one and that actually changes a little bit because it's either whether
fired or not in this case activation function or what value should come out when it's fired but in these models
we're looking at just the boltzmann restricted layers so this is what causes them to fire either they don't or they
do it's a yes or no true false all or nothing it accepts a weighted sum of the inputs the bias as input to any
activation function so whatever activation function is it needs to have the sum of the weights times the input
so each input if you remember on that model and let's just go back to that model real quick and then you always have to add a bias and you can look at
the bias if you remember from your euclidean geometry you draw a straight line formula for that line has a
y-coordinate at the end it's always cx plus m or something like that where m
is where it crosses the coordinates if you're doing a straight line with these weights it's very
similar but a lot of times we just add it in as its own weight we take it as a node of a one value coming in and then
we compute its new weight and that's how we compute that bias just like we compute all the other weights coming in
the node which gets fires depends on the y value and then we have a step function and the step function this is where
remember i said is going to get complicated and simple all at the same time we have a lot of different step functions we have the sigmoid function
we have just the standard step function we have the relu it's pronounced like re
the from the sun and liu like a name so relu function and we have the tangent h
function and if you look at these they all have something similar they all either force it to be
one value or the other they force it to be in the case of the first three is zero or one and in the last one it's
either a minus one or a one and you can easily convert that to a zero one yes no true false and on this one of the most
common ones is the step function itself because there is no middle value there is no
uh discrepancy that says well i'm not quite sure but as you get into different models probably the most commonly used
used to be the sigmoid was most commonly used but i see the relu used more often really depending on what you're doing
you just have to play with these and find out which one works best depending on the data and your output
the reason to have a non-zero one answer or something kind of in the middle is
when you're looking at this and it's coming out you can actually process that middle ground as part of the answer into
another neural network so it might be that the rayleigh function says hey this is only a 0.6 not a 1. and even though
the 1 is what's going into the next neural network or the next hidden layer as an input the 0.6 value might also be
going in there to let you know hey this is not a straight up one or straight up zero it's someplace in the middle this
is a little uncertain what's coming out here so it's a very powerful tool open the basic neural network you usually
just use the step function it's yes or no let's take a big step back and take a kind of an
overview the next function is what is a cost function that we're going to cover
this is so important because this is your end result that you're going to do over and over again and use to decide
whether the model is working or not whether you need to try a different step function whether you need to try a different activation whether you need to
try a fully different model used so what is the cost function cost function is a measure to evaluate how
good your model's performance is it is also referred as loss or error used to
compute the error of the output layer during back propagation there's our back propagation where we're training our
model that's one of our key words mean squared error is an example of a popular
cost function and so here we have the cost function c equals half of y minus y
predicted and then you square that so the first thing is um you know real quick if you haven't done statistics
this is not a percentage it's not a percentage of how accurate it is it's just a measurement of the error and we
take that error for training it and we push that error backwards through the neural network and we use that through
the different training functions depending on what model you're using to train the neural network so when you
deploy the network you're usually done training it because it takes a lot of computational force to train it um this
is a very simple model and so you deploy the trained one but we want to know how your error is and so how do we do that
well you split your data part of your data is for training and part of your data is for testing and then we can also
test the error on there so it's very important and then we're going to go one more step on this we've got to look at
both the local and the global setup it might work great to test your data on
what you have on your computer but that's different than in the field so when we're talking about all these different tests in the error test as far
as your loss you don't you want to make sure that you're in a closed environment when you do initial testing but you also
want to open that up and make sure you follow up with the testing on the larger scale of data because it will change it might not fit the larger scale there
might be something in there in the way you brought the data in specifically or the data group you used or any of those
could cause an error so it's very important to remember that we're looking at both the local and the global context
of our error and just one other side note on a lot of the newer models of neural networks by comparing the error
we get on the data our training data with a portion of the test data we can actually figure out how good the model
is whether it's overfitted or not we'll go into that a little bit more as we go into some of the different models so we
have our output we're able to figure out the error on it based on the square means usually although there's
other functions used so we want to talk about what is gradient descent another vocabulary word gradient descent is an
optimization algorithm to minimize the cost function or to minimize the error aim is to find the local or global
minima of a function determine the direction the model should take to reduce the error so as we're looking at
this we have our squared error that we just figured out the cop based on the cost function it says how bad is my
model fitting the data i just put through it and then we want to reduce that error so how do you figure out what
direction to do that in well it could be that you're looking at just that line of that line of data coming in so that
would be a local minima we want to know the error of that particular setup coming in and then you have your global
your global minima we want to minimize it based on the overall data we're putting through it and with this we can
figure out the global minimum cost we want to take all those local minimum cost of each piece of data coming in and
figure out the global one how we're going to adjust this model to fit all the data we don't want it to be biased
just on three or four lines of data coming in we wanted to kind of extrapolate a general answer for all the
data coming in but this of course uh we mentioned it briefly about back propagation this is where really comes
in handy is training our model neural network technique to minimize the cost function helps to improve the
performance of the network back propagates the error and updates the weights to reduce the error so as you
can see here is a very nice depiction of a back propagation we have our predicted
y coming out and then we have since it's a training set we already know the answer and the answer comes back and
based on case of square means was one of the functions we looked at one of the activation functions based on cost
function that cost function then depending on what you choose for your back propagation method and there's a
number of them we'll change the weights it will change the weight going to each of the one of those nodes in the hidden layer and then based upon the error
that's still being carried back it'll change the way it's going to the next hidden layer and then it computes an
error level on that and sends that back up and you're going to say well if it computes the air into the first hidden
layer and fixes it why would it stop there well remember we don't want to create a biased neural network so we
only make small adjustments on these weights we don't make a big adjustment that changes everything right off the
bat so no matter how far back you go you're always going to have a small amount of error and that's still going to continue to go all the way back up
the hidden layers for right now focus on the back propagation is taking that
error moving it backwards on the neural network to change the weights and help program it so that it'll have the
correct answers so far we've been talking about forward propagation neural networks everything goes forwards goes
left to right but let's just take a little detour let's see what is the difference between a feed forward neural
network and a recurrent neural network now this is in the function not when we're training it using the back
propagation so you've got new information coming in and you want to get the answer and there's a couple different networks out there and we want
to know we have a feed forward neural network and we have a new vocabulary term recurrent neural network a feed
forward neural network signals travel in one direction from input to output no feedback loops considers only the
current input cannot memorize previous inputs one example of one of these feed
forward neural networks and we've covered a number of them but one of the ones that has a big highlight nowadays is the cnn a convolutional neural
network tensorflow the one put out by google is probably most known for their cnn where the information goes forward
it first takes a picture splits it apart goes through the individual pixels on the picture so it picks up a different
reading then calculates based on that goes into a regular feed forward neural network and then gives your
categorization on there now we're not covering the cnn today but we do have a video out that you can look up on
youtube put out by simply learn the convolutional neural network wonderful tutorial check that out and learn a lot
more about the convolutional neural network but you do need to know that the cnn is a forward propagation neural
network only so it's only moving in one direction so we will look at a recurrent neural network signals travel in both
directions making it a looped network considers the current input along with the previous received inputs for
generating the output of a layer has the ability to memorize past data due to its internal memory and you can see they
have a nice image here we have our input and for some reason they always do the recurrent neural network
in reverse from bottom up in the images kind of a standard although i'm not sure why your x goes into your hidden layer
and your hidden layer the answer for part of the answer from that it generates feeds back into the hidden
layer so now you have an input of both x and part of the hidden layer and then that feeds into your output now if we go
back to the forward let me just go back a slide and we're looking at our forward propagation network one of the tricks
you can do to use just a forward propagation network is if you're in a what they call a time sequence that's a
good term to remember are a time series meaning that a sequential data each term
comes after the other you can trick this by creating your input nodes as with the
history so if you know that you have values one five and seven going in and you know what the output is from one
what those outputs are you can expand the input to include the history input that's one of the ways to trick a
forward propagation network into looking at that but when you deal with the recurrent neural network you let the
hidden layer do that for you it sends that data and reprocesses it back into itself what are some of the applications
of recurrent neural network the rnn can be used for sentiment analysis and text
mining getting up early in the morning is good for health it's a positive sentiment one of the catches you really
want to look at this when you're looking at the language is that i could switch this around and totally negate the
meaning of what i'm doing so would no longer be positive so when you're looking at a sentence knowing the order
of the words is as important as the meaning of the words you can't just count how many good words there are
versus bad words to get some positive sentiment you now have to know what they're addressing and there's lots of
other different uses uh kids are playing football or soccer as we call it in the us rn can help you caption an image so
based on previous information coming in it refeeds that back in and you have a image setter and then time series
problems like predicting the prices of stocks in a month or quarter or sell a product can be solved using an rnn and
this is a really good example you have whatever your stocks were doing earlier this month will have a huge effect on
what they're doing today if you're investing so having an rnn model a recurrent neural network feeding into
itself what's happening previously allows it to take that model and program in the whole series without having to
put in the whole a month at a time of data you only put in one day at a time but if you keep them in order it will
look back and say oh this because what happened yesterday i need some information from that and i'm going to use that to help predict today's and so
on and so on we're going to go back to our activation functions remember i told you rayleigh was one of the most common
functions used uh so let's talk a little bit more about relu and also softmax softmax is an activation function that
generates the output between zero and one it divides each output such that the
total sum of the outputs is equal to one it is often used in the output layers soft max l of the n equals e to l on the
n over the absolute value of e to the l so what does this function mean i mean what is actually going on here so we
have our output nodes and our output nodes are giving us let's say they gave us 1.2 0.9 and 0.4 as a human being i
look at that and i say well the greatest value is 1.2 so whatever category that is if you have three different
categories maybe you're not just doing if it's a cat or it's a dog or oh let's say it's a cow and we had cats and dogs
earlier why the cats and dogs are hanging out with a cow i don't know but we have a value and it might say 1.2 is
a cat 0.9 is the dog and 0.4 is a cow for some reason it thinks that there's a chance of it being any one of these
three items and that's how it comes out of the output layer well as a human i can look at 1.2 and say this is definitely what it is it's definitely a
cat or whatever it is maybe it's looking at different kinds of cars might be a bitter whether it's a car truck or a
motorcycle maybe that'd be a better example well from a computer standpoint that might be a little confusing because
they're just numbers waving at us and so with the soft max we want all those numbers to always add up to one so when
i add three numbers together i want the final output to be one on there and so it goes through this formula changes
each of these numbers in this case it changes them to 0.46 0.34 and 0.20 they
all add up to 1. and that's a lot easier to register because it's very set it's a set output it's never going to be more
than one it's never going to be less than 0. and so you can see here that there's probably a pretty high chance that it's the first one so you're a
human being we have no problem knowing that but this output can then also go into say another input so it might be
an automated car that's picking up images and it says that image in front of us is probably a big truck we should
deal with it like it's a big truck it's probably not a motorcycle um or whatever those categories are that's the soft max
part of it but now we have the relu well what where is the relu coming from well the relu is what's generating the 1.2
and the 0.9 and the 0.4 and so if you remember our relu stands for rectified
linear unit and is the most widely used activation function we looked at a number of different activation functions
including tangent h the step function remember i said the step function is really used if that's what your actual
output is because then you know it's a zero or one but the ray loop if you have that as your output you now have a
discrepancy in there and if that's going into another neural network or another process having that discrepancy is
really important and it gives an output of x if x is positive and 0 otherwise so
it says my x value is going to be somewhere between 0 or 1 and then the usually unless it's really uncertain the
output's usually a 1 or 0s and then you have that little piece of uncertainty there that you can send forward to
another network or you can look at to know that there's uncertainty involved and is often used in the hidden layers
this is what's coming out of the hidden layers into the output layer usually or as we reference the convolutional neural
network the cnn you'd have to go to another video to review the relu is the most common used
for convolutional part of that network it has a bunch of little pieces that are very simplified looking at all the
different images or different sections of the map and the rayleigh works really good for that like i said there's other
formulas used but that this is the most common one and you'll see that in the hidden layers going maybe between one layer and the next layer so just a quick
recap we have our soft max which means that if you have numerous categories
only one of them is going to be picked but you also want to have some value attached to it how well it picked it and
you put that between 0 1 so it's very standardized so we have our soft max we looked at that let's go back one we
looked at that here where it transforms the numbers and then we have our relu function which takes the information and
the summation and puts it between a zero and a one where it's either clearly a zero or depending on how confident our
model is it'll go between the zero and one value what are hyper parameters well
this is a great interview question hyper parameters when you are doing neural networks this is what you're playing
with most the time once you've gotten the data formatted correctly a hyperparameter is a parameter whose
value is set before the learning process begins determines how a network is trained and the structure of the network
this includes things like the number of hidden units how many hidden layers are you going to have and how many nodes in
each layer learning rate learning rate is usually multiplied once you've figured out the error and how much you
want to change the weights we talked about or i mentioned it early just briefly you don't want to just make a huge change otherwise you're going to
have a biased model so you only take some little incremental changes and that's what the learning rate is is those small incremental changes epics
how many times are you going to go through all the data in your training set so one epic is one trip through all
the data and there's a lot of other things depending on which model you're working with and which programming
script you're working with like the python sk learn package will have a slightly different than say google's
tensorflow package which will be a little bit different than the spark machine learning package so these are
just some examples of the hyper parameters and so you see in here we have a nice image of our data coming in
and we train our model then we do a comparison to see how good our model is and then we go back and we say hey this
this model is pretty good but it's biased so and we send it back and we change our hyper parameters to see if we
can get an unbiased model or we can have a better prediction on it it matches our data closer what will happen if learning
rate is set too low or too high we have a nice couple graphs here we have one
over here so the learning rate set to load you can see that it slowly works its way down the curve and on the right
you can see a learning rate set too high it's just bouncing back and forth when your learning rate is too low that's
what we studied at two slides over that's what the learning rate was training of the model will progress very
slowly as we are making very tiny updates to the weights we'll take many updates before reaching the minimum
point so i just mentioned epic going through all the data you might have to go through all the data a thousand times
instead of 500 times for it to train learning rate too high causes undesirable divergent behavior to the
loss function due to drastic updates and weights at times it may fail to converge or even diverge so if you have your
learning rate set too high and it's training too quickly maybe you'll get lucky and it trains after one epic run
but a lot of times it might never be able to train because the weights are changing too fast they they flip back and forth too easy and you see down here
we've introduced two new terms converge and diverge a converge means that our
model has reached a point where it's able to give a fairly good answer for all the data we put in all those weights
have adjusted and it's minimized the error diverged means that the data is so chaotic that it can never manage to to
train to that data the data is just too chaotic for it to train so we have two new words there converge and diverge are
important to know also what is dropout and batch normalization dropout is a
technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data it doubles
the number of iterations needed to converge the network so here we have our standard neural network and then after
applying dropout now it doesn't mean we actually delete the node the node is still there and we're still going to use
that node what it means is that we're only going to work with a few of the nodes
a lot of times i think the most common one right now used is 20 percent uh so you'll drop out 20 of the nodes when you
do your training you reverse propagate your data and then you'll randomly pick another 20 nodes the next time you go
through an epic data training so each time you go through one epic you will randomly pick 20 of those nodes not not
to mess with and this allows for less overfitting of the data so by randomly doing this you create some i
guess it just kind of pulls some nodes off to the side it says we're going to handle the data later on so we don't over fit batch normalization is a
technique to improve the performance and stability of neural network the idea is to normalize the inputs in every layer
so that they have mean output and activation of zero and standard deviation of one this question covers a
lot of different things which is great it's a great uh interview question because it pulls in that you have to
understand what the mean value is so a mean output activation of zero that means our average activation is zero so
when you normalize it remember usually we're going between minus one and one on a lot of these it's a very standard
setup so you have to be very aware that this is your mean output activation of zero and then we have our standard deviation of one so we wanna keep our
error down to just a one value the benefits of this doing a batch normalization is it provides
regularization it trains faster higher learning rates and weights are easier to
initialize what is the difference between batch gradient descent and stochastic gradient descent batch
gradient descent batch gradient computes the gradient using the entire data set it takes time to converge because the
volume of data is huge and weights update slowly so you can look at the batches a lot of times if you're using
big data batch the data in but you still go through a full epic you still go through all the data on there so batch
gradient descent means you're going to use it to fit all the data and look for convergence there stochastic gradient
descent stochastic gradient computes the gradient using a single sample it converges much faster than batch
gradient because it updates weight more frequently explain overfitting and underfitting and how to combat them
overfitting happens when the model learns the details and noise and the training data to the degree that it
adversely impacts execution of the model on the new information it is more likely to occur with non-linear models that
have more flexibility when learning a target function an example of this would be if you're looking at say cars and
trucks and motorcycles it might only recognize trucks that have a certain
box-like shape it might not be able to notice a flatbed truck unless it's only a specific kind of flatbed truck or only
ford trucks because that's what it saw on the training set this means that your model performs great on your train data
and great on maybe a small test amount of data but when you go to use it in the real world it leaves out a lot and start
and is not very functional outside of your small area your slow laboratory data coming in under fitting doing the
opposite when you under fit your data underfitting alludes to a model that is neither well trained on training data
nor can generalize to new information usually happens when there is less and improper data to train a model has a
verb performance and accuracy so if you're using underfitted data and you generate a model and you distribute that
in a commercial zone you'll have a lot of people unhappy with you because it's not going to give them very good answers
so we've explained overfitting and under fitting so now we want to ask how to combat them combating overfitting and
underfitting resampling the data to estimate the model accuracy k-fold cross-validation having a validation
data set to evaluate the model so we do the resampling we're randomly going to be picking out data and we'll run it a
few times to see how that works depending on our random data and how we sample the data to generate our model
and then we want to go ahead and validate the data set by having our training data and then keeping some data
on the side testing data to validate it how are weights initialized in a network
initializing all weights to zero all the weights are set to zero this makes your model similar to a linear model so if
you have linear data coming in doing a basic setup like that might work all the neurons in every layer perform the same
operation given the same output and making the deep net useless right there is a key word it's going to be useless
if you initialize everything to zero at that point be looking into some other machine learning tools initializing all
weights randomly here the weights are assigned randomly by initializing them very close to zero it gives better
accuracy to the model since every neuron performs different computations and here we have the weights are set randomly we
have our input layer the hidden layers and the output layer and w equals np random random in layer size l layer size
l minus one this is the most commonly used is to randomly generate your weights what are the different layers in
cnn convolutional neural network first is the convolutional layer that performs
a convolutional operation we have our other video out if you want to explore that more so go into detail exactly how
the c the convolutional layer works on the cnn as far as creating a number of smaller picture windows to go over the
data the second step is as a rayleigh layer relu brings non-linearity to the network and converts all the negative
pixels to zero output is rectified feature map so it goes into a mapping feature there pooling layer pulling is a
down sampling operation that reduces the dimensionality of the feature map so we have all our relu layer which is pulling
all these little maps out of our convolutional layer it's taking that picture and little creating little tiny
neural networks to look at different parts of the picture uh then we need to pull it together and then finally the
fully connected layer so we flatten our pulling layer out and we have a fully connected layer recognizes and
classifies the objects in the image and that's actually your forward propagation reverse propagation training model
usually i mean there's a number of different models out there of course what is pooling in cnn and how does it
work pooling used to reduce the spatial dimensions of a cnn performs down
sampling operation to reduce the dimensionality creates a pooled feature map by sliding a filter matrix over the
input matrix i mentioned that briefly on the previous slide it's important to know that you have if you see here they
have a rectified feature map and so each one of those colors like the yellow color that might be one of the a smaller
little neural network using the relu you'll look at it'll just kind of go over the main picture and look at all
the different areas on the main picture so you might step one two three four spaces and then you have another one
that's also looking at features and it has a two seven eight five each one of those is a map so it might be the first
one might be a map looking for cat ears the second one looking for human eyes when it does this you then have this
rectified feature map looking at these different features and the max pooling with the two by two filters in a stride
a two stride means instead of skipping every pixel you're gonna go every two pixels you take the maximum values and
you can see over here we look at a pulled feature map one of the features says hey i had a max value of eight so
somewhere in here we saw a human eye labeled as eight pretty high label and maybe seven was a human hand and maybe
four was cat whiskers or something that we thought might be cat whiskers four is kind of a low number in this particular
case compared to the other ones so you have your full pool feature map you can see the process here is we have our
stepping we look for the max value and then we create a pool feature map of the max values how does a lstm network work
that's long short term memory so the first thing to know is that an lstms are a special kind of recurrent neural
network capable of learning long-term dependencies remembering information for long periods of time is their default
behavior we did look at the rnn briefly talked about how the hidden layer feeds back into itself with the lstm as a much
more complicated feedback and you can see here we have the hidden layer of t minus one and hidden layer that's what
the h stands for hidden layer of t and the formula is going in as we can see here we have the hidden layers we have t
minus one and then h of t where t stands for time so this is a series remember working with series we want to remember
the past and you can see you have your x your input of t and that might be a frame in a
video as the frame comes in they usually use in this one the tangent h activation
formula but you also see that it goes through a couple other formulas the omega formula and so when it combines
these that then goes into the next layer your next hidden layer that then goes into the data that's submitted to the
next input so you have your x of t plus one so when you have that coming in then you have your h values coming forward
from the last process and depending on how many of these omega structures you put in there depends on how long term
the memory gets so it's important to remember this is more for your long-term recurrent neural networks the three
steps in an lstm step one decides what to forget and what to remember step 2
selectively update cell state values so based on what we want to remember and forget we want to update those cell
values and then decides what part of the current state make it to the output so now we have to also have an output on
there what are vanishing and exploding gradients this is a great question that affects all our neural networks while
training an rnn your slope can become either too small or too large and this makes the training difficult when the
slope is too small the problem is known as vanishing gradient so our slope we have our change in x and our change in y
when the slope decreases gradually to a very small value sometimes negative and makes training difficult when the slope
tends to grow exponentially instead of decaying this problem is called exploding gradient the slope grows
exponentially you can see a nice graph of that here issues in gradient problem long training time poor performance and
low accuracy what is the difference between epic batch and iteration and deep learning epic an epic represents
one iteration over the entire data set so that's everything you're going to go ahead and put into that training model
batch we cannot pass the entire data set into the neural network at once so we divide the data set into a number of
batches and then iteration if we have 10 000 images as data and a batch size of
200 then the epic should run 10 000 times over 200 so that means we have our
total number over the 200 equals 50 iteration so in each epoch we're running over all the data set we're going to
have 50 iterations and each of those iterations includes a batch of 200
images in this case why tensorflow is the most preferred library in deep learning well first tensorflow provides
both c plus plus and python apis that makes it easier to work on has a faster compilation time than other deep
learning libraries like cross and torch tensorflow supports both cpus and gpus
computing devices so right now tensorflow is at the top of the market because it's so easy to use for both
programmer side and for hardware side and for the speed of getting something up and running what do you mean by
tensor in tensorflow tensor is a mathematical object represented as a rays of higher dimension these arrays of
data with different dimensions and ranks that are fed as input to the neural network are called tensors and you can
see here we have a tensor of dimensions five comma four so it's a two dimensional tensor coming in you can
look at an image like this that each one of those pixels is a different value if it's a black and white so it might be
zero and ones and then each one represents a black and white image in a color photo you might
either find a different value system or you might have a tensor value that has the xy coordinates as we see here plus
the colors so you might have three more different dimensions for the three different images the red the blue and
the yellow coming in and even as you go from one layer or one tenser to the next
these layers might change we might flatten them might bring in numerous in the case of the convergence neural
network we have all those smaller different mappings of features that come in so each one of those layers coming
through is a tensor if it has multiple dimensions coming in and weights attached to it what are the programming
elements in tensorflow well we have our constants constants are parameters whose value does not change to define a
constant we use tf.constant command example a equals tf.constant 2.0 tf
float32 so it's a tensorflow value of 32. b equals tf constant 3.0 print a b
if we did a print of a b we'd have tf.constant and then of course b is that
instance of it variables variables allow us to add new trainable parameters to graph to divide a variable we use
tf.variable command and initialize them before running the graph in session example w equals tf variable 0.3 d type
tfloat32 or b equals the tf variable minus three comma d type float32 placeholders placeholders allow us to
feed data to a tensorflow model from outside a model and permits a value to be assigned later to define a
placeholder we use tf placeholder command example a equals tf placeholder b equals a times two with the tf session
as sess result equals session run b comma feed dictionary equals a 3.0 print
result uh so we have a nice example there replace holder session a session is run to evaluate the nodes this is
called as the tensorflow run time so for example you have a equals tf constant 2.0 b equals tf constant 4.0 c equals a
plus b and at this point you'd go ahead and create a session equals tf session and then you could evaluate the tensor c
print session run c that would input c as an input into your session what do you understand by a computational graph
everything in tensorflow is based on creating a computational graph it has a network of nodes where each node
performs an operation nodes represent mathematical operation and edges represent tensors since data flows in a
form of a graph it is also called a data flow graph and we have a nice visual of
this graph or graphic image of a computational graph and you can see here we have our input nodes our add multiply
nodes and our multiply node at the end and then we have the edges where the data flows so we have from a going to c
a going to d you can see we have a two flowing four flowing explain generative adversarial network along with an
example suppose there is a wine shop that purchases wine from dealers which they will resell later we have a dealer
going to the wine or shop owner that then sells it for a profit but there are some malfactor dealers who sell fake
wine in this case the shop owner should be able to distinguish between fake and authentic wine the forger will try to
different techniques to sell fake wine and make sure certain techniques go past the shop owner's check so here's our
forager fake wine shop owner the shop owner would probably get some feedback from the wine experts that some of the
wine is not original the owner would have to improve how he determines whether a wine is fake or authentic goal
of forger to create wines that are indistinguishable from the authentic ones goal of shop owner to accurately
tell if the wine is real or not there are two main components of generative adversarial network and we refer to as a
noise vector coming in we have our forger who's going to generate fake wine and then we have our real authentic wine
and of course our shop owner has to figure out whether it's real or fake the generator is a cnn that keeps producing
images that are closer in appearance to the real images while the discriminator tries to determine the difference
between real and fake images the ultimate aim is to make the discriminator learn to identify real and
fake images what is an auto encoder the network is trained to reconstruct its
inputs it is a neural network that has three layers here the input neurons are equal to the output neuron the network's
target outside is same as the input it uses dimensionality reduction to restructure the input input image comes
in we have our latin space representation and then it goes back out reconstructing the image it works by
compressing the input to a latin space representation and then reconstructing the output from this representation what
is bagging and boosting bagging and boosting are ensemble techniques where the idea is to train multiple models
using the same learning algorithm and then take a call so we have in here where we're bagging we take a data set
and we split it we have our training data and our test data very standard thing to do then we're going to randomly
select data into the bags and train your model separately so we might have bag one model one bag two model two bag
three model three and so on in boosting the emphasis is to select the data points which give wrong output in order
to improve the accuracy so in boosting we have our data set again we split it to test data and train data and we'll
take a bag one and we'll train the model data points with wrong predictions then go into bag two and we then train that
model and repeat so we've covered a number of questions that you might get
during an interview hopefully this helps you get land that perfect job we want to thank you for joining us here at simply
learn again wwe get certified get ahead we have a number
of other resources out there so if you want to learn more about the rnn recurrent neural network or the cnn the
convolutional neural network is one of the main things we have a tensorflow out there you can come and