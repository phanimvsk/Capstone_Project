"you've probably read in the news that",
 'deep learning is the secret recipe',
 'behind many exciting developments and',
 "has made many of our world's dreams and",
 'perhaps also nightmares come true',
 'who would have thought that deep minds',
 'alphago could be at least a doll in a',
 'boat game which boasts in more possible',
 'moves than there are atoms in the entire',
 'universe',
 'a lot of people including me never saw',
 "it coming it seemed impossible but it's",
 'here now deep learning is everywhere',
 "it's beating physicians are diagnosing",
 "cancer it's responsible for translating",
 'web pages in a matter of mere seconds to',
 'the autonomous vehicles by weimo and',
 'tesla',
 'hi my name is jason and welcome to this',
 "course in deep learning where you'll",
 'learn everything you need to get started',
 'with deep learning in python how to',
 'build remarkable algorithms capable of',
 "solving complex problems that weren't",
 "possible just a few decades ago we'll",
 'talk about what deep learning is and the',
 'difference between artificial',
 "intelligence and machine learning i'll",
 'introduce new networks what they are and',
 'just how essential they are to deep',
 "learning you're going to learn about how",
 'deep learning models train and learn and',
 'the various types of learning associated',
 'supervised unsupervised and',
 "reinforcement learning we're going to",
 'talk about loss functions optimizers the',
 'grading descent algorithm the different',
 'types of neural network architectures',
 'and the various steps involved in deep',
 'learning',
 'this entire course is centered on the',
 'notion of deep learning but what is it',
 'deep learning is a subset of machine',
 'learning which in turn is a subset of',
 'artificial intelligence which involves',
 'more traditional methods to learn',
 'representations directly from data',
 'machine learning involves teaching',
 'computers to recognize patterns in data',
 'in the same way as our brains do so as',
 "humans it's easy for us to distinguish",
 "between a cat and a dog but it's much",
 'more difficult to teach a machine to do',
 "this and we'll talk more about this",
 'later on in this course before i do that',
 'i want to give you a sense of the',
 'amazing successes of deep learning in',
 'the past',
 'in 1997 gary kasparov the most',
 'successful champion in the history of',
 "chess lost to ibm's deep blue one of the",
 'first computer or artificial systems',
 'it was the first defeat of a reigning',
 'world chess champion by a computer',
 "in 2011 ibm's watson competed in game",
 'show jeopardy against his champions brad',
 'rotter and ken jennings and won the',
 'first prize a million dollars',
 'in 2015 alphago a deep learning computer',
 "program created by google's deepmind",
 'division defeated lisa dole an 18 time',
 'world champion at go a game of google',
 'more times complex than chess',
 'but deep learning can do more than just',
 'betas at boat games it finds',
 'applications anywhere from self-driving',
 'vehicles to fake news detection to even',
 'predicting earthquakes',
 'these were astonishing moments not only',
 'because machines beat humans at their',
 'own games but because of the endless',
 'possibilities that they opened up',
 'what followed such events have been a',
 'series of striking breakthroughs in',
 'artificial intelligence machine learning',
 'and yes deep learning',
 'to put it simply deep learning is a',
 'machine learning technique that learns',
 'features and tasks directly from data by',
 'running inputs through a biologically',
 'inspired neural network architecture',
 'these neural networks contain a number',
 'of hidden layers through which data is',
 'processed allowing for the machine to go',
 'deep in its learning making connections',
 'and weighing input for the best results',
 "we'll go over neural networks in the",
 'next video so why deep learning',
 'the problem with traditional machine',
 'learning algorithms is that no matter',
 "how complex they get they'll always be",
 'machine like they need a lot of domain',
 'expertise human intervention and are',
 "only capable of what they're designed",
 'for',
 'for example if i show you the image of a',
 'face you will automatically recognize',
 "it's a face",
 'but how would a computer know what this',
 'is well if we follow traditional machine',
 "learning we'd have to manually and",
 'painstakingly define to a computer what',
 'it faces for example it has eyes ears',
 'and mouth but now how do you define an',
 'eye or a mouth to a computer well if you',
 'look at an eye the corners are at some',
 "angle they're definitely not 90 degrees",
 "they're definitely not zero degrees",
 "there's some angle in between so we",
 'could work with that and train our',
 'classifier to recognize these kinds of',
 'lines in certain orientations',
 'this is complicated',
 'for ei practitioners and the rest of the',
 "world that's where deep learning holds a",
 'bit of promise',
 'the key idea in deep learning is that',
 'you can learn these features just from',
 'raw data so i can feed a bunch of images',
 'or faces to my deep learning algorithm',
 "and it's going to develop some kind of",
 'hierarchical representation of detecting',
 'lines and edges and then using these',
 'lines and edges to detect eyes and a',
 'mouth and composing it together to',
 'ultimately detect the face',
 'as it turns out the underlying',
 'algorithms for training these models',
 'have existed for quite a long time',
 'so why has deep learning gaining',
 'popularity many decades later',
 'well for one data has become much more',
 "pervasive we're living in the age of big",
 'data and these algorithms require',
 'massive amounts of data to effectively',
 'be implemented',
 'second we have hardware and architecture',
 'that are capable of handling the vast',
 'amount of data and computational power',
 'that these algorithms require hardware',
 "that simply wasn't available a few",
 'decades ago',
 'third building and deploying these',
 'algorithms models as i call is extremely',
 'streamlined with the increasing',
 'popularity of open source software like',
 'tensorflow and pytorch',
 'deep learning models refer to the',
 'training of things called neural',
 'networks',
 'neural networks form the basis of deep',
 'learning a sub-field of machine learning',
 'where algorithms are inspired by the',
 'structure of the human brain',
 'just like neurons make up the brain the',
 'fundamental building blocks of a neural',
 'network is also a neuron',
 'neural networks take in data they train',
 'themselves to recognize patterns in this',
 'data and predict outputs for a new set',
 'of similar data',
 'in a new network information propagates',
 'through three central components that',
 'form the basis of every neural network',
 'architecture the input layer the output',
 'layer and several hidden layers between',
 'the two',
 "in the next video we'll go over the",
 'learning process of a neural network',
 'the learning process of a neural network',
 'can be broken into two main processes',
 'forward propagation and back propagation',
 'full propagation is the propagation of',
 'information from the input layer to the',
 'output layer we can define our input',
 'layer as several neurons x1 through xn',
 'these neurons connect to the neurons of',
 'the next layer through channels and they',
 'are assigned numerical values called',
 'weights the inputs are multiplied to the',
 'weights and their sum is sent as input',
 'to the neurons in the hidden layer where',
 'each neuron in turn is associated to a',
 'numerical value called the bias which is',
 'then added to the input sum',
 'this weighted sum is then passed through',
 'a non-linear function called the',
 'activation function which essentially',
 'decides if that particular neuron can',
 'contribute to the next layer',
 "in the output layer it's basically a",
 'form of probability the neuron with the',
 'highest value determines what the output',
 'finally is',
 "so let's go over a few terms",
 'the weight of a neuron tells us how',
 'important the neuron is the higher the',
 'value the more important it is in the',
 'relationship the bias is like the new on',
 'having an opinion to the relationship it',
 'serves to shift the activation function',
 'to the right or to the left if you have',
 'had some experience with high school',
 'math you should know that adding a',
 'scalar value to a function shifts a',
 'graph either to the left or to the right',
 'and this is exactly what the bias does',
 'it shifts the activation function to the',
 'right or to the left that propagation is',
 'almost like for propagation except in',
 'the reverse direction information here',
 'is passed from the output layer to the',
 'hidden layers not the input layer',
 'but what information gets passed on from',
 "the output layer isn't the output layer",
 'supposed to be the final layer where we',
 'get the final output',
 'well',
 'yes but no bad propagation is the reason',
 'why new networks is so powerful it is',
 'the reason why new networks can learn by',
 'themselves',
 'in the last step before propagation a',
 'new network spits out a prediction',
 'this prediction could have two',
 'possibilities either right or wrong',
 'in bad propagation the new network',
 'evaluates its own performance and checks',
 'if it is right or wrong if it is wrong',
 'the network uses something called a loss',
 'function to quantify the deviation from',
 'the expected output and it is this',
 "information that's sent back to the",
 'hidden layers for the weights and biases',
 "to be adjusted so that the network's",
 "accuracy level increases let's visualize",
 'the training process with a real example',
 "let's suppose we have a data set this",
 'dataset gives us the weight of a vehicle',
 'and the number of goods carried by that',
 'vehicle and also tells us if those',
 'vehicles are cars or trucks',
 'we want to go through this data trade a',
 'new networks to predict cars or trucks',
 'based on their weights and goods to',
 "start off let's initialize the neural",
 'network by giving it random weights and',
 'biases these can be anything we really',
 "don't care what these values are as long",
 "as they're there",
 'in the first entry of a data set we have',
 'vehicle weight equal to a value which in',
 'this case is 15 and good as two',
 "according to this it's a car",
 'we now start moving these input',
 'dimensions through the newer network so',
 'basically what we want to do is take',
 'both the inputs multiply them by their',
 'weight and add a bias',
 'and this is where the magic happens we',
 'run this weighted sum through an',
 'activation function',
 "okay now let's say that the output of",
 'this activation function is 0.001',
 'this again is multiplied by the weight',
 'and added to the bias and finally in the',
 'output layer we have a guess',
 'now according to this neural network the',
 'type of vehicle with weight 15 and goods',
 '2 has a greater probability of being a',
 'truck of course this is not true and a',
 'new network knows this so we use back',
 "propagation we're going to quantify the",
 'difference between the expected result',
 'and the predicted output using a loss',
 "function in bank propagation we're going",
 'to go backwards and adjust our initial',
 'rates and biases remember that during',
 'the initialization of the neural network',
 'we chose completely random weight and',
 'biases',
 'well during back propagation these',
 'values will be adjusted to better fit',
 'the prediction model',
 'okay so that was one iteration through',
 'the first piece of the data set in the',
 'second entry we have vehicle weight 34',
 'and goods 67.',
 "we're going to use the same process as",
 'before multiply the input with the',
 'weight and add a box pass this result',
 'into an activation function and repeat',
 'till the output layer check the error',
 'difference and employ back propagation',
 'to adjust the weights in the biases your',
 'new network will continue doing this',
 'repeated processor for propagation',
 'calculating the error and then back',
 'propagation for as many entries there',
 'are in this data set',
 'the more data you give the newer network',
 'the better it will be at predicting the',
 "right output but there's a tradeoff",
 "because too much data and you'll end up",
 'with a problem like overfitting which',
 "i'll discuss later in this course but",
 "that's essentially how a new network",
 'works you feed input the network',
 'initializes with random weights and',
 'biases that are adjusted each time',
 'during back propagation',
 "until the network's gone through all",
 'your data and is now able to make',
 'predictions',
 'this learning algorithm can be',
 'summarized as follows first we',
 'initialize the network with random',
 "values for the network's parameters or",
 'the weights in the biases we take a set',
 'of input data and pass them through the',
 'network we compare these predictions',
 'obtained with the values of the expected',
 'labels and calculate the loss using a',
 'loss function',
 'we perform back propagation in order to',
 'propagate this loss to each and every',
 'weight and bias',
 'we use this propagated information to',
 'update the weights and biases of neural',
 'network with the gradient descent',
 'algorithm in such a way that the total',
 'loss is reduced and a better model is',
 'obtained',
 'the last step is continue iterating the',
 'previous steps until we consider that we',
 'have a good enough model',
 "in this section we're going to talk",
 'about the most common terminologies used',
 'in deep learning today',
 "let's start off with the activation",
 'function',
 'the activation function serves to',
 'introduce something called non-linearity',
 'into the network and also decides',
 'whether a particular neuron can',
 'contribute to the next layer',
 'but how do you decide if the neuron can',
 'fire or activate',
 'well we had a couple of ideas which led',
 'to the creation of different activation',
 'functions',
 'the first idea we had is how about we',
 'activate the neuron if it is above a',
 'certain value or threshold if it is less',
 "than the threshold don't activate it",
 'activation function a is equal to',
 'activated if y is greater than some',
 "threshold else it's not",
 'this is essentially a step function its',
 'output is 1 or activated when value is',
 'greater than 0. its output is activated',
 'when value is greater than some',
 'threshold and outputs not activated',
 'otherwise',
 'great so this makes an activation',
 'function for a neuron no confusions life',
 'is perfect except there are some',
 'drawbacks with this to understand about',
 'it think about the following',
 'think about a case where you want to',
 'classify multiple such neurons into',
 'classes say class 1 class 2 class 3 etc',
 'what will happen if more than one neuron',
 'is activated all these neurons will',
 'output a one',
 'well how do you decide',
 'now how do you decide which class it',
 "belongs to it's complicated right you",
 'would want the network to activate only',
 'one neuron and the other should be zero',
 'only then you would be able to say it',
 'was classified properly',
 'in real practice however it is harder to',
 'train and converge it this way it would',
 'be better if the activation was not',
 'binary and instead some probable value',
 'like 75 activated or 16 activated',
 "there's a 75 chance that it belongs to",
 'class 2 etc',
 'then if more than one neuron activates',
 'you could find which neuron fires based',
 'on which has the highest probability',
 "okay maybe you'll ask yourself i want",
 'something to give me a more analog value',
 'rather than just saying activated or not',
 'activated something other than in binary',
 'and maybe you would have thought about a',
 'linear function a straight line function',
 'where the activation is proportional to',
 'the input by a value called the slope of',
 'the line',
 'this way it gives us a range of',
 "activations so it isn't binary",
 'activation we can definitely connect a',
 'few neurons together and if more than',
 'one fires we could take the maximum',
 'value and decide based on that so that',
 'is okay too and what is the problem with',
 'this',
 'well if you are familiar with gradient',
 "descent which i'll come to in just a bit",
 "you'll notice that the derivative of a",
 'linear function is a constant',
 "makes sense because the slope isn't",
 'changing at any point',
 'for a function f of x is equal to mx',
 'plus c the derivative is m this means',
 'that the gradient has no relationship',
 'whatsoever with x this also means that',
 'during back propagation the adjustments',
 'made to the weights and the biases',
 "aren't dependent on x at all and this is",
 'not a good thing additionally think',
 'about if you have connected layers no',
 'matter how many layers you have if all',
 'of them are linear in nature the',
 'activation function of the final layer',
 'is nothing but just a linear function of',
 'the input of the first layer',
 'pause for a bit and think about it',
 'this means that the entire neural',
 'network of dozens of layers can be',
 'replaced by a single layer remember a',
 'combination of linear functions in the',
 'linear manner is still another linear',
 'function',
 "and this is terrible because we've just",
 'lost the ability to stack layers this',
 'way no matter how much we stack the',
 'whole network is still equivalent to a',
 'single layer with single activation next',
 "we have a sigmoid function and if you've",
 'ever watched a video on activation',
 'functions this is the kind of function',
 'used in the examples a sigmoid function',
 'is defined as a if x is equal to 1 over',
 '1 plus e to the negative x',
 'well this looks smooth and kind of like',
 'a step function what are its benefits',
 'think about it for a moment',
 'well first things first it has',
 'non-linear nature combinations of this',
 'function are also non-linear great so',
 'now we can stack layers',
 'what about non-binary activations yes',
 'that too this function outputs an analog',
 'activation unlike the step function and',
 'also has a smooth gradient an advantage',
 'of this activation function is that',
 'unlike the linear function the output of',
 'this function is going to be in the',
 'range zero to one inclusive compared to',
 'the negative infinity to infinity of the',
 'latter',
 'so we have activations bound in a range',
 "and this won't blow up the activations",
 'and this is great and sigmoid functions',
 'are one of the most widely used',
 'activation functions today',
 "but life isn't always rosy and sigmoids",
 'two tend to have the share of',
 'disadvantages',
 'if you look closely between x is equal',
 'to negative two and x is equal to two',
 'the y values are very steep any small',
 'changes in values of x in that region',
 'will cause values of y to change',
 'drastically',
 'also towards either end of the function',
 'the y values tend to respond very less',
 'to changes in x',
 'the gradient at those regions is going',
 'to be really really small',
 'almost zero and it gives rise to the',
 'vanishing gradient problem which just',
 'says that if the input to the activation',
 'function is either large or small the',
 'sigmoids are going to squish that down',
 'to a value between zero and one and the',
 'gradient of this function becomes really',
 "small and you'll see why when we talk",
 'about gradient descent this is a huge',
 'problem another activation function that',
 'is used is the tan h function',
 'this looks very similar to sigmoid in',
 'fact',
 "mathematically this is what's known as a",
 'shifted sigmoid function',
 'okay so like the sigmoid it has',
 'characteristics that we discussed above',
 'it is nonlinear nature so we can stack',
 'layers it is bound to arrange from',
 "negative one to one so there's no",
 'worrying about the activations blowing',
 'up',
 'the derivative of the tanning function',
 'however is steeper than that of the',
 'sigmoid so deciding between the sigmoid',
 'and the tanh will really depend on your',
 'requirement of the gradient strength',
 'like sigmoid tanh is also a very popular',
 'and widely used activation function',
 'and yes like the sigmoid tanh does have',
 'a vanishing gradient problem the',
 'rectified linear unit or the value',
 'function is defined as a of x is equal',
 'to the max from 0 to x at first look',
 'this would look like a linear function',
 'right the graph is linear in the',
 'positive axis',
 'let me tell you rather is in fact',
 'non-linear nature and combinations of',
 'relu are also non-linear great so this',
 'means that we can stack layers however',
 'unlike the previous two functions that',
 'we discussed is not bounded the range of',
 'the relu is from zero to infinity this',
 'means there is a chance of blowing up',
 'the activation',
 'another point i would like to discuss',
 'here is sparsity of inactivation imagine',
 'a big neural network with lots of',
 'neurons using a sigmoid or a tanning',
 'will cause almost all the neurons to',
 'fire in an analog way',
 'this means that almost all activations',
 'will be processed to describe the',
 "network's output in other words the",
 'activation would be dense',
 'and this is costly ideally we want only',
 'a few neurons in the network to activate',
 'and thereby making the activations pass',
 'and efficient',
 "here's where the relu comes in imagine a",
 'network with randomly initialized',
 'weights and almost 50 percent of the',
 'network yields zero activation because',
 'of the characteristic relu it outputs',
 'zero for negative values of x',
 'this means that only 50 percent of the',
 'neurons fire sparse activation making',
 'the network lighter but when life gives',
 'you an apple it comes with a little worm',
 'inside',
 'because of that horizontal line in relu',
 'for negative values of x the gradient is',
 'zero in that region which means that',
 'during back propagation the weights will',
 'not get adjusted during descent this',
 'means that those neurons which go into',
 'that state will stop responding to',
 'variations in the error simply because',
 'the gradient is zero nothing changes',
 'this is called the dying value problem',
 'this problem can cause several neurons',
 'to just die and not respond thus making',
 'a substantial part of the network',
 'passive rather than what we want out of',
 'there are workarounds for this one way',
 'especially is to simply make the',
 'horizontal line into a non-horizontal',
 'component by adding a slope usually the',
 'slope is around 0.001',
 'and this this new version of the relu is',
 'called leaky value the main idea is that',
 'the gradient should never be zero one',
 'major advantage of the relu is the fact',
 "that it's less computationally expensive",
 'than functions like tannage and sigmoid',
 'because it involves simpler mathematical',
 'operations',
 'this is a really good point to consider',
 'when you were designing your own deep',
 'neural networks great so now the',
 'question is which activation function to',
 'use',
 'because of the advantages that relu',
 'offers does this mean that you should',
 'use reload for everything you do',
 'or could you consider sigmoid and tan h',
 'well both',
 "when you know the function that you're",
 'trying to approximate has certain',
 'characteristics you should choose an',
 'activation function with which will',
 'approximate the function faster leading',
 'to faster training processes',
 'for example a sigmoid function works',
 'well for binary classification problems',
 'because approximating our classifier',
 'functions as combinations of the sigmoid',
 'is easier than maybe the relu this will',
 'lead to faster training processes and',
 'larger convergence',
 'you can use your own custom functions',
 "too if you don't know the nature of the",
 "function you're trying to learn i would",
 'suggest you start with relu and then',
 'work backwards from there',
 'before we move on to the next section i',
 'want to talk about why we use non-linear',
 'activation functions as opposed to',
 'linear ones',
 'if you recall in my definition of',
 'activation functions i mentioned that',
 'activation functions serve to introduce',
 'something called non-linearity in the',
 'network for all intensive purposes',
 'introducing non-linearity simply means',
 'that your activation function must be',
 'non-linear that is not a straight line',
 'mathematically linear functions are',
 'polynomials of degree 1 that when',
 'graphed in the x y plane are straight',
 'lines inclined to the x-axis at a',
 'certain value we call this the slope of',
 'the line',
 'non-linear functions are polynomials of',
 'degree greater than one',
 "and when graphed they don't form",
 'straight lines rather than more curved',
 'if we use linear activation functions to',
 'model our data then no matter how many',
 'hidden layers our network has it will',
 'always become equivalent to having a',
 'single layer network and in deep',
 'learning we want to be able to model',
 'every type of data without being',
 'restricted as would be the case should',
 'we use linear functions',
 'we discussed previously in the learning',
 'process of neural networks that we',
 'started with random weight and biases',
 'the neural network makes a prediction',
 'this prediction is compared against the',
 'expected output and the weights and',
 'biases are adjusted accordingly',
 'well loss functions are the reason that',
 "we're able to calculate that difference",
 'really simply a loss function is a way',
 'to quantify the deviation of the',
 'predicted output by the neural network',
 "to the expected output it's as simple as",
 'that nothing mo nothing less',
 'there are plenty of loss functions out',
 'there for example under regression we',
 'have squared error loss absolute ever',
 'loss in huber loss in binary',
 'classification we have binary cross',
 'entropy and hinge loss in multi-class',
 'classification problems we have the',
 'multi-class cross entropy and the',
 'callback liability divergence loss and',
 'so on',
 'the choice of the best function really',
 "depends on what kind of project you're",
 'working on different projects require',
 'different loss functions',
 "now i don't want to talk any further",
 "loss functions right now we'll do this",
 'under the optimization section because',
 "that's really where most functions are",
 'utilized',
 'in the previous section we dealt with',
 'loss functions which are mathematical',
 'ways of measuring how wrong predictions',
 'made by neural network are',
 'during the training process we tweak and',
 'change the parameters or the weights of',
 'the model',
 'to try and minimize that loss function',
 'and make our predictions as correct and',
 'optimized as possible',
 'but how exactly do you do that how do',
 'you change the parameters of your model',
 'by how much and when',
 'we have the ingredients how do we make',
 'the cake',
 'this is where optimizers come in they',
 'tied together the loss function and',
 'model parameters or the weight and',
 'biases by updating the network in',
 'response to the output of the loss',
 'function',
 'in simpler terms optimizers shape and',
 'mold your model into more accurate',
 'models by adjusting the weights and the',
 'biases',
 'the loss function is its guide it tells',
 "the optimizer whether it's moving in the",
 'right or the wrong direction',
 'to understand this better',
 'imagine did you have just killed mount',
 'everest and now you decide to descend',
 "the mountain blindfolded it's impossible",
 'to know which direction to go in you',
 'could either go up which is away from',
 'your goal or go down which is towards',
 'your goal but to begin you would start',
 "taking steps using your feet you'll be",
 "able to gauge whether you're going up or",
 'down',
 'in this analogy you resemble the neural',
 'network going down your goal is trying',
 'to minimize the error and your feet are',
 'resemblance of the loss functions they',
 "measure whether you're going in the",
 'right way or the wrong way',
 "similarly it's impossible to know what",
 "your model's weights should be right",
 'from the start but with some trial and',
 'error based on the loss function you',
 'could end up getting there eventually',
 'we now come to grading descent often',
 'called the grand daddy of optimizers',
 'grading descent is an iterative',
 'algorithm that starts up at a random',
 'point in the loss function and travels',
 'down its slope in steps until it reaches',
 'the lowest point or the minimum of the',
 'function',
 'it is the most popular optimizer we use',
 "nowadays it's fast robust and flexible",
 "and here's how it works",
 'first we calculated what a small change',
 'in each individual weight would do to',
 'the loss function',
 'we adjust each individual weight based',
 'on its gradient that is take a small',
 'step in the determined direction',
 'the last step is to repeat the first and',
 'the second step until the loss function',
 'gets as low as possible i want to talk',
 'about this notion of a gradient the',
 'gradient of a function is the vector of',
 'the partial derivatives with respect to',
 'all independent variables the gradient',
 'always points in the direction of the',
 'steepest increase in the function',
 'suppose we have a graph like so with',
 'loss on the y-axis and the value of the',
 'weight on the x-axis',
 'we have a little data point here that',
 'corresponds to the randomly initialized',
 'weight to minimize this loss that is to',
 'get this data point to the minimum of',
 'the function',
 'we need to take the negative gradient',
 'since we want to find the steepest',
 'decrease in function',
 'this process happens iteratively through',
 'the losses as minimized as possible',
 "and that's grading descent in a nutshell",
 'when dealing with high dimensional data',
 "sets that is a lot of variables it's",
 "possible you'll find yourself in an area",
 "where it seems like you've reached the",
 'lowest possible value for your loss',
 "function but in reality it's just a",
 'local minimum',
 'to avoid getting stuck in a local minima',
 'we make sure we use the proper learning',
 'rate',
 'changing our weights too fast by adding',
 'or subtracting too much that is taking',
 'steps that are too large or too small',
 'can hinder your ability to minimize the',
 'loss function',
 "we don't want to make a jump so large",
 'that we skip over the optimal value for',
 "a given weight to make sure this doesn't",
 'happen we use a variable called the',
 'learning rate',
 'this thing is usually just a small',
 'number like 0.001',
 'that we multiply the gradients by to',
 'scale them this ensures that any changes',
 'we make to our weights are pretty small',
 'in math talk taking steps that are too',
 'large can mean that the algorithm will',
 'never converge to an optimum at the same',
 "time we don't want to take steps that",
 'are too small because then we might',
 'never end up with the right values for',
 'our weights in math talk steps that are',
 'too small might lead to our optimizer',
 'converging on a local minimum for the',
 'loss function but never the absolute',
 'minimum',
 'for a simple summary just remember that',
 'the learning rate ensures that we change',
 'our weight at the right pace not making',
 'any changes that are too big or too',
 'small',
 'instead of calculating the gradients for',
 'all your training examples on every part',
 "of the gradient descent it's sometimes",
 'more efficient to only use a subset of',
 'the training examples each time',
 'stochastic gradient descent is an',
 'implementation that either uses batches',
 'of examples at a time or random examples',
 'on each pass',
 'stochastic gradient descent uses the',
 'concept of momentum momentum accumulates',
 'gradients of the past steps to dictate',
 'what might happen in the next steps also',
 "because we don't include the entire",
 'training set',
 'sjd is less computationally expensive',
 "it's difficult to overstate how popular",
 'gradient descent really is back',
 'propagation is basically gradient',
 'descent implemented on a network there',
 'are other types of optimizers based on',
 'gradient descent that are used today ad',
 'grad adapts the learning rate',
 'specifically to individual features',
 'that means that some of the weights in',
 'your data set will have different',
 'learning rates than others',
 'this works really well for sparse data',
 'sets where a lot of input examples are',
 'missing',
 'at a grad has a major issue though the',
 'adaptive learning rate tends to get',
 'really really small over time rms prop',
 'is a special version of adegrad',
 'developed by professor jeffrey hinton',
 'instead of letting all the gradients',
 'accumulate for momentum it accumulates',
 'gradients in a fixed window',
 'rms prop is similar to add a prop which',
 'is another optimizer that seeks to solve',
 'some of the issues that atograd leaves',
 'open atom stands for adaptive moment',
 'estimation and is another way of using',
 'past gradients to calculate the carbon',
 'gradient atom also utilizes the concept',
 'of momentum which is basically our way',
 'of telling the neural network whether we',
 'want past changes to affect the new',
 'change by adding fractions of the',
 'previous gradients to the current one',
 'this optimizer has become pretty',
 'widespread and is practically accepted',
 'for use in training new networks',
 "it's easy to get lost in the complexity",
 'of some of these new optimizers just',
 'remember that they all have the same',
 'goal minimizing the loss function and',
 'trial and error will get you there',
 'you may have heard me referring to the',
 'words parameters quite a bit and often',
 'this word is confused with the term',
 'hyperparameters',
 "in this video i'm going to outline the",
 'basic difference between the two a model',
 'parameter is a variable that is internal',
 'to the new network and whose values can',
 'be estimated from the data itself',
 'they are required by the model when',
 'making predictions these values define',
 'the skill of the model on your problem',
 'they can be estimated directly from the',
 'data',
 'and are often not manually set by the',
 'practitioner',
 'and oftentimes when you save your model',
 "you are essentially saving your model's",
 'parameters parameters are key to machine',
 'learning algorithms and examples of',
 'these include the weights and the biases',
 'a hyper parameter is a configuration',
 'that is external to the model and whose',
 'value cannot be estimated from data',
 "there's no way that we can find the best",
 'value for a model hyper parameter on a',
 'given problem we may use rules of thumb',
 'copy values used in other problems or',
 'search for the best value by trial and',
 'error',
 'when a machine learning algorithm is',
 'tuned for a specific problem such as',
 "when you're using grid search or random",
 'search then you are in fact tuning the',
 'hyper parameters of the model in order',
 'to discover the parameters that result',
 'in more skillful predictions',
 'model hyper parameters are often',
 'referred to as parameters which can make',
 'things confusing so a good rule of thumb',
 'to overcome this confusion is as follows',
 'if you have to specify a parameter',
 'manually then it is probably a hyper',
 'parameter',
 'parameters are inherent to the model',
 'itself some examples of hyper parameters',
 'include the learning rate for training',
 'on your network the c in sigma',
 'hyper parameters for sport vector',
 'machines and the k and k newest',
 'neighbors',
 'we need terminologies like epochs batch',
 'size and iterations only when the data',
 'is too big which happens all the time in',
 "machine learning and when we can't pass",
 'all this data to the computer at once so',
 'to overcome this problem we need to',
 'divide the data set into smaller chunks',
 'give it to our computer one by one and',
 'update the weights of the new network at',
 'the end of every step to fit it into the',
 'data given',
 'one epoch is when an entire data set is',
 'passed forward and backward through the',
 'network once',
 'in a majority of deep learning models we',
 'use more than one epoch i know it does',
 'make sense in the beginning why do we',
 'need to pass the entire data set many',
 'times through the same neural network',
 'passing the entire data set through the',
 'network only once is trying to read the',
 "entire lyrics of a song once he won't be",
 'able to remember the entire song',
 'immediately you have to reread the',
 'lyrics a couple more times before you',
 'can say you know the song by memory',
 'the same is true with the neural network',
 'we pass the data set multiple times',
 "through the neural network so it's able",
 'to generalize better',
 'gradient descent is an iterative process',
 'and updating the parameters and back',
 'propagation in a single pass or one',
 'epoch is not enough as the number of',
 'epochs increases the more the parameters',
 'are adjusted leading to a better',
 'performing model',
 'but too many epochs could spell disaster',
 'and lead to something called overfitting',
 'where a model has essentially memorized',
 'the patterns in the training data and',
 "performs terribly on data it's never",
 'seen before',
 'so what is the right number of epochs',
 'unfortunately there is no right answer',
 'the answer is different for different',
 'data sets sometimes your data set can',
 'include millions of examples passing',
 'this entire data set at once becomes',
 'extremely difficult so what we do',
 'instead is divide the data set into a',
 'number of batches rather than passing',
 'the entire dataset once the total number',
 'of training examples present in a single',
 'batch is called a batch size iterations',
 'is the number of batches needed to',
 'complete one epoch',
 'note the number of batches is equal to',
 'the number of iterations for one epoch',
 "let's say that we have a data set of 34",
 '000 training examples if we divide the',
 'data set into batches of 500 then it',
 'will take 68 iterations to complete one',
 'epoch',
 'well i hope that gives you some kind of',
 'sense about the very basic terminologies',
 'used in deep learning before we move on',
 'i do want to mention this and you will',
 "see this a lot in deep learning you'll",
 'often have a bunch of different choices',
 'to make how many hidden layers should i',
 'choose or which activation function must',
 'i use and where and to be honest there',
 'are no clear-cut guidelines as to what',
 'your choice should always be',
 "that's a fun part about deep learning",
 "it's extremely difficult to know in the",