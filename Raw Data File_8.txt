Welcome everyone to this lecture on Multiple Linear Regression. In the preceding lectures
we saw how to regress a single independent variable to a dependent variable. Particularly
we were developing a linear model between the independent and dependent variable.
We also saw various measures by which we can assess the model that we built. In this lecture
we will extend all of these ideas to multiple linear regression which consists of one dependent
variable, but several independent variables. So, as I said that we have a dependent variable which
we denote by y and several independent variables which we denote by the symbols x j, where j
equals 1 to p. There are p independent variables which we believe affect the dependent variable.
We will try to develop a linear model between the dependent variable y and these independent
p independent variables x j, j equals 1 to p. In general we can write this linear model as before
we can say y the dependent variable is equal to beta naught, an intercept, plus beta 1 times x 1
plus beta 2 times x 2 and so on up to b beta times p x p, where beta 1, beta 2, beta p represents the
slope parameters or the effect of the individual independent variables on the dependent variable.
In addition we also have an error. This error is due to error in the dependent
variable measurement of the dependent variable. In ordinary least squares we always assume that the
independent variable measurements are perfectly measured and do not have any error whereas,
the dependent variable may contain some error and that error is indicated as epsilon. We do not know
what this quantity is, we assume that it is a random quantity with 0 mean and some variance.
If we take the ith sample corresponding to this measurement of x 1 to x p and y corresponding y
we can say that the ith sample dependent variable y i is equal to beta naught plus beta 1 x 1 i,
the ith sample value of the independent variable 1. Similarly, x 2 y beta 2 times x 2 y,
where x 2 y represents the value of the second independent variable for the ith
sample and so on plus an error i that corrupts the measurement of y i and so on for i equals 1 to n.
We assume we have small n number of samples that we have obtained. And our
aim is to find the values, best estimates, of beta naught beta 1 beta 2 up to beta p
using these n sample measurements of x’s corresponding y. This is what we
call multiple linear regression because we are fitting a linear model and there are
many independent variables and we therefore, call the multiple linear regression problem.
Again in order to find the best estimates of the parameters beta naught to
beta p we actually set up the minimization of the sum squared of errors. In order to
set it up in a compact manner using vectors and matrices,
we de ne the following notations. Let us de ne the vector y where which consists of all the n
measurements of the dependent variable y 1 to y n, we have also done one further things we
have subtracted the mean value of all these measurements from each of the observations.
So, the first one represents the first sample value of the dependent variable
y 1 minus the mean value of y over all the measurements, y bar. So,
the first sample is mean shifted value of the first observation,
the second coefficient or second value in this vector is the second sample value minus the
mean value of the dependent variable and so on for all the n observations we have.
So, these are the mean shifted values of all the n samples for the dependent variable.
Similarly we will construct a matrix x where the first column corresponds to variable,
independent variable 1. Again what we do is take the sample value of the first
independent variable and subtract the mean value of the first independent variable. That means,
we take the mean of all these n samples for the first variable and subtract it
from each of the observations of the first independent variable.
So, the first coefficient here will be x 11 represents the sample value of the
first independent variable, first sample first independent variable,
minus the mean value of the first independent variable. And we do this for all n measurements
of the first independent variable. Similarly we do this for the second independent variable
and arrange it in the second column. So, this one represents the observation the
first observation of the second independent variable minus the mean value of the second
independent variable and we do this for all p variables independent variables.
So, this particular matrix x that we get will be a n cross p matrix,
n is the number of rows p is the number of columns. You can view the first row as actually
the sample, first sample, of all independent variables for the first sample. Of course,
we have being shifted that value. And the second row is the second sample and so on
and each column represents a variable. So, first column represents the first
independent variable and the last column represents the pth independent variable.
So, similarly we will represents represent all the coefficients beta except beta naught in a
vector form beta 1 to beta p as a column vector. Here basically as a I am sorry a row vector. So,
beta 1 is the first coefficient, beta p is the coefficient corresponding pth variable. So,
we have beta vector which is a p cross 1 vector we can also de ne epsilon, the noise vector,
as epsilon 1 to epsilon n corresponding to all the n observations. Now having defined
this notation we can write our linear model in the form y equals x times beta plus epsilon.
Notice that we have not included beta naught. We have eliminated that in
directly by doing this mean subtraction I will show you how that happens. But you can
take it that right now we have only interested in the slope parameters this linear model only
involves the slope parameters beta 1 to beta p, does not involve the beta naught parameter
because that has been effectively removed from the linear model using this mean subtraction idea.
So, we can write our linear model compactly as y equals x beta plus
epsilon and we also make the usual assumptions about the error that it
is a 0 mean vector in this case because it is a multivariate vector 0 is a vector. So,
epsilon expected value epsilon 0 implies epsilon is a random vector with 0 mean and the variance,
covariance matrix of epsilon is assumed to be sigma squared identity.
Sigma squared identity in this form it means all the epsilons, epsilon 1 to epsilon n,
have all have the same variance sigma squared homoscedastic assumption. And we also assume
that epsilon 1 and epsilon 2 are uncorrelated or epsilon i and epsilon j are uncorrelated if i is
not equal to j, in which case we can write the covariance matrix of epsilon as sigma squared i.
Now, under this assumption we can go ahead and say we want to find the estimateds of beta so as
to minimize the sum square of the errors. So, epsilon transpose epsilon is a compact way of
saying the sum of all errors ,error squared of all the errors, in all the n measurements. So,
expanding this is nothing, but sigma epsilon i squared i equals 1 to n, that is compactly
written like this and this is what we want to minimize, but epsilon itself can be written
as y minus x beta. So, we can write this whole thing as y minus x beta transpose y minus x beta.
We want to minimize this which is a function of beta by finding the best value of beta. So,
if we setup this optimization problem to minimize the sum squared errors to
find beta we will we can show. We can by differentiating that objective function
with respect to beta and setting it equal to 0 we get what are called the first order
conditions and these first order conditions will result in the following set of linear
equations. We will get X transpose X into beta equals X transpose y.
Now, this is a p cross, remember X is a n cross p matrix. So, X transpose is p cross n. So,
this is square matrix X transpose X is a square matrix of size p cross p and multiplied by the p
cross n vector and similarly it is p cross 1 on the right hand side. So, these are p equations
in p variables. The linear equations in beta x is all known y is known. So, right hand side is
like the if you, are what we, reinterpret this as a times some x is equal to b. It is a set
of linear equations p equations in p variables which can be easily solved if a is invertible.
So, we assume that X transpose X is a full rank matrix invertible. The meaning of this
will become little clearer later, and if it is not invertible we will have to do other things which
we will again talk in another lecture. But at for the time being let us assume that X transpose X
which is a square matrix is invertible it is a full rank matrix and then you can easily find
the solution for beta hat solve this linear set of equations by taking a inverse b which is exactly
X transpose X inverse X transpose y. So, beta hat the coefficient vector can be found by this thing.
And this is the solution that minimizes the sum squared errors, this objective function
that we have written. So, once we get beta 1 the slope parameters beta naught can be estimated as
the mean value of y minus the mean vector X transpose times the slope parameter. Notice
that this is very similar to what we have in the univariate case where it says beta naught estimate
is nothing but y bar minus x bar into beta 1. So, it is very similar to that you can see.
You can also compare the solution for the slope parameters, for the,
with the univariate case which says beta 1 hat is SXy divided by SXX. Notice that
X transpose y represents SXy and X transpose X represents SXX in the uni-variate case you were
diving SXy by SXX in the multivariate case division is represented by an inverse. So,
you get X transpose X inverse terms times X transpose y. So,
you can see the it is very very similar to the solution for the univariate case except that
these are matrices and vectors and therefore, you have to be careful. You cannot simply divide it
as matrix times inverse times a vector that is a solution for beta which is slope parameters.
You can also estimate beta naught and beta 1 by doing what is called
augmentation of the X vector with a constant value 1 1 1 in the final thing,
but I did not use that approach because the mean subtraction approach is a much
better approach for estimating whether if for estimating beta naught and beta hat, beta slope
parameters because this is applicable even to another case called the total least squares.
The augmentation approach is valid only for ordinary least squares you cannot use it for
total least squares which we will see again later. So, that is why I use the mean subtraction route
in order to obtain the estimates of the slope parameter first followed by the estimation of
beta naught using the estimates of the slope parameters in this manner.
Now, you can also derive properties of these parameters beta. We can show
that the X vector value of beta hat is beta which just means it is an unbiased estimate
just as in the univariate case and we can also get the variance of this beta
hat the in this case it is a covariance matrix because it is a vector and we can
show that the covariance matrix is sigma squared times this X transpose X inverse.
Now, again you can go back and look at the univariate case. There the vari-ance of beta
1 slope parameter will be sigma squared by SXX in this case it is sigma squared
into X transpose X inverse. So, X transpose X represents SXX. Sigma squared is the variance
of the error corrupting the dependent variables. We may have a priori knowledge sometimes in most
cases we may not be able to know this value of sigma squared we may not be given this. So,
we have to estimate the sigma squared from data and we will show how to get this.
These two parameters that actual we can show the first parameter says that the estimates of beta
1 the slope parameters are unbiased. So, beta hat are unbiased estimator it is an unbiased estimator
of the of the true value beta. Moreover you can show that among all linear estimators because
beta hat is a linear function of y. Notice that X transpose X inverse X transpose is
nothing but matrix which basically multiplies the measurements y. So, beta hat can be interpreted
as a linear combination of the measurements. Therefore, it is known as a linear estimator.
Among all such linear estimators we can show that beta hat has the least variance. Therefore,
it is called a blue estimator or a unbiased estimator with the best linear unbiased estimator
that is what it blue represents, best in the sense of having the least variance.
Now, we can also estimate as I said sigma squared from the data and that sigma squared estimate is
nothing but the after you fit the linear model you can take the predicted value for the ith sample
from the linear model and compute this residual y i minus y i hat which is the measured value
minus the predicted value for the ith sample, square it take the sum or all possible samples,
n samples, divided by n minus p minus 1. Again if you go back to your linear case univariate
case you will find that the denominator is n minus 2. Here you have n minus p minus
1 because you are fitting p plus 1 parameters p is slope parameters plus 1 o set parameter.
Therefore out of the n measurements p plus 1 are taken away for the deriving
the estimates. Only the remaining things are the degrees of freedom or the variability in
the residuals is cost by the remaining n minus p minus 1 measurements and that is
why you are diving by n minus p minus 1 whereas, in the univariate case you
would have divided by n minus 2 because you are estimating only two parameters there.
So, you can see a one to one similarity between the univariate regression problem and the multi
multiple linear regression problem in every derivation that we have given here. Now,
once we have estimated sigma hat, the variance of the error used from the data
you can go back and construct confidence intervals for each slope parameter we can
show that the true slope parameter lies in this confidence interval for any confidence
interval you may choose 1 minus alpha, alpha represents like a level of significance.
So, if you say alpha is equal to 0.05, 1 minus alpha would represents 0.95. So,
that will be a 95 percent confidence interval. Correspondingly I will find the critical value
from the t distribution n with n minus p minus 1 degrees of freedom and this represents alpha
by 2 the upper lower value probability value from the t distribution and this is the upper
critical value where the probability area under the curve beyond the value is alpha by 2. So,
n minus p minus 1 represents the degrees of freedom notice that in
the univariate case it would have been n minus 2, very very similar.
So, the confidence interval for beta j for any given alpha can be computed using this
particular formula and the term here se of beta hat j represents the standard
deviation of the estimate of beta hat j and that is given by the diagonal element,
diagonal element here of this quantity with sigma square replaced by the estimate here.
So, we have computed the standard deviation of the, of the parameter beta hat j estimated
parameter beta hat j by using the estimated value of sigma multiplied by the diagonal
element of X transpose X. So, we are fitting the diagonal elements of the covariance matrix of
beta parameters that is all we have done. So, this represents the diagonal element
or the square root of the diagonal element which represents standard deviation of the
estimated value of beta which is what is used in order to construct this confidence interval.
So, every one of this can be computed from the data as you can see and you can construct. Now,
the confidence level can later be used for testing whether the
estimated parameter beta is significant or insignificant as we will see later.
Now, we will, can also compute the correlation between y and y hat which tells you whether the
predicted value from the linear model is, resembles or closely related to,
the measured value. So, typically we will draw a line between the y the measured value
and the predicted value and see whether it is these things fall on the 45 degree line
and if it does then we think that the t is good. Another way of doing this is to find
the correlation coefficient between y and y hat which is simply using the standard thing
y i minus y bar multiplied by y i hat minus y hat bar. Summed over all quantities divided by
the standard deviation of in y and the standard deviation in y hat that is for normalization.
We could also use the coefficient of determination, R squared, just as we did
for the univariate case. We can compute R squared as 1 minus sum squared error by sum squared total.
Which is nothing, but numerator is y i minus y i hat, the residual squared divided by y i minus y
bar squared which is the variance in y basically. So, if we take 1 minus this we will, actually we
can show whether using the independent variables have we been able to get a better t. If we have
obtained a very good t then the numerator will be close to 0 and R squared will be close to 1.
On the other hand if we are not improved the t because of x’s any of the x’s,
then the numerator will be almost equal to the denominator and therefore, this one will be close
to 0. So, value of R square close to 1 as before represents indication of a good linear t whereas,
a value close to 0 indicates the t is not good. We can also compute adjusted
R square to account for the degrees of freedom notice that the numerator has n minus p minus 1
degrees of freedom whereas, the denominator has n minus 1 degrees of freedom therefore, we can
do an adjusted R squared which divides the SSE by the appropriate degrees of freedom.
We can say this is the error due to per degree of freedom that is there in the t whereas,
the denominator represents the error because we have fitted only the o set parameter there
are n minus 1 degrees of freedom this is the error per degree of freedom. So, this kind of a
thing is also a good indicator instead of using R squared we can use adjusted value of R square. So,
these are all very very similar again to the univariate linear regression problem.
So, we can use, we can check R squared and see whether the values close to
one and if it is we can say maybe linear model is good to t the data,
but that is not a confirmatory test. We have to do the residual plot as we did in linear regression,
univariate linear regression, and that is what we are going to do further. So, we are going to
find whether the fitted model is adequate or it can be reduced further. What this reduced
further means we will explain. In the univariate case there is only one independent variable,
but here there are several independent variables. Maybe not all independent variables have an effect
on y. Some of the independent variables may be irrelevant. So, one way of trying to find
whether a particular independent variable has an effect is to test the corresponding coefficient.
Notice we have already defined the confidence interval for each coefficient and we can see
whether the confidence interval contains 0, in which case we can say the corresponding
independent variable does not have a significant effect on the dependent variable and we can
perhaps drop it. Or, we can also do what we call the test, F test, just as we did
univariate regression problem we test whether the full model is better than the reduced model.
The reduced model contains no independent variables whereas, the full model can contain
all or some of the independent variables. You can do many kinds of test and we will do this. So,
we can test whether the reduced model which contains only the constant
intercept parameter is a good t as opposed to including all the independent variables,
some or all the independent variables, that is what we call the full model.
We will consider a specific case here where we do the F test statistic for
the case when we have a reduced model and compare it with the full model. The reduced
model we will consider with k parameters specifically let us consider the reduced
model with only one parameter which means that we have only the constant intercept
parameter we will not include any of the independent variables. And compare it
with the full model which contains all of the independent variables including the intercept.
So, the reduced model is one which contains only the o set parameter and
no independent variables the full model is a case where we consider
all the independent variables and the intercept parameter. So, the number of
parameters we are estimating in the reduced model is only 1, so k equals 1 and the full
model is the case where we have all the independent variables p independent
variables. So, we are estimating p plus 1 parameters in the full model.
So, what we do is perform a fit and compute the sum squared errors which
is nothing but the difference between y the measured value and the predicted value. So,
we will first take the model containing only the o set or the intercept parameter and estimate. In
this case of course, y bar will be the best estimate. And we will compute sum squared
errors which is nothing but the variance of the measured measurements for the dependent
variable. Then we will also perform a linear regression containing all the parameters,
independent variables, and in this case we will if we compute the difference between
y and y predicted and take the sum squared errors that is the SSE of the full model.
So, when we want to compare whether we want to accept the full model as compared to the
reduced model what we do is take the difference in the sum squared errors remember the sum squared
errors for the reduced model will become greater than the sum squared errors for the full model
because the full model contains more number of parameters and therefore, you get a better fit.
So, the difference in the fit which is difference in the sum squared errors between the reduced
model fit and the full model fit that is the numerator, divided by what we call the degrees
of freedom. Notice the full model as p plus 1 parameters p independent variable plus the o set
and the reduced model in this particular case contains only 1 parameter, so, k equals 1
So, the degrees of freedom will be p. So, you divide this difference in the sum squared errors
by p denominator is the sum squared errors of the full model which contains n minus p
minus 1 degrees of freedom because p plus 1 parameters have been fitted therefore,
the degrees of freedom is the total number of measurements minus p minus 1. So,
we divide the sum squared errors for the denominator by the number of degrees of
freedom and then take this ratio as defined and that is your F statistic.
Now, in order to reject, if we want to reject the null hypothesis,
or if we want to test the null hypothesis against this alternative we find the test
criteria for the alpha level of significance. We will take it from the F distribution where
the numerator degrees of freedom is p plus 1 minus k for this particular case it is exactly
p and the denominator degrees of freedom is n minus p minus 1 and alpha level of significance
we use and we compute the test criteria, critical value from the F distribution.
Then we compare the test statistic with the critical value and if the test statistic
exceeds the critical value at this level of significance ,then we reject the null
hypothesis. That is we will say the full model is better choice and the independent variables
do make a difference. And this is a standard thing that R function will provide. This
particular comparison between the reduced model which has no independent variables and the full
model which contains all the independent variables in multi linear regression.
Of course, you can choose different reduced models and compare with the full model. For example,
you can take the reduced model by leaving out only one of the independent variables. So,
that will have p parameters, we can compare it with the full model and again perform a
test to decide whether the inclusion of that independent variable makes a difference or not.
So, this kind of combination can be done depending on what stage you are and that will be using in
what we call the sequential method for subset selection that will be discussed in the later
lecture. But essentially the R functions only provide a comparison between the reduced model
which contains no independent variable and the full model which contains all of the independent
variables. Let us go through simple example in order to what you call revisit these ideas.
So, in this case we have what is called the price data where customers are being asked
to rate the food and the other aesthetics of a particular restaurant and we also and cost
of the particular dinner also data obtained for these restaurants. And the location of
these restaurants whether on they are on the east side of a particular street in
New York or the west side. Typically New York Westside is probably a little poorer whereas,
the east side probably is a little richer neighborhood.
So, location of the restaurant also would indicate, would have a effect on, the price. So,
these are the four independent variables people data was obtained on. The quality of the food,
the decor and service all this was rated by the customers and the location of this
restaurant and the price of dinner in that served in that restaurant was also taken.
So, you would expect that the quality of the food the service level all of this
would have a very direct influence on the price in the restaurant and
a linear model was built between y and the independent variables x 1 to x 4.
So, before we build a model we do a scatter plot as usual and visualization. And here
you because there are several independent variables we have not just one plot scatter
plot between y and x 1. For example, in this case remember price is y, y versus x 1 this
particular plot shows the correlation or the scatter plot for y versus x 1 or y versus food,
price versus food. The second one is the scatter plot will be price and decor.
The third one is the scatter plot between price and service and the
last one is price versus location. And similarly you can actually develop a
scatter plot between food and decor which is here or food and service and so on.
Even though we consider all these variables that we have obtained like food ,decor,
service, location as independent. It is possible when we select these
vari-ables they are not truly independent there might be interdependencies between
the what we call so called independent variables that can give rise to problem in regression which
we will see later the what we call the effect of collinearity. But a scatter plot may reveal some
dependencies, inter dependencies, between the independent so called independent variables.
So, for example, if we look at the scatter plot between food and decor it is seems to
be completely randomly distributed this does not seem to be any quite correlation. However,
food and service seems to be very strongly correlated there seems to
be a linear relationship between food and service.
So, perhaps you do not need to include both these variables we will see later that that
it is true. But in this just a scatter plot itself reveals some interesting features
and so we will now go ahead and say perhaps a linear mode between price and food and decor is,
seems to be pointed out or, indicated by this scatter plots let us go ahead and build one.
And if we apply the R function lm to this data set and we examine the
output. We will get this output from R and it tells that the intercept term
is minus 24.02 and the slope parameters, the coefficient multiplying food is 1.5,
the coefficient multiplying decor is 1.9 and so on so forth.
It also gives you the standard error for each coefficient as well as the offset parameter
which is nothing but the sigma value for the estimated quantities and it also gives you
the probability values p values as we call them. And notice if the p value is very low
it means that this coefficient is significant. We cannot take it that this value is. Any low
value of this indicates that the corresponding coefficient is significantly different from 0.
So, in this case the first three has very low p values and therefore, they are significant,
but service has a high p value therefore, it seems to indicate that this coefficient
is insignificant is equal almost equal to 0 that is what this indicates. If you
look at the east which is this independent location parameter that as does not have
a very low p value.But it is still not bad 0.03 and therefore, it is significant only
is insignificant only if you take a level of significance of 0.025 or something like that.
If you take 0.1 or 0.05 and so on you will still consider this east ,this coefficient,
to be significant and that is what this is basically pointing out this star indicates that.
So, now we will go ahead and try to actually look at the F value also, the F statistics says that
the full model as compared to the reduced model of using only the intercept is actually significant.
Which means the constant model is not good and including these variables results in a better
t or explanation of the price and therefore, you should actually include this. Whether you
should include all of them or only some of them we can do different kinds of test to find that.
What we have done in this particular case is only compare the model with-out any of these
independent variables which is called the constant model with all of these variables
included that is the only two model comparisons we have made. The reduced model is one containing
only the interceptor and the full model is one which contains intercept and all four
independent variables and thus the p value it has given the corresponding F statistic.
So, we are saying that including these independent variables is important in explaining the price.
But it may turn out that all of them is not necessary and that we will we will
examine further. So, the corresponding t that we obtained is this. As I said that from the,
what you call, the confidence interval for the slope parameter for service,
we can say that we can remove this it is insignificant and perhaps we can remove
this and try the t. For the time being let us actually remove this and try the t.
We have done that. We have only included now food, decor and east and done the regression again and
it turns out that regression thing is still what you call the R squared value is improved
not improved significantly, but not reduced and F value is significant and we get the more or less
the same coefficients for the other parameters also the intercept and the slope parameters.
It indicates that x 3 is not adding any value to the prediction of y. The rea-son for this as
we said if you look at the scatter plot service and food are very strongly correlated therefore,
only either food or service needs to be included in order to explain price and
not both right. And in this case service is being removed, but you can try removing
food as the variable and try to t between price, decor and decor service and east and
you will find that the regression is as good as retaining food and eliminating service.
R squared value and the f statistics seems to indicate that we can go ahead with the linear
model, but we should further examine the standardized residual plot for concluding
whether the linear model is or not. There should no pattern in the residuals. So,
let us actually do the residual plot.
Here we have taken the standardized residuals and plotted it against the, what is called
the predicted price value or the fitted value. Remember this is y i hat, y i hat has only one
variable. So, you need to generate only one plot and we have also shown here the, in red lines,
the confidence interval for the standardized residuals and anything above this outside of
this interval indicates outliers. So, for example, 56, sample number 48, sample number 30 and 109 and
so on so forth may be possible outliers and, but there is no pattern in the standardized residuals.
It is spread randomly within this boundary and therefore, we can say since there is no
pattern a linear t is acceptable. So, here the quality of the t is shown here. So,
the actual price measured value versus the y i hat the predicted
value is shown and a linear model seems to explain the data reasonably well.
The last thing is we have these outliers if you want to improve the t you may want to
remove let us say the outlier which is farthest away from the boundary.
For example, you may want to remove 56 and redo the linear regression multi multiple
linear regression and again repeat it until there are no outliers. That will
improve the R squared value and the fit quality of the fit little more.
So, we have not done that we leave this as exercise for you. So, what we have done is
we have seen that whatever was valid for the univariate regression can be extended to the
multi multiple linear regression except that scalars there will get replaced by vectors and
matrices corresponding. What was a variance there it will become a variance covariance matrix here,
what was a vector there scalar there might mean scalar might become a mean vector here.
So, you will see a one to one correspondence,
but the residuals plot and interpretation of confidence interval for beta all of this,
the F statistic more or less similar. Except that understand in the multiple
linear regression there are several independent variables all of them may not be relevant.
We may be able to take only a subset and I will actually handle subset selection as a
separate lecture. For the time being we are just done a significance test
on the coefficient in order to identify the irrelevant independent variables,
but there are better approaches and we will take it up in the following lectures.

Let us continue our lectures on optimization for data science. In this lecture we will
look at multivariate optimization non-linear optimization. However, in the previous lectures
we saw the unconstrained version of this problem. In this lecture we will look at problems of this
type where we have what are called equality constraints. I will explain that is presently.
.
And I am going to explain this using a very simple example, but before we dive into this example let
us set some context and then ask the question as to why we should be interested in this in a
course on data science. The reason why we should be interested in this in a course on data science;
the reason why we are interested in constraints in optimization is as I mentioned before, we look at
optimization from a data science viewpoint because we are trying to minimize error in many cases,
when we try to solve data science problems. And when we minimize error, we said we could use some
kind of gradient based algorithm what we called as the learning algorithm to solve the problem.
In some cases while we are trying to optimize or minimize our error or the objective function,
we might know some information about the problem that we want to incorporate in the solution. So,
if for example, you are trying to uncover relationships between several variables and you
do not know how many relationships are there, but you know for sure that certain relationships exist
and you know what those relationships are, then when you try to solve the data science problem
you would try to constrain your problem to be such that the known relationships are satisfied.
So, that could pose an optimization problem where you have constraints in particular
equality constraints and there are several other cases where you might have to look
at the constrained version of the problem while one solves data science problems. So,
it is important to understand how these problems are solved. Once we look at equality constraint
problems we will look at in-equality constraints which is even more relevant. For example,
the algorithms for inequality can with inequality constraints are very useful
in data science algorithm that is called support vector machines and
so on. So we going to look at both equality and inequality constraints.
Now, let us look at this problem right here. So we are interested in mini-mizing the function here
which is 2 x 1 squared plus 4 x 2 squared and like we discussed before this is an objective
function in 2 variables x 1 and x 2. So, real visualization of this would be in 3 dimensions
where the objective function value is plotted in the z direction. However, we know that we
can work with contour plots and if you look at this picture here, you see these contour plots.
These are plots where each of this contour is a constant objective function contour and again you
would see that these are ellipses because if you look at constant contour plots then you have 2 x
1 squared plus 4 x 2 square equal k this you will see is an ellipse that is what is plotted here.
Now, if you look at the optimum value for this function and just by inspec-tion you can see
that the optimum value is 0 because this are functions where I have terms which are squares
of the decision variables. So, there are 2 terms here 2 x 1 squared plus 4 x 2 squared and the
lowest value that each one of these could take has to be 0 and that basically means the unconstrained
minimum is at x 1 equal to 0 x 2 equal to 0 the objective value at that point is also 0.
So, which is what is shown here in this point right here. So, if I had no constraints I would
say the optimum value is 0 and it is at 0 0 the star point here. Now let us see what happens if
I introduce a constraint in this case I am going to introduce a very simple linear constraint. So,
let us assume that we have a constraint of this form which is 3 x 1 plus 2 x 2 equals 12.
Now what this basically means is the following. You are looking for a
solution to x 1 and x 2 which also satis es this constraint that is what it means. So,
though I know the very best value for this function is 0 from a minimization viewpoint,
I cannot use that value because the 0 0 point might not satisfy this equation. So,
you will notice that if I put x 1 equals 0 x 2 equal to 0 it does not satisfy this equation. So,
the unconstrained solution is not the same as the constrained solution.
Now, let us see how we solve this problem. To understand this we rst start by representing
this constraint in this 2 dimensional space. So, since this is a linear constraint you will see
that it is a line which is here which is what is given by this constraint. Now since we said that
whatever solution I get it should sat-isfy this constraint would translate to saying,
I can only pick solutions from this line because only points on this line will satisfy
this constraint. Now, you notice that you could pick many many points on this line. So,
for example, you could pick this point and the objective function value at that point
would be corresponding to a contour which intersects this point. So, if you pick a
point here then you would have a corresponding objective function value. Now you could pick a
point here then you would see that it would have another objective function value which
would be corresponding to the constant function contour that intersects the line at that point.
So, you notice that as you pick different points on this line the objective function takes
different values and what we are interested in is the following. Of all the points on this line,
I want to pick that one point where the objective function value is the minimum. To understand this
let us pick two points and see what happens. So, if I pick a point here and let us say I pick a
point here and ask the question which one of these points is better from a minimization
of the original function view point. The way to think about this is the following.
So, when I pick a point here I know that the objective function value would be based on the
contour which intersects at that point and if you compare these 2 points you will see that
this point is worse o than this point, from a minimization viewpoint. This is because if you
look at these contours these are contours of increasing objective function values and the
contour that intersects this point is within the contour that intersects the line at this point.
So, basically what that means, is because this is the direction of increasing objective function
value the contour intersecting this point is inside the contour intersect-ing at this
point. So, that basically means this function takes a lower value at this point on the line.
So, as you go along the line you see that the value keeps changing and my job is to nd that
particular point where the objective function value is the min-imum. So, this is a basic idea
of constrained optimization solution that we are looking for. The key point to notice here
is that the unconstrained minimum is not the same as the constraint minimum. If it turns out
that the unconstrained minimum itself is on the constraint line then both would be the same, but
in this case we clearly see that the unconstrained minimum is di erent from the constrained minimum.
.
So, when I have just only one constraint, how do I solve this problem? I will first
give you the result and then explain the result again by going back to the previous
slide and then showing you another viewpoint and then how that leads to this solution.
So, if you typically take a multivariate function f of x in the unconstrained version,
let us assume that x is x 1 x 2 and x n. We know that the optimum solution is grad
f equals 0 which basically means that dou f dou x 1 dou f dou x 2 all the way up to
dou f dou x n equal 0. So, this when I expand this further I will get equations
dou f dou x 1 equal 0 dou f dou x 2 equal 0 dou f duo x 1 0 and so on.
Now, notice that if there are n variables there will be n equations of this form. So,
I have n equations in n variables. So, I can solve for it and find a solution and
once I find a solution to nd out whether it is a maximum or minimum or a saddle point I calculate
the second derivative and then construct the hessian matrix and then depending on whether
the hessian matrix is positive definite positive negative definite or semi definite and so on we
can make judgments about whether the point is a minimum point maximum point and so on.
Now, let us see what happens if I am trying to solve a problem where I have to minimize
f of x which is x 1 x 2 all the way up to x n. But like I said before let us assume that
I also introduced one constraint and I am going to introduce let us say a constraint which is of the
form h of x 1 x 2 all the way up to x n equal to 0. So, this is an equality constraint. So,
we can always write a constraint in this form, even if you have some number on the right hand
side you can always move to the left hand side and write this constraint as something equals
So, basically my job is to find the minimum point for f subject to this
constraint. I will just give you the result and then we will see how we get this result.
So, when I want to solve for this problem the result is in this case grad f equal to 0
itself gave us the result. In this case it will turn out that the result is the following. So,
we can write the negative of grad has to be equal to some lambda gradient of h. So,
you can write this as negative or drop the negative and the sign of the value
lambda will take care of that, but I am writing in this particular form.
So, just to expand this basically says I have something like this I have domain is dou f dou x
1 all the way up to dou f dou x n equals lambda dou h dou x 1 all the way up to dou h dou x n. So,
if we expand this further I am going to get n equations. In this case I got these equations
to be 0 in this case I am going to get equations of the form minus dou f dou x 1 equals lambda
dou h dou x 1 will be one equation, the second equation will be minus dou x 2 equals lambda
dou h dou x 2 and so on, the last equation will be minus dou f duo x n equals lambda h dou x n.
So, now notice much like before I have n equations the difference being in the unconstrained case
I had zeros on the right hand side in the constrained case I have these terms on the
right hand side. Nonetheless these equations are in now n plus 1 variable because I have
my x 1 x 2 all the way up to x n and I have also introduced a new variable lambda right here. So,
my equations are in n plus 1 variables. I have only n equations at this point,
but notice that I have one more equation that I need to use and that equation is
the following if I nd some solution x 1 to x n which satis es all of these equations.
Then that also has to satisfy the constraint. So, here we are only talking about the gradient
form of the various functions, but the equation which repre-sents the constraints also needs to
be satis ed by any solution that we get for the constrained optimization problem. So, with these n
equations I will also get another equation which is that h of x 1 x 2 x n has to be equal to 0.
Now, you notice that in this case with one linear constraint I have n plus 1 equation in
n plus 1 variables. So, I can solve this, so to reiterate the di erence between the constrained
and unconstrained case was, in the unconstrained case we had n equations in n variables,
in the constraint case with just one constraint we have n plus 1 equations in n plus 1 variables.
Now, you might ask what happens, if there are
there is more than one con-straint there are let us say 2 constraints.
So, we will see what happens if there are 2 constraints,
so in this case we are going to minimize f of x and now let us say we have 2 constraints we
are going to say h 1 x equal to 0 h 2 x equal to 0. In this case what is going
to happen is the following the solution to the constrained optimization problem
is going to be minus grad f equals lambda 1 grad of h 1 plus lambda 2 grad of h 2.
So, if you look at this and the one constraint case you will see
the similarities. In the one constraint case we introduced one extra parameter,
in the 2 constraints case the x introduced 2 extra parameters. In the one constraint case,
we simply said grad f minus grad f equal lambda grad h, in this case this lambda
grad h becomes a sum of lambda grad h 1 lambda 1 grad h 1 plus lambda 2 grad has 2 and so on.
Now, if there are 3 equality constraints, then you would have 3 terms here and if there are
l equality constraints you will have something like this here where this is sum of l terms. So,
that part is clear the other part that we need to worry about is if
I have enough equations to solve for all the variables when I have 2 constraints,
that basically means I have introduced 2 new variables lambda 1 and lambda 2.However, this
equation irrespective of the number of equality constraints you have will always be n equations.
Because grad f would be dou f dou x 1 dou f dou x 2 dou f 2 x n so these are all n by 1 vectors. So,
this will just give me n equations. Nonetheless if I had 2 equality constraints
I need to find the 2 extra equations which are directly given by the constraints. So,
since the optimum point has to satisfy both the constraints, I get one extra equation x
1 x equals 0 and the other extra equation is 2 x equal to 0. So, if you have 2 equality
constraints in n variables I will have n plus 2 variables and n plus 2 equations and we will
always find this to be true because if I had 3 equality constraints, I will have n plus 3
variables which would be x 1 to x n lambda 1, lambda 2, lambda 3 and this gradient equation
will always give me n equations and the 3 extra consent would have given me the 3 extra equations.
So, I will have n plus 3 equations and plus 3 variables which can be directly generalized to
this form where I have l equality constraints. So, let us see in the single constraint case
how we get an expression like this that that would be easy to see in the single constraint
case. This expression where we have the sum of these terms is slightly more complicated
to understand. I am not going to do that. I am going to give you an intuitive feel for why this
is true in the single equation case and once you understand that with a little bit more effort you
should be able to think about why it should be true for more equality constraints and so on.
So, we go back to the previous slide and when I was discussing this slide and explaining how
equality constraints affect the optimum solution. I said there are many points on
this line which could all be feasible solutions. Feasible solutions meaning
those are all solutions which can satisfy this equation. Nonetheless, there are some points
out of those or one point which would give me the lowest objective function value. So,
when we looked at candidate solutions for this optimization problem we were looking
at the points on this line that that we are interested in because that is a constraint.
Now, let us take a slightly different viewpoints and then look at the same
problem from an objective function viewpoint. Now from an objective function viewpoint if
you did not constrain me at all and you said you could do anything you want,
then I would pick this point as a solution. Now when you look at this point and then say
well this is a best point I have let me find out whether it satisfies my constraint and then you
substitute this point into this and then you gure out it does not satisfy the constraint.
So, you say look I have to do something because I am forced to satisfy the constraint. So,
you will say let me lose a little bit in terms of an
objective function perspective and then see whether I can meet the constraint.
So, when we say I want to lose a little bit basically you know as we mentioned
before these are contours where the objective function value increases and those are actually
not good from a minimization viewpoint. So, while I am here since the constraint is not
satis ed, I am willing to lose a little to see whether I can satisfy the constraint and
maybe I go to a point here and then this is a constant objective function contour point. So,
if I am willing to give up something that basically means I am going and sitting on
different points on this contours and as I am pushed away and away from this minimum point I
am losing more and more in terms of the objective function value. That is I am increasing the
objective function value. Now logically if you keep extending this argument you will see that,
let us say this is the first point I moved here which is basically worse than this because you see
this is a contour which is going to be outside of this contour, but I moved here I made my objective
function worse, but I still am not satisfying the constraint. So, I give some more I come to
this point and I see a contour and this point is worse than this because this contour is outside
this contour and if I extract this argument let us say I keep making things worse and the
only reason I am making these worse is because I am forced to satisfy this equality constraint.
So, I come up to let us say here and this is still a contour which is much worse than
my original solution, but it is still not enough for me to satisfy my constraint. So,
if you keep repeating this process, you are going to nd a contour here where I touch this line for
the first time. So, when I touch this line for the rst time is the point at which for the rst time,
when I give up my objective functions value, I am also able to satisfy the constraint. Now once I
find a contour like that which touches this line then there is no incentive for me to go further
beyond because going further beyond would mean I would be making my objective function worser.
So, when I just touched that line that is the best compromise because that is where I become
feasible for the rst time and going any further would only make my objective function worse. So,
geometrically what this would mean is I keep making my objective function worse
till a contour just touches this line. So, at that point this line will become a tangent to
that contour and remember what that contour is that contour is f of x equal to k for some k
So, we have to choose a k in such a way that this contour, when I have the constraint,
the constraint becomes a tangent to this contour and optimum point. This geometric fact when you
represent it as equations, you would get the form that I showed which is minus grad of f is
lambda times grad of h. So, this is for the one constraint case, now when you have many equality
constraints while it is not as easy to see that as it is in the single equality constraint case
the statement that minus grad f let us say if I have 2 is lambda 1 grad h 1 plus lambda 2 grad h
2 this statement basically says that this gradient negative of the gradient has to be written as a
linear combination of grad of h 1 and grad of h 2 and that has a geometric interpretation.
So, that is the reason why we get these conditions for optimization with equality
constraints the key point that I want you to remember is that as you have more and
more equality constraints you will have to introduce more and more parameters lambda
1 lambda 2 and so on. However, there will always be enough equations and variables.
So, let us look at a numerical example to bring all the ideas together. We are going
to use the same example that we had looked at in the previous slides. So, we are trying
to minimize 2 x 1 square 4 plus 4 x 2 squared as objective function subject to this constraint. So,
we wrote the following equations for the optimum point for identifying the optimum point we said
minus grad of f has to be lambda grad of h and then we have h of x equal to 0. So,
this is what we wrote let us do the computation so that we understand this.
So to calculate grad of f which is basically dou f dou x 1 dou f dou x 2. So, if you look
at this objective function. So, f dou x 1 will be simply 4 x 1 and dou f dou x 2 will be 8 x 2. So,
we have grad of f so negative grad of f would be negative. Let us look at grad of h. So,
h is this equation so if I do go h dou x 1 dou h dou x 2. So, this is going to be equal
to dou h dou x 1 will be simply 3 and dou h dou x 2 will be simply 2 so we have this.
So, now, let us see what this equation becomes. So, this equation you would see is if you put
this in a bracket and you will see this easily. So, you have the first equation is minus dou
f dou x 1 equal to lambda dou h dou x 1 and we have minus 4 x 1 equal to 3 times lambda,
similarly the second equation would turn out to be minus 8 x 2 is equal to 2 times lambda
and this equation is basically the same equation as the constraint equation that
we have here. And as we mentioned before we thought the constraint that would have been 2
variables and you would have got 2 equations which would have been grad f equal to 0.
But with an equality constraint we have added a new parameter lambda. So,
we need 3 equations in x 1 x 2 and lambda we do have these 3 equations here and when
you solve these 3 equations you will get this solution and this is your optimum solution in
the constrained case which is different from the optimum solution in the unconstrained case which
would have been 00. So, in other words we have given up on the value of the objective function.
However, this is a point which would satisfy this equation of the line. So,
that is how we deal with equality constraints in an optimization problems. I already described how
these are useful or why we should study them in the rst place from a data science perspective.
With this I will conclude this lecture and in the next lecture we will look at how we handle
inequality constraints and I will also explain why we are interested in understanding how
optimization problems are solved with inequality constraints from a data science perspective.
Thank you and I look forward to seeing you again in the next lecture.

We come to the last topic in this series of lectures on optimization for data science.
Till now we saw how to solve unconstrained multivariate optimization problems. Then we
saw how to solve multivariate optimization problems with equality constraints. Now we
move on to multivariate optimization problems with inequality constraints.
Before I start explaining some of the key ideas in these types of problems,
let us look at why we might need this optimization technique in data science.
Remember in one of the earlier lectures I
talked about data for people who like south Indian restaurants and so on.
So, if you remember that example I said there are a group of people who might like certain type of
restaurant there might be a group of people who do not like that kind of restaurant and
so on. Then if we were to do a classifier which probably is a line like this then,
as we are trying to solve an optimization problem to identify a classifier like this,
we have to impose the constraint that all these data points will have to be on one
side of the line and all of these data points will have to be on the other side of line and
from our lecture on half spaces and hyper planes in linear algebra we know that if this equation
is something like this which is linear equation, then it might be that if the normal is in this
direction then this direction is said that if I substitute a value of this point into this it
is greater than equal to 0 and on this side it is less than equal to 0 with 0 being the line.
So, now, notice that for each point if we were to write the condition in terms of the
equation of the line and then you would see that these become inequality constraints. So,
there may be as many in equality constraints there are points and so on.
So, you see why we might be interested in imposing inequality constraints and optimization
problems from a data science viewpoint. A more sophisticated version of this idea is what is
used in one of the data science algorithms called support vector machine. Though we will not study
that technique in this first course on data science for engineers, I just wanted to point
out that this class of optimization problems is very important from a data science viewpoint.
Now, let us go back to the same example that we had before,
where we had the equality constraint and then we tried to solve the optimization
problem and then just make that equality into an inequality. So, in this case let us assume
what was equal to 12 in the last lecture has now become less than equal to 12 and let us
understand intuitively what happens to problems this of this type. Now in the previous case we
said when we have an equality constraint we said we are interested in any point on this line as a
candidate solution these are all called the feasible points and of all of these points
we were trying to pick the point which will give me the minimum objective function value.
Now, when I have the optimization problem where I have this inequality and let us assume this
is less than equal to in this picture what we have plotted is that the original unconstrained
optimum or the unconstrained minimum is still the red star. Now if you look at this and then
say this less than equal to 12 it can be basically rewritten as 3 x 1 plus 2 x 2 minus 12 less than
equal to 0 and remember that this is of the form in transpose x plus b less than equal to 0. So,
it is going to be one half space depending on how n is defined. In this case it will
turn out that this is the half space that is represented by this equation.
So, the di erence between the equality constraint and the inequality con-straint is the following,
in the equality constraint we had every point on this line being a feasible solution when you
make this as a inequality constraint. Then what happens is if you think of
this line is extended all the way, any point to this half space now becomes a
feasible solution and because every point in this half space is a feasible solution
the unconstrained minimum also becomes a feasible point so the constrained minimum
and unconstrained minimum are the same so this is an interesting thing to see.
So we saw that in the case of equality the unconstrained and constrained minimum were
different, but when we made this into an inequality with the less than equal
to sign we see that the constrained and unconstrained minimum are the same points.
.
Now, let us try and see what happens if I flip the sign and then said this is greater
than equal to 12. Then what would happen is if you were to extend this line all the way,
then the feasible region is to this side. Now, you asked the question where does the
optimum lie? You will notice that again we know the best solution is here and
as we move away from this solution, we will see that we are losing out on the objective
function value and as before we know any point on this line or to the side is a
feasible point and any point on this line is also feasible because of this equality sign.
Now, making the same arguments that we made in the case of the equality constraint problem, we
will see that I give up on my optimality, that is I keep going through contours of larger and larger
size where the optimum value keeps increasing and when a contour particular contour touches this
line exactly at one point then I have a feasible point which is going to satisfy this constraint,
the equality part of the constraint. So, it is satisfying the general constraint and that is
the worst I have lost in terms of how much my objective function has increased its value by.
Anything more would be unnecessary because if I move little further I am going to make
my objective function value worse. However, there is no need to do that because I already
found a feasible point here. So, this case the constrained minimum becomes
the same as the minimum that we achieved with the equality constraint case. So,
you see that depending on what type of inequality these different things can happen.
.
Now, from a general mathematical formulation viewpoint, we are going to generalize this.
What I am going to show is I am going to show you the general conditions. Now for this course
we might not be using this later in any data science algorithm. Nonetheless this
forms a basic idea for more sophisticated data science algorithms like s v m and so on. So,
it is worthwhile to pay attention and then understand this completely. So,
general multivariate optimization problem we can say has both equality and inequality constraints.
So, I have the objective function, I have m equality constraints and l inequality
constraints. Notice that we can always write the equality condition in this form where I
have 0 on the right hand side because if you have something on the right hand side
I simply move it to the left hand side and get a 0 on the right hand side. Similarly,
any condition inequality condition whether it is greater than equal to 0 or less than
equal to 0 I can always put it in this form. If the original condition is already in less
than equal to form then it is the same as this if it is in the greater than equal to form then
what I do is I multiply by a negative and then I have this condition has less than equal to 0.
So, now, I call this my constraint so I can again put it in this form. Now this becomes
a little more complicated formulation and I am going to show you the conditions for this
in the next slide. Which will look a little complicated in terms of all the math that is
there. What I am going to do is I am just going to simply read out the conditions in the next
slide and then we will take a particular example and then demonstrate how the conditions work.
.
These are the conditions for multivariate optimization problem with both equality
and inequality constraints to be at it is optimum value and let us look at this carefully. Remember
when there was only equality constraints, we had this part of the expression where we had
grad of f plus linear sum of lambda times grad h for each one of the equality constraints.
Now, when you also have inequality constraints, what happens to this equation is,
just like how we added a single parameter for every equality constraint, we add a parameter
for each inequality constraints. So, if there are m inequality constraints there will be m
parameters and in typical optimization books and literature people use lambda as a scalar
multiple for each one of these constraints. So, if there are l equality constraints, there will
be l lambdas and people use the nomenclature of mu for the inequality constraints. So,
if we have m mus there will be, there will be m mus corresponding to the m inequality constraints.
So, the difference between this condition in the equality and inequality case
is that for every one of these inequality constraints you add more linear combinations of mu
times delta g. So, look at this, this is the same form of as this except that I used lambda here and
mu here, but I take a take a grad of h and grad of g. That is the first set of conditions then
much like how we had the constraints equality constraints also as part of conditions in the
previous case, I am going to have the optimum solution satisfy all of this equality constraints.
Now the lambda is some real number, so as many real numbers as there are equality constraints
and much like how I still need to have this equality condition satis ed the optimum point,
I need to have the inequality constraint also to be satisfied by the optimum point.
So this ensures that the optimum point is in the feasible region.
Now this real di erences between the equality constraint condition and the
inequality constrained situation shows up. We also have additional constraints,
which are of this form, these are called complementary slackness condition.
So, what this says is if you take a product of the inequality constraint and the corresponding
mu i then that has to be 0. Basically what it means is either mu j is 0 in which case
this is free to be any value such that this condition is satisfied or this is 0 in which
case I have to compute a mu and the mu that I compute has to be such that it is a positive
number or it is greater than equal to 0. So, this condition is there to ensure that
whatever optimum point that you have, there is no possibility of improvement -any more
improvement- from the optimum point so that is the reason why this condition is there.
Now just keep in mind that if you are seeing course on optimization for the first time it
is not very easy or natural to understand this constraints right away. However, what we are going
to do is in the next slide we will take an example and then show you how these things work. One thing
that I want you to keep in mind is if we had let us say an unconstrained optimization problem
objective function in n variables, I always look at whether the optimum conditions have enough
equations and variables for me to be able to solve the system of equations and clearly you know in
the unconstrained case you have n equations and n variables and I clearly made the point
in the equality constraint case that for every equality constraint you add an extra parameter.
However, you will have enough equations and variables because when you write the
first condition which is of this form you will get the n equations and you will get as many
equality constraints that need to be satisfied as there are lambdas. So that also works out
properly. Now we will see whether the same thing happens in the case of inequality constraints.
.
There is a speci c problem in solving this -what are called the KKT conditions-the
conditions that I showed in the previous slide are called the KKT conditions. It
is not easy to solve the KKT conditions directly in the inequality case because
of the complimentary slackness condition which says either mu could be 0 or z could be 0. So,
we have to make a choice as to which is 0 so that makes this a combinatorial problem.
So, in general the KKT conditions are generally used to verify if a solution
that we have is an optimal solution. However, optimization algorithms will
have di erent ways of solving this problem and there are methods called penalty methods,
active set methods and so on, we are not going to talk about those methods here.
Nonetheless I just want you to understand how the solution works out in an analytical case.
Let us take a look at a numerical example to bring together all the
ideas that we have described till now. In this particular case it is a multivariate
optimization problem which is actually called quadratic programming and this is called
quadratic programming because the objective function is quadratic and the constraints
are linear. Those types of problems are called quadratic programming problems.
I think this is the same objective that we have been using till now in the several examples. So,
let us say this is the objective function and let us assume that we have constraints of the form
shown here. Let us assume the first constraint is 3 x 1 plus 2 x 2 is less than equal to 12,
the second constraint is 2 x 1 plus 5 x 2 is greater than equal to 10 and the third constraint
is x 1 is less than equal to 1. Now I am not doing anything more to this problem, but nonetheless I
just want you to remember that to be consistent with whatever we have been saying this should be
converted to a less than equal to constraint which we will see how that happens in the next slide.
Now let us look at this pictorially. Remember that the value of the objective function is going to be
plotted in the z direction coming out of the plane of the screen that you are seeing. So,
the representation of that are these contours that we see here,
which are constant objective function contours. So, I am just trying to see and explain how this
picture speaks to both the objective function and the constraints. So, the objective function
is actually represented in this picture as this constant value contours. So, if I am
moving on this the objective function value the same we have repeated this several times.
Now in this x 1 x 2 plane, let us look at how these constraints look. So,
I have this equation which is the equation of a line. So, when we talk about the first constraint
which is less than equal to 12, then any point to this side of the line is a feasible point. Now,
when we look at this constraint here then the equation of the line is 2 x 1 plus 5 x 2 equals
to 10 and whenever I have something greater than equal to 10, this region is a feasible
region and this is the x 1 equal to 1 line and x 1 less than equal to 1 would be this region.
So, if you put all of these regions together the only region which is feasible is shaded in brown
colour here. So, if you take a point any point in this region it will be satisfying this constraint
because it is to this side of this line, it will satisfy the second constraint because it is to
the side of the line and it will satisfy the third constraint because it is to this side of the line.
Notice that if you take any point anywhere else you will not be feasible. For example,
a point here would violate constraint 1, but it would be feasible from constraint 2 and 3
viewpoint nonetheless all the constraints have to be satisfied. Now similarly if you take a
point here while it satis es constraint 1 and 3 it will violate constraint 2.
Now, if you notice the optimum point is going to lie here and we are going to try and find
out this value through the conditions that we described in the last few slides.
.
So, what we do here is the following. So, we de ne something called a Lagrangian,
which basically will boil down to the kind of conditions that we have been talking
about I will explain this quickly. So, we take the objective function value or the objective function
expression here and then there are no equality constraints here, there are only inequality
constraints. I said we had one parameter age for each of these inequality constraints.
So, this is already in a form that is palatable to us which is less than equal to form. So, I just
simply write this as mu one times 3 x 1 plus 2 x 2 minus So, 3 x 1 minus plus 2 x 2 minus 12 will be
less than equal to 0. So, this is already in the less than equal to form, when I look at the second
equation, I want to make it into a less than equal to form then what I do is I said 10 minus 2 x 1
minus 5 x 2 is less than equal to 0. So, if I put this here this is into the less than equal to form
and then corresponding to this if I have mu 1 corresponding to this mu 2 and corresponding to
this I have mu 3 I have x 1 minus 1 is less than equal to 0 so you see that term here.
Then what you do is, you differentiate this with respect to x 1 and x 2. You will get the first 2
conditions that we have been talking about for the con-strained case with equality constraint,
unconstrained case and so on. So, the 2 equations for the 2 decision variables so,
when you differentiate this whole expression with respect to x 1 4
x 1 comes out of this term right here and this is only a function of x 2. So,
differential with respect to x 1 will become 0 and I will have 3 mu 1 x 1 when I di erentiate
this with respect to x 1 I get 3 mu 1 and from the second term I will get minus 2 mu
2 and from the third term I will get mu 3. So, this equal to 0. So basically this equation
you have and then when I differentiate the same expression with respect to x 2.
So, I get 8 x 2 from this here and from the second term I get 2 mu 1,
I get minus 5 mu 2 from this term and from this term I get nothing because it is only
a function of x 1. So, I get this equals 0 so, this basically gives you the condition that that
we have talked about which is of the form in the equality constraint case remember I said
you have minus grad has to be equal to sigma lambda I grad of h i and in the inequality
case you also add plus sigma mu i grad of g i if g i are the inequality constraints.
So, you kind of back out these 2 equations from this constraint and you have 2 equations here
because grad of x is of size 2 by 1 h is 2 by 1 2 by 1 because there are 2 decision variables
and these are scalars and this is a linear weighted sum. So, if you notice all of this,
you see that I have 2 equations. However, I have 5 variables that I need to compute,
I need to compute a value for x 1, I need to compute a value for x 2 and then I need
to compute mu 1, mu 2 and mu 3. So, let us see how we do that. We go back and add the
complementary slackness conditions. Also keep in mind that other than this we also have to make
sure that whatever solution we get still has to satisfy the 2 inequality constraints also.
So, whatever solution we get should still be such that 2 x 3 x plus 2 x 2 minus 12
is less than equal to 0 and 10 minus 2 x minus 5 x 2 is less than equal to 0. So,
these 2 also have to be valid, but because these are inequality constraints we cannot actually use
them to solve for anything. Once we solve for these variables we have to still verify whether
this conditions are satisfy or not. So, going back to the complementary slackness condition
we saw that we will have mu times g is 0. So, this is corresponding to the rst inequality constraint,
this is corresponding to the second inequality constraint and this is corresponding to the
third inequality constraints and we also have to have this mu i greater than equal to 0. So,
all of these conditions have to be satisfied for an optimum point.
Now, it is interesting to notice something here for let us just take this as an example
already we have 2 equations, I have already mentioned that the 2 equations are here. So,
I am looking for another 3 equations to compute mu 1, mu 2, and mu 3. So, that will be a total
of 5 equations and 5 variables. So, for this to be 0 you could say in this equation either mu 1
is 0 or whatever is inside the bracket is 0. So, we could say one possibility is whatever
is inside the bracket is not 0 in which case mu 1 has to be 0. So, the key point that I
want to make here is if we say whatever is in the bracket is not 0, then that automatically
gives me the value for mu 1. So, out of the 5 variables here I have already computed one,
similarly if I say whatever is inside here is not 0 then mu 2 has to be 0 and whatever
is inside here is not 0 mu 3 has to be 0 then I have already computed values for mu 1, mu 2, mu 3.
Then I could substitute those values back into this equation and I have 2 equations I can
calculate x 1 and x 2. So, this is one option. So, in one of the previous slides I had mentioned that
this becomes a combinatorial problem because we could also assume that this goes to 0. So,
this term goes to 0 and now let us assume actually this is nonzero, this is nonzero,
let us see what happens to that case do we have enough equations and variables.
So, in this case we will have 1 equation, 2 equations, the third equation will
be mu 2 has to be 0 because we have assumed this is not 0 the fourth equa-tion has to be
mu 3 is 0 because we have assumed this is not 0. Now the fth equation becomes the one which we have
assumed which is the term inside the bracket is 0. So, in which case again I have 1 2 3 and then mu
2 equal to 0 mu 3 equal to 0 as 5 equations in 5 variables. You could for example, assume that this
and this are 0 in which case I have to compute mu 2 and mu 1 and then let us say this is not 0
then mu 3 is 0, but in that case also you will have 1 equation, 2 equation this equal to 0 is
1 equation and this equal to 0 is 1 equation. So, again I have 5 equations and 5 variables
So, whichever assumption you make as far as these equations are concerned you will have
enough equations and enough variables. However, for some of those choices when you actually
find a solution and try to plug this back into this right here it might not satisfy this. So,
in which case that is not a viable option for us from an optimization viewpoint. And
in some cases this might be satisfied, but the mu that you calculate out of the
equations you get might not be positive. So, it might get negative mu in which case again
this is not an optimum solution. So, let us see how this happens for this example.
Now, I am going to use one notation here so, that we can understand the table in the next line. So,
whenever I assume that an equality constraint is exactly satis ed; that means, when I say 3 x 1
plus 2 x 2 minus 12 equals 0, then we say this constraint is active it is active because the
point is already on the constraint. If I take a point here then that is not on the line so,
that is basically less than equal to 0 so, I will say it is inactive. So, for every
constraint I can say whether it is active or inactive. So, if this constraint is active;
that means, 3 x 1 plus 2 x 2 minus 12 is 0. If this constraint is active; that means,
2 x 1 plus 5 x 2 minus 10 equals 0 and if this constraint is active; that means, x 1 equals 1.
So, in the example that we are considering right now, there are 3 inequality constraints
and as I mentioned in the previous slide if the inequality constraint is exactly satis ed that
does it becomes an equality constraint then we call that an active constraint and if the
constraint is not exactly satisfied we call it as an inactive constraint. And now we
have 3 inequality constraints and each of these constraints could be either active or inactive.
So, there are 2 possibilities for each of these constraints and since we have 3 constraints there
are 2 to the power 3 possibilities which equals the 8 possibil-ities that we have here. So,
what I am going to do in this case is we are going to enumerate all possibilities for you
to get a good understanding of how this approach works when you have inequality constraints. So,
let me pick let us say a couple of rows from this table to explain the
ideas behind how this works and what we are going to do is in the
next slide we are actually going to see graphically what each of this case means.
So, let us look at the rst row for example, here the choice we have made is all the 3
inequality constraints are active. That means, they all become equality constraints. Notice
something interesting remember there are 2 decision variables. So, each inequality
constraint is basically representing one half space for a line and when they become
active each of these constraints become an equality constraint each of them become line.
Now when all 3 are active then we have 3 equations in 2 dimensions right. So,
there are only decision variables x 1 and x 2, but I have 3 equations in those 2 variables and
from our linear algebra lectures we know that when we have more equations than variables in
many cases we are not going to find a solution for the 2 variables which will satisfy all the
3 equations. So, in this case you cannot solve this problem it is infeasible because though I
have enough equations a subset of these equations 3 equations are in 2 variables and I cannot nd
a valid solution. We will understand what this is geometrically in the next slide.
Let us look at some other condition here. Let us pick for example, row 5. So, if you look at row 5,
we have made a choice that the rst constraint is active, the second constraint is inactive and the
third constraint is inactive, that basically means the first constraint is equal to 0,
the second and third constraints have to be less than equal to 0,
which needs to be tested after we go through the solution process. Now much like how I
described before in this case also we will be able to find 5 equations and 5 variables and we
can solve for x 1 and x 2 which is shown here and we have solved for mu which is shown here.
Now if you just look at this solution, you will you will say it seems to satisfy everything
because I have a solution for x 1 and x 2 and I have mu 1, mu 2, mu 3 where mu 1 is minus 4.36
and mu 2 and mu 3 are 0. However, when you look at this mu you will see that one of the mus is
negative. Which basically means that this cannot be an optimum point based on the conditions we
showed in a couple of slides back, not only that on top of it when you actually put these 2 values
into the constraint x 1 is less than equal to 1 it is not satis ed because x 1 is 3.27.
So, if you take this row for example, it looks like both this mu being positive is
not satisfied and this constraint is also not satisfied. An interesting thing to note
here is we have to go back and check only the constraints that we have as-sumed to be inactive
because the active constraint is already at 0. So, whatever solution again will automatically
satisfy the active constraint. Let us say row 6 for example, here we have made a choice that the
rst constraint is inactive, the second constraint is active and the third constraint is inactive.
Again we will get 5 equations and 5 variables and we will look at the solution here. It again
looks good from the x view point.We have solved for x 1 and x 2, 1.21 and 1.51.
Unlike the previous case in this case the mus also looked good because mus have to be positive. So,
I have 0 to 0.450. So, it looks like this, this is a good candidate for an
optimum point. But once you have satisfied all of this you have to still go back and
look at whether the inactive constraints are satisfied by this solution point right
here and the inactive constraint here is x 1 has to be less than equal to 1,
but if you notice that 1.21 does not satisfy this constraint. So, this cannot be an optimum either.
When you look at row 4, where we have assumed constraint 1 is inactive and 2 and 3 are active,
I get a solution x 1 1.6. I get all mus to be positive which is also one of the conditions
for an optimum point and then I have to only verify this constraint here because other 2
are active constraints and they are providing equations for us to solve so they are like they
are going to be valid. Now when you put these 2 values into the first constraint
you will check that it actually satisfies the inequality also. So, this is a case where all
constraints and KKT conditions are satis ed. So, this is the optimum point so, the optimum
solution is 1.6 which is what we had indicated in the slide with the geometry of this problem.
So, let us look at each of this case in terms of where the optimum lies and how we interpret
this. So, if you take the case one where we took all the constraints to be active we said
we have 2 variables and 3 equations and that is not satis able in general. Geometrically what
this means is we want the point to lie on all the 3 lines at the same time because we
have assumed all 3 lines are active concerned. That means, all of them have to be equal to 0.
So, you find that you cannot get a point where the all 3 lines equations are satis ed. So,
if I want to satisfy 2, I will get a point here nonetheless it would not satisfy the
other constraint and so, on. So, this is the case of not all 3 lines intersect so we do not
have a solution here. Similarly you can look at each of these cases and you can see what happens
in each of these and you will notice that only in case 4 will you have the solution
that you got from the Kuhn Tucker condition and the actual optimum being satisfied.
In every other case you will see that there is some problem or other. So,
if you go back I think we looked at case 5 and 6 then the solution for case 5 is this,
the solution for case 6 is this and we said these two are not good solutions because they
violate the condition x 1 is less than equal to 1 which basically seen here because the
feasible region is to this side of line x 1 is less than equal 1, but this point is violating
this constraint and here again you see that this point is violating this constraint.
So, in summary what you need to know is that in the unconstrained case it is very clear
that I just simply write this grad of f is 0 as the condition and I will have enough equations
and variables there will be n equations and n variables. In the constraint case with equality
constraints I will get the same n equations, but the form will be slightly different we
write that as minus grad f is sigma lambda i grad of h i and since we add as many variables
as there are equality constraints and since the equality constraints have to be satisfied
those give you the extra equation. So, you will have enough equations and variables.
In the inequality constraint case, the first n equations come from a very similar form where
minus grad f is sigma lambda i grad of h i plus sigma mu i grad of g i that gives you
n equations and corresponding to every one of those lambda corresponding to equality
constraints you will get so many equations which are the equalities have to be satisfied. The
only complication comes in when you have these inequality constraints where either
you know you have can have one or the other be 0 that is what we call as complementary slackness.
So, we have this mu times g going to be 0. So, if g is 0 we call that as an active
constraint in which case we have to calculate the mu corresponding to it and in the optimum
point mu will be greater than equal to 0. Now depending on the form in which
you write whether you write all of these constraints or as inequality constraints
less than equal to or greater than equal to and also in terms of whether you write the
original equation as minus grad f equal to this sum on the right hand side or grad of
equal to sum on the right hand side the sine of mu in different textbooks and different
papers might be reported as either they have to be positive or they have to be negative.
So, you have to be careful about the conditions when you look at those, but if you stick to this
type of writing the equations, where you write minus grad of a sigma lambda grad h i plus sigma
mu grad g i and if you write all the constraints as less than equal to inequality constraints,
then mus have to be positive for the point to be an optimum point which is what we saw here.
As I mentioned before while this unconstrained case is a very very important case in machine
learning algorithms such as s v m and so on the constrained case I mean we
might not be using this quite a bit in this course. Nonetheless for the sake of
completeness and for the sake of giving the foundations for understanding other
data science algorithms that outside of this course that you might go and study.
I have also described the key ideas behind how to solve constrained optimization problems when you
have equality and inequality constraints. Keep in mind that while I have shown you the conditions,
I have not shown a proof or a derivation of these conditions in a formal manner.
The equality constraint case I appealed to intuition to tell you why the conditions
turn out to be the way they are. However, all of this can be formally proved and you
can derive these conditions based on formal mathematical arguments.
So, with this we conclude this portion on optimization for data science now we have
all the tools that we need to understand data science problems. The next set of lectures what
I am going to do is I am going to introduce to you different types of data science problems
that we encounter, how do we think about these data science problems. Is there some formal way
of thinking about these problems; that we can use to solve a variety of problems and
then we will move on to regression as a function approximation tool and we will look at different
clustering techniques that are used in data science and then we will nally conclude with
one particular technique called principle component analysis which is very useful for
engineers and end with more general example of how one solves real life data science problems.
So, I hope to see you continue these lectures and understand more of data
science. Thank you.

In the previous lecture we described unconstrained non-linear optimization
in the univariate case or when there was only one decision variable. In this
lecture we are going to see how this is extended to cases where there are
multiple variables that act as decision variables in the optimization problem.
So, when you look at these types of problems,
a general function z could be some non-linear function of decision variables x 1 to x n. So,
there are n variables that we could manipulate or choose to optimize this function z. A simple
demonstration of this in a two dimensional case is root of x 1 squared plus x 2 squared.
Notice that we could explain univariate optimization using pictures in two dimensions
that is because in the x direction we had the decision variable value and in the y direction
we had the value of the function. So, you could see something like this and then say this is the
minimum and so on. However, when you just extend this problem to two dimensions then you have to
have 3-dimensional plots and in dimensions higher than 2, if the decision variables are more than 2
then it is di cult to visualize. So, what we are going to do is we are going to explain some of the
main ideas in multivariate optimization through pictures such as the one that I have shown on
the left side of the slide and as I mentioned before since even for cases where there are two
decision variables we need to go to 3-dimensions and that is simply because if this is x 1 and
this is x 2 or this is x 1 and this is x 2, I need a third dimension to describe the value
of the function x 1 comma x 2 equal to 0. So, the objective function value becomes the third axis.
So, let us look at how we think about this unconstrained optimization when there are multiple
variables. Take this picture for example, if you look at this picture right here on the left hand
side of the slide you will notice that the minimum point is somewhere around here, which in this case
happens to be 0 0 this is touching the x y, x 1, x 2 plane and that is a solution minimum point is 0.
Nonetheless if you start moving in the x 1, x 2 plane then when you compute the
objective function at di erent points. Let us say I compute the objective function at this
point then this is going to be outside the plane and this is a value of the objective
function at this point. And if I compute the objective function at this point it is going
to be outside the plane this is going to be the value of the objective function at
this point and so on and if I go this direction I might come here and so on.
So, what we are going to do is we are going to be in the space
of decision variables and we are going to try and find an optimum
solution because those are the values that we are actually choosing. So, for example,
if let us say I have a point here on the space of the decision variables then the corresponding
objective function value is this and clearly we know that that is not the minimum point.
So, what we need to do is we have to figure out some how to get here. Notice that the point at
which the decision variables take values such that the function is a minimum is also in the
decision variable space. So, essentially when we keep changing the values for the
decision variables we are basically moving in this plane; however, while we are moving this
plane we are looking at the values in the z direction to nd out whether the point that
we have reached is a minimum or not. To better visualize this we draw what are called contour
plots which I show on the right hand side of this picture. So, think about a plane that cuts this
objective function plot parallel to the x 1, x 2 surface. So, for example, let us say you think of
a plane like this which is parallel to this and it is going to cut the objective function plot.
Now, if you have a plane that is parallel to the x 1, x 2 surface then what we are going to see
is we are going to have the objective function value be a constant across the plane because
when you project it here it is going to be at a particular f of x 1, x 2 value or z value. So,
what one could say then is that if I cut this surface with the plane parallel to x 1,
x 2 surface then I am going to get what are called contours on the x 1, x 2 surface. So,
you want to think about it this way. So, here is a plane that is cutting the surface. So,
on the plane wherever the surface is cut you are going to have a contour and what
we are going to do is we are going to project that contour onto this x 1, x 2 surface. So,
that is the plot here. So, for example, we could take z equal to 5 and then have that
plane cut this surface then let us see what the projection of that in x 1, x 2 axis will be.
So, we know that we are going to keep or hold the z equal to 5 as a constant. So,
you will get this equation root of x 1 squared plus x 2 squared equal to 5 which will give you
x 1 squared x 2 squared equal to 5 squared. So, this we all realize as equation of a
circle centered at the origin with a radius of 5, which is what you see in this plot. Similarly if
you say k equal to 4 you will get this contour plot and k equal to 3, k equal to 2 and so on.
Now, an interesting thing to notice is if I start with some decision variable values here
and let us say I want to improve my objective function, I know that if I pick a contour like
this and then from here if I keep moving on this contour I am not going to make any improvement
to my objective function value. I also know that as I go away from this point in this direction,
let us say I go to a new point, then that would be on a contour where the value of k is larger
than where I was here. So that means, I have increased my objective function. So,
if I move in any of these directions I am going to increase my objective function.
So, the one way in which I should move to decrease my objective function is to
move in this direction because however, much I decide to move if I let us say I
move here then this is the contour. So, the objective function value on
this contour is less than this contour. So, this point is a better point then this one.
So, this is the basic idea about how we optimize this function.
Now, as I speak you would have noticed that there are two decisions that I need
to make. The one decision that I need to make is of all of these directions,
what direction should I choose? So, I have to sit here and then make a choice about the direction
that I need to gure out. And once I choose a particular direction let us say I choose
this direction how far should I go in this direction is another decision I should make.
So, for example, if I go here I have made some improvement to my objective function
let us say if I go here I have made much more improvement to my objective function,
but let us say if I go here I have actually made my objective function worse. So,
there are two important things that I need to decide, one is the direction in which I should
move in the decision variable surface and once I figure out which direction I should move in how
much should I move in the direction. So, those are two important questions that we need to answer.
We will answer these questions when we look at numerical methods of solving these kinds
of unconstrained optimization problems. In this lecture what we are going to do is we
are going to show you the analytical conditions for minimum in a multivariate problem. Before we
do that just like we saw in the univariate case, let us say this is f of x x and then
I have a function like this which has only one minimum which is a global minimum and
then I also showed another case where we have a function may be like this where this is one
minimum this is one minimum both are minima this is a local minimum and this is a global minimum.
This same thing happens in the multivariate case also. Here is an interesting example of a function
where there are two decision variables let us say x 1 and x 2 and the function is of this form. If
you plot this in a three d plot you will get this you can see there are many hills and valleys in
this and you will notice if we do the projection of this on to the x 1, x 2 surface you will see
that there are several minima in this picture and you will see that the global minimum is here.
So, you can see how hard it can become in case of functions like this, where if you if you let
us say, you start from here, then clearly you know one of the good things to do would be to
go to this minimum. And you will be here and from here when you look at the conditions for minimum
there will not be any difference between the conditions here and the conditions here in terms
of the first order and second order conditions that we talked about in the univariate case
we will see what the equivalent conditions are in the multivariate case subsequently.
However, from just those conditions you will not see any difference, nonetheless if you actually
compute the objective function value at this point and this point this will be much smaller
than this. However, when you are here you have no reason to suspect that a point like this really
exists unless you do considerable analysis. So, in cases like this what you will have to do is,
you have to see whether you can improve it further, that basically means though
you know locally you are very good here you have to do some sacrifice and then try and
see whether there are other points which could be better. So, there are algorithms which will
let you jump here and then maybe will jump here and so on, but these are all algorithms
where it is very difficult in a general case to prove that I will go and hit the global minimum.
Now, remember this is something important to note particularly from a data science viewpoint because
let us say this is your error surface. Just as a idea for you to think about how important these
concepts are and you are trying to fit a model and the best parameters for the model are here.
But a typical optimization algorithm would get stuck anywhere in any of these local optima.
From a data science viewpoint what it means is that the error is not as small as it could be
here. However, from the model viewpoint if you were to change the parameters from this value
in any direction you change, you will be finding out that the error actually increases in the local
region. So, there is very little incentive to improve your objective function value,
sorry a very little incentive to move away from this point because locally
you are increasing your objective function value. So, ultimately your algorithm might
nd parameters which while may be acceptable are not the best. So,
this is one problem that needs to be solved really to have good efficient data science algorithms.
So, let us get back to finding out analytically how we solve this problem. So, if you have a
multivariate optimization problem where you have z is f of x 1, x 2 all the way up to
x n. And in the univariate case let us just contrast this with the univariate case. So,
let us say z equal to f of x just one variable. Then remember we said the necessary condition
for a minimum is that I should have dz dx equal to f prime x equal 0 and then we said
d squared z dx squares equal to f double prime x is greater than 0 for minimum. So,
these are the conditions that we described in the previous lecture.
So, the derivative in a single dimensional case becomes what we call as a gradient in the
multivariate case. So, in this case we have dz dx or df dx. However since there are many variables
we have many partial derivatives and the gradient of the function f is a vector such that in each
component I compute the derivative of the function with respect to the corresponding variable. So,
for example, this is dou f dou x 1 is the rst component dou f dou x 2 is the second component
and dou f dou x n is the last component. So, this replaces this in the single variable case.
And this is replaced by what we call as a hessian matrix in the multivariate case. So,
this is a matrix of dimension n by n. The first component is dou squared f dou x 1 squared,
the second component is dou squared f dou x 1 dou x 2 and so on, and you fill out the row
like this. So, the notation that we have used is we have used the x 1 in the front here and x
2 here for second column, x 3 here for the third column and so on, x n here for the last column.
Now, this is with respect to variable x 1. Now, you can do the same thing with respect
to variable x 2. Notice here that x 2 has come before x 1 because it is in the second
row and this diagonally is always with respect to the same variable differentiated twice. So,
this is dou squared f two x 2 squared dou squared f dou x and squared and so on. Also notice that
this hessian will be a symmetric matrix because for most functions this equals this and similarly
you will have dou squared f dou x 1 dou x three the next term here will be dou squared f dou x 3,
dou x 1 which will be the same. So, the hessian matrix is going to be symmetric.
Remember in the linear algebra lecture we said that we will be seeing symmetric matrices quite
a bit and here is a symmetric matrix that is of importance from an optimization viewpoint.
Now, what we need to do is we need to see how these conditions in the
uni-variate case translate to the multivariate case.
So, much like what we did in the univariate case, we are going to do a Taylor series approximation
and what I have done here is I have just written it till two terms there are more terms here.
But what we are going to do is we are going to make the argument that if you make the distance
between the point that you are at and the next point that you are going to choose very small,
then whatever is the leading term in the sum is going to decide the sign of the whole sum.
So, in other words if you take this whole thing right here if you keep
making this as small as possible or as small as needed then what will happen is,
the fact that whether this in nite sum is positive or negative can be identified only
by the first term and if that is positive then the whole sum series sum is going to
be positive and so on. So, that is the kind of logic that we are going to use again here.
Much like before we said that if I keep making this small I need to
only look at this here and much like the univariate case if this does not go to 0,
I can make this term either positive or negative. To see this if I take a particular direction and
then say delta f transpose alpha. If this turns out to be negative this number, then
if I go in the opposite direction of minus alpha I will get grad of alpha this will be positive;
that means, that I will have a point here which can be either larger than this or smaller than
this and if I can find a point such that this is smaller than this then this cannot be a minimum.
So, whatever you do unless this goes to 0, I cannot ensure that this is a minimum point. So,
the rst condition that we will get is that this is 0. And once that is 0 then I am left
with the just this term right here and if you notice this term is of the form delta
transpose the hessian matrix let me use H here delta. We know that this is a symmetric matrix
and let us also make sure we understand this clearly the function f is a scalar
function and you can see that here also H is an n by n matrix here lambda transpose will be 1
by n and lambda will be n by 1. So, when I do this I will get one by one which is a scalar.
Now, irrespective of this x bar, if this is a positive number then we can
say irrespective of whatever direction you take this will always be greater than this
in the local region which would qualify this point x star as a minimum point. So,
that is the important idea that that you should remember.
So, we come back to this in the last slide I wrote this as delta transpose
H delta greater than 0, H is symmetric. H is basically this second derivative matrix. Now,
we did not see this in the linear algebra lectures, but if I need this condition to be
satisfied irrespective of whatever delta is then we call this H as a positive definite matrix. So,
if H is positive definite then this will be greater than 0 for all delta not equal to 0,
clearly when delta equal to 0 this will be equal to 0.
So, how do I check if a matrix that I compute is positive definite or not.
Remember from the linear algebra lecture we said if I have a symmetric matrix,
then I will have the eigenvalues as being real. So, symmetric matrices always have
real eigenvalues and the eigenvalues could be positive or negative in this case. Now, the linear
algebraic result for positive definite matrix is that if this matrix has let us say n eigenvalues,
and if all of these eigenvalues are greater than 0 then this matrix is called positive definite.
In other words if all the eigenvalues of this matrix are greater than 0,
it is automatically guaranteed that whenever we compute this for any delta direction we
will always get a positive quantity. So, this has already been proved. So,
if you want this to be positive for any direction why do we want this we want this because we want
f of x star to be the lowest value in its neighborhood and that we said will happen
if this is positive for any delta or for every delta this should be positive. That condition
can be translated to H being positive definite and H being positive de nite can be translated
to the condition that lambda 1 to lambda n the n eigenvalues of H are strictly greater than 0.
Now, what this does is the following. So, in a multivariate case it gives us a way to identify
points that could be optimum points and once we identify those points we can compute this hessian
matrix at those points and then computation of the eigenvalues of this hessian matrix would
allow us to determine whether the point is a maximum point or a minimum point and so on. So,
this is the complete equivalent of what we did in the univariate case.
So, to summarize in the univariate case the two conditions are f prime has to be
zero and f double prime has to be greater than 0. In the multivariate case these
translate to grad f equal to 0 and the Hessian matrix being positive definite.
Let us take a very very simple example and identify an optimum solution.
So, consider this multivariate example. So, there are two decision variables and
this is a function in terms of these two decision variables.
So, what you can do is you can first construct this grad f vector which is dou f dou x 1. So,
that would be dou x 1 dou x 1 will be 1, this will be a 0 term this will be 4 time 2 times x 1,
8 x 1 this would be minus x 2 and this would be 0. So, the rst term is dou f dou x 1 here
similarly when you do dou f dou x 2, I will have a term corresponding to this two. This
when differentiated with respect to x 2 will go to 0 corresponding to this I will have a
minus x 1. Now, and corresponding to this I left 4 x 2 which is what we have here.
So, we have these two equations that we need to solve. So, when we solve this and get let
us say one of the solutions x 1 star x 2 star is this here. I can check whether
this is a maximum point or a minimum point, to do that what I have to do is I have to do this
second derivative matrix. So, the way you do the second derivative matrix is the following. So,
the first term is dou squared f dou x 1 squared. So, we already have dou f dou x 1.
So, if you differentiate it with respect to x 1 you will get this term. So,
the only term remaining will be 8 which is what we see here and when we look at this we
already have dou f dou x 2. So, we have to differentiate this with respect to x 1. So,
the only term remaining will be minus 1 which will be here and I already told you
this is a symmetric matrix. So, you can simply fill in the minus 1 here and to
get this term I already have dou f dou x 2 here I differentiate this again with respect to x 2. So,
the only thing that will be remaining would be 4 which is here. So, I have this. Now, what I need
to do is I need to compute the eigenvalues for this and when I compute the eigenvalues for this
I nd the eigenvalues to be both positive, that means, that this is a minimum point.
Now, when we look at this equation here there are two equations in two variables
and both are linear equations. So, there is going to be only one solu-tion here
and it turns out that that solution is a minimum for this function. So,
this finishes our lecture on multivariate optimization in the unconstrained case.
What we will do in the lectures that follow, we will look at some numerical methods for solving
these types of problems. We will introduce the notions of how to solve these problems
when there are constraints. We look at two types of constraints, one are what we call as equality
constraints the other type of constraints are inequality constraints. So, we will pick up from
here in the next lecture. Thank you.




In this lecture we will continue with Unconstrained Multivariate
Optimiza-tion. In the last lecture we saw the conditions for a point
to be an optimum point a minimum point for multivariate functions.
We described multivariate functions as functions with several decision variables. What we are going
to do in this lecture is to look at the same conditions and show how you can numerically
solve these optimization problems. And the reason why we are teaching this in a data science course
is the following, if you think of any data science algorithm, you can think of it as some form of an
optimization algorithm and the techniques that you will see in today’s class are also used in
solution to those data science problems or data science algorithms. And you will see
these numerical methods as what is called as learning rule in machine learning and so on.
So, let us look at an unconstrained multivariate optimization problem. In
unconstrained multivariate optimization problems we are going to solve these using what we call
as a directional search. The idea here is the following, if you are on the top of a mountains
keying and you are interested in reaching the bottom most point from where you are pictorially
shown through this picture here, you will see that there are several different points in this
surface. This is a point which is at the bottom of the hill. So, we call this as a minimum point.
However, this is a local minimum because right next to it there is another point
which is even lower which we call as the global minimum. We also see that there is
a local maximum here a global maximum here and there are also points such as
these which are called the saddle points. When you look at optimization algorithms,
the aim is for us to reach this point. We want to avoid points like this because as I described
before when we reach these kinds of regions while locally we cannot make our algorithm nd anything
better we know that globally this is not the best. So, in that sense we could do better.
Nonetheless this is an OK point if it is not very far in terms of the per-formance from a
global minimum. However, we want to avoid points like these saddle points and so on, because as you
see even in this picture you know saddle points could be very far away from the actual solution.
So, the aim is to reach the bottom most region. Typically what you would do is the following. So,
if you are at a particular point here and then you say look let me go to the bottom of the hill
as fast as possible, then you would look around and nd the direction where you will go down the
fastest. So, this is a direction we call as the steepest descent. So, the direction in
which I can go down really fast and I will nd that direction and then go down the direction.
Now, the way optimization algorithms work is the following, you are at a
point you find the steepest descent direction,
and then what you do is you keep going in that direction till a sensible amount of
time or in this case the length of the step that you take in that direction.
The reason for this is the following, the reason is you could find this as
the steepest descent direction and you could keep going in this direction,
but let us say beyond this point you really do not know whether this is going to be the steepest
descent at that point also. In other words is this going to continue to be the steepest
descent till I get to my best solution. Now, that is something that is you cannot guarantee easily.
So, smarter strategy would be to find the steepest direction at wherever you are and
then find how long you are going to move along this direction, go to the next point in the
direction and then at that point reevaluate all the directions, and then nd new steepest descent
direction. If it turns out that the direction that you are on is continuing to be the steepest
descent direction you continue to go on that direction, if not you find a new direction
and then go in the direction. So, that is a basic idea of all steepest descent algorithms.
Now, notice that you know we do the steepest descent and let us say we end up here, then at
that point you will nd no direction where you can improve your objective function that is you cannot
minimize your objective function anymore. In which case you are stuck in the local minimum. So,
there are optimization algorithms which, when they try to get out of the local minimum. The only way
to do it is let us say if you are here the only way to get to global minimum is to really climb up
a little more and then nd directions and maybe you will nd another direction which takes you here.
In some cases we might construct these optimization algorithms in such a way that you
might actually make your objective function worse in the interim, looking for better solutions than
your local optimum solution. So, that is what we have written here sometimes we might even climb
the mountain to get better perspective. So, for example, if you are here and then say if you look
at this you are going to land up here maybe you can go in this direction climb up actually get
a better perspective and then come down here. So, some of these are mathematically formulated
and solved. So, the basic idea of how these algorithms work is explained in this slide.
Now, let us see the mathematics behind whatever I described in the previous slide. So, the current
point that I am at is what I would call as x k. So, k stands for the k th iteration. So,
at the 0 th iteration you will start with x 0, move to a point x 1 and at the rst iteration
you will start at x 1 and move to x 2 and so on, till you nd the solution that you are happy with.
The discussion that we had in the previous slide is seen here mathematically. So,
what we are interested in is nding a new point which is better than x
generally. And the notion of steepest descent is the following or for that
matter any other search direction is the following. So, I have this point and I am
going to move in a direction that I have chosen. If I choose the steepest descent
then I choose that direction, if it is some other direction we choose that direction.
And then what I am going to say is I have a current point I have chosen a direction in
which I want to move the only other thing that I need to gure out is how much should I move
in this direction, and remember from vectors we talked about in the linear algebra portion of this
course. If I start from x k and then keep moving in the direction of s k this is what it will be I
will be on this line. Now, the question is this x k plus 1 where is it placed. So, if alpha is
very small it will be here slightly bigger x k plus 1 will be here even bigger x k plus 1 will
be here and so on. So, how do I find this step length that I need to take in this direction.
So, notice that something interesting has happened. If I am in a particular point. So,
remember this is also vector because we are talking about multivariate problems. So,
there are several decision variables. So, this will be an n by 1 vector if
I have n decision variables. So, this could be something like x 1 k,
x 2 k all the way up to x n k and these are the current values for variables x 1,
x 2 all the way up to x n which are the decision variables that we are trying to find. Now,
for this equation to be correct this is also going to be n by 1 vector the search direction and this
is essentially a scalar this is just a number that we need to nd out which is a step length.
Now, we will show you what the steepest descent direction is, but you gure out this direction
somehow. So, this is going to be a direction vector. So, this direction vector will be
something like s 1 k, s 2 k all the way up to s n k. So, let us assume that we have somehow gured
this out, then the real question is what is the step length. So, one idea is to gure out the step
length, so that this when substituted into the objective function is an optimum in some sense.
So, that is what we are going to try and do. So, the key take away from this is that if
you are at a current point which you know and if you somehow gure out a search direction,
then the only thing that you need to then calculate is the step length. And
since step length is a scalar what happens is a multivariate optimization problem has been broken
down into a search direction computation and nding the best step length in that
direction which is a univariate optimization because we are looking for a scalar alpha.
In general this kind of equation that we see here you will see in many places as we look at
machine learning algorithms in clustering, in neural networks and many other places,
in machine learning techniques this is called the learning rule. Why is
it called the learning rule? It is called the learning rule because you are at a point here
and you are going to a new point you are learning to go to a point which is better
than wherever you are and I mentioned to you before that we could think of this machine
learning algorithms as being optimization problems solutions to optimization problems.
So, if you talk about neural networks one of the well known algorithms is what is called a
back propagation algorithm. It turns out that the back propagation algorithm is nothing but
the same gradient descent algorithm. However, because of the network and several layers in
the network it is basically gradient descent we are including an application of a chain
rule which we all know from our high school. Similarly in clustering algorithms you would
see that clustering algorithms would turn out to be minimization of a Euclidean distance now.
So, let us now, focus on the steepest descent and the optimum step size that we need to take. So,
the steepest descent algorithm is the following. At iteration k you start at a point x k. Remember
with all of these optimization algorithms you would have to start with something
called an initialization which is x naught and this is true for your machine learning
algorithms also. All of them have to start at some point and depending on where you start,
when you go through the sequence of steps in the algorithm you will end up at some
point let us call x star, and in many cases if the problem is non convex that is there
are multiple local minima and global minima the point that you will end up
is de-pendent on not only the algorithm, but also the initial point that you start with.
That is the reason why in some cases if you run the same algorithm many
times and if the choice of the initialization is randomized,
every time you might get slightly different results. So, to interpret the difference
in the results you have to really think about how the initialization is done. So,
that is an important thing to remember later when we when we learn machine learning algorithms.
So, as I said before, we start at this point x k and then we need to find a
search direction and without going into too much detail the steepest descent will
turn out to be a search direction s k which is basically the negative of gradient of f of x,
where f of x is your objective function. So, if f of x is an objective function of
the form with this many decision variables then grad f is basically dou f dou x 1 all
the way up to dou f dou x 1 and negative grad f would be this, and we keep this as the search
direction s k equal to negative grad f and this is called the steepest descent search direction.
The key thing that I really want you guys to notice here is the following, x k is known,
the function is known. So, to get to x k plus 1 x k is known since the function is known we
know also the grad of f and s k is given as the grad of negative grad of f evaluated at x k. So,
basically this is going to be let us say some functional form minus g 1 x all the way
up to g n x all you are going to do is simply substitute for this x, x k. So, that basically
gives you the search direction. So, this is given this is calculated once this is given.
Then the only thing that I need to find out is this alpha k and the way they
are value for alpha k is found out is by looking at this f of x k plus 1. Now,
substitute this x k plus 1 into this. So, you are going to have f of x k plus alpha s k alpha k,
s k. In this you know this you know this. So, this f is going to simply become a function
of alpha right. So, let me put alpha k here. So, this is going to be a function of alpha.
Now, in the previous slide we talked about this and we said alpha is a scalar. So,
it is just one variable. So, this becomes a univariate optimization or
a univariate minimization problem. So, this is a critical idea that I want you to understand.
Now, any univariate minimization algorithm can be deployed to find out what alpha k is. So,
you deploy a univariate minimization algorithm nd a value for alpha k and then plug this back
in then you have your algorithm for your multivariate optimization which will go
something like this. So, you start with x naught then x 1 is going to be x naught
plus alpha naught s naught. So, x naught is given based on your initialization,
s naught is calculated, alpha naught is optimized for, then you go on to x 2 is x 1 plus alpha 1 s
1 and so on. And you keep doing this till you use some rule for convergence you say at some
point the algorithm is converged. So, that point is what I am going to call as x star.
Connection to machine learning algorithms is the following this alpha is usually called as the
learning rate. You could either optimization nd out or you could actually pick a value for this
and then say let us run the algorithm let us not optimize for this alpha naught alpha 1 and so on,
I will give you a xed value of alpha, alpha xed. So, you simply run your algorithm with
xed value of alpha which is x naught x 1 will be x naught plus alpha xed s naught,
x 2 will be x 1 plus alpha xed x 1 and so on. So, that is also something that you could do.
Nonetheless this is the critical equation which is used to optimize an objective function.
In the next lecture we will look at a numerical example that illustrates
some of the ideas that we have seen. Now, see you in the next lecture.
Thanks.

Let us continue our lectures on optimization for data science. I am going to start out
this lecture by showing you a numerical example of how gradient descent works in
optimization. In many cases this is also called the learning rule in machine learning algorithms.
Let us look at a function f of X which is 4 x 1 squared plus 3 x 1 x 2 plus 2.4 x 2
squared minus 5.5 x 1 minus 4 x 2. We are interested in minimizing this function. As
you can notice this is a function of two variables x 1 and x 2. So,
there are two decision variables that we can choose values for, to minimize this function.
As I mentioned before if it was a univariate optimization, then you could visualize the
picture in two dimensions with the y axis being the objective function value and the
x axis being the decision variable. Now, since we have a function with two variables x 1 and
x 2 we visualize this in 3 dimensions, you have the two dimensions x 1 and x 2 on the
plane below here and you have the z axis which is f of X which is the objective function value. So,
we are trying to look at how an algorithm would minimize this function in a numerical fashion.
Now, notice from the previous lecture we described that while you try to minimize these functions you
do what are called contour plots. And if you looked at the previous slide you would notice
that x 1 and x 2 are in a plane and the objective function is a value that is projected outside the
plane. So, if you think about constant objective function values then what you think about is a
constant z value in the previous graph, and a constant z value would be a plane which
will be parallel to the plane of the decision variables x 1 and x 2. So, when that cuts the
surface that we saw in the previous graph then you have what are called these contour plots.
So, while we are trying to minimize a function which is represented in the z direction,
all the changes to the decision variables are being done in a 2-dimensional plane which is
shown here. For example, each of these curves that we see here are contour plots and as I mentioned
before these contour plots are plots where the objective function takes the same value. So,
from the previous slide you would notice that if you were to pass a plane through the surface
you would see a curve like this would be traced on the surface, and as you move the
surface up and down the size of the contour will increase or decrease corre-spondingly.
Now, the way most optimization problems work is the following. So,
let us say we start with some value for x 1 and x 2. So, we simply guess
a value for that and this is what we call as initialization in the optimization. So,
let us assume that we initialized this problem at x naught. So, you would notice that this
initialization basically says here is your value for x 1 and here is a value for x 2. So,
we have picked some x 1, x 2, and we have initialized this problem.
Now, we know that the constant objective function values are contour plots and if we look at this
equation here what we are saying is that the function f of X which we saw from the previous
slide. If we were to find a constant value for this function then I have to set this equal to
k. Then I can look at this equation and then say how would this constant contour plot for
f of X look in the x 1, x 2 plane and you would notice that this is quadratic in this case. It
is actually going to be an ellipse in for this particular function. So, this ellipse that we
trace would be for some particular value of k and our initialization point is here.
Now, the way to interpret this is to say if we were to keep moving on this contour you
would make no improvement through your objective function that is your objective function will
not decrease because it is a constant contour plot. Now, in gradient descent we wrote this
equation where we had x k plus 1 equals x k plus alpha k sk, we said this is the current point,
this is the step length and this is the direction in which we should move.
So, let us look at this picture here. When we start we have x naught which is initialization
which is this point. What we need to do is we need to find a direction in which to move
and once we find a direction in which we would like to move, then we will find out a learning
rule or a learning constant which will take us to the next point. So, initial guess in this
case that we have chosen is about 2 2 and f of X naught value when you substitute this 2 2 is 19.
So, the next step is the following. So, we are going to say x 1 is x naught. Now,
if I take this direction as minus grad f which is what we discussed last time which I have written
here as f prime then, the equation will be the new point x 1 is x naught minus alpha times f
prime x naught. So, this grad is evaluated at the point that you are currently at. So,
the same equation becomes this here. So, just to illustrate the idea of how an optimization
approach works we are going to pick some alpha here, which we have picked as 0.135 here,
there is a way in which you can automate this and here we are just using this number.
Now, if I take grad of f, so when I differentiate this function with respect to x 1, I will get from
this term 8 x 1, from this term I will get 3 x 2, and from this term I will get minus 5.5. So,
dou f dou x 1 is going to be this term. And dou f dou x 2 I am going to get 3 x 1 from this term,
5 x 2 through this and this minus 4 I am going to get from here. So,
this is dou f dou x 2. So, that is your f prime or grad of f.
Now, what you need to do is, once you identify this if you look at this equa-tion this
direction which is a gradient direction has to be evaluated at x naught and since our original
x naught is 2 2, I am going to substitute the value of 2 2 into these equations. So,
I have 8 times 2 plus 3 times 2 minus 5.5, and 3 times 2 plus 5 times 2 minus 4 and
this is the learning parameter and this is our original point which gives me a nu point x 1
after simple computation. And when you compute the function value at x 1 you notice that the
original function value was 19. Now, it is come down to this number right here.
So, the point that we are trying to make is this direction is actually a good direction and then
you move in the direction and you find that your objective function value decreases which
is what which was our original intent because we are trying to minimize this function. And as
I mentioned before this, gradient descent is usually called the learning rule. So,
when you have parameters let us say that you are trying to learn for a particular problem
this keeps adjusting this equation keeps adjusting the parameters till it serves
some purpose and this adjustment is usually called the learning rule in machine learning.
So, this is the new point that we are right and if you find out this value which is 0.0399
and then set this k to be this number whatever was f of X 1 which is 0.0399, then you would
notice that the equation form remains the same except this constant has changed. So, this is
continuing to be an ellipse, but it is an ellipse that are shrunk from your original ellipse. So,
the constant contour plot, this blue plot, is the plot at which f of X will take a value 0.0399. So,
wherever you are on this blue curve or the blue contour the objective function
value is the same. So, this is a rst step of the learning rule that we see.
Now, let us proceed to the next iteration. The next iteration is pretty much exactly the same.
What you can see here is now, we start with X 1 which was what we identi ed from the previous
iteration and X 2 is X 1 minus alpha f prime X 1. So, pretty much we are doing the same thing
I substitute the value of X 1 here alpha remains the same and I do the same dou f dou x, but now,
I evaluate the gradient at the new point X 1. So, if you notice here in the last slide we
had put 2 and 2 for these values, but in this you would see I am using the X 1 value minus 0.2275,
0.3800 and so on. And in this case the learning rate remains constant,
but in more sophisticated algorithms or algorithms where you could actually optimize
the size of this learning parameter as we go along in the algorithm.
Nonetheless, the ideas are pretty much the same only that this number will keep changing
iteration to iteration. Now, we get a new value X 2 and notice that this new value of f X 2 when
substituted into this function f of X 2 give you even smaller objective function value. In fact,
the objective function has become negative. So, this new point is shown here as X 2. Now,
again much like how we discussed the previous iteration in the last slide, in this iteration
if you were to take the function f of X and then set it equal to minus 2.0841 then that would be
again an elliptical contour and that contour is actually described by this green contour.
So, one more iteration you can simply follow through the steps same thing
here x 2 from the previous slide the new X 3 value which is X 2 minus alpha. Now,
again the gradient is evaluated at the new point and the alpha remains the same and now
you notice from the previous slides. Let us go back quickly to the previous slide
and see what the value was. The value of the objective function was minus 2.0841.
Now, when you look at the new point X 3 the objective function value has decreased even
more it has become minus 2.3341. So, we notice that at every step of the algorithm the objective
function keeps improving for us here in this problem improvement means the objective function
value keeps coming down and since at every point and the objective function value keeps
coming down our hope is at some point it will hit the minimum value. How do you understand if
it is a minimum value or not? It is something that that we will discuss in the next slide.
Nonetheless all I want you to notice is that at every iteration the value of the objective
function keeps coming down because we are trying to minimize the objective function,
and you can see the kind of improvement you get as we keep zooming down this picture, this was our
original red elliptical contour which is kind of going outside the frame. So, this is a big
elliptical contour from where we started, but we have improved the objective function to come here.
And just to make the connection between data science and optimization if this objective
function were an error in some function that you are approximat-ing your initial
error is very large and as we learn or as the machine learns to approximate the function,
what it does it keeps improving and it keeps nding out new points which could
be the parameter values that that you are trying to nd out such that
the error keeps decreasing. So, that is what is happening here in this example.
So, you could go through this process. The third iteration maybe gives us this
X 3 and then the next iteration gives you an f X 4 value which is this and you
can notice that the objective function value has come down.
Also notice a couple of other interesting things that we see as we go along through
this optimization procedure. If you noticed, I think the first value was very high for f of X
and after the first iteration the objective function value came down quite a bit,
and then we have gradually keep improving the objec-tive function value. And as you
get closer and closer to the optimum the gradual improvement in your objective function value in
the iterations that come later in the algorithm are going to be less. What I mean is I the very
first improvement when we went from X naught to X 1 was a large improvement. But when we go
from X 2 to X 3 and X 3 to X 4, the improvement in the objective function, while the objective
function is improving, the improvement amount of improvement keeps decreasing.
And you would expect that because as you get closer and closer to the optimum you
know that the optimum point is where the gradient goes to 0. So, as close to the
optimum if the function is reasonably continuous then you are going to have
derivatives which are very small and if you notice each of these steps,
this is one thing that dictates the size of the step you take. And if this is a constant value
the size of the step you are going to take is going to come down and also the improvement in
your objective function is going to come down. But keep in mind the objective function keeps
improving all I am saying is that the amount by which it improves will keep coming down.
So, if you do this for a few more iterations you will get to the optimum point which is
this solution 0.5, 0.5 and the function value at this optimum point turns out to be this. Now,
the couple of things that I would address here. So, when you write an algorithm like this or
when an algorithm like this works you have to tell the machine to stop doing this algorithm
at some point. Which is what in optimization terminology called as convergence criteria.
And the way the convergence criteria works is the following there are many ways in which you
could you could post a convergence criteria and then say this the algorithm has converged.
So, we talked about the decision various values themselves decision variable values themselves.
So, there is let us say we are at x k and we are nding a new variable x k plus 1 we have this we
have also the value of the function at x k and the value of the function at x k plus 1, and we have
also got the gradient of the function at x k, and the gradient of the function at x k plus 1. So,
what I am trying to show here is that when I am moving from x k to x k plus 1, I could compute
all of these quantities. So, you could post a convergence criteria on any of these. So,
for example, you could say the difference between this and this, in a vector of different sense,
if that is becoming smaller and smaller then you might stop your algorithm.
So, the logic behind this is that if you are making minor modi cations to your parameters
you can keep doing it to try to get to perfect value, but at some point it starts making not
much of a difference. So, you could use this norm as we call it which is the difference
between these two values at two different iterations as a condition for saying the
algorithm converges. That is when this becomes small enough you say the algorithm has converged.
You could also simply take the difference between the objective function values in
two iterations for example. When that becomes very very small you could think about saying
that the algorithm has converged. Or you could take the derivative at every point
and then when the derivative norm becomes very small you could say the algorithm converges.
The logic between these two are that in this case we are saying well we are doing this,
but we are not really improving our objective function.So, I am going to be happy with
whatever I get at some point and then say if you do not improve significantly and
what is significant is something that you define I am going to stop the algorithm.
So, you could do that. Or when you do the norm of this you know ultimately
at the optimum value you know the gradient has to be 0, that means,
the norm of this vector has to be 0. So, when grad f becomes very close to 0 then you could
say I have converged my algorithm and I am going to stop the algorithm at that point. So,
in typical optimization packages or software there are these various options that you can
use to ask for convergence to be detected and the algorithm to stop at that point.
So, this gives you an idea of how the analytical expression that we started with for maximum or
minimum is converted into a gradient rule and these are all called as gradient based
optimization algorithms. And then we showed you a numerical example of how actually this
gradient based optimization algorithm works in practice. We also made the connection between
these algorithms and machine learning and as I mentioned before most of the machine
learning tech-niques you can think of them as some form of an optimization algorithm and the
gradient descent is one algorithm which is used quite a bit in solving data science problems.
Couple of other things to notice are that the direction for changing your values iteration
by iteration, in this case we have taken it as a steepest descent. There are many other ways
of doing this you can choose directions in using other ideas that many other algorithms use. So,
we here in this introductory course on data science we focused on the most common and the
simplest of the search directions which is the negative of the gradient at that point.
And again these algorithms also keep changing the value of the learning
parameter or the step length as they would call it in optimization algorithms,
iteration to iteration. In this case we have kept that to be a constant just to make sure that we
explain the fundamental ideas rst before moving on to more complicated concepts. Nonetheless,
I just want you to remember that this learning parameter is something that
could be changed optimally from iteration to iteration in a given optimization algorithm.
So, with this I hope you have got a reasonable idea of univariate and multi-variate optimization,
unconstrained non-linear optimization. What we are going to do in the next lecture
is to look at how we can introduce constraints into this formulation,
and what effect does a constraint have on the formulation and how do we solve constrained
optimization problems. And as I mentioned before these constraints could be of two types,
equality constraints and inequality constraints. We will see how we can solve optimization problems
with equality constraints and inequality constraints. So, I will see you in the
next lecture. Thank you.

This module on Statistical Modeling will introduce you the Basic Concepts
in Probability and Statistics that are necessary for performing data analysis.
The module is divided into two parts: in the first part we will provide you an
introduction to random variables and how they are characterized using probability
measures and probability density functions, and in the second part of this module we
will talk about how parameters of these density functions can be estimated and
how you can do decision making from data using the method of hypothesis testing.
So, we will go on to characterizing random phenomena; what they are?
And how probability can be used as a measure for describing such phenomena?
Phenomena can actually be either considered as deterministic phenomena whose outcome can be
predicted with the high level of con dence can be considered as deterministic. For example,
if you are given the information about the date of birth from an Aadhaar card of a person you
can predict with a high degree of confidence the age of the person up to let us say number of days.
Of course, if you are asked to predict the age of the person to a hour or a minute the date
of birth from an Aadhaar card is insufficient. Maybe you might need the information from the
birth certificate, but if you want to predict the age with higher degree of precision,
let us say to the last minute, you may not be able to do it with the same level
of con dence. On the other hand, stochastic phenomena are those there are many possible
outcomes for the same experimental conditions and the outcomes can be predicted with some
limited confidence. For example, if you toss a coin you know that you might get a head or
a tail but you cannot say with 90 or 95 percent confidence, it will be a head or
a tail. You might be able to say it only with a 50 percent con dence if it is a fair coin.
Such phenomena we will call it as stochastic. Why are we dealing with stochastic phenomena.
Because, all data that you actually obtain from experiments contain some errors these errors
can either be, because we do not know all the rules that govern the data generating process,
that is, we do not know all the laws we may not have knowledge of all the causes that
affects the outcomes and therefore, this is called modeling error. The other kind
of errors is due to the sensor itself. Even if we know everything the sensor that we use for
observing these outcomes may themselves contain errors. Such errors are called measurement errors.
So, inevitably these two errors are modeled using probability density func-tions and therefore,
the outcomes are also predicted with certain con dence intervals, which we derive. The types of
random phenomena can either be discrete where the outcomes are finite - For example, in a coin toss
experiment we have only two outcomes either a heads or tail or a throw of a dice where we have
6 outcomes - or it could be a continuous random phenomena which where we have an in nite number
of outcomes such as the measurement of a body temperature which could vary between let us say 96
degrees to about 105 degrees depending on whether the person is running a temperature or not.
So, such continuous variable things which have random outcomes are called continuous.
Random phenomena we will try to describe all the notions of probability and so on
using just the coin toss experiment. In this particular case we are looking at the discrete
random variable or a random phenomena where we actually have a single coin toss whose outcomes
are described by H and T. The sample space is the set of all possible outcomes. So, in
this case the sample space consists of these two outcomes H and T denoted by the symbols H and T.
On the other hand if you are having two successive coin tosses, then there can be 4 possible outcomes
either you might get a head in the first toss followed by a head in the second toss or a head
in the first toss followed by a tail and so on. So, these are the four possible outcomes denoted
by the symbol HH, HT, TH and TT and that constitutes what we call the sample space.
The set of all possible outcomes an event is some subset of this sample space. For example,
for the two coin toss experiment if we consider that and consider the event of receiving a head
in the first toss then there are two possible outcomes that con-stitute this even space which
is HH and HT we call this event a which is the observation of a head in the first toss.
Outcomes of the sample space for example, HH, HT,
TH and TT can also be considered as events. These events are known as elementary events.
Now, associated with each of these events we de ne a probability. It is a measure which
assigns a real value to every outcome of a random phenomena. When we assign this probability it has
to follow certain rules. The first condition is that the probability we assign to any event
should be bounded between 0 and 1 and that means, probabilities are non negative and it is less than
1. For any event that you might consider also the probability of the entire sample space should be
equal to 1, which means 1 of the outcomes should occur; that is, what it means when
you say P of S is equal to 1. And finally, the probability measure should also satisfy
this condition that if you consider two exclusive events and say whether one or the other occurs.
The probability that either A or B occurs is the sum of probability of A and probability of B;
if A and B are exclusive events. The notion of exclusive events will be discussed in the
subsequent slide. So, these are the though three rules that you should follow. When
you assign a probability the easiest way of interpret interpreting probability is as a
frequency. For example, as an experimentalist you might want to do the coin toss experiment
let us say 10,000 times N times and then count the number of times a particular outcome is observed.
For example, let us say you are counting the number of times head occurs.
Let us say NA is the number of times that the outcome corresponding to the
head occurs, then the probability of head occurring can be defined as NA by N. So,
this you can see is bounded between 0 and 1, and if you look at the other outcome it will
be N minus N A by N and therefore, it will add up the probability of the sample space
will be equal to 1. This way of defining how has a problem, because if you do that
toss 10,000 times instead of 1,000 times you might get a slightly different number.
So, the best way of interpreting this as a frequency is in the limit as N tends to
infinity and that is what we do as an assignment. If it is a fair coin then if we toss the coin a
large number of times large meaning million billion times, then the probability of head
occurring would be approximately equal to 0.5 and the probability of tail occurring will be
approximately 0.5, if it is a fair coin and that is what we have assigned as probabilities.
Now, we can go on to de ne two important types of events what is called the independent set of
events. Two events are said to be independent, if the occurrence of one has no influence on
the occurrence of other. That is, even if first event occurs we will not be able to
make any improvement about the predictability of B if A and B are independent formally. In
probability it is the way we consider two events to be independent is if the
probability of A intersection B which means A joint occurrence of A and B can be obtained
by multiplying their respective probabilities which is probability of A into probability of B.
Let us illustrate this by A by A example of the coin toss experiment. Sup-pose you toss
the coin twice. Now if you tell me that the first toss is a head then does it allow you
to improve the prediction of a head or a tail in the second toss? Clearly you will
say well does not matter whether the first toss was a head or a tail the probability
of head occurring as the second toss is still 0.5. That means, information you
provide me about the first toss has not changed my predictability of head or tail in the second toss.
So, if we look at the joint probability of two successive heads which is the head in the first
toss and the head in the second toss, because we consider them as independent events we can obtain
the probability of this two successive heads as a probability in the first toss of head in the
first toss multiplied by the probability of head in the second toss which is 0.5 into 0.5 and 0.25.
So, all the four outcomes in the case of two coin toss experiment, we will have a probability
of 0.25, whether you get two successive heads or two successive tails or a head or a tail or
a tail in the head all will be 0.25 and this way we actually assign the probabilities for
the two coin toss experiment from the probability assignment of a single coin toss experiment. Now,
mutually exclusive events are events that preclude each other. Which means,
if you say that event A has occurred then it implies B has not occurred, then A and B are
called mutually exclusive events one excludes the other occurrence of one excludes the other.
So, let us look at the coin toss experiment again. Two coin tosses in suc-cession we
can look at the event of two successive heads as precluding the occurrence of
a head followed by a tail. If you tell me two successive heads have occurred,
it is clear that the event of head followed by a tail has not occurred. So, these are mutually
exclusive events. The probability of either receiving two successive heads or a head and
followed by a tail can be obtained in this case by simply adding their respective probabilities
because they are mutually ex-clusive events. So, we can say the probability of either a
HH or a HT which is nothing but the event of a head in the first toss is simply 0.25 plus 0.25
which is 0.5 which is obtained by a basic laws of probability of mutually exclusive events.
Now, there are other rules of probability that we can derive and these can be done
using Venn diagrams. So, here we have illustrated this idea of using Venn
diagram to derive probability rules by for the 2 coin toss experiment. In the two coin
toss experiment the sample space consists of 4 outcomes denoted by HH, HT, TH and TT.
We are interested in the event A, which is a head in the first toss. This consists of two outcomes
HH and HT, which is indicated by this red circle. A compliment is the set of all events that exclude
A which is nothing but the set of outcomes TH and TT is known as a complement. Now from the
rules of probability you can actually derive the probability of a compliment is nothing but the
probability of the entire sample space minus the probability of A, which is one. Which is
because probability of S is 1 minus probability of A notice the probability of A. In this case
is the probability of HH which is 0.25 plus the probability of HT which is 0.25 equal to 0.5. So,
eventually we get the proba-bility of a compliment with this TH and TT equals 0.5. This could have
also been computed by looking at the probability of TH plus the probability of TT which is 0.5.
So, it verifies that probability of A complement is 1 minus probability of A. Now you can consider
a subset, in this case even be denoted by the blue circle of two successive heads notice two
successive heads it is a subset of receiving a head in the first toss which is A event A. So,
we can claim that if B is a subset of A, then the probability of B should be less than the
probability of A. You can verify that the probability of B is two successive heads
which is 0.25 is less than the probability of A which is 0.5. You can also compute the
probability joint probability of two events A and B, which is not joint probability,
but the probability of A or B which is given by P of A union B can be derived
as P of A probability of A plus probability of B minus the probability of joint occurrence of
A and B. Let us consider this example of receiving a head in the first toss
which is event A and receiving a head in the second toss which is event B.
So, receiving a head in the second toss consists of two outcomes HH and TH denoted by the blue
circle. Now notice that they have a common event of two successive heads which belongs
to both A and B. So, A and B are not mutually exclusive, but have a common outcome. Now in
order to compute the probability of A or B which means either I receive a head in the first toss or
I receive a head in the second toss, then this comes to three outcomes and together gives you
a probability of 0.75 which we can count from the respective probabilities of H T,
H H and TH, but this can also be derived by looking at the probability of A which is 0.5
plus the probability of B; which is 0.5 minus the probability of A intersection B which is the
probability of HH which itself can be computed by multiplying the probability of receiving a first
hedge in the first toss and the probability of head in the second toss which is 0.25.
So, overall gives you 1 minus 0.25 which is 0.25 which is what we can derive by
counting the respective adding up the respective probabilities of the mutu-ally exclusive events
HT, HH and TH. So, such rules or things can be proved by using Venn Diagrams in a simple manner.
Now that is an important notion of conditional probability, which is
used when two events are not independent. So, if two events are not independent;
then if you can provide me some information about A, it will influence the predictability
of B vice versa if you tell me some information about the occurrence of B then this information
will improve the predictability of A, if the two events are not independent. So,
we de ne what is called the conditional probability, that is, the probability of
event B occurring given that event A has occurred can be obtained by this formula which is the
probability of A and B simultaneously occurring divided by the probability of A occurring.
Give of course, assuming that probability of A is greater than 0. Now using this notion
of conditional probability of B, given A and this formul,a we can derive what is called the
Bayes rule, which simply says the conditional probability of A given B multiplied by the
probability of B is the conditional probability of B given A multiplied by probability of A.
This rule can be easily derived from the first rule by simply interchanging A and
B and deriving the conditional probability of A given B multiplied by probability of B,
which is the A inter-section B and right hand side. In this also is A intersection B,
both of these are equal to A intersect probability of A intersection B. We can
also derive another rule for probability of A which is probability of conditional
probability of A given B multiplied by the probability of B plus the probability of
conditional probability of A given B complement multiplied by probability of B complement.
Notice that B and B complement are mutually exclusive and therefore con-ditional event
to A given B and A given B complement are mutually exclusive and therefore,
you are able to add the probabilities. So, let us illustrate this by a two coin toss
experiment. Let us consider the event A which is a head in the first toss and event B which
is two successive heads. Notice that A and B are not independent and which you can easily verify
by computing the probabilities also. So, if you do not give me any information about event A,
I will tell you that the probability of receiving two successive heads is 0.25, which is the
probability of heads in the first toss multiplied by the probability of head in the second toss.
However, if you tell me that you are observed event A; that means,
that the first toss is a head, in this case then the probability of event B is
actually im-proved. I can tell now there is a 50 percent chance of getting probability event B,
because you have already told me that the first toss is a head. So, notice that I can compute
this probability conditional probability of B given A. Using the first rule which
is probability of A intersection B which is 0.25 divided by the probability of A which is 0.5. So,
this probability of B given A is 0.5 which has improved my ability to predict B, because I have
used some information you have given about point event A. Now, if B and A were totally independent,
then information that you are provided to A will not a ect the probability of predicting
predictability of B it would have remain the same in this case it does not remain the same.
So, B and A are not independent. We
will illustrate again a example all these ideas of probability.
Suppose we have a manufacturing process where we actually have manufac-tured 1,000 parts out
of which 50 parts are defective. Now from the collection of parts produced in a day,
we randomly choose one part and ask this question would this part that we have selected picked,
would it be a defective part or what is the probability it will be a non defective part.
Clearly, because there are 50 defective parts and each of these parts can be uniformly picked,
we know that the probability of A is the number of defective parts divided
by the total number of parts which is 50 by 100, 1,000. On the other hand,
the probability of picking a non defective part is the complement of this, which is 950 divided
by 1,000. Now let us assume that we have picked one part kept it aside and we draw
a second part without replacing the first part into the pool. We are interested in the outcome
whether the second part that we have picked is it a defective part or a non defective part.
Suppose you do not tell me anything about what happened in the first pick, then I will say that
the probability of picking as defective part even in the second is unchanged it is 50 by 1,000. Let
us see how this comes about. At this point it may not be clear that it is 50 by 1,000,
but we will show this formally. Now let us assume I give you some information about A. Suppose,
I tell you that the first part that you do was a defective part, then clearly the total number of
defective parts have decreased to 49 and the total number of parts has decreased to 999.
So, the probability of picking a defective part in the second pick given that you
picked a defective part in the first pick is 49 by 999. On the other hand,
if you tell me that the first draw is non defective which means the total number of
parts again as reduce to 999, but the number of defective parts in the pool still remains at 50.
So, the probability of picking as defective part in the second round
given that the first pick was non defective is 50 by 999. Now, according to the rules
of conditional probability we can compute the probability of C, by probability of C given A;
which is 49 by 999 multiplied by the probability of A; which is 50 by 1,000
plus the probability of C given A compliment. Remember, A complement is nothing, but B.
So, the probability of C given A complement is 50 by 999 multiplied by the probability
of A complement which is nothing, but 950 by 1,000, which we have actually shown in
the first case. So, if you add up all these probabilities. You will find that you get
50 by 1,000 which is that if you do not give me any information about. What has happened
in the first pick? Whether you replace the part or whether you do not replace the part
the probability of picking a defective part in the second ring is 50 by 1,000.
Non obvious, but it is the same if you do not give me any information about the first pick it
does not matter, whether you replace the part or you do not replace the part your predictability
your ability to predict still remains the same 50 by 1,000. On the other hand clearly,
if you give me some information I am able to change the probably either decreases or
increases depending on what was the outcome of the first pick.
Now it is very interesting to actually ask the inverse question. If you tell me
some information about the second pick would it actually change your ability to predict
the outcome of the first pick? It turns out it does, because these are not independent
events. You can ask the question, what is the probability of getting a defective part
in the first pick given that you had a defective pick in the second round.
Now, if you apply again the rules of conditional probability. You can say probability of A given
C is probability of A intersection C divided by probabil-ity of C, but probability of a
intersection C can be written as probability of C given a conditional probability C given multiplied
by probability of A. So, the whole thing is conditional probability of C given A multiplied
by probability of A divided by probability of C property of C given A.We have computed as 49 by
999; probability of A is 50 by 1,000 divided by probability of C which is 50 by 1,000.
So, nally, I get probability of A given C is 49 by 99. Notice probability of A itself is 50 by 1,000,
but it has now reduced to 49 by 999, because you told me that the second
pick was a defective part clearly it seems to be that somehow the first pick information is
dependent on the second pick information which is obviously true because you have
not done a replacement here. If you have done A replacement on the other hand,
you will nd that the outcome of C will be completely independent of outcome of A and
you will not be able to improve or decrease the predictability of A in the first pick.
So, all these ideas of conditional probability independent events; mutually exclusive events
will be repeatedly used in the application of data analysis and we will see how.

In the previous lecture I introduced the concept of Random Phenomena and how such phenomena can
be described using probability measures. In this lecture, I am going to introduce the
notion of random variables and the idea of probability mass and density functions. We
will also see how to characterize these functions and how to work with them?
So, a random variable is a function which maps the outcomes of a sample
space to a real line. So, there is a unique real number that is associated
to every outcome in the sample space. Why do we need a notion of a random variable?
Let us take the example of a coin toss experiment in which the outcomes are
denoted by symbols H and T. H refers to the head and T refers to the tail. Unfortunately,
we will not be able to do numerical computations with such rep-resentation therefore,
what we do is to map these outcomes to points on the real line. For example,
we map H to 0 and tail to 1. The random variable or function that maps the outcomes of a sample
space to this real life that is what we are referring to as a random variable.
Now, if the outcomes of random phenomena are already numbers such as the true of a dice,
then we do not need to do this extra e ort. We can work with the outcomes themselves in
that case and call them random variables. We will see how we can do numerical computations
once we have such a map and before in a way similar way we have discrete and continuous
random phenomena. We will describe discrete random variables that maps
functions of discrete phenom-ena to the real line and continuous random variable
which maps outcomes of a continuous random phenomena to intervals in the real life.
So, just as we had a probability measure to characterize outcomes of random phenomena,
we will have a similar probability measure that
characterizes or is associated with this random variable.
The notion of a probability mass or a density function is a measure that maps the outcomes of
a random variable to values between 0 and 1. For ex-ample, if we take the coin toss experiment and
associate a random variable x whose outcomes are 0 and 1, then we associate a probability
for x is equal to 0.5 and a probability for x is equal to 1 which is another 0.5. Notice
that this association should follow the same laws of probability that we described in the
last lecture. This is a fair coin and that is why the outcomes are given equal probability.
Now, in the case of a continuous random variable, we define what is known as a
probability density function, which can be used to compute the probability for every outcome of
the random variable within an interval. Notice, in the case of a continuous random variable,
there are infinity of outcomes and therefore, we cannot associate a probability with every
outcome. However, we can associate a probability that the random variable lies within some finite
interval. So, let us call this random variable x which can take any value in the real line
from minus in nity to in nity, then we de ne the density function f of x, such that the probability
that the variable lies in an interval a to b is de ned as the integral of this function from a to b.
So, the integral is an area. So, the area represents the probability. So,
this is denoted on the right hand side, you can see a function and here we show
how the random variable the probability that the random variable lies between
minus 1 to town 2 is denoted by the shaded area. That is how we de ne the probability
and f of x which defines this function is called the probability density function.
Again, since this has to obey the laws of probability the integral from minus infinity
to infinity of this function or the area under the entire curve should be equal to 1 and obviously,
the area is non zero therefore it obeys the second law also we actually described in the
last lecture. We can also define what is called the cumulative density function,
which is denoted by capital F and this is the probability that the random variable x lies in
the interval minus infinity to b for every value of b you can compute this value function value and
this is nothing, but the integral between minus infinity and b of this density function f of x dx.
That is known as the cumulative density function for b equals minus in nity the
cumulative density function value will be 0 and b equals in nity you can verify that the
cumulative density function takes the value one. So, this cumulative density function goes from
0 to 1 as the upper limit of the interval goes from minus infinity to plus infinity.
Now, let us look at some sample probability mass functions. Let us take the case of n
coin tosses of an experiment where we have do n coin tosses and we are asking
the question what is the probability of obtaining exactly k heads in n tosses.
Let us assume p is the probability of obtaining a head in any toss. We also assume that these
tosses are independent. So, let us define a random variable x that represents the number of
heads that we obtain in these n coin tosses. Now the sample space for x, you can verify goes from
takes the value 0 or 1 or n. 0 represents that you observe 0 heads in all the n tosses 1 represents
we exactly observe 1 head in n tosses and n represents that all the tosses results in a head.
So, clearly each of these outcomes have a certain probability which we can compute
and this probability can be computed as follows: let us take the case of k heads in n tosses. Now
let us look at one outcome which results in such a such a event. Here I listed the
first k tosses as resulting in a head and the remaining n minus k tosses resulting in a tail.
Clearly the probability of this particular event is p power k,
because I am getting p successive heads and these out tosses are independent. So,
p power k is the probability that you will get k successive heads and 1 minus p power n minus
k would represents the probability that you will get n minus k successive tails. So, p power k
into 1 minus p power n minus k represents the probability of this outcome. However, this is
only one such outcome of obtaining n k heads. You can rearrange these heads and it is equal
into picking k heads out of n tosses. The number of ways in which you can pick k heads out of n
tosses is de ned by this combination n factorial divided by k factorial into n minus k factorial.
So, the probability nally, of receiving k heads in n tosses which is denoted by f the random variable
taking the value k is given by the probability, which is defined on the right hand side here,
this distribution for various values of k. For example, x is equal to 0,
x is equal to one can be computed and such a distribution will be called as the probability
mass function. For this particular random variable and this particular distribution is
called a binomial distribution. As an example of the binomial distribution mass function is shown
on the right hand side for n is equal to 20 and taking the probability p is equal to 0.5 clearly,
it shows the probability of receiving 0 heads in 20 tosses is extremely small
and similarly the probability of obtaining 20 heads in 20 tosses loss is also small
as expected the probability of obtaining ten heads has the maximum value as shown.
The vertical line here represents the probability value for a particular number
of heads being observed in n tosses. So, clearly since it is a fair coin,
we should expect that out of the 20 tosses, 10 heads are most likely to be obtained
and this has the highest probability. This distribution is characterized by a parameter
p a single parameter p and we can also see for large enough n it tends to this bell
shaped curve which is the what is called the Gaussian distribution , which will make use
of for large n instead of using the binomial distribution which is computationally more
di cult we can approximate by a Gaussian distribution for computational purposes.
That is an example of a probability mass function of a discrete random variable.
We have now considered a continuous random variable. In this case we will look at what is
called the normal density function which is shown on the right. Usually this normal density function
is used to characterize what we call random errors in data and it has this density function as given
by this 1 by root 2 pi sigma e power minus x minus mu whole square by 1 sigma squared.
Now, this particular density function has two parameters
mu and sigma and it has the shape as shown here like a bell shaped curve,
which is the normal density function. Notice that it is symmetric and in a particular case
of this normal density function is when mu equals 0 and sigma is 1 and such a normal
density function with mean mu 0 and sigma 1 is called a standard normal distribution.
Again, if you want to compute the probability of this, that the standard normal variable
lies within some interval. Let us say 1 and 2 you have to compute the shaded region. Unfortunately,
you cannot analytically do this integration of this function
between 1 and 2 you have to use numerical procedures and the our package contains,
such functions for computing the probability numerically such that
the variable lies within some given interval we will see such computations a little later.
Another continuous random variable who is characterized by density function known
as the chi square density function. We do not need to remember the form of this function it
has them gamma function and so, on. Again this density function is characterized by
one parameter which is n called the degrees of freedom. Notice that this function takes values
only between the random variable which follows this distribution takes values only between 0
and infinity. The probability to have negative values is defined to be exactly equal to 0 and
it turns out that this distribution arises when you square a standard normal variable.
So, if you see, the square of a standard normal variable will be a chi square distribution with
one degree of freedom. If you take n independent standard normal variables and square and add all
of them that will result in a chi square distribution with n degrees of freedom,
n representing the number of standard normal variables which you have squared and added to
get this new random variable. We can show later that the distribution for sample variance follows
a chi square distribution and therefore, it is used to make inferences about sample variances.
There are other examples of probability density functions such as the uniform
density function and exponential density function which we have not touched upon.
I would T distribution and. So, on which you can actually look
up what we want to do is describe some properties of these density functions.
Just as you take a function and talk about properties such as derivatives we can talk
about moments of a probability density function. And these moments are described
by what is called the symbol expectation e of some function. So, in this particular case,
I have taken the function to be x power k and expectation of x power k is de ned as the integral
between minus infinity to infinity of x power of a k multiplied by the density function f of x dx
this is called the moments of the distribution. If k equals 1, you will call it the rst moment.
If k equals 2 you will call the second moment and so on so, forth. So, if you give all the
moments of a distribution it is equivalent to stipulating the density function completely.
Typically we will usually specify only 1 or 2 or 3 moments of the distribution and work
with them for discrete distributions. You can similarly describe this moment;
in this case expectation of x power k is defined as summation -integrations replaced by summation-
over all possible outcomes of the random variable x. I represents the outcome and k
is the power to which you have raised. So, x I power k probability of obtaining the outcome x
I which is very similar to this integration procedure except that the finite number of
outcomes and therefore, we have replaced the probability f of xdx with p of x I
and the value x power k with the outcome x I power k and integral as a summation.
Now there are two important moments that we described for a distribution.
What is called the mean or the first moment which is de ned as expectation of x and the
variance which is defined as denoted by the symbol sigma squared and this
is defined as the expectation of x minus the first moment which is mu whole square.
This is the function that we want to take the expectation of, which can be obtained by
x minus mu the whole square f of x dx. In the case of a continuous distribution we can show
that the variance sigma squared is expectation of x squared which is the second moment of the
distribution about 0 minus mu squared. This proof is left to you, you can actually try
to prove this; the standard deviation is defined as the square root of the variance.
Now, for a Gaussian random variable, if I take the expectation of x which is the mean,
we can show that is the first parameter mu in the density function and the variance
which is denoted defined as expectation of x minus mu the whole squared turns
out to be equal to sigma squared which is the second parameter of the distribution.
So, the parameters that we have used to characterize the normal variable is mu the
mean and sigma squared which is the variance. Typically sigma square tells you how wide the
distribution will be and mu tells me what the value is at which the density function attains
the highest probability most probable value. So, mu is also known as the centrality parameter and
sigma squared is essentially the width of the distribution tells you how far the values are
spread around the central value mu symbolically. We defined this normal distribution variable
random variable as distributed as N represents the normal distribution and the parameters are
defined by mu and sigma square which completely defines this density function. A standard normal
or standard Gaussian random variable is a particular random variable, which has normally
distributed random variable which has mean 0 and standard deviation one and denoted by this symbol.
Now, we can show that if x is a normally distributed random variable with mean mu and
sigma squared, then if you take a linear function of this x denoted by ax plus b, where a and b are
some constants, then we can show that y you will also have a normal distribution. But its mean will
be our expected value will be a mu plus b and its variance would be a squared sigma square.
Now we can use this linear transformation to do something called standardization,
which you will see often in many many application of hypothesis testing or estimation of parameters,
where we simple define if a random variable is normally distributed with mean mu and sigma
squared variance. Then we can de ne a new random variable z which is x minus mu by sigma. That is,
you subtract the mean from the variable random variable and divide it by the standard deviation
that, this new random variable is a linear transformation or a linear function of x and
therefore, we can apply the previous rule to show that z is a standard normal distributed
variable which means it has a mean 0 and a standard deviation of variance equal to 1,
this is this process is also known as standardization.
Now in our there are several functions that allow you to compute probability given a value or the
value given the probability. So, we will see some couple of examples of such functions. For example,
if you give a value x and ask what is the probability that this continuous random
variable lies between the interval minus infinity to this capital x value that you are given then,
obviously, I have to perform this integral minus infinity to x of the density function.
And as I said this can only be done numerically and there is a function
for doing this it is called p xxx. In the case of a normal distribution you call it p
norm you give it the value upper limit in this case and then you define the mean and standard
deviation of the two parameters of the normal distribution and you also specify something;
whether you want the upper tail probability or the lower tail probability. We will see
what it is and this function will give you the probability value.
This value of this integral, notice this integral is nothing but the area under the curve between
minus in nity to x, if lower tail is equal to TRUE it will give you this integral value area
between minus in nity and x. On the other hand if lower tail is FALSE, then it will give the in
area under the curve between x n infinity. So, x will be taken as the lower limit and infinity
is the higher limit if you say lower dot tail is equal to TRUE it will take excess the upper limit
and do the area in under the curve f of x between minus in nity and x the default value is TRUE. So,
this norm can be replaced by other distributions like chi square exponent uniform and so on,
so forth to give the probability for other distribution given this value x.
Now, the parameters of the distribution must also be speci ed for every case in the normal
distribution. There is only there are two parameters, but other distributions such
as chi squared may have one parameter such as the degrees of freedom and exponent will have
one one parameter such as the parameter lambda and so on so forth. As I said the
lower dot tail tells you whether you want the area to the left of x or to the right of x.
Then other functions in R one of them is q norm which actually does what is called
the inverse probability; here you give the probability and ask what is the limit X. So,
here I have if you give the probability to q norm with the mean and standard deviation
parameters of the normal distribution and you say the lower tail is equal to 2,
then it will actually compute the value of X such that the integral between minus in nity to
X of this density function is equal to the given value p you are specified p and calculating X.
In p norm you are specifying x and computing p. So, this is called the
in-verse probability function as before if you actually say lower dot trail is equal to FALSE,
then this integral will be replaced by x to in nity such that x to infinity of
f of x dx is equal to p and it will find the value of x. There are other
functions called d norm which computes the density function value at a given x and r
norm which are used to generate random numbers from this given distribution.
Now, having seen the distribution of single random variable,
let us move on to the joint distribution of two random variables. In general,
we will be dealing with the multivariate distributions which are joint distribution
of several random variables, but first we will look at the joint probability density
function of two continuous random variables x and y denoted by the function f of x comma y.
And the way this density function is used to compute the probability is as before the joint
probability that x is less than or equal to a minus in nity being the other assumed to
be the other limit and y less than or equal to b or y ranging from minus infinity to b,
the joint probability of these two variables lying in these intervals is denoted as computed
as the integral minus infinity to b and double integral minus infinity to a f of x y dx dy.
That is the basically the volume of this particular function f of x comma y. Now when there
are two variables you can also de ne other than the variance of x and y which are denoted as sigma
squared x and sigma squared y. You can also define what is called the covariance between x and y and
this is defined as the expectation of x minus mu x mu being the mean or expectation of x I multiplied
by y minus expectation of y which is denoted by mu y this product is the expectation of this product
function is defined as the covariance between x and y and denoted by the symbol sigma x y.
Now, the correlation between x and y is the standardized or normalized form of this
covariance which is nothing but sigma x y divided by the standard deviation of x and the standard
deviation of y -this is denoted by the symbol rho x y. We can show that rho x y varies between minus
1 to plus 1 depending on the extent of correlation between x and y. Typically, when sigma x y equal
0, we rho x y will be equal to 0 and we say that the random variables x and y are uncorrelated.
On the other hand, if x and y are independent, then the joint density function of f of x x
comma y can be written as the product of the individual density functions or marginal density
functions of x and y. That is f of x into f of y. This is the extension of this notion
of independent variables in terms of probability we defined in the previous lecture where we said
the probability joint probability of x and y is basically probability of x into probability of y.
But this is a generalization which defines the notion of independence of two random variables.
Now, we can now extend this idea of joint distribution of two variables to joint
distribution of n variables. Here I have defined the vector x which consists of n random variables
x one to x n. And specifically we look at this multivariate normal distribution we denoted by
the symbol x multivariate normal with mean vector mu and covariance matrix sigma. Now
each of these x I components x 1, x 2 and so on have their respective means. If you
put them in the vector form, we get this mean vector symbolically written as expectation of
x which is a multi dimensional integral, we get this value mu which is known as the mean vector.
And similar to the variance we de ned what is called the covariance matrix which is de ned as
expectation of x about the mean mu or x minus mu into x minus mu transpose. Remember this is
a matrix of variables because x is a vector and if you take the expectation of each of
these elements of this matrix you will get this matrix called the variance covariance matrix.
In fact, we can write the multi dimensional normal distribution also has a very similar
form if you look at it 1 by 2 pi we had square root of 2 pi. In this case it will be 2 pi n
by 2 n represents number of dimension of this vector and we had sigma in this case. We have
the square root of this matrix covariance matrix sigma and we had exponents minus
half we had a quadratic form. In this case the quadratic form is x minus mu transpose
sigma inverse x minus mu which is similar to x minus mu square divided by sigma square.
So, it has very similar form we do not need to know the form. We need to know
how to interpret mu and sigma. And if you look at the structure of sigma you will find that
it is a square matrix with the diagonally elements representing the variance of each
of the elements that is sigma squared x 1 is the variance of x 1 and sigma squared x 2,
the variance of x 1 and so on, so forth. And the of diagonal elements representing the covariance
between x 1 and x 2 or x 1 and x 3, x 2 and x 3 and so on ,so forth pair wise covariance.
Those are the o diagonal elements for example, sigma x 1, x 2 represents the
covariance between x 1 and x 2 sigma x 1, x n represents the covariance between x 1 and
x n this particular matrix is symmetric and we completely characterized the multivariate
normal distribution by specifying this mean vector mu and the covariance matrix sigma.

In the preceding two lectures, I introduced the concepts of
probability. Probability provides a theoretical framework for providing,
for performing statistical analysis of data. Statistics actually deals with the
analysis of experimental observations that we have obtained. So, in this lecture I will
introduce you to a few measures statistical measures and how they are used in analysis.
So, what is the need for performing statistical
analysis when we have already talked about probability density functions and so on?
Typically, when we are actually doing analysis we do not know the entire sam-ple space. We may also
not know all the parameters of the distribution from which the samples are being withdrawn.
Typically we actually obtain only a few samples of the total number of avail population. So, from
this finite sample we have to derive conclusions about the probability density function of the
entire population and also infer, make inferences about the parameters of these distributions.
So, the sample or observation set is supposed to be sufficiently representative of the entire
sample space. Let us take an example. Suppose you want to actually find out the average
height of people in the world, you cannot go and sample just people or take heights
of American people alone because they are known to be much taller compared to the Asian people.
So, when you take samples you should take examples from let us say America, from Europe from Asia and
so on, so forth. So that you get a representative of the entire population of this world. So,
this is called proper sampling procedures and these are dealt with in the design of
experiments. We will assume that we have obtained a sample; you have done the due
diligence and obtained the representative sample of whatever population you are trying to analyze.
Now, with basic definition of population is the set of all possible outcomes of this
random expire experiment. We have already defined this as the sample space. What you
are going to obtain is just a few examples or samples and these are called the sample
set. Its a nite set of observations obtained through whatever experiment
that you are going to conduct. Now from this sample set you want to make inferences which
are conclusions that you derive regarding the population itself, which you do not know its
either the probability density function of the population or the parameters of the population.
You have to note that when you actually lose such inferences, your inference
is also stochastic or uncertain because the sample that you have drawn are also uncertain;
they are not representative of the entire population. So, you should expect that
your inferences are also uncertain and therefore, when you provide the answers, you should actually
provide also the con dence interval associated with these estimates that you have deriving.
So, that is one of the reasons that we actually studied probability density functions because
then you can characterize all the estimates that you have obtained from the sample in
terms of this confidence interval and so on or the probability that you will obtain this value.
So, let us actually look at some typical analysis statistical analysis. We can divide
the statistical analysis into two parts, the graphical part or graphical analysis
where we use plots and graphs in order to have a visual feel of the entire data. The other way
of doing is to actually do quantitative computations or numerical computations,
where you try to summarize the entire sample sent by a few parameters. Example
we will talk about mean and variance and so on. Notice that we have taken hundreds of
data points or experiments, you cannot go and tell somebody all the hundred values,
you cannot reel off all these values that will not be possible for somebody to digest.
Summary statistics that we do numerically allows you to get a feel for the entire data
set that you have obtained without knowing the individual obser-vations. And that is
why the these are very useful and they are also called summary statistics. Now
inferential statistics deals with two kinds of problem estimation problem,
where we try to estimate parameters of the probability density function.
We did talk about parameters such as the expected value or the rst moment and the second moment and
so on, and different distributions are different number of parameters and how do we estimate these
parameters from a small sample that we obtain. And how do you give a confidence region for these
estimates that is called estimation and the other kind of decision making that we want to do is;
we want to judge whether particular value is 0 or not. The parameters of the distribution
and such decision making that we do from a sample is called hypothesis testing.
We want to know whether a customer will continue to remain with you
or will leave you for another vendor, based on whatever offers that you are making. So,
these are come under the category of hypothesis testing. We will first deal with the descriptive
statistics and in the next lecture we will deal with inferential statistics.
So, some of the summary statistics that we can define for a sample or what we
call measures of central tendency, it is the kind of the center point
of this entire sample you might say. And let us define what is called the mean,
these are measures that you are familiar with from your high school courses in mathematics.
So, let us recap some of these. The mean of a sample is defined as the summation of all the data
points that you obtain divided by the number of datapoints that you have. So, that is also denoted
by the symbol x bar and its also called the mean or the average of the sample. This particular
thing as I said can be viewed as a central point of the entire sample that you have got.
And we can show that this estimate that we obtained of the sample the average is the
best estimate in some sense. Later on, we will set up what is called the least squares method
of estimating parameters. And if you set up this particular criterion for estimating parameters
you will nd that x bar is the best estimate that you can get from the given sample of data. We can
also show some properties of this estimate. For example, we can prove that this x bar represents
an unbiased estimate of the population mean mu which you do not know anything about.
What do we mean by the unbiased estimate? Expectation of x bar is mu. This can be
analytically proven for any kind of distribution. And in order to prove
understand what this means you say that suppose you take a sample of N points
and you get an estimate x bar. And you repeat this experiment and draw another
random sample from the population of N points and get another value of x bar.
And you average all these x bars that you get from different experimental sets,
then the average of these averages will tend to this population mean. That is
a way of interpreting this statement that its an unbiased estimate. There
are other properties of estimates we will see that we want the demand,
but this is an useful and important property of estimates that you should always check.
The one unfortunate aspect of this particular statistic or mean is that it is if there is one
bad data point in your sample, by mistake you have made a wrong entry, then your estimate of
x bar can be significantly affected by this bad value. The bad value is what we call
an outlier and even a single outlier in your data can give rise to a bad estimate of x bar.
Let us take one example we have taken 20 cherry trees and we have measured the heights of the
cherry trees in terms in feet and we got let us say the set of bunch of numbers; generated these
randomly from a normal distribution with some mean which is 70 and the standard deviation of 10. So,
the population mean is 70 and the population standard deviation is 10 and I got these
values. You can use r r norm for example, in r in order to generate such data points.
Now, if you take this sample of 20 points and compute the mean, you get a value of 69.25 which
is very close to the population mean. So, you see that it is a good estimate of the population mean,
even though you did not know what that value was until I told you. Now on the other hand
if I take the rst data point 55 and add a bias wrongly enter it as 105 let us
say by adding 50 to ta and then recompute this mean, I will find that the mean becomes 71.75.
It starts deviating from 70 you see more significantly. A single bias in
this sample actually caused your estimate to become poorer. That is what we mean by
saying that x bar will get affected by outliers in the data. We can de ne other
measures of central tendency which are robust with respect to the outliers,
even if the outlier exists, it does not change by much and we will see what that such a measure is.
Another measure of central tendency is what is called a median. The median is a value
such that 50 percent of the data points lie below this value and 50 percent of
the experimental observations are greater than this value. So, you like to find out
that value below which half the data points lie and above which half the data points
lie. For doing this you have to order all the observations that you have got from smallest
to highest and then find out the middle value. Let us do this through an example.
So, same 20 cherry trees data I have looked at, this data point I have ordered from the
smallest to the largest. And if you look at it the tenth point 1, 2, 3, 4, 5, 6, 7, 8, 9, 10;
tenth point is 67 because there are even number of points, the eleventh point is 71 and you take the
average between this and call that the median. If there are odd number of points then you can
take the middle point just as it is because there are even number of points, you take the average of
the mid midpoints, in this case the tenth and the eleventh point and that gives you a median of 69.
Suppose we add a bias in the first data point as before and make this 105 and then reorder
the data and find out the again the median; we find that the median has not changed.
So, the presence of an outlier in this particular case has not affected the median at all and that
is why we call this a robust measure even if there is a bad data point in your samples. You can also
show that this estimate is the best estimate in some sense. In this case the merit that you are
using is what is called the absolute deviation. That is you are asking what is the estimate which
deviates from the individual observations in the absolute sense to the least extent?
And it turns out that the median is such a point, such an estimate. So, when there are
outliers typically we would like to use this as a central measure rather than the mean.
A mode is another measure of central tendency and this value is the value that occurs most often
or what is called the most probable value. And if you take the example of this 20 cherry trees data,
we find that the most probable value, the value that repeats more often, is 67.
Again you said this is 3 consecutive, 3 occurrences of this as compared to any other
data point and that is called the mode. And in a distribution if it is a continuous distribution,
this represents the highest value of this maximum value of the density function. And
you should expect most of the data to be clustered around this most probable
value. Sometimes distribution may have two modes. What is called a bimodal distribution
in which case if you sample from such a distribution, you will nd two clusters
one clusters around the one of the modes and another cluster around the second mode. So,
you should interpret the mode as that value around which you will find most of the data points.
The another measure which characterizes a sample set is what we call the measures of
spread and tells you how widely their data is ranging. So,
one of the measures is what to call the sample variance denoted by the symbol s
squared and that is de ned as the data point x i minus the sample average that
you have already computed. This deviation of the observation from the sample mean u square.
And add over all the data points n data points and divide the this particular sum
squared value by N minus 1, such a measure is called the sample variance. And again you can
prove that the sample variance is an unbiased estimated estimate of the population variance
and the square root of the sample variance is also known as the standard deviation.
Now, just like the mean, the sample variance happens to be also a very susceptible to
outliers. So, if you have a single outlier, the sample variance, our sample standard deviation
can become very poor estimate of the population parameter. So, we define another measure of spread
which is called the mean absolute deviation somewhat similar to the median. In this case
instead of taking the sum squared deviation, we take the absolute deviation of the data
point from the mean; you can also take it from the median if you wish. So, deviation of the
observation from the mean or the median, you take the absolute value of this deviation sum
over all the end points and divide by N and that is what is called the mean absolute deviation.
Again whether you should divide by N or N minus 1 is a point to be noted. Typically we divide
by N minus 1 to indicate that if you have only one data point; it is not possible to estimate s
squared. For example, if you have one data point, s squared will turn out to be 0 because the mean
will be equal to the point. So, really speaking you have only N minus 1 data points to estimate
the spread. So, that is why we divide by N minus 1 to indicate that one data point has been used up
to estimate the sample mean or the median whatever the parameter that you are actually estimated.
So, similarly here also you can divide by N minus 1 to indicate that only N minus 1
data points were available for obtaining the mean absolute deviation. A third measure of
spread is what is called the range that is basically the difference between the
maximum and minimum value. All of these give you indication of how much the data is spread
around the central measure which is the mean or the mode or the median as the case may be.
So, let us take an example of the 20 cherry trees we have computed the variance from the
given data. And we find out that its actually 70.5 5132 and if we take the standard deviation;
we will find its 8.4. As I told you that I had used a standard deviation of 10 to generate
this data from a normal distribution. And we find that the sample standard deviation is a
reasonably good measure of the population parameter which we did which was unknown.
On the other hand, if I add outlier of 50 units to the first data point and recompute s squared
and s, it turns out s squared turns out to be 212 and you can see if I take the square root
it might be around 14, which is signficantly deviating from the population parameter 10. So,
a single outlier can cause the standard deviation and the variance to become very poor and therefore
cannot be trusted as a good estimate of the population standard deviation or variance.
On the other hand, let us look at the mean absolute deviation. In this case I have,
if we do not have an outlier, we get a mean absolute deviation of 6. 9,
which is not too bad compared to 10. The moment you have an outlier,
the mean absolute deviation shifts to 9.5, it comes closer that is not what is important,
but it does not change much just because of the presence of the outlier. So, this is a
much more robust measure. In fact, if you take the mean absolute deviation from the median,
it would be even better in terms of robustness with respect to the outlier. The range of the data
can be obtained as the maximum and minimum value and I have just simply reported it.
So, these are measures of spread. So, even if I do not give you the entire 20 data points and I tell
you the mean is, let us say 69 and the standard deviation is 8.5, then you can say that the data
will spread typically between 69 plus or minus 2 times the standard deviation which is 16. So,
the lowest value will be about 53 and the highest value will be about 85 and it turns
out if you look at the highest and maximum value and that is what it turns out to be.
So, plus or minus 2 times the standard deviation from the mean would rep-resent
about 95 percent of the data points if the distribution is normal. For other distributions
you can derive these kind of intervals if you wish, but just giving two numbers allows me
to tell you some properties of the sample and that is the power of these sample statistics.
Now, there are some important properties of the sample mean and variance which we will use in
hypothesis testing. So, I want to recap some of these. If you have observations drawn from the
normal distribution with some population parameter mu and population variance sigma squared. And let
us say you draw N capital N observations for all from this distribution; let us assume these draws
or samples that you are drawn are independent, it does not have a bias in any manner.
And if you compute the sample average from this set of samples independent samples,
then you can prove that x bar is also normally distributed with the same mean
population mean mu. Which means the expected value of x bar is mu as I
told you before and the expected variance of x bar however, is sigma squared by N.
So, the variance of x bar is actually lower than the variance of the individual observations.
The important point here to be noted is, if I have N repeats from the same distribution and
if I take the average of them, the average will be less noisy than the original observations.
So, one simple way of dealing with noise and reducing the noise content in observations
is to take n observations at the same experimental condition and average them.
The average will contain less variability or less noise and it will reduce the variance
of this average will be 1 by N times the variance in your individual observations. So,
what we call the noise will be reduced by square root of N where N is the number of samples.
Now, if you look at the sample variance and want to characterize the distribution of the sample
variance, we can show again if you draw samples from the normal distribution with some mean mu
and variance sigma squared and these observations I am going to assume are mutually independent.
Then if you take N minus 1 times the sample variance divided by the popu-lation variance,
we can show that this particular measure is a chi squared dis-tribution with N
minus 1 degrees of freedom. We already saw the chi squared is a distribution
of a random variable which varies between 0 and in nity. And that distribution can
be used to characterize s squared and we can later on do hypothesis testing,
whether the sigma squared is some value and so on, using these distributions we will see.
Now, those are numerical methods of actually doing analysis of a sample data. What we want
to do is also graphical analysis this is something that you should always do when
you are given a data set. The first and foremost that you should do is to do some plotting to get
a visual appeal because the mind is capable of inferring things what numbers do not tell you.
So, my suggestion is always when you have a data set, if you can plot and visualize it please do.
So, let us see some of the standard plots again. Some of it you might have already encountered in
your high school days. We will start with what is known as the histogram. Here I am given a sample
set and what we do is rst divide this sample set into small ranges; we de ne a small range
and count how many observations fall within that range or within each interval.And then we plot
the width of the interval or the interval size of the x axis and the number of data points we
see in that interval as the y axis, we call it the frequency and that is on the y axis.
So, let us take this example of the cherry trees. We have 20 data points. What I did was divided
into small intervals of 5 feet which means I asked what are the number of cherry tree heights which
are falling in the range 50 to 55, 55 to 60, 60 to 65 and so on, so forth. And I find between 50
and 55 there are no trees within that height, we find 4 trees with the height between 55 and
60 which we can easily see there is one data point here, there is second data point here,
there is a third data point here and fourth data point is 60; so, the edge.
So, the 4 data points lying between 55 and 60 and similarly we find there are two data points lying
just above 60 and up to 65 and so on, so forth. And that is what we plotted as a rectangle for
each interval and this is known as a histogram. In fact, if I take 100 such examples and I plot,
you will nd this standard bell shaped curve. And that is because I drew these samples from
the normal distribution. In this case because it is 20, you are not able to clearly see its
bell shaped. You can see that the most of the data points are clustered around the middle
point which is around 70 and you can see highest number 6 there. So, at least that is borne out.
You have other kinds of plots. One is called the box plot, which is used most often in
sometimes in visualizing stock prices. Here you will compute quantities called quartiles Q 1,
Q 2 and Q 3 and the minimum and maximum values in the range. What are quartiles? Quartiles are
basically an extension of the idea of median. Q 2 is exactly the median which means half the
number of points fall below the value of Q 2 and half the number of points are exactly about Q 2.
Similarly, Q 1 represents the 25 percent value which means 25 percent of the observations fall
below Q 1. 75 percent above Q 1 and Q 3 implies that 75 percent of the data points fall below
Q 3 and 25 percent above Q 3. And once you have these values, the median, the quartiles and the
minimum maximum, you can plot what is called the box and whisker plot in the box the median
is the center value and the lower quartile Q 1 and the upper quartile is also plotted.
And the box is drawn between Q 1 and Q 3 clearly showing where the me-dian is. In this case its
shown as symmetric, but generally need not be, either Q 2 might be closer to Q 1 or Q
3. We also show the minimum and maximum values; here in this case the lowest observation the
highest observation and those are called the whiskers. This gives you an idea better idea
of the spread of the data than just giving you standard deviation or the mean absolute
deviation and so on. This gives you a little more information about the spread of the data.
So, as an example if you take the 20 cherry trees and sort them out,
sort it from the lowest to highest value, and we try to compute the median it turns
out the median is the average of the tenth and eleventh point which is 69. Then the quartile
one can actually be computed by just taking the rst half which is the 10 points and computing
the median of the first 10 points which turns out to be 64. And the Q 3 can be computed as
the median of the other half from 60 from 71 to 83 and that turns out to be around 75. So,
the min and max of course, is 50 and 83 and therefore, you can perform this plot.
The third kind of plot which is very useful is to know about the distribution of the
data and this is called the probability plot the p-p plot or the q-q plot. Here
instead of determining just Q 1, Q 2, Q 3 you compute several quantiles. And then plot these
quantiles against the distribution which you think this data might follow. And if
the data falls on the 45 degree line, then you can conclude that the sample
data has been drawn from the appropriate distribution you are testing it against.
So, this is useful for visually figuring out from which distribution did the data come from. So, I
have taken this example of these 20 cherry trees. I first standardized them, standardization means
we remove the mean and divide by the standard deviation and we get the values. The 20 values
as these are called the standardized values I have sorted them from the lowest to highest.
Now if you look at the 10 percent quantile, I can say that out of 20 points two points rst
two points fall below 1.679 minus 1.679. So, minus 1.679 represents the 10 percent
quantile similarly minus 1.1016 represents to 20 percent quantile and so on, so forth. For example,
the 50 percent quantile or the median quantile, in this case the standardized measure, will
be and which is around these two points around let us say minus point or around 0 close to 0.
Notice that for a normal distribution, 50 percent of the data will lie below 0 and
that is what this also seem to indicate. Now if you go to the standard normal distribution
and try to compute the value below which the probability is 0.1 which is the lower tail
probability we talked about. Then you will find the value is around let us say minus 1.
7- 1.5 whatever that value turns out to be. So, that is the 10 percent quantile. Similarly you
ask what is the value below which 20 percent of the data lies or what is the value below
which 20 percent of a standard normal distribution values will have a probability of area under the
curve of 0.2 and that value you take that is the second 20 percent quantile and so on, so forth.
Then you plot the actual value obtained for the sample which is minus 1.67 against the
standard normal contact and that is what is called the probability plot. So now,
if this data has been drawn from the normal distribution then you should find a curve
like this. I did not plug the normal probability plot for these 20 points, but for some other set
of data. But typically if you find if you think that this data comes from the normal distribution,
then you will find that in the normal probability plot the data will align itself on the 45 degree
line and then you can conclude yes that the data has come from the distribution.
You can test this against any distribution. In this case, I have shown you
how to test it against the normal distribution. You can take the quantiles from a uniform
distribution or from the chi squared distribution what have you and the plot
these sample quantiles against the expected population quantiles. And if they fall on
the 45 degree line then you know that it comes from the appropriate distribution.
So, this is useful for determining visually the distribution from which the data have been drawn.
Now the last kind of plot which is useful in data analy-sis is what is called the scatter plot. The
scatter plot plots one random variable against another. So, if you have two random variables,
let us say y and x and I want to know whether there is any relationship between y and x,
then one way of visually verifying this dependency or interdependency is to plot y versus x.
So, in this case we have taken some data corresponding to 100 students for which I
mean students have spent time preparing for a quiz and they have obtained marks in that quiz. So,
you should find typically if you spend more time study you should obtain typically more marks. And
that is what this is trying to show on the x axis is a time in minutes that we have plotted. And the
y axis we have plotted the marks obtained by the student and you can see there is an alignment.
The marks obtained seem to be dependent on the time spent and in fact,
in this particular case you find that it looks like a linear dependency. So,
you can plot a line approximate line through these data points we will show how to fit such lines
using regression and how to obtain the parameters of this line that we have actually indicated here.
But more importantly if the random variable y, in this case the quiz marks has a dependency on
the study time, then you will see an alignment of the data. On the other hand if there is no
dependency you will nd a random spread, this data will be spread all around with no clear pattern,
in which case you will say that there are these two variables are more or less
independent and you do not have to discover a relationship between these variables.
So, this is a plot which we will do in order to assess dependency between two
variables and then proceed for further for analysis. In the next lecture we will take
you through some decision making using hypothesis testing and how
to perform estimation of parameters using sample data. See you in the next lecture.
