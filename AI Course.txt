artificial intelligence applications are being widely used in various fields to solve business problems in this video
you will learn the important skills that will help you understand ai and its implementation better our highly proficient trainers will take
you through this course so let's look at the agenda for today's topic we will start by learning the basics of
ai from a short animated video then you will learn the top 10 ai technologies and companies for 2022
next you will understand machine learning in detail and explore some of the top machine learning algorithms that
are used to build intelligent machines so here you will look at linear regression logistic regression decision
tree k-means clustering k nn pca and other techniques such as regularization
reinforcement learning and q-learning moving forward you will understand the working of artificial neural networks
and deep learning algorithms such as convolutional neural network recurrent neural network keras image
classification and sequential model so let's get started picture this a machine that could
organize your cupboard just as you like it or serve every member of the house a customized cup of coffee makes your day
easier doesn't it these are the products of artificial intelligence but why use
the term artificial intelligence well these machines are artificially incorporated with human-like

intelligence to perform tasks as we do this intelligence is built using complex
algorithms and mathematical functions but ai may not be as obvious as in the
previous examples in fact a.i is used in smartphones cars social media feeds
video games banking surveillance and many other aspects of our daily life the
real question is what does an ai do at its core here is a robot we built in our
lab which is now dropped onto a field in spite of a variation in lighting landscape and dimensions of the field
the ai robot must perform as expected this ability to react appropriately to a
new situation is called generalized learning the robot is now at a crossroad
one that is paved and the other rocky the robot must determine which path to
take based on the circumstances this portrays the robot's reasoning ability
after a short stroll the robot now encounters a stream that it cannot swim across using the plank provided as an
input the robot is able to cross this stream so our robot uses the given input

and finds the solution for a problem this is problem solving these three
capabilities make the robot artificially intelligent in short ai provides
machines with the capability to adapt reason and provide solutions
well now that we know what ai is let's have a look at the two broad categories an ais classified into

weak ai also called narrow ai focuses solely on one task
for example alphago is a maestro of the game go but you can't expect it to be
even remotely good at chess this makes alphago a weak ai

you might say alexa is definitely not a weak ai since it can perform multiple
tasks well that's not really true when you ask alexa to play despacito it picks
up the keywords play and despacito and runs a program it is trained to

alexa cannot respond to a question it isn't trained to answer for instance try
asking alexa the status of traffic from work to home alexa cannot provide you this
information as she is not trained to and that brings us to our second category of

ai strong ai now this is much like the robots that
only exist in fiction as of now ultron from avengers is an ideal example
of a strong ai that's because it's self-aware and eventually even develops emotions
this makes the ai's response unpredictable you must be wondering well how is
artificial intelligence different from machine learning and deep learning we saw what ai is machine learning is a
technique to achieve ai and deep learning in turn is a subset of machine
learning machine learning provides a machine with the capability to learn from data and
experience through algorithms deep learning does this learning through ways inspired by the human brain this
means through deep learning data and patterns can be better perceived ray kurzweil a well-known futurist
predicts that by the year 2045 we would have robots as smart as humans
this is called the point of singularity well that's not all in fact elon musk predicts that the
human mind and body will be enhanced by ai implants which would make us partly
cyborgs so here's a question for you which of the below ai projects don't exist yet
a an ai robot with citizenship b a robot with a muscular skeletal
system c ai that can read its owner's emotions d ai that develops emotions over time
give it a thought and leave your answers in the comment section below since the human brain is still a mystery it's no
surprise that ai2 has a lot of unventured domains for now ai is built to work with humans
and make our tasks easier however with the maturation of technology we can only wait and watch
what the future of ai holds for us well that is artificial intelligence for you
in short do not forget to leave your answer to the quiz in the comment section below also like share and
subscribe to our channel if you enjoyed this video stay tuned and keep learning
now let's look at the different types of artificial intelligence so ai can be classified based on
capabilities and functionalities under capabilities there are three types of artificial intelligence
they are narrow ai general ai and super ai under functionalities we have four types
of artificial intelligence reactive machine limited memory theory of mind and self-awareness
let's look at them one by one first we will look at the different
types of artificial intelligence based on capabilities
so what is narrow ai narrow ai also known as vki focuses on
one narrow task and cannot perform beyond its limitations it aims at a single subset of cognitive
abilities and advances in that spectrum applications of narrow ai are becoming increasingly common in our day-to-day
lives as machine learning and deep learning methods continue to evolve
apple siri is a simple example of an arrow ai that operates with a limited predefined range of functions
siri often has challenges with tasks outside its range of abilities ibm
watson supercomputer is another example of narrow ai which applies cognitive computing machine learning and natural
language processing to process information and answer your questions ibm watson once outperformed human
contestant ken jenkins to become the champion on the popular game show jeopardy
other examples of narrow ai include google translate image recognition software recommendation systems spam
filtering and google's page ranking algorithm
next we have general artificial intelligence or general ai general ai also known as strong ai has
the ability to understand and learn any intellectual task that a human can
general artificial intelligence has received a one billion dollar investment from microsoft through openai
it allows the machine to apply knowledge and skills in different contexts ai researchers and scientists have not
achieved strong ai so far to succeed they would need to find a way to make machines conscious programming a full
set of cognitive abilities fujitsu built the k computer which is
one of the fastest computers in the world it is one of the most notable attempts at achieving strong ai
it took 40 minutes to simulate a single second of neural activity so it is difficult to determine whether or not
strong ai will be achieved in the near future tianhe 2
is a supercomputer created by china's national university of defense technology it currently holds the record for cps at
33.86 petaflops although it sounds exciting the human brain is estimated to be capable of one
example now cps means characters per second that a system can process
third in the list of ai that is based on capabilities we have super ai super ai exceeds human intelligence and
can't perform any task better than a human the concept of artificial super intelligence sees ai evolve to be so
akin to human emotions and experiences that it doesn't just understand them it evokes emotions needs beliefs and
desires of its own its existence is still hypothetical some of the key characteristics of super ai include the
ability to think solve puzzles make judgments and decisions on its own
now if you have enjoyed watching this video so far please make sure to subscribe to our youtube channel and hit
the bell icon to stay updated with all the latest technologies also if you have any questions related
to this video please put it in the chat section our team of experts will help you address your questions
moving ahead now we will see the different types of artificial intelligence based on functionalities
in this category first we have reactive machine a reactive machine is the basic form of
ai that does not store memories or use past experiences to determine future actions
it works only with present data they simply perceive the world and react to it reactive machines are given certain
tasks and don't have capabilities beyond those duties
ibm's deep blue which defeated chess grandmaster gary kasparov is a reactive machine that sees the pieces on a chess
board and reacts to them it cannot refer to any of its prior experiences and cannot improve with
practice deep blue can identify the pieces on a chessboard and know how each moves
it can make predictions about what moves might be next for it and its opponent
it can choose the most optimal moves from among the possibilities deep blue ignores everything before the
present moment all it does is look at the pieces on the chessboard as it stands right now and
choose from possible next moves up next we have limited memory limited
memory ai learns from past data to make decisions the memory of such systems is
short-lived while they can use this data for a specific period of time they cannot add
it to a library of their experiences this kind of technology is used for
self-driving vehicles they observe how other vehicles are moving around them in the present and as
time passes that ongoing collected data gets added to the static data within the ai machine
such as lean markers and traffic lights they are included when the vehicle decides to change lanes to avoid cutting
of another driver or being hit by a nearby vehicle mitsubishi electric
is a company that has been figuring out how to improve such technology for applications like self-driving cars
then we have theory of mind theory of mind represents a very advanced class of technology and exists
as a concept this kind of ai requires a thorough understanding that the people and the
things within an environment can alter feelings and behaviors it should be able to understand people's
emotions sentiment and thoughts even though a lot of improvements are
there in this field this kind of ai is not complete yet one real world example of theory of mind
ai is kismet a robot head made in the late 90s by a massachusetts institute of
technology researcher kismet can mimic human emotions and recognize them
both abilities are key advancements in theory of mind ai but kismet can't follow gazes or convey
attention to humans sophia from hanson robotics is another example
where the theory of mind ai was implemented cameras within sophia's eyes combined with computer algorithms allow her to
see she can follow faces sustain eye contact and recognize individuals
she is able to process speech and have conversations using natural language subsystem
finally we have self-awareness self-awareness ai only exists
hypothetically such systems understand that internal traits states and conditions and
perceive human emotions these machines will be smarter than the human mind
this type of ai will not only be able to understand and evoke emotions in those it interacts with but also have emotions
needs and beliefs of its own while we are probably far away from creating machines that are self-aware we
should focus our efforts towards understanding memory learning and the ability to base decisions on past
experiences i am keef tana and today we shall be talking about the top 10 artificial intelligence technologies
before getting into the video here's a gentle reminder to subscribe to our channel and hit the bell icon so that
you wouldn't miss any update from us so without further ado let's quickly jump
into the topic in the list we have natural language generation smart
devices virtual agents speech recognition augmented reality
machine learning robotic process automation decision management
deep learning and finally image recognition please note that the list is in no
particular order natural language generation
communicating or conveying the information efficiently and transparently is very much crucial in
communication to say the right words in the right sequence to convey the message clearly
humans often find it tricky and when it comes to machines it's trickier
natural language generation is a sub-discipline of artificial intelligence and is a trendy technology
that converts the structured data into a native language the machines are programmed in such a
way that the algorithms convert the data into a desirable format for the users
this technology is widely used in customer service report generation and
summarizing business intelligence insights some of the sample vendors are a tvo
automated insights sas cambridge semantics digital reasoning narrative
science etc on the list next we have smart devices
with every passing day smart devices are becoming more popular and more trendy
technologies that people were using once upon a time are now remodeled and
modified as smart devices they can be used in almost every
industry to improve efficiency and optimize the operations smart devices are everyday objects made
intelligent with advanced computations which include artificial intelligence
and machine learning they are electronic gadgets that are able to connect share and interact with
its users and other smart devices some of the smart devices are
smart watches smart glasses smart phones smart speakers
etc virtual agents they are computer generated intelligence
that provides online customer assistance they can effectively communicate with
humans in most of the web and mobile applications we see chatbots as customer
service agents who interact with us answering our queries in simple virtual agents are a
manifestation of a technology which aims to create an effective
but digital impersonation of humans virtual agents can make reservations
book an appointment place an order and can even avail product information
some of the best examples of virtual agents are google assistant and amazon
alexa and the companies that provide virtual agents are microsoft google amazon and
assist ai next we have speech recognition
speech recognition is yet another subdiscipline of artificial intelligence which is used to convert human speech
into a comprehensive and useful format for computer applications to process
it can be said as a bridge between computer interactions and humans
it's presently used on interactive voice response systems and mobile applications
some of the companies offering speech recognition services are nuanced communications open text nice and varied
systems augmented reality yet another trendy technology in the
field of artificial intelligence augmented reality is an enhanced version
of the real physical world that is achieved using sound digital visual
elements and other sensory stimuli delivered via technology
augmented reality uses the existing environment and adds information to it to make a new artificial environment it
alters the perception of the real world environment augmented reality is making its way into
several retail stores that make makeup selection and furnishing much more fun
and interactive if getting your learning started is half the battle what if you could do that for
free visit skill up by simply learn click on the link in the description to know more
machine learning machine learning is an application and an important branch of artificial
intelligence which refers to machines being able to learn by themselves without being explicitly programmed
it enables systems to learn and improve from experience automatically
machine learning platforms are becoming more and more popular with the help of algorithms
apis big data and applications and training tools
enterprises are heavily investing in machine learning to reap its benefits for the various domains
machine learning is widely used for categorization and prediction
some of the sample vendors are amazon fractal analytics
google microsoft etc robotic process automation
configuring a robot to interpret communicate and analyze data is robotic
process automation it uses scripts and other methods to automate the human actions which in turn
supports efficient business processes robotic process automation is widely
used in those areas where the physical presence of a human is dangerous
fields like warfare mining etc is where it's currently being used
we need to remember that artificial intelligence is not meant to replace humans but instead it is to complement
their abilities and reinforce human talent companies that are focusing on robotic
process automation are pika systems automation anywhere blue prism
etc next on the list we have decision management decision management is yet another
technology in artificial intelligence that helps companies make valid decisions by providing up-to-date and
relevant information and performing analytic functions artificial intelligence when
combined with decision management helps translate customer data into predictive models of key trends
it also helps in making quick decisions avoidance of risks and in the automation
of the process decision management is widely used in the health care sector financial sector
insurance sector e-commerce and many more some of the companies that provide this
service are informatica advanced systems concepts pika systems
mana etc deep learning deep learning is a branch of artificial
intelligence that mimics the workings of the human brain in processing data
this data is used for a variety of purposes like recognizing speech detecting objects translating languages
and making decisions deep learning uses hierarchical level of artificial neural networks for carrying
out the process of machine learning which is used across several industries for various applications
some of the sample vendors are deep instinct fluid ai math works
saffron technology etc at last we have image recognition
image recognition is the process of detecting an object or feature in a digital image or video
it can be used to analyze clients and their opinions verify users based on
their faces detect license plates and diagnose diseases the present-day image
recognition is comparable to human visual perception many social media platforms like
facebook use this technology to enhance image search and aid visually impaired
users clarify gums and sense time provide this technology
decades ago the term artificial intelligence was just another frankenstein story but today it could be
a citizen in your next door right from his genesis artificial intelligence was programmed to probe
into human life and eliminate the fortifications of the impossibilities in many areas like healthcare
transportation communication and much more there are hundreds and thousands of
organizations tech communities and engineers worldwide working round the
clock to make miracles a reality and today we are going to discuss the top 10
amongst those who are contributing a significant part so here we are the top 10 artificial
intelligence companies to watch in 2022. at number 10 we have this ai
this ai is a revolutionary approach to dramatically improvise the way healthcare is delivered
with ai focuses on implementing artificial intelligence to diagnose and
detect abnormalities in the human body via ct cta mri x-ray ekg ultrasound etc
this ai coordinates both the patient and healthcare professionals at the same time
the patient reports are shared via mobile to all the concerned medical teams results are astonishing with vis-a-i
there is a significant improvement in productivity and reduction of time required for patient treatment and
discharge this ai was founded in 2016 and established in november 2021.
the company runs with 223 employees and the headquarters is in san francisco
this ai mainly focuses on improving and filling all the gaps between patients and healthcare professionals
this ai offers hands and salaries to its employees in india the salaries range from 4 lakhs
to 18 lakhs per annum and in america the wages range from 110 000
to 210 000 dollars per annum so at number nine we have farmwise
ten years ago an automated wheat killing robo was science fiction
now its rise of practical weed killing robos with millimeter precision thanks
to farmwise the first product of the californian-based company was an automated robo-weeder based on ai
computer vision and also robotics the robot would eliminate weeds from the
vegetable fields farmvise intends to build innovative systems and efficient procedures that
help farmers streamline farming with efficiency farmwise was established in the year
2020 the company functions with 50 employees on board and actively looking
for expansion the company follows the procedure of paying farming equipment employees and
contract personnel hourly farm buys makes an annual turnover of 8.5 million us dollars on average an
aiml engineer at farm wise earns anywhere around 113 000 per annum
up next we have amp robotics at number eight we have amp robotics
while others are busy trying to revolutionize mankind with ai amp robotics came up with an innovative idea
that happens to be a kind gesture of gratitude towards nature amp ai designed an artificial
intelligence robo that recycles e-waste plastic waste and waste from
construction sites and much more the robo precisely identifies paper
plastics and metals by learning and recognizing them via color size shape
and a few more factors the colorado based company is currently operational with 137 employees in total
and looking forward to expand the idea globally ap robotics aims to reimagining
recycling and to create a world without waste through artificial intelligence
amp robotics pays handsome salaries to their employees with an average salary of an employee amp
robotics varies between ninety six thousand dollars to one thirty one thousand dollars per annum
coming up next is a giant at number seven we have automation anywhere
automation anywhere is a global robotics automation and software company founded
in the year 2003 the company's headquarters is located in san jose
california the global rpa leader offers cloud
native web-based intelligent automation solutions to the world's leading cloud
companies automation anyways automation 360 offers
a variety of intelligent tools and self-learning systems to automate every
redundant task automation anywhere is focusing on transforming the companies to be rpa
oriented and the current kovit situations have triggered the need for automation even more
a recent survey states the rpa adoption rates will increase by 57
in just one more year the globally recognized multinational company works with more than 2
200 plus employees on board the company pays decent salaries as well
at number six we have microsoft ai microsoft integrated ai into cloud
computing microsoft's ai platform provides robust solutions to cloud related problems microsoft's ai division
is headquartered in redmond washington with around 5000 employees specifically
in the ai division and aims to train and certify 15 000 employees by 2022.
microsoft ai is a robust framework designed for developing ai solutions using machine learning data science
robotics iot and more ai enable services accelerate
productivity to a greater level microsoft azure uses ai to get best of
breed infrastructure with enterprise great security availability compliance
and manageability microsoft also pays handsome salaries to their employees
the average salary of an employee at the microsoft ai division earns up to 160
000 dollars per annum at number five we have data robo data robo has its headquarters in boston
it is one of the ai cloud leaders providing a unified platform to its cloud users to accelerate the delivery
of ai to the cloud data robo aims to understand and leverage the inherent capabilities of
the platform data robo has the caliper to help business analysts in building predictive
analytics with no knowledge of machine learning it uses automated ml to build and deploy
accurate predictive models in a short span of time the global cloud leader currently
operates with more than a thousand employees worldwide data robo compensates its employees with
handsome salaries an average employee at data robo earns up to 160 000 per year
and up to three lakh four hundred thousand rupees in india at number four we have amazon the alexa
maker amazon is one of the tech giants interested in adopting ai into its
various divisions amazon calls its ai services amazon lex amazon poly amazon
recognition and amazon machine learning these services are accessible through a
software api call or console the amazon ai suite of services can have text or
voice conversations with an end user using a conversational chat ops
interface the seattle-based tech giant is currently using ai for its smart home
solutions ai companies like alexa and its business intelligence needs amazon is also
conducting trials on drone delivery to save time and efforts to deliver goods
and services to remote locations with ease amazon ai aims to provide biometric
payments to its customers at cash-free stores the approach is still in trials
amazon calls it just walk out technology amazon has enormous teams and pays
decent salaries to its employees for example an employee at amazon can earn up to 20 lakh rupees per annum in india
and about a hundred thousand dollars per annum in united states of america
at number three we have data bricks databricks is one of the best data driven companies
data bricks lake houses combine data warehouses reliability performance and
governance with the openness and flexibility of data lakes data brick is headquartered in san
francisco california and has 15 office locations across 11 countries
the company announces plans on delivering keynote on data and machine learning product innovations and the
rise of the lake house architecture advancements databricks currently operates with 2000
plus employees globally the organization compensates its employees with handsome salaries an
average employee at databricks makes about 154 000 us dollars and around 2.1
lakh rupees per annum in india next we have meta at number two we have
meta meta formerly known as facebook is innovating an advanced way of social
interaction meta has integrated artificial intelligence and it's being called the meta ai
meta ai has its headquarters in toronto canada meta ai has redesigned a new material
and touch sensor with potential to boost the development of metawars
a team of researchers at the metawars have worked jointly with scientists at
carnegie mellon university to build reskin a plastic three millimeter thick
membrane with magnetic particles capable of measuring touch and sensation
a thrilling experiment meta has under its sleeve i would say meta currently
operates remotely with about 350 employees on board critiques also state that meta pays a
vast salary to its ai experts with an average salary of 1 lakh 750 000
paid to the employees at meta ai at number one we have google ai
google ai is the division of google dedicated to artificial intelligence
ceo sundar pichai announced it at google i o 2017 google ai has its headquarters
in california google ai is demonstrating its quantum computing supremacy with its new and
powerful cinema processor google's ai is one of the most widespread technologies
it's almost everywhere from your language translations to helping people with speech impairments
to have a successful conversation social media interactions shopping lists
and assisting users with gps sending alerts to its users about natural
calamities and saving them google ai has announced that its research team is designing next
generation ai chips for a smart future in addition a new ai model allows
artificial intelligence to perform chip design that more efficiently solves unique aspects of any problem with an
artificial neural network google ai currently works with our four divisions with massive teams we have
come to an end of this session on the top 10 ai companies to watch out for in
2022. hello and welcome to machine learning tutorial part one this is part one of a machine learning series put on
by simply learn my name is richard kirschner i'm with the simply learn team that's www.simplylearn.com
get certified get ahead what's in it for you today well we'll start off with a
brief explanation of why machine learning and what is machine learning and then we'll get into a few of the
types of machine learning machine learning algorithms linear regression decision trees support vector machine
and finally we'll do a use case where we're going to classify whether recipe is of a cupcake or a muffin using the
svm or the support vector machine sounds like a delicious way to explore machine learning so why machine learning why do
we even care about having these computers come up and be able to do all these new things for us well because
machines can now drive your car for you still very in the infant stage but it's just exploding as we see with google's
waymo and then uber had their program which unfortunately crashed they know that this is huge this is going to be
the huge industry to change our whole transportation infrastructure machine learning is now used to detect over 50
eye diseases do you know how amazing that is to have a computer that double checks for the doctor for things they
might miss that's just huge in the health industry pretty soon they actually do already have that in some
areas where maybe not for eyes but for other diseases where they're using the camera on your phone to help
pre-diagnose before you go and see the doctor and because the machine can now unlock your phone with your face i mean
that's just cool having it being able to identify your face or your voice and be able to turn stuff on and off for you
depending on where you're at and what you need talking about an ultimate automation or world we live in and as we
dig in deeper we have a nice example of facebook as you can see here they have the facebook post with halloween comment
yes if you want it order here nobody likes spam post on facebook that annoy
them into interacting with likes shares comments and other actions i remember
the original ones were all if you don't click on here you will have bad luck or some kind of fear factor well this is a
huge thing in a social media when people are getting spammed and so this tactic known as engagement bait takes advantage
of facebook's news feed algorithm by choosing engagement in order to get the greater reach
to eliminate engagement bait the company reviewed and categorized hundreds of thousands of posts to train a machine
learning model that detects different types of engagement bait so in this case we have we're using facebook but this is
of course across all the different social media they have different tools for building and the facebook scroll
jiff will be replaced kind of like a virus coming in there and notices that there's a certain setup with facebook
and it's able to replace it and they have like vote baiting react baiting share baiting they have all these
different these are kind of general titles but there certainly are a lot of way of baiting you to go in there and click on something so they fed all this
this data was fed into the machine and then they have the new post the new post comes up that takes over part of the
facebook setup and that's what you're looking at you're looking at this new post that's replaced like a virus has replaced that so what facebook do to
eliminate this is they start scanning for keywords and phrases like this and checks the click-through rate so starts
looking for people who are clicking through it without even looking at it or clicking through it it's not something
that normally would be clicked through once facebook has scanned for these keywords and phrases it is now able to
identify the spam coming in and this makes your life easier so you're not getting spammed it's not like walking
through an airport and a lot of countries you have like hundreds of people trying to sell you timeshare come
join us sign up for this eliminates that annoyingness so now you can just enjoy your facebook and your cat pictures or
maybe it's your family pictures mine is family certainly people like their cat pictures too another good example is
google's deepmind project alphago a computer program that plays a board game go has defeated the world's number one
go player and i hope i say his name right kiji the ultimate go challenge game of three of three was on may 27
2017 so it's just last year that this happened and what makes this so
important is that you know go is just is a game so it's not like you're driving a car or
something in our real world but they are using games to learn how to get the
machine learning program to learn they want it to learn how to learn and that is a huge step a lot of this is still in
its infant stage as far as development as we saw what happened with the as i referred to earlier the uber cars they
lost their whole division because they jumped ahead too fast so it's still an infant stage but boy is this like the
beginning of just an amazing world that is automated in ways we can't even imagine what tomorrow's going to look
like we've looked at a lot of examples of machine learning so let's see if we can give a little bit more of a concrete
definition what is machine learning machine learning is the science of making computers learn and act like humans by
feeding data and information without being explicitly programmed we see here we have a nice little
diagram where we have our ordinary system and your computer nowadays you can even run a lot of the stuff on a
cell phone because cell phones advance so much and then with artificial intelligence and machine learning it now
takes the data and it learns from what happened before and then it predicts what's going to come next and then
really the biggest part right now in machine learning that's going on is it improves on that how do we find a new
solution so we go from descriptive words learning about stuff and understanding how it fits together to predicting what
it's going to do to post scripting coming up with a new solution and when we're working on machine learning
there's a number of different diagrams that people have posted for what steps to go through a lot of it might be very
domain specific so if you're working on photo identification versus language
versus medical or physics some of these are switched around a little bit or new things are put in they're very specific
to the domain this is kind of a very general diagram first you want to define your objective very important to know
what it is you're wanting to predict then you're going to be collecting the data so once you've defined an objective
you need to collect the data that matches you spend a lot of time in data science collecting data in the next step
preparing the data you've got to make sure that your data is clean going in there's the old saying bad data in bad
answer out or bad data out and then once you've gone through and we've cleaned
all this stuff coming in then you're going to select the algorithm which algorithm are you going to use you're
going to train that algorithm in this case i think we're going to be working with svm the support vector machine then
you have to test the model does this model work is this a valid model for what we're doing and then once you've
tested it you want to run your prediction you want to run your prediction or your choice or whatever output it's going to come up with and
then once everything is set and you've done lots of testing then you want to go ahead and deploy the model remember i
said domain specific this is very general as far as the scope of doing something a lot of models you get
halfway through and you realize that your data is missing something and you have to go collect new data because
you've run a test in here someplace along the line you're saying hey i'm not really getting the answers i need so there's a lot of things that are domain
specific that become part of this model this is a very general model but it's a very good model to start with and we do
have some basic divisions of what machine learning does that's important to know for instance do you want to
predict a category well if you're categorizing thing that's classification for instance whether the stock price
will increase or decrease so in other words i'm looking for a yes no answer is it going up or is it going down and in
that case we'd actually say is it going up true if it's not going up it's false meaning it's going down this way it's a
yes no zero one do you want to predict a quantity that's regression so remember
we just did classification now we're looking at regression these are the two major divisions in what data is doing
for instance predicting the age of a person based on the height weight health and other factors so based on these
different factors you might guess how old a person is and then there are a lot of domain specific things like do you want to
detect an anomaly that's anomaly detection this is actually very popular right now for instance you want to
detect money withdrawal anomalies you want to know when someone's making a withdrawal that might not be their own account we've actually brought this up
because this is really big right now if you're predicting the stock whether to buy stock or not you want to be able to
know if what's going on in the stock market is an anomaly use a different prediction model because something else
is going on you got to pull out new information in there or is this just the norm i'm going to get my normal return
on my money invested so being able to detect anomalies is very big in data science these days
another question that comes up which is on what we call untrained data is do you
want to discover structure in unexplored data and that's called clustering for instance finding groups of customers
with similar behavior given a large database of customer data containing their demographics and past buying
records and in this case we might notice that anybody who's wearing certain set
of shoes go shopping at certain stores or whatever it is they're going to make certain purchases by having that
information it helps us to market or group people together so that we can now explore that group and find out what it
is we want to market to them if you're in the marketing world and that might also work in just about any arena you
might want to group people together whether they're based on their different areas and investments and financial
background whether you're going to give them a loan or not before we even start looking at whether they're a valid customer for the bank you might want to
look at all these different areas and group them together based on unknown data so you're not you don't know what the data is going to tell you but you
want to cluster people together that come together let's take a quick detour for quiz time oh my favorite so we're going
to have a couple questions here under quiz time and we'll be posting the answers in this part two of this
tutorial so let's go ahead and take a look at these quiz times questions and hopefully you'll get them all right it'll get you thinking about how to
process data and what's going on can you tell what's happening in the following cases of course you're sitting there
with your cup of coffee and you have your checkbox and your pen trying to figure out what's your next step in your data science analysis
so the first one is grouping documents into different categories based on the
topic and content of each document very big these days you know you have legal
documents you have maybe it's the sports group documents maybe you're analyzing newspaper postings
but certainly having that automated is a huge thing in today's world b identifying handwritten digits in
images correctly so we want to know whether they're writing an a or capital a c
what are they writing out in their hand digit their handwriting c behavior of a website indicating that
the site is not working as designed d predicting salary of an individual
based on his or her years of experience with hr hiring uh set up there so stay
tuned for part two we'll go ahead and answer these questions when we get to the part two of this tutorial or you can
just simply write at the bottom and send a note to simply learn and they'll follow up with you on it
back to our regular content now these last few bring us into the next topic which is another way of
dividing our types of machine learning and that is with supervised unsupervised
and reinforcement learning supervised learning is a method used to enable
machines to classify predict objects problems or situations based on labeled data fed to the machine and in here you
see we have a jungle of data with circles triangles and squares then we label them we have what's a circle
what's a triangle what's a square and we have our model training and it trains it so we know the answer very important
when you're doing supervised learning you already know the answer to a lot of your information coming in so you have a
huge group of data coming in and then you have a new data coming in so we've trained our model the model now knows
the difference between a circle a square a triangle and now that we've trained it we can send in in this case a square and
a circle goes in and it predicts that the top one's a square and the next one's a circle and you can see that this
is uh being able to predict whether someone's going to default on a loan because i was talking about banks earlier supervised learning on stock
market whether you're going to make money or not that's always important and if you are looking to make a fortune in
the stock market keep in mind it is very difficult to get all the data correct on the stock market it is very it
fluctuates and weighs you really hard to predict so it's quite a roller coaster ride if you're running machine learning
on the stock market you start realizing you really have to dig for new data so we have supervised learning and if you
have supervised we should need unsupervised learning in unsupervised learning machine learning model finds
the hidden pattern in an unlabeled data so in this case instead of telling it what the circle is what a triangle is
and what a square is it goes in there looks at them and says for whatever reason it groups them together maybe
they'll group it by the number of corners and it notices that a number of them all have three corners a number of
them all have four corners and a number of them all have no corners and it's able to filter those through and group
them together we talked about that earlier with looking at a group of people who are out shopping we want to
group them together to find out what they have in common and of course once you understand what people have in
common maybe you have one of them is a customer at your store or you have five of them our customer your store and they
have a lot in common with five others who are not customers at your store how do you market to those five who aren't
customers at your store yet they fit the demographics of who's gonna shop there and you'd like them to shop at your store not the one next door of course
this is a simplified version you can see very easily the difference between a triangle and a circle which might not be so easy in marketing reinforcement
learning reinforcement learning is an important type of machine learning where an agent learns how to behave in an
environment by performing actions and seeing the result we have here where the in this case a baby it's actually great
that they used an infant for this slide because the reinforcement learning is very much in its infant stages but it's
also probably the biggest machine learning demand out there right now or in the future it's going to be coming up
over the next few years is reinforcement learning and how to make that work for us and you can see here where we have our
action in the action on this one it goes into the fire hopefully the baby didn't it's just a little candle not a giant
fire pit like it looks like here when the baby comes out and the new state is the baby is sad and crying because they
got burned on the fire and then maybe they take another action the baby's called the agent because it's the one
taking the actions and in this case they didn't go into the fire they went a different direction and now the baby's happy and laughing and playing
reinforcement learning is very easy to understand because that's how as humans that's one of the ways we learn we learn
whether it is you burn yourself on the stove don't do that anymore don't touch the stove in the big picture being able
to have a machine learning program or an ai be able to do this is huge because now we're starting to learn how to learn
that's a big jump in the world of computer and machine learning and we're going to go back and just kind of go
back over supervised versus unsupervised learning understanding this is huge because this is going to come up in any
project you're working on we have in supervised learning we have labeled data we have direct feedback so
someone's already gone in there and said yes that's a triangle no that's not a triangle and then you predict an outcome
so you have a nice prediction this is this this new set of data is coming in and we know what it's going to be and then with unsupervised training it's not
labeled so we really don't know what it is there's no feedback so we're not telling it whether it's right or wrong
we're not telling it whether it's a triangle or a square we're not telling it to go left or right all we do is
we're finding hidden structure in the data grouping the data together to find out what connects to each other and then
you can use these together so imagine you have an image and you're not sure what you're looking for so you go in and
you have the unstructured data find all these things that are connected together and then somebody looks at
those and labels them now you can take that label data and program something to predict what's in the picture so you can
see how they go back and forth and you can start connecting all these different tools together to make a bigger picture
there are many interesting machine learning algorithms let's have a look at a few of them hopefully this gives you a
little flavor what's out there and these are some of the most important ones that are currently being used we'll take a
look at linear regression decision tree and the support vector machine let's start with a closer look at linear
regression linear regression is perhaps one of the most well-known and well-understood algorithms in statistics
and machine learning linear regression is a linear model for example a model that assumes a linear relationship
between the input variables x and the single output variable y
and you'll see this if you remember from your algebra classes y equals mx plus c
imagine we are predicting distance traveled y from speed x our linear regression model representation for this
problem would be y equals m times x plus c or distance equals m times speed plus c
where m is the coefficient and c is the y-intercept and we're going to look at two different variations of this first
we're going to start with time is constant and you can see we have a bicyclist he's got a safety gear on
thank goodness speed equals 10 meters per second and so over a certain amount of time his distance equals 36
kilometers we have a second bicyclist who's going twice the speed or 20 meters per second and you can guess if he's
going twice the speed and time is a constant then he's going to go twice the distance and that's easy to compute 36
times 2 you get 72 kilometers and so if you had the question of
how fast would somebody is going three times that speed or 30 meters per second is you can easily compute the distance
in our head we can do that without needing a computer but we want to do this for more complicated data so it's
kind of nice to compare the two but let's just take a look at that and what that looks like in a graph so in a linear regression model we have
our distance to the speed and we have our m equals the ve slope of the line
and we'll notice that the line has a plus slope and as the speed increases distance also increases hence the
variables have a positive relationship and so your speed of the person which equals y equals mx plus c distance
traveled in a fixed interval of time and we could very easily compute either following the line or just knowing it's
3 times 10 meters per second that this is roughly 102 kilometers distance that this third bicyclist has traveled one of
the key definitions on here is positive relationship so the slope of the line is
positive as distance increase so does speed increase let's take a look at our second example where we put distance is
a constant so we have speed equals 10 meters per second they have a certain distance to go and it takes them 100
seconds to travel that distance and we have our second bicyclist who's still doing 20 meters per second since he's
going twice the speed we can guess that he'll cover the distance in about half the time 50 seconds and of course you
could probably guess on the third one 100 divided by 30 since he's going three times the speed you can easily guess
that this is 33.333 seconds time we put that into a linear regression model or graph if the
distance is assumed to be constant let's see the relationship between speed and time and as time goes up the amount of
speed to go that same distance goes down so now your m equals a minus ve slope of
the line as the speed increases time decreases hence the variable has a negative relationship again there's our
definition positive relationship and negative relationship depended on the slope of the line and with a simple
formula like this and even a significant amount of data let's see with the mathematical
implementation of linear regression and we'll take this data so suppose we have this data set where we have x y x equals
1 2 3 4 5 standard series and the y value is 3 2 2 4 3. when we take that
and we go ahead and plot these points on a graph you can see there's kind of a nice scattering and you could probably
eyeball a line through the middle of it but we're going to calculate that exact line for linear regression and the first
thing we do is we come up here we have the mean of x i and remember mean is
basically the average so we added five plus four plus three plus two plus one and divide by five and that simply comes
out as three and then we'll do the same for y we'll go ahead and add up all those numbers and divide by five and we
end up with the mean value of y of i equals 2.8 where the x i reference is
it's an average or means value and the y i also equals a means value of y and when we plot that you'll see that we can
put in the y equals 2.8 and the x equals 3 in there on our graph we kind of give
it a little different color so you can sort it out with the dashed lines on it and it's important to note that when we
do the linear regression the linear regression model should go through that dot
now let's find our regression equation to find the best fit line remember we go ahead and take our y equals mx plus c so
we're looking for m and c so to find this equation for our data we need to find our slope of m and our coefficient
of c and we have y equals mx plus c where m equals the sum of x minus x
average times y minus y average or y means and x means over the sum of x
minus x means squared that's how we get the slope of the value of the line and we can easily do that by creating some
columns here we have x y computers are really good about iterating through data and so we can easily compute this and
fill in a graph of data and in our graph you can easily see that if we have our x
value of one and if you remember the x i or the means value is three one minus
three equals a negative two and two minus three equals a negative one so on
and so forth and we can easily fill in the column of x minus x i y minus y i
and then from those we can compute x minus x i squared and x minus x i times
y minus y i and you can guess it that the next step is to go ahead and sum the different columns for the answers we
need so we get a total of ten for our x minus x i squared and a total of two for
x minus x i times y minus y i and we plug those in we get two tenths
which equals 0.2 so now we know the slope of our line equals 0.2 so we can calculate the value of c that'd be the
next step is we need to know where it crosses the y-axis and if you remember i mentioned earlier that the linear
regression line has to pass through the means value the one that we showed earlier we can just flip back up there
to that graph and you can see right here there's our means value which is 3 x equals 3 and y equals 2.8 and since we
know that value we can simply plug that into our formula y equals 0.2 x plus c
so we plug that in we get 2.8 equals 0.2 times 3 plus c and you can just solve
for c so now we know that our coefficient equals 2.2 and once we have
all that we can go ahead and plot our regression line y equals 0.2 times x
plus 2.2 and then from this equation we can compute new values so let's predict
the values of y using x equals 1 2 3 4 5 and plot the points remember the 1 2 3 4
5 was our original x values so now we're going to see what y thinks they are not what they actually are and we plug those
in we get y of designated with y of p you can see that x equals 1 equals 2.4 x
equals 2 equals 2.6 and so on and so on so we have our y predicted values of
what we think it's going to be when we plug those numbers in and when we plot the predicted values along with the axial values we can see the difference
and this is one of the things that's very important with linear aggression in any of these models is to understand the error and so we can calculate the error
on all of our different values and you can see over here we plotted x and y and y predict and we've drawn a little line
so you can sort of see what the error looks like there between the different points so our goal is to reduce this
error we want to minimize that error value on our linear regression model minimizing the distance there are lots
of ways to minimize the distance between the line and the data points like sum of squared errors sum of absolute errors
root mean square error etc we keep moving this line through the data points to make sure the best fit line has the
least square distance between the data points and the regression line so to recap with a very simple linear
regression model we first figure out the formula of our line through the middle and then we slowly adjust the line to
minimize the error keep in mind this is a very simple formula the math gets even though the math is very much the same it
gets much more complex as we add in different dimensions so this is only two dimensions y equals mx plus c but you
can take that out to x z i j q all the different features in there and they can plot a linear
regression model on all of those using the different formulas to minimize the error let's go ahead and take a look at
decision trees a very different way to solve problems in the linear regression model decision tree is a tree-shaped
algorithm used to determine a course of action each branch of a tree represents a possible decision occurrence or
reaction we have data which tells us if it is a good day to play golf and if we
were to open this data up in a general spreadsheet you can see we have the outlook whether it's a rainy overcast
sunny temperature hot mild cool humidity windy and did i
like to play golf that day yes or no so we're taking a census and certainly i wouldn't want a computer telling me when
i should go play golf or not but you can imagine if you got up in the night before you're trying to plan your day
and it comes up and says tomorrow would be a good day for golf for you in the morning and not a good day in the
afternoon or something like that this becomes very beneficial and we see this in a lot of applications coming out now
where it gives you suggestions and lets you know what would be fit the match for you for the next day or the next
purchase or the next uh whatever you know next mail out in this case is tomorrow a good day for playing golf
based on the weather coming in and so we come up and let's uh determine if you should play golf when the day is sunny
and windy so we found out the forecast tomorrow is going to be sunny and windy and suppose we draw our tree like this
we're going to have our humidity and then we have our normal which is if it's if you have a normal humidity you're
going to go play golf and if the humidity is really high then we look at the outlook and if the outlook is sunny
overcast or rainy it's going to change what you choose to do so if you know that it's a very high humidity and it's
sunny you're probably not going to play golf because you're going to be out there miserable fighting off the mosquitoes that are out joining you to
play golf with you maybe if it's raining you probably don't want to play in the rain but if it's slightly overcast you get just the right
shadow that's a good day to play golf and be outside out on the green now in this example you can probably make your
own tree pretty easily because it's a very simple set of data going in but the question is how do you know what to
split where do you split your data what if this is much more complicated data where it's not something that you would
particularly understand like studying cancer they take about 36 measurements of the cancerous cells and then each one
of those measurements represents how bulbous it is how extended it is how sharp the edges are something that as a
human we would have no understanding of so how do we decide how to split that data up and is that the right decision
tree but since the question is going to come up is this the right decision tree for that we should calculate entropy and
information gain two important vocabulary words there are the entropy and the information gain entropy entropy
is a measure of randomness or impurity in the data set entropy should be low so
we want the chaos to be as low as possible we don't want to look at it and be confused by the images or what's
going on there with mixed data and the information gain it is a measure of decrease in entropy after the data set
is split also known as entropy reduction information gain should be high so we
want our information that we get out of the split to be as high as possible let's take a look at entropy from the
mathematical side in this case we're going to denote entropy as i of p of and
n where p is the probability that you're going to play a game of golf and n is the
probability where you're not going to play the game of golf now you don't really have to memorize these formulas there's a few of them out there
depending on what you're working with but it's important to note that this is where this formula is coming from so when you see it you're not lost when
you're running your programming unless you're building your own decision tree code in the back and we simply have a
log squared of p over p plus n minus n over p plus n times the log squared of n
of p plus n but let's break that down and see what actually looks like when we're computing that from the computer
script side entropy of a target class of the data set is the whole entropy so we have
entropy play golf and we look at this if we go back to the data you can simply
count how many yeses and no in our complete data set for playing golf days
in our complete set we find we have five days we did play golf and nine days we did not play golf and so our i equals if
you had those together nine plus five is fourteen and so our i equals five over fourteen and nine over 14 that's our p
and n values that we plug into that formula and you can go 5 over 14 equals 0.36 9 over 14 equals
0.64 and when you do the whole equation you get the minus 0.36 log root squared
of 0.36 minus 0.64 log squared root of 0.64 and we get a set value we get
0.94 so we now have a full entropy value for the whole set of data that we're
working with and we want to make that entropy go down and just like we calculated the entropy out for the whole
set we can also calculate entropy for playing golf and the outlook is it going to be overcast or rainy or sunny and so
we look at the entropy we have p of sunny times e of three of two and that just comes out how many sunny days
yes and how many sunny days no over the total which is five don't forget to put the we'll divide that five out later on
uh equals p overcast equals four comma zero plus rainy equals two comma three
and then when you do the whole set up we have five over fourteen remember i said
there was a total of five five over fourteen times the i of three of two plus 4 over 14 times the 4 comma 0 and
514 over i of 2 3. and so we can now compute the entropy of just the part
that has to do with the forecast and we get 0.693 similarly we can calculate the
entropy of other predictors like temperature humidity and wind and so we look at the gain outlook how much are we
going to gain from this entropy play golf minus entropy play golf outlook and we can take the original 0.94 for the
whole set minus the entropy of just the rainy day and temperature and we end up
with a gain of 0.247 so this is our information gain remember we define entropy and we define an
information gain the higher the information gained the lower the entropy the better the information gain of the
other three attributes can be calculated in the same way so we have our gain for temperature equals 0.029
we have our gain for humidity equals 0.152 and our gain for a windy day equals 0.048 and if you do a quick
comparison you'll see the 0.247 is the greatest gain of information so
that's the split we want now let's build the decision tree so we have the outlook is it going to be sunny
overcast or rainy that's our first split because that gives us the most information gain and we can continue to
go down the tree using the different information gains with the largest information we can continue down the
nodes of the tree where we choose the attribute with the largest information gain as the root node and then continue
to split each sub node with the largest information gain that we can compute and although it's a little bit of a tongue
twister to say all that you can see that's a very easy to view visual model
we have our outlook we split it three different directions if the outlook is overcast we're going to play and then we
can split those further down if we want so if the outlook is sunny but then it's also windy if it's windy we're not going
to play if it's not windy we'll play so we can easily build a nice decision tree
to guess what we would like to do tomorrow and give us a nice recommendation for the day so we want to
know if it's a good day to play golf when it's sunny and windy remember the original question that came out tomorrow's weather report is sunny and
windy you can see by going down the tree we go outlook sunny outlook windy we're not going to play golf tomorrow so our
little smart watch pops up and says i'm sorry tomorrow's not a good day for golf it's going to be sunny and windy and if
you're a huge golf fan you might go uh-oh it's not a good day to play golf we can
go in and watch a golf game at home so we'll sit in front of the tv instead of being out playing golf in the wind
now that we've looked at our decision tree let's look at the third one of our algorithms we're investigating support
vector machine support vector machine is a widely used classification algorithm the idea of support vector machine is
simple the algorithm creates a separation line which divides the classes in the best possible manner for
example dog or cat disease or no disease suppose we have a labeled sample data
which tells height and weight of males and females a new data point arrives and we want to
know whether it's going to be a male or a female so we start by drawing a line we draw decision lines but if we
consider decision line 1 then we will classify the individual as a male and if we consider decision line two then it'll
be a female so you can see this person kind of lies in the middle of the two groups so it's a little confusing trying to figure out which line they should be
under we need to know which line divides the classes correctly but how the goal
is to choose a hyperplane and that is one of the key words they use when we talk about support vector machines
choose a hyperplane with the greatest possible margin between the decision line and the nearest point within the
training set so you can see here we have our support vector we have the two nearest points to
it and we draw a line between those two points and the distance margin is the distance between the hyperplane and the
nearest data point from either set so we actually have a value and it should be equal the distance between the two
points that we're comparing it to when we draw the hyperplanes we observe that line one has a maximum distance so we
observe that line one has a maximum distance margin so we'll classify the new data point correctly and our result
on this one is going to be that the new data point is mel one of the reasons we call it a hyperplane versus a line
is that a lot of times we're not looking at just weight and height we might be looking at
36 different features or dimensions and so when we cut it with a hyperplane it's
more of a three-dimensional cut in the data multi-dimensional it cuts the data a certain way and each plane continues
to cut it down until we get the best fit or match let's understand this with the help of an example problem statement i
always start with a problem statement when you're going to put some code together we're going to do some coding now classifying muffin and cupcake
recipes using support vector machines so the cupcake versus the muffin let's have
a look at our data set and we have the different recipes here we have a muffin recipe that has so much flour i'm not
sure what measurement 55 is in but it has 55 maybe it's ounces
but a certain amount of flour certain amount of milk sugar butter egg baking powder vanilla and salt and so based on
these measurements we want to guess whether we're making a muffin or a cupcake and you can see in this one we
don't have just two features we don't just have height and weight as we did before between the male and female in
here we have a number of features in fact in this we're looking at eight different features to guess whether it's
a muffin or a cupcake what's the difference between a muffin and a cupcake turns out muffins have more
flour while cupcakes have more butter and sugar so basically the cupcake's a little bit more of a dessert where the
muffins a little bit more of a fancy bread but how do we do that in python how do we code that to go through
recipes and figure out what the recipe is and i really just want to say cupcakes versus muffins like some big
professional wrestling thing before we start in our cupcakes versus muffins we are going to be working in python
there's many versions of python many different editors that is one of the strengths and weaknesses of python is it
just has so much stuff attached to it it's one of the more popular data science programming packages you can use
in this case we're going to go ahead and use anaconda in jupyter notebook the
anaconda navigator has all kinds of fun tools once you're into the anaconda
navigator you can change environments i actually have a number of environments on here we'll be using python36
environment so this is in python version 3.6 although it doesn't matter too much which version you use i usually try to
stay with the 3x because they're current unless you have a project that's very specifically in version 2x 27 i think is
usually what most people use in the version 2. and then once we're in our jupiter notebook editor i can go up and
create a new file and we'll just jump in here in this case we're doing svm muffin
versus cupcake and then let's start with our packages for data analysis
and we almost always use a couple there's a few very standard packages we use
we use imports import import
numpy that's for number python they usually denote it as np that's very comma
that's very common and then we're going to import pandas as pd
and numpy deals with number arrays there's a lot of cool things you can do with the numpy setup
as far as multiplying all the values in an array in a numpy array data array
pandas i can't remember if we're using it actually in this data set i think we do as an import it makes a nice data frame
and the difference between a data frame and a numpy array is that a data frame is more like your excel spreadsheet you
have columns you have indexes you have different ways of referencing it easily viewing it and there's additional
features you can run on a data frame and pandas kind of sits on numpy so they you need them both in there
and then finally we're working with the support vector machine so from sk learn we're going to use the sk
learn model import svm support vector machine
and then as a data scientist you should always try to visualize your data some data
obviously is too complicated or doesn't make any sense to the human but if it's possible it's good to take a second look
at it so you can actually see what you're doing now for that we're going to use two packages we're going to import
matplotlibrary.piplot as plt again very common and we're going to import seaborn as sns
and we'll go ahead and set the font scale in the sns right in our import line that's what this
semicolon followed by a line of data we're going to set the sns and these are great because the seaborn sits on top of
matte plot library just like pandas hits on numpy so it adds a lot more features
and uses and control we're obviously not going to get into matplot library and cborn that'd be its own tutorial we're
really just focusing on the svm the support vector machine from sklearn and
since we're in jupiter notebook we have to add a special line in here for our matplot library and that's your
percentage sign or ambersign matplot library in line now if you're doing this in just a
straight code project a lot of times i use like notepad plus plus and i'll run it from there you don't have to have
that line in there because it'll just pop up as its own window on your computer depending on how your computer's set up because we're running
this in the jupyter notebook as a browser setup this tells it to display
all of our graphics right below on the page so that's what that line is for remember the first time i ran this i
didn't know that i had to go look that up years ago it's quite a headache so matplot library inline is just because
we're running this on the web setup and we can go ahead and run this make sure all our modules are in they're all
imported which is great if you don't have an import you'll need to go ahead and pip use the pip or
however you do it there's a lot of other install packages out there although pip is the most common and you have to make
sure these are all installed on your python setup the next step of course is we got to look at the data you can't run
a model for predicting data if you don't have actual data so to do that let me go and open this up and take a look and we
have our cupcakes versus muffins and it's a csv file or dot csv meaning
that it's comma separated variable and it's going to open it up in a nice uh spreadsheet for me and you can see up
here we have the type we have muffin muffin muffin cupcake cupcake cupcake and then it's broken up into flour milk
sugar butter egg baking powder vanilla and salt so we can do is we can go ahead and look
at this data also in our python
let us create a variable recipes equals we're going to use our pandas module
dot read csv remember it's a comma separated variable
and the file name happened to be cupcakes versus muffins oops i got double brackets there
do it this way there we go cupcakes versus muffins
because the program i loaded or the the place i saved this particular python program is in the same folder we get by
with just the file name but remember if you're storing it in a different location you have to also put down the full path on there
and then because we're in pandas we're going to go ahead and you can actually inline you can do this but let
me do the full print you can just type in recipes.head
in the jupiter notebook but if you're running in code in a different script you need to go ahead and type out the whole print recipes dot head
and panda's nose is that's going to do the first five lines of data and if we
flip back on over to the spreadsheet where we opened up our csv file
you can see where it starts on line two this one calls it zero and then two three four five six is going to
match go and close that out so we don't need that anymore and it always starts at zero
and these are it automatically indexes it since we didn't tell it to use an index in here so that's the index number
for the left hand side and it automatically took the top row as labels so pandas
using it to read a csv is just really slick and fast one of the reasons we love our pandas
not just because they're cute and cuddly teddy bears and let's go ahead and plot our data
and i'm not going to plot all of it i'm just going to plot the sugar and flour
now obviously you can see where they get really complicated if we have tons of different features
and so you'll break them up and maybe look at just two of them at a time to see how they connect
and to plot them we're going to go ahead and use seaborn so that's our sns and the command for that is sns.lmplot
and then the two different variables i'm going to plot is flour and sugar
data equals recipes the hue equals type and this is a lot of fun because it
knows that this is pandas coming in so this is one of the powerful things about pandas mixed with seabourn and
doing graphing and then we're going to use a palette
set one there's a lot of different sets in there you can go look them up for seabourn or do a regular fit regular
equals false so we're not really trying to fit anything and it's a scatter kws
a lot of these settings you can look up in seabourn half of these you could probably leave off when you run them somebody played with this and found out
that these were the best settings for doing a seaborn plot let's go ahead and run that
and because it does it in line it just puts it right on the page and you can see right here that just
based on sugar and flour alone there's a definite split
and we use these models because you can actually look at and say hey if i drew a line right between the middle of the
blue dots and the red dots we'd be able to do an svm and a hyper
plane right there in the middle then the next step is to
format or pre-process
our data and we're going to break that up into two parts
we need a type label and remember we're going to decide whether it's a muffin or cupcake well a
computer doesn't know muffin or cupcake it knows zero and one so what we're going to do is we're going
to create a type label and from this we'll create a numpy array
and p where and this is where we can do some logic we take our recipes from our panda
and wherever type equals muffin it's going to be zero and then if it doesn't
equal muffin which is cupcakes it's going to be one so we create our type label
this is the answer so when we're doing our training model remember we have to have a training data this is what we're
going to train it with is that it's 0 or 1. it's a muffin or it's not
and then we're going to create our recipe features and if you remember correctly from right
up here the first column is type so we really don't need the type columns that's our muffin or cupcake
and in pandas we can easily sort that out we take our value recipes
dot columns that's a pandas function built into pandas
we've got values converting them to values so it's just the column titles going across the top
and we don't want the first one so what we do is since it always starts at zero we want one
colon till the end and then we want to go ahead and make this a list
and this converts it to a list of strings
and then we can go ahead and just take a look and see we're looking at for the features make sure it looks right
let me go ahead and run that and i forgot the s on recipes so we'll
go ahead and add the s in there and then run that and we can see we have flour milk sugar butter egg
baking powder vanilla and salt and that matches what we have up here right where
we printed out everything but the type so we have our features and we have our label
now the recipe features is just the titles of the columns and we actually need the ingredients
and at this point we have a couple options one we could run it over all the ingredients
and when you're doing this usually you do but for our example we want to limit it so you can easily see what's going on
because if we did all the ingredients we have you know that's what uh
seven eight different hyper planes that would be built into it we only want to look at one so you can see what the svm
is doing and so we'll take our recipes and we'll do just flour and sugar
again you can replace that with your recipe features and do all of them but we're going to do just flour and sugar and we're going to convert that to
values we don't need to make a list out of it because it's not string values these are actual
values on there and we can go ahead and just print ingredients and you can see what that
looks like uh and so we have just the amount of flour and sugar just the two sets of
plots and just for fun let's go ahead and take this over here and
take our recipe features and so if we decided to use all the
recipe features you'll see that it makes a nice column of different data so it just strips out all the labels and everything we just
have just the values but because we want to be able to view this easily in a plot later on we'll go
ahead and take that and just do flower and sugar and we'll run that you'll see it's just
the two columns so the next step is to go ahead and fit
our model we'll go ahead and just call it model and it's a svm we're using a package
called svc in this case we're going to go ahead and
set the kernel equals linear so it's using a specific setup on there and if we go to the reference on their website
for the svm you'll see that there's about there's eight of them here three of them are for regression
three are for classification the svc support vector classification is
probably one of the most commonly used and there's also one for detecting outliers
and another one that has to do with something a little bit more specific on the model but svc and sbr are the two
most commonly used standing for support vector classifier and support vector regression remember regression is an
actual value a float value or whatever you're trying to work on and sbc is a classifier so it's a yes no
true false but for this we want to know zero one muffin cupcake
if we go ahead and create our model and once we have our model created we're going to do model dot fit and this is
very common especially in the sk learn all their models are followed with the fit command
and what we put into the fit what we're training with it is we're putting in the ingredients which in this case we
limited to just flour and sugar and the type label is it a muffin or cupcake
now in more complicated data science series you'd want to split into we won't
get into that today we split it into uh training data and test data and they even do something where they split it
into thirds where a third is used for whether you switch between which one's training and test there's all kinds of
things go into that it gets very complicated when you get to the higher end not overly complicated just an extra
step which we're not going to do today because this is a very simple set of data and let's go ahead and run this and now
we have our model fit and i got an error here so let me fix that real quick
capital sbc it turns out i did it lowercase
support vector classifier there we go let's go ahead and run that
and you'll see it comes up with all this information that it prints out automatically these are the defaults of the model
you notice that we change the kernel to linear and there's our kernel linear on the printout and there's other different
settings you can mess with we're going to just leave that alone for right now for this we don't really need
to mess with any of those so next we're going to dig a little bit
into our newly trained model and we're going to do this so we can show you on a graph
and let's go ahead and get the separating
we're going to say we're going to use a w for our variable on here we're going to do model
dot coefficient underscore zero so what the heck is that again we're
digging into the model so we've already got a prediction and a train this is a math behind it that we're
looking at right now and so the w is going to represent two
different coefficients and if you remember we had y equals mx plus c
so these coefficients are connected to that but in two-dimensional it's a plane
we don't want to spend too much time on this because you can get lost in the confusion of the math so
if you're a math whiz this is great you can go through here and you'll see that we have a equals minus w0 over w of one
remember there's two different values there and that's basically the slope that we're generating
and then we're going to build an xx what is xx we're going to set it up to a
numpy array there's our np dot line space so we're creating a line
of values between 30 and 60. so it just creates a set of numbers for x
and then if you remember correctly we have our formula y equals the slope times x
plus the intercept well to make this work we can do this as y y
equals the slope times each value in that array that's the neat thing about
numpy so when i do a times x x which is a whole numpy array of values it multiplies a across all of them
and then it takes those same values and we subtract the model intercept that's here uh
where we had mx plus c so that'd be the c from the formula y equals mx plus c
and that's where all these numbers come from a little bit confusing because it's digging out of these different arrays
and then we want to do is we're going to take this and we're going to go ahead and plot it so plot the parallels to separating
hyperplane that pass through the support vectors and so we're going to create b equals a
model support vectors pulling our support vectors out there here's our y y which we now know is a
set of data and we have we're going to create y y down equals a times x x plus
b one minus a times b zero and then model support vector b is going
to be set that to a new value the minus one set up and y y up equals a times x x
plus b one minus a times b zero and we go ahead and just run this to
load these variables up if you wanted to understand a little bit more of what's going on you can see if we print
y y let me just run that you can see it's an array this is a line it's going to have in this case between
30 and 60 so it's going to be 30 variables in here and the same thing with yy up yy down
and we'll plot those in just a minute on the graph so you can see what those look like i'll just go ahead and delete that out
of here and run that so it loads up the variables nice clean slate
i'm just going to copy this from before remember this our sns our seabourn plot lm plot flower sugar
and i'll just go ahead and run that real quick so you can see remember what that looks like it's just a straight graph on there
and then one of the new things is because seaborn sits on top of pie plot
we can do the pi plot for the line going through and that is simply plt dot plot
and that's our xx and yy our two corresponding values xy
and then somebody played with this to figure out that the line width equals two in the color black would look nice
so let's go ahead and run this whole thing with the pie plot on there and you can see when we do this it's
just doing flour and sugar on here corresponding line between the sugar and
the flour and the muffin versus cupcake
and then we generated the support vectors the y y down and y y up so let's
take a look and see what that looks like so we'll do our pl plot
and again this is all against x x our x value but this time we have y y
down and let's do something a little fun with this we can put in a k
dash dash that just tells it to make it a dotted line
and if we're going to do the down one we also want to do the up one
so here's our yy up and when we run that it
both sets a line and so here's our support and this is what you expect you expect these two
lines to go through the nearest data point so the dashed lines go through the nearest muffin and the nearest cupcake
when it's plotting it and then your svm goes right down the middle so it gives it a nice split in our data and you can
see how easy it is to see based just on sugar and flour which one's a muffin or a cupcake
let's go ahead and create a function to predict
muffin or cupcake i've got my recipes i pulled off the
internet and i want to see the difference between a muffin or a cupcake and so we need a
function to push that through and uh create a function with def and let's call it muffin or cupcake and
remember we're just doing flour and sugar today we're not doing all the ingredients and that actually is a pretty good split you really don't need
all the ingredients you know it's flour and sugar and let's go ahead and do an if else
statement so if model predict
is of flour and sugar equals zero so we take our model and we do it run a
predict it's very common in sk learn we have a dot predict you put the data in
and it's going to return a value in this case if it equals zero then print you're looking at a muffin recipe
else if it's not zero that means it's one then you're looking at a cupcake recipe
that's pretty straightforward for a function or def for definition d e f is
how you do that in python and of course we're going to create a function you should run something in it and so let's run a cupcake and we're
going to send it values 50 and 20 a muffin or cupcake i don't know what it is and let's run this and just see what it
gives us and it says oh it's a muffin you're looking at a muffin recipe so it very easily predicts whether we're
looking at a muffin or a cupcake recipe let's plot this
there we go plot this on the graph so we can see what that actually looks like and i'm just going to copy and paste it
from below where we're plotting all the points in there so this is nothing different than we did before if i run it
you'll see it has all the points and the lines on there and what we want to do is we want to add another point
and we'll do plt plot and if you remember correctly we did for
our test we did 50 and 20. and then somebody went in here and
decided we'll do a y-o for yellow or it's kind of a oranges yellow color is going to come out marker size nine those
are settings you can play with somebody else played with them to come up with the right setup so it looks good and you
can see there it is graphed clearly a muffin in this case in cupcakes versus muffins
the muffin has won and if you'd like to do your own muffin cupcake
contender series you certainly can send a note down below and the team at simply
learn will send you over the data they use for the muffin and cupcake and that's true of any of the data we didn't
actually run a plot on it earlier we had men versus women you can also request that information to
run it on your data setup so you can test that out so to go back over our setup we went
ahead for our support vector machine code we did a predict 40 parts flower 20
parts sugar i think was different than the one we did whether it's a muffin or a cupcake hence we have built a
classifier using svm which is able to classify if a recipe is of a cupcake or
a muffin which wraps up our cupcake versus muffin today in our second tutorial we're going to cover k-means
and linear regression along with going over the quiz questions we had during our first tutorial
what's in it for you we're going to cover clustering what is clustering k
means clustering which is one of the most common use clustering tools out there including a flowchart to
understand k-means clustering and how it functions and then we'll do an actual python live demo on clustering of cars
based on brands then we're going to cover logistic regression what is logistic regression
logistic regression curve and sigmoid function and then we'll do another python code demo to classify a tumor as
malignant or benign based on features and let's start with clustering suppose
we have a pile of books of different genres now we divide them into different groups like fiction horror education and
as we can see from this young lady she definitely is into heavy horror you can just tell by those eyes and the maple
canadian leaf on her shirt but we have fiction horror and education and we want to go ahead and divide our books up well
organizing objects into groups based on similarity is clustering and in this
case as we're looking at the books we're talking about clustering things with known categories but you can also use it
to explore data so you might not know the categories you just know that you need to divide it up in some way to
conquer the data and to organize it better but in this case we're going to be looking at clustering in specific
categories and let's just take a deeper look at that we're going to use k means clustering k means clustering is
probably the most commonly used clustering tool in the machine learning library k-means clustering is an example
of unsupervised learning if you remember from our previous thing it is used when
you have unlabeled data so we don't know the answer yet we have a bunch of data that we want to cluster to different
groups define clusters in the data based on feature similarity so we've
introduced a couple terms here we've already talked about unsupervised learning and unlabeled data so we don't
know the answer yet we're just going to group stuff together and see if we can find an answer of how things connect
we've also introduced feature similarity features being different features of the data now with books we can easily see
fiction and horror and history books but a lot of times with data some of that information
isn't so easy to see right when we first look at it and so k-means is one of those tools where we can start finding
things that connect that match with each other suppose we have these data points and want to assign them into a cluster
now when i look at these data points i would probably group them into two clusters just by looking at them i'd say
two of these group of data kind of come together but in k means we pick k clusters and assign random centroids to
clusters where the k clusters represents two different clusters we pick k clusters and cyrano centroids to the
clusters then we compute distance from objects to the centroids now we form new
clusters based on minimum distances and calculate the centroids so we figure
out what the best distance is for the centroid then we move the centroid and recalculate those distances repeat
previous two steps iteratively till the cluster centroid stopped changing their positions and become static
repeat previous two steps iteratively till the cluster centroid stopped changing and the positions become static
once the clusters become static then k-means clustering algorithm is said to be converged and there's another term we
see throughout machine learning is converged that means whatever math we're using to figure out the answer has come
to a solution or it's converged on an answer should we see the flowchart to understand make a little bit more sense
by putting it into a nice easy step-by-step so we start we choose k
we'll look at the elbow method in just a moment we assign random centroids to clusters and sometimes you pick the
centroids because you might look at the data in a graph and say oh these are probably the central points then we
compute the distance from the objects to the centroids we take that and we form new clusters
based on minimum distance and calculate their centroids then we compute the distance from objects to the new
centroids and then we go back and repeat those last two steps we calculate the distances so as we're doing it brings
into the new centroid and then we move the centroid around and we figure out what the best which objects are closest
to each centroid so the objects can switch from one centroid to the other the centroids are moved around and we
continue that until it is converged let's see an example of this suppose we have this data set of seven
individuals and their score on two topics a and b so here's our subject
in this case referring to the person taking the test and then we have subject a where we see
what they've scored on their first subject and we have subject b and we can see what they score on the second subject now let's take two farthest
apart points as initials cluster centroids remember we talked about selecting them randomly or we can also
just put them in different points and pick the furthest one apart so they move together either one works okay depending
on what kind of data you're working on and what you know about it so we took the two furthest points one
and one and five and seven and now let's take the two farthest apart points as
initial cluster centroids each point is then assigned to the closest cluster with respect to the distance from the
centroids so we take each one of these points in there we measure that distance and you can see that if we measured each
of those distances and you use it the pythagorean theorem for a triangle in this case because you know the x and the
y and you can figure out the diagonal line from that or you just take a ruler and put it on your monitor that'd be
kind of silly but it would work if you're just eyeballing it you can see how they naturally come together in
certain areas now we again calculate the centroids of each cluster so cluster one
and then cluster two and we look at each individual dot there's one two three
we're in one cluster uh the centroid then moves over it becomes 1.8 comma 2.3
so remember was that one and one well the very center of the data we're looking at would put it at the one point
roughly 2 2 but 1.8 and 2.3 and the second one if we wanted to make the
overall mean vector the average vector of all the different distances to that centroid we come up with 4 comma 1 and 5
4. so we've now moved the centroids we compare each individual's distance to its own cluster mean and to that of the
opposite cluster and we find build a nice chart on here that the as we move the centroid around we now have
a new different kind of clustering of groups and using euclidean distance between the points and the mean we get
the same formula you see new formulas coming up so we have our individual dots distance to the mean centroid of the
cluster and distance to the mean centroid of the cluster only individual 3 is nearer to the mean
of the opposite cluster cluster two then its own cluster one and you can see here
in the diagram where we've kind of circled that one in the middle so when we've moved the cluster the centroids of
the clusters over one of the points shifted to the other cluster because it's closer to that group of individuals
thus individual 3 is relocated to cluster 2 resulting in a new partition and we regenerate all those numbers of
how close they are to the different clusters for the new clusters we will find the actual cluster centroids so now we move
the centroids over and you can see that we've now formed two very distinct clusters on here on comparing the
distance of each individual's distance to its own cluster mean and to that of the opposite cluster we find that the
data points are stable hence we have our final clusters now if you remember i brought up a concept earlier k-mean on
the k-means algorithm choosing the right value of k will help in less number of iterations and to find the appropriate
number of clusters in a data set we use the elbow method and within sum of
squares wss is defined as the sum of the squared distance between each member of
the cluster in its centroid and so you see we've done here is we have the number of clusters and as you do
the same k-means algorithm over the different clusters and you calculate what that centroid looks like and you
find the optimal you can actually find the optimal number of clusters using the elbow the graph is called as the elbow
method and on this we guessed it two just by looking at the data but as you can see
the slope you actually just look for right there where the elbow is in the slope and you have a clear answer that we want two different to start with k
means equals two a lot of times people end up computing k-means equals two three four five until they find the
value which fits on the elbow joint sometimes you can just look at the data and if you're really good with that
specific domain remember domain i mentioned that last time you'll know that where to pick those numbers or
where to start guessing at what that k value is so let's take this and we're going to use a use case using k-means clustering
to cluster cars into brands using parameters such as horsepower cubic inches make year etc so we're going to
use the data set cars data having information about three brands of cars toyota honda and nissan we'll go back to
my favorite tool the anaconda navigator with the jupiter notebook and let's go
ahead and flip over to our jupiter notebook and in our jupiter notebook i'm going to go ahead and just paste the
basic code that we usually start a lot of these off with we're not going to go too much into this code because we've
already discussed numpy we've already discussed matplot library and pandas that'll be being the number array pandas
being the pandas dataframe and map plot for the graphing and don't forget since if you're using the jupiter notebook you
do need the matplot library inline so that it plots everything on the screen if you're using a different python
editor then you probably don't need that because it'll have a pop-up window on your computer we'll go ahead and run
this just to load our libraries and our setup into here the next step is of course to look at our data which i've
already opened up in a spreadsheet and you can see here we have the miles per gallon cylinders cubic inches horsepower
weight pounds how heavy it is time it takes to get to 60 my card is probably on this one at about
80 or 90. what year it is so this is you can actually see this is kind of older cars and then the brand toyota honda
nissan so the different cars are coming from all the way from 1971 if we scroll down
to the 80s we have between the 70s and 80s the number of cars that they've put out and let's uh we come back here we're
going to importing the data so we'll go ahead and do data set equals and we'll
use pandas to read this in and it's uh from a csv file remember you can always post this in the comments and request
the data files for these either in the comments here on the youtube video or go to simplylearn.com and request that the
car csv i put it in the same folder as the code that i've stored so my python
code is stored in the same folder so i don't have to put the full path if you store them in different folders you do have to change this and double check
your name variables and we'll go ahead and run this and we've chosen data set arbitrarily because it's a data set
we're importing and we've now imported our car csv into the data set as you know you have to prep the data so
we're going to create the x data this is the one that we're going to try to figure out what's going on with and then
there's a number of ways to do this but we'll do it in a simple loop so you can actually see what's going on so we'll do
for i and x dot columns so we're going to go through each of the columns
and a lot of times it's important i'll make lists of the columns and do this because i might remove certain columns
or there might be columns that i want to be processed differently but for this we can go ahead and take x of i
and we want to go fill in a and that's a pandas command but the question is what are we going to fill the missing data
with we definitely don't want to just put in a number that doesn't actually mean something and so one of the tricks
you can do with this is we can take x of i and in addition to that we want to go
ahead and turn this into an integer because a lot of these are integers so we'll go ahead and keep it integers and let me add the bracket here
and a lot of editors will do this they'll think that you're closing one bracket make sure you get that second bracket in there if it's a double
bracket that's always something that happens regularly so once we have our integer of x of i this is going to fill
in any missing data with the average and i was so busy closing one set of brackets i forgot that the mean is also
has brackets in there for the pandas so we can see here we're going to fill in all the data with the average value for that column so if there's missing data
it's in the average of the data it does have then once we've done that we'll go ahead and loop through it again
and just check and see to make sure everything is filled in correctly and we'll print and then we
take x is null and this returns a set of the null value or the how many lines are
null and we'll just sum that up to see what that looks like and so when i run this and so with the x what we want to do is
we want to remove the last column because that had the models that's what we're trying to see if we can cluster
these things and figure out the models there is so many different ways to sort the x out for one we could take the x
and we could go data set our variable we're using and use the i location one
of the features that's in pandas and we could take that and then take all
the rows and all but the last column of the data set
and at this time we could do values we just converted to values so that's one way to do this and if i let me just put
this down here and print x it's a capital x we chose and i run this
you can see it's just the values we could also take out the values and it's not going to return anything
because there's no values connected to it what i like to do with this is instead of doing the ilocation which
does integers more common is to come in here and we have our data set and we're
going to do data set dot or data set dot columns and remember
that lists all the columns so if i come in here let me just mark that as red
and i print data set dot columns
you can see that i have my index here i have my mpg cylinders everything including the brand which we don't want
so the way to get rid of the brand would be to do data columns of everything but
the last one minus one so now if i print this you'll see the brand disappears
and so i can actually just take data set columns minus one
and i'll put it right in here for the columns we're going to look at and let's un mark this
and unmark this and now if i do an x dot head
i have a new data frame and you can see right here we have all the different columns except for the brand at the end
of the year and it turns out when you start playing with the data set you're going to get an
error later on and it'll say cannot convert string to float value and that's
because if for some reason these things the way they recorded them must been recorded as strings so we have a neat
feature in here on pandas to convert and it is simply convert objects
and for this we're going to do convert oops convert underscore
numeric numeric equals true and yes i did have to go look that
up i don't have it memorized the convert numeric in there if i'm working with a lot of these things i remember them but
depending on where i'm at what i'm doing i usually have to look it up and we run that oops i must have missed something
in here let me double check my spelling and when i double check my spelling you'll see i missed the first underscore
in the convert objects when i run this it now has everything converted
into a numeric value because that's what we're going to be working with is numeric values down here
and the next part is that we need to go through the data and eliminate null values
most people when they're doing small amounts working with small data pools discover afterwards that they have a null value and they have to go back and
do this so you know be aware whenever we're formatting this data things are going to
pop up and sometimes you go backwards to fix it and that's fine that's just part of
exploring the data and understanding what you have and i should have done this earlier but let me go ahead and increase the size of
my window one notch there we go
easier to see so we'll do 4i in working with x dot
columns we'll page through all the columns and we want to take x of i and we're going to change that
we're going to alter it and so with this we want to go ahead and fill in
x of i pandas has the fill in a and that just fills in any non-existent
missing data then we'll put my brackets up and there's a lot of different ways to fill
this data if you have a really large data set some people just void out that data because
and then look at it later in a separate exploration of data one of the tricks we can do is we can
take our column and we can find the means and the means is in other are quotation
marks so we take the columns we're going to fill in the non-existing one with the means the problem is that returns a
decimal float so some of these aren't decimals certainly let me be a little careful of
doing this but for this example we're just going to fill it in with the integer version of this keeps it on par
with the other data that isn't a decimal point
and then what we also want to do is we want to double check a lot of times you do this first part
first to double check then you do the fill and then you do it again just to make sure you did it right so we're going to go through and test
for missing data and one of the ways you can do that is simply go in here
and take our x of i column so it's going to go through the x of i column it says
is null so it's going to return any any place there's a null value it actually goes through all the rows of each column
is null and then we want to go ahead and sum that so we take that and we add the sum value and these are all pandas so is
null is a panda command and so is sum and if we go through that we go ahead and run it
and we go ahead and take and run that you'll see that all the columns have zero null values so we've now tested and
double checked and our data is nice and clean we have no null values everything is now a number value we turned it into
numeric and we've removed the last column in our data and at this point we're actually going
to start using the elbow method to find the optimal number of clusters so we're now
actually getting into the sk learn part the k means clustering on here
i guess we'll go ahead and zoom it up one more not so you can see what i'm typing in here
and then from sk learn we're going to our sk learn
cluster we're going to import k means
i always forget to capitalize the k and the m when i do this so it's capital k capital m k means
and we'll go and create a array wcss equals when we get an empty
array if you remember from the elbow method from our slide
within the sums of squares wss is defined as the sum of square distance
between each member of the cluster and its centroid so we're looking at that change in differences as far as a square
distance and we're going to run this over a number of k-mean values
in fact let's go for i in range we'll do 11 of them
rain 0 11 and the first thing we're going to do is we're going to create the actual
[Music] all lower case
and so we're going to create this object from the k-means that we just imported
and the variable that we want to put into this is in clusters we're going to set that equals
to i that's the most important one because we're looking at how increasing the number of clusters
changes our answer there are a lot of settings to the k means
our guys in the back did a great job just kind of playing with some of them the most common ones that you see in a
lot of stuff is how you init your k-means so we have k-means plus plus
this is just a tool to let the model itself be smart how it picks the centroids to start with its initial
centroids we only want to iterate no more than 300 times we have a max iteration we put in there
we have the infinite the random state equals zero you really don't need to worry too much about these when you're
first learning this as you start digging in deeper you start finding that these are shortcuts that will speed up the
process as far as the setup but the big one that we're working with is the in clusters
equals i so we're going to literally train our k means 11 times we're going
to do this process 11 times and if you're working with big data you
know the first thing you do is you run a small sample of the data so you can test all your stuff on it and you can already
see the problem that if i'm going to iterate through a terabyte of data 11 times and then the k-means itself is
iterating through the data multiple times that's a heck of a process so you got to be a little careful with
this a lot of times though you can find your elbow using the elbow method find your optimal number on a sample of data
especially if you're working with larger data sources so we want to go ahead and take our k-means and we're just going to fit it
if you're looking at any of the sklearn very common you fit your model and if you remember correctly our variable
we're using is the capital x and once we fit this value we go back to
the array we made we want to go and just depend that value on the end
and it's not the actual fit we're pinning in there it's when it generates it it generates the value you're looking
for is inertia so k means dot inertia will pull that specific value out that
we need and let's get a visual on this we'll do our plt plot
and what we're plotting here is first the x axis which is range 0
11 so that will generate a nice little plot there and the wcss for our y-axis
it's always nice to give our plot a title and let's see we'll just give it the
elbow method for the title and let's get some labels so let's go ahead and do plt
x label and we'll do a number of clusters for that
and plt y label and for that we can do oops there we go
wcss since that's what we're doing on the plot on there and finally we want to go ahead and display our graph which is
simply plt dot oops dot show there we go and because we have
it set to inline it'll appear in line hopefully i didn't make a type error on there
and you can see we get a very nice graph you can see a very nice elbow joint there at 2 and again right around three and four
and then after that there's not very much now as a data scientist if i was looking
at this i would do either three or four and i'd actually try both of them to see what the
output look like and they've already tried this in the back so we're just going to use three as a setup on here
and let's go ahead and see what that looks like when we actually use this to show the different kinds of cars
and so let's go ahead and apply the k-means to the car's data set and basically we're going to copy the
code that we loop through up above where k-means equals k-means number of clusters and we're just going to set the
number of clusters to 3. since that's what we're going to look for and you can do 3 and 4 on this and
graph them just to see how they come up differently be kind of curious to look at that but for this we're just going to set it to 3. go ahead and create our own
variable y k means for our answers and we're going to set that equal to
whoops a double equal there to k means but we're not going to do a fit we're
going to do a fit predict is the setup you want to use and when you're using untrained models you'll see a slightly
different cause usually you see fit and then you see just the predict but we want to both fit and predict the k-means
on this and that's fit underscore predict and then our capital x is the data we're working with
and before we plot this data we're going to do a little pandas trick we're going to take our x value and we're going to
set x as matrix so we're converting this into a nice rows and columns kind of set up but we
want the we're going to have columns equals none so it's just going to be a matrix of data in here
and let's go ahead and run that a little warning you'll see these warnings pop up because things are
always being updated so there's like minor changes in the versions and future versions instead of matrix now that it's
more common to set it dot values instead of doing as matrix but math matrix works
just fine for right now and you'll want to update that later on but let's go ahead and dive in and plot this and see
what that looks like and before we dive into plotting this data i always like to
take a look and see what i am plotting so let's take a look at y k means i'm just going to print that out
down here and we see we have an array of answers we have 2 1 0 2 1 2
so it's clustering these different rows of data based on the three different spaces it
thinks it's going to be and then let's go ahead and print x and see what we have for x and we'll see
that x is an array it's a matrix so we have our different values in the array
and what we're going to do it's very hard to plot all the different values in the array so we're only going to be
looking at the first two or positions zero and one
and if you were doing a full presentation in front of the board meeting
you might actually do a little different than and dig a little deeper into the different aspects because this is all
the different columns we looked at but we look at columns one and two for this to make it easy
so let's go ahead and clear this data out of here and let's bring up our plot and we're going to do a scatter plot
here so plt scatter and this looks a little complicated so let's
explain what's going on with this we're going to take the x values
and we're only interested in y of k means equals zero the first cluster
okay and then we're going to take value zero for the x axis and then we're gonna do the same thing here
we're only interested in k means equals zero but we're going to take the second column so we're looking
at the first two columns in our answer or in the data and then the guys in the back played
with this a little bit to make it pretty and they discovered that it looks good with as a size equals 100 that's the
size of the dots we're going to use red for this one and when they were looking at the data and
what came out it was definitely the toyota on this we're just going to go ahead and label it toyota again that's
something you really have to explore in here as far as playing with those numbers and see what looks good we'll go
ahead and hit enter in there and i'm just going to paste in the next two lines which is the next two cars
and this is our nissa and honda and you'll see with our scatter plot we're now looking at where y underscore
k means equals one and we want the zero column and y k means equals two again we're looking at
just the first two columns zero and one and each of these rows then corresponds to nissan and honda
and i'll go ahead and hit enter on there and finally let's take a look and put the centroids on there
again we're going to do a scatter plot and on the centroids you can just pull
that from our k-means the model we created dot cluster centers
and we're going to just do all of them in the first number and all
of them and the second number which is 0 1 because you always start with zero and one
and then they were playing with the size and everything to make it look good we'll do a size of 300 we're going to
make the color yellow and we'll label them it's always good to have some good labels
centroids and then we do want to do a title plt title
and pop up there plt title cause you always want to make your graphs look pretty and we'll call
it clusters of car make and one of the features of the
plot library is you can add a legend it'll automatically bring in it
since we've already labeled the different aspects of the legend with toyota nissan and honda
and finally we want to go ahead and show so we can actually see it and remember it's inline so if you're using a
different editor that's not the jupiter notebook you'll get a pop-up of this and you should have a nice set of
clusters here so we can look at this we have a clusters of honda and green toyota and red
nissan and purple and you can see where they put the centroids to separate them
now when we're looking at this we can also plot a lot of other different data on here as far because we only looked at
the first two columns this is just column one and two or zero one as you label them in computer scripting but you
can see here we have nice clusters of car make and we were able to pull out the data and you can see how just these
two columns form very distinct clusters of data so if you were exploring new data you
might take a look and say well what makes these different almost going in reverse you start looking at the data and pulling apart
the columns to find out why is the first group set up the way it is maybe you're doing loans and you want
to go why is this group not defaulting on their loans and why is the last group defaulting on their loans and why is the
middle group 50 percent defaulting on their bank loans and you start finding ways to
manipulate the data and pull out the answers you want
so now that you've seen how to use k-mean for clustering let's move on to the next topic
now let's look into logistic regression the logistic regression algorithm is the
simplest classification algorithm used for binary or multi-classification problems and we can see we have our
little girl from canada who's into horror books is back that's actually really scary when you think about that with those big eyes in the previous
tutorial we learned about linear regression dependent and independent variables so to brush up y equals mx
plus c very basic algebraic function of y and x the dependent variable is the
target class variable we are going to predict the independent variables x1 all the way
up to xn are the features or attributes we're going to use to predict the target class we know what a linear regression
looks like but using the graph we cannot divide the outcome into categories it's really hard to categorize 1.5 3.6
9.8 for example a linear regression graph can tell us that with increase in number
of hours studied the marks of a student will increase but it will not tell us whether the student will pass or not in
such cases where we need the output as categorical value we will use logistic regression and for that we're going to
use the sigmoid function so you can see here we have our marks 0 to 100 number
of hours studied that's going to be what they're comparing it to in this example and we usually form a line that says y
equals mx plus c and when we use the sigmoid function we have p equals 1 over
1 plus e to the minus y it generates a sigmoid curve and so you
can see right here when you take the ln which is the natural logarithm i always
thought it should be nl not ln that's just the inverse of e your e to the
minus y and so we do this we get ln of p over 1 minus p equals m times x plus c
that's the sigmoid curve function we're looking for and we can zoom in on the function and you'll see that the
function as it derives goes to 1 or to 0 depending on what your x value is and
the probability if it's greater than 0.5 the value is automatically rounded off to 1 indicating that the student will
pass so if they're doing a certain amount of studying they will probably pass then you have a threshold value at
the 0.5 it automatically puts that right in the middle usually and your probability if it's less than 0.5 the
value rendered off to zero indicating the student will fail so if they're not studying very hard they're probably
going to fail this of course is ignoring the outliers or that one student is just a natural genius and doesn't need any
studying to memorize everything that's not me unfortunately i have to study hard to learn new stuff
problem statement to classify whether a tumor is malignant or benign and this is
actually one of my favorite data sets to play with because it has so many features and when you look at them you
really are hard to understand you can't just look at them and know the answer so it gives you a chance to kind of dive
into what data looks like when you aren't able to understand the specific domain of the data but i also want you
to remind you that in the domain of medicine if i told you that my probability
was really good it classified things that say 90 or 95 and i'm classifying whether you're going
to have a malignant or a b9 tumor i'm guessing that you're going to go get it tested anyways so you got to remember
the domain we're working with so why would you want to do that if you know you're just going to go get a biopsy
because you know it's that serious this is like an all or nothing just referencing the domain it's important it
might help the doctor know where to look just by understanding what kind of tumor
it is so it might help them or aid them and something they missed from before so let's go ahead and dive into the code
and i'll come back to the domain part of it in just a minute so use case and we're going to do our normal imports
here where we're importing numpy pandas seaborn the matplot library and we're
going to do matplot library inline since i'm going to switch over to anaconda so let's go ahead and flip over there and
get this started so i've opened up a new window in my anaconda jupiter notebook
and by the way jupiter notebook you don't have to use anaconda for the jupiter notebook i just love the interface and all the tools and anaconda
brings so we got our import numpy as in p for our numpy number array we have our
pandas pd we're going to bring in seaborn to help us with our graphs as sns
so many really nice tools in both seaborne and matplot library and we'll do our matplot library.piplot as plt and
then of course we want to let it know to do it in line and let's go and just run that so it's all set up
and we're just going to call our data data not creative today equals pd and this happens to be in a
csv file so we'll use the pd.read underscore csv
and i happen to name the file i renamed it data4p2.csv you can of course write in
the comments below the youtube and request for the dataset itself or go to the simplylearn website and we'll be happy to supply that for you
and let's just open up the data before we go any further and let's just see what it looks like in a spreadsheet
so when i pop it open in a local spreadsheet this is just a csv file comma separated variables we have an id
so i guess the categorize this for reference or what id which test was done the diagnosis m for
malignant b for b9 so there's two different options on there and that's what we're going to try to predict is
the m and b and test it and then we have like the radius mean or average the texture average
perimeter mean area mean smoothness i don't know about you but unless you're a
doctor in the field most of the stuff i mean you can guess what concave means just by the term concave but i really
wouldn't know what that means in the measurements they're taking so they have all kinds of stuff like how smooth it is
uh the symmetry and these are all float values we just page through them real quick and you'll see there's i believe
36 if i remember correctly in this one so there's a lot of different values
they take and all these measurements they take when they go in there and they take a look at the different growth the tumorous growth
so back in our data and i put this in the same folder as a code so i saved
this code in that folder obviously if you have it in a different location you want to put the full path in there and
we'll just do pandas first five lines of data with the data
dot head when we run that we can see that we have pretty much what we just looked at we
have an id we have a diagnosis if we go all the way across you'll see all the different columns coming across
displayed nicely for our data and while we're exploring the data our
seaborn which we referenced as sns makes it very easy to go in here and do
a joint plot you'll notice that very similar to because it is sitting on top
of the plot library so the joint plot does a lot of work for us and we're just going to look at the first two
columns that we're interested in the radius mean and the texture mean we'll just look at those two columns
and data equals data so that tells it which two columns we're plotting and then we're going to use the data that we
pulled in let's just run that and it generates a really nice graph on here
and there's all kinds of cool things on this graph to look at i mean we have the texture mean and the radius mean obviously the axes you can also see
and one of the cool things on here is you can also see the histogram they show that for the radius mean whereas the
most commons radius mean come up and where the most common texture is so we're looking at the tech the
on each growth its average texture and on each radius its average
radius on there gets a little confusing because we're talking about the individual objects average and then we
can also look over here and see the histogram showing us the median or how
common each measurement is and that's only two columns so let's dig
a little deeper into seaborn they also have a heat map and if you're not familiar with heat maps a heat map just
means it's in color that's all that means heat map i guess the original ones were plotting heat density on something
and so ever since then it's just called a heat map and we're going to take our data and get our corresponding numbers to put
that into the heat map and that's simply data dot c-o-r-r for that that's a pandas expression
let's remember we're working in a pandas data frame so that's one of the cool tools in pandas for our data and this is
pull that information into a heat map and see what that looks like and you'll see that we're now looking at
all the different features we have our id we have our texture we have our area our compactness concave points
and if you look down the middle of this chart diagonal going from the upper left to bottom right
it's all white that's because when you compare texture to texture they're identical so they're 100 percent or in
this case a perfect one in their correspondence and you'll see that when you look at say
area or right below it it has almost a black on there when you compare it to texture so these have almost no
corresponding data they don't really form a linear graph or something that you can look at and say how connected they are they're very scattered data
this is really just a really nice graph to get a quick look at your data doesn't so much change what you do but it
changes verifying so when you get an answer or something like that or you start looking at some of these individual pieces you might go hey that
doesn't match according to showing our heat map this should not correlate with
each other and if it is you're going to have to start asking well why what's going on what else is coming in there
but it does show some really cool information on here and we can see from the id
there's no real one feature that just says if you go across the top line that
lights up there's no one feature that says hey if the area is a certain size then it's going to be b9 or malignant it
says there's some that sort of add up and that's a big hint in the data that we're trying to id this whether it's
malignant or b9 that's a big hint to us as data scientists to go okay we can't solve this with any one feature
it's going to be something that includes all the features or many of the different features to come up with the solution for it and while we're
exploring the data let's explore one more area and let's look at data dot is
null we want to check for null values in our data if you remember from earlier in
this tutorial we did it a little differently where we added stuff up and summed them up you can actually with
pandas do it really quickly data dot is null and summit and it's going to go across all the columns so when i run
this you're going to see all the columns come up with no null data
so we've just just to re hash these last few steps we've done a lot of exploration we have
looked at the first two columns and seeing how they plot with the seaborn with a joint plot which shows
both the histogram and the data plotted on the xy coordinates and obviously you can do that more in
detail with different columns and see how they plot together and then we took and did the seaborne
heat map the sns dot heat map of the data and you can see right here where it did a nice job
showing us some bright spots where stuff correlates with each other and forms a very nice combination or points of
scattering points and you can also see areas that don't and then finally we went ahead and
checked the data is the data null value do we have any missing data in there very important step because it'll crash
later on if you forget to do this step it will remind you when you get that nice error
code that says no values okay so not a big deal if you miss it but it's no fun having to go back when
you're you're in a huge process and you've missed this step and now you're 10 steps later and you've got to go
remember where you were pulling the data in so we need to go ahead and pull out our
x and our y so we just put that down here and we'll set the x equal to
and there's a lot of different options here certainly we could do x equals all the columns except for the first two
because if you remember the first two is the id and the diagnosis so that certainly would be an option
but we're going to do is we're actually going to focus on the worst the worst radius the worst texture parameter area
smoothness compactness and so on one of the reasons to start dividing your data
up when you're looking at this information is sometimes the data will be the same data
coming in so if i have two measurements coming in to my model it might overweigh
them it might overpower the other measurements because it's measuring it's basically taking that information in
twice that's a little bit past the scope of this tutorial i want you to take away from this though is that we are dividing
the data up into pieces and our team in the back went ahead and said hey let's just look at the worst so i'm going to
create a an array and you'll see this array radius worst
texture worse perimeter worst we've just taken the worst of the worst and i'm just going to put that in my x so this x
is still a pandas data frame but it's just those columns and our y if you
remember correctly is going to be oops hold on one second it's not x it's data
there we go so x equals data and then it's a list of the different columns the worst of the worst and if we're going to
take that then we have to have our answer for our y for the stuff we know and if you remember correctly we're just
going to be looking at the diagnosis that's all we care about is what is it diagnosed is it b9 or
malignant and since it's a single column we can just do diagnosis oh i forgot to put the
brackets or the there we go okay so it's just diagnosis on there and we can also
real quickly do like x dot head if you want to see what that looks like and y dot head
and run this and you'll see it only does the last one i forgot about that if you don't do print you can see
that the the y dot head is just mmm because the first ones are all malignant and if i run this the x dot head is just
the first five values of radius worse texture worse parameter worst area worst and so on
i'll go ahead and take that out so moving down to the next step we've built our two
data sets our answer and then the features we want to look at in data science it's very important to
test your model so we do that by splitting the data
and from sklearn model selection we're going to import train test split so
we're going to split it into two groups there are so many ways to do this i noticed in one of the more modern ways
to actually split it into three groups and then you model each group and test it against the other groups so you have
all kinds and there's reasons for that which is past the scope of this and for this particular example isn't necessary
for this we're just going to split it into two groups one to train our data and one to test our data and the
sklearn dot model selection we have train test split you could write your own quick
code to do this we just randomly divide the data up into two groups but they do it for us nicely
and we actually can almost we can actually do it in one statement with this where we're going to generate four variables
capital x train capital x test so we have our training data we're going to
use to fit the model and then we need something to test it and then we have our y train so we're going to train the answer and then we have our test so this
is the stuff we want to see how good it did on our model and we'll go ahead and take our train
test split that we just imported and we're going to do x and our y are
two different data that's going in for our split and then the guys in the back came up
and wanted us to go ahead and use a test size equals 0.3 that's test underscore
size random state it's always nice to kind of switch your random state around but not that important
what this means is that the test size is we're going to take 30 percent of the data and we're going to put that into
our test variables our y test and our x test and we're going to do 70 into the x
train and the y train so we're going to use 70 of the data to train our model and 30 to test it
let's go ahead and run that and load those up so now we have all our stuff split up and all our data ready to go and now we
get to the actual logistics part we're actually going to do our create our model so let's go ahead and bring that
in from sklearn we're going to bring in our linear model and we're going to import logistic regression that's the
actual model we're using and this we'll call it log model stereo model and let's just set this
equal to our logistic regression that we just imported so now we have a variable log model set
to that class for us to use and with most the models in the sk learn we just need to
go ahead and fix it fit do a fit on there and we use our x train
that we separated out with our y train and let's go ahead and run this so once we've run this we'll have a model
that fits this data that 70 percent of our training data
and of course it prints this out that tells us all the different variables that you can set on there there's a lot of different choices you can make but
for word though we're just going to let all the defaults sit we don't really need to mess with those on this particular example there's nothing in
here that really stands out as super important until you start fine tuning it
but for what we're doing the basics will work just fine and then let's we need to go ahead and test out our model is it
working so let's create a variable y predict and this is going to be equal to
our log model and we want to do a predict again very
standard format for the sk learn library is taking your model and doing a predict on it and we're going to test why
predict against the y test so we want to know what the model thinks it's going to be that's what our y predict is and with
that we want the capital x x test so we have our train set and our test
set and now we're going to do our y predict and let's go ahead and run that
and if we uh print y predict
let me go ahead and run that you'll see it comes up and it presents and i prints a nice array of
b and m for b9 and malignant for all the different test data we put
in there so it does pretty good we're not sure exactly how good it does but we can see that it actually works and is functional
was very easy to create you'll always discover with our data science that as you explore this
you spend a significant amount of time prepping your data and making sure your data coming in is
good there's a saying good data in good answers out bad data
in bad answers out that's only half the thing that's only half of it
selecting your models becomes the next part as far as how good your models are and then of course fine-tuning it
depending on what model you're using so we come in here we want to know how good this came out so we have our y
predictor log model.predict x test
so for deciding how good our model is we're going to go from the
sklearn.metrics we're going to import classification report and that just reports how good our model
is doing and then we're going to feed it the model data let's just print this out and we'll take our classification report
and we're going to put into there our test our actual data so this is what
we actually know is true and our prediction what our model predicted for that data on the test side
and let's run that and see what that does so we pull that up you'll see that we
have a precision for b9 and malignant b and m
and we have a precision of 93 at 91 a total of 92 so it's kind of the average
between these two 92 there's all kinds of different information on here your f1 score
your recall your support coming through on this and for this i'll go ahead and just flip
back to our slides that they put together for describing it and so here we're going to look at the precision
using the classification report and you see this is the same printout i had up above some of the numbers might be
different because it does randomly pick out which data we're using so this model is able to predict the type of tumor
with 91 percent accuracy so we look back here let's you'll see
where we have uh b9 and england it actually is 92 coming up here but we're looking about a 92 91 precision and
remember i reminded you about domains and we're talking about the domain of a medical domain with a very catastrophic
outcome you know at 91 or 92 percent precision you're still going to go in there and have somebody do a biopsy on
it very different than if you're investing money and there's a 92 percent chance you're going to earn 10 percent
and 8 chance you're going to lose 8 percent you're probably gonna bet the money because at that odds it's pretty
good that you'll make some money and in the long run you do that enough you definitely will make money and also with
this domain i've actually seen them use this to identify different forms of cancer that's one of the things that
they're starting to use these models for because then it helps the doctor know what to investigate so that wraps up
this section we're finally we're going to go in there and let's discuss the answer to the quiz asked in machine
learning tutorial part one can you tell what's happening in the following cases grouping documents into
different categories based on the topic and content of each document this is an
example of clustering where k-means clustering can be used to group the documents by topics using bag of words
approach so if you've gotten in there that you're looking for clustering and hopefully you had at least one or two
examples like k-means that are used for clustering different things then give yourself a two thumbs up
b identifying handwritten digits in images correctly this is an example of classification the
traditional approach to solving this would be to extract digit dependent features like curvature of different digits etc and then use a classifier
like svm to distinguish between images again if you got the fact that it's a classification example give yourself a
thumb up and if you're able to go hey let's use svm or another model for this give yourself those two thumbs up on it
c behavior of a website indicating that the site is not working as designed this
is an example of anomaly detection in this case the algorithm learns what is normal and what is not normal usually by
observing the logs of the website give yourself a thumbs up if you got that one and just for a bonus can you think of
another example of anomaly detection one of the ones i use it for my own business is detecting anomalies in stock markets
stock markets are very fickled and they behave very radical so finding those erratic areas and then finding ways to
track down why they're erratic was something released in social media was something released you can see where
knowing where that anomaly is can help you to figure out what the answer is to it in another area d predicting salary
of an individual based on his or her years of experience this is an example of regression this problem can be
mathematically defined as a function between independent years of experience and dependent variables salary of an
individual and if you guess that this was a regression model give yourself a thumbs up and if you were able to
remember that it was between independent and dependent variables and that terms
give yourself two thumbs up summary so to wrap it up we went over what is k means and we went through also
the chart of choosing your elbow method and assigning a random centroid to the clusters computing the distance and then
going in there and figuring out what the minimum centroids is and computing the distance and going through that loop
until it gets the perfect centroid and we looked into the elbow method to choose k based on running our clusters
across a number of variables and finding the best location for that we did a nice example of clustering cars with k means
even though we only looked at the first two columns to make it simple and easy to graph you can easily extrapolate that
and look at all the different columns and see how they all fit together and we looked at what is logistic regression we
discussed the sigmoid function what is logistic regression and then we went into an example of classifying tumors
with logistics i hope you enjoyed part two of machine learning thank you for joining us today
for more information visit www.simplylearn.com again my name is richard kirschner a
member of the simply learn team get certified get ahead if you have any questions or comments feel free to write
those down below the youtube video or visit us at simplylearn.com we'll be happy to supply you with the data sets
or other information as requested
hi there if you like this video subscribe to the simply learn youtube channel and click here to watch similar
videos turn it up and get certified click here today we're going to cover the k nearest
neighbors allowed to refer to as k n n and k n is really a fundamental place to
start in the machine learning it's the basis of a lot of other things and just the logic behind it is easy to
understand and incorporated in other forms of machine learning so today what's in it for you why do we need k n
n what is k n how do we choose the factor k
when do we use k n n how does k n algorithm work and then
we'll dive in to my favorite part the use case predict whether a person will have diabetes or not that is a very
common and popular used data set as far as testing out models and learning how
to use the different models in machine learning by now we all know machine learning models make predictions by
learning from the past data available so we have our input values our machine learning model builds on those inputs of
what we already know and then we use that to create a predicted output is that a dog little kid looking over
there watching the black cat cross their path no dear you can differentiate between a cat and a dog based on their
characteristics cats cats have sharp claws uses to climb
smaller length of ears meows and purrs doesn't love to play around dogs they
have dull claws bigger length of ears barks loves to run around you usually don't see a cat running around people
although i do have a cat that does that where dogs do and we can look at these we can say we can evaluate the sharpness
of the claws how sharp brother claws and we can evaluate the length of the ears and we can usually sort out cats
from dogs based on even those two characteristics now tell me if it is a cat or a dog not
question usually little kids know cats and dogs by now unless they live a place where there's not many cats or dogs so
if we look at the sharpness of the claws the length of the ears and we can see that the cat has a smaller ears and
sharper claws than the other animals its features are more like cats it must be a
cat sharp claws length of ears and it goes in the cat group because knn is
based on feature similarity we can do classification using knn classifier so
we have our input value the picture of the black cat it goes into our trained model and it predicts that this is a cat
coming out so what is knn what is the k n algorithm
k nearest neighbors is what that stands for is one of the simplest supervised machine learning algorithms mostly used
for classification so we want to know is this a dog or is not a dog is it a cat
or not a cat he classifies a data point based on how his neighbors are classified knn stores all available
cases and classifies new cases based on a similarity measure and here we gone from cats and dogs right into wine
another favorite of mine k n stores all available cases and classifies new cases based on a similarity measure and here
you see we have a measurement of sulfur dioxide versus the chloride level and then the different wines they've tested
and where they fall on that graph based on how much sulfur dioxide and how much chloride k and k n is a perimeter that
refers to the number of nearest neighbors to include in the majority of the voting process and so if we add a
new glass of wine there red or white we want to know what the neighbors are in this case we're going to put k equals 5.
we'll talk about k in just a minute a data point is classified by the majority of votes from its five nearest neighbors
here the unknown point would be classified as red since four out of five neighbors are red so how do we choose k
how do we know k equals five i mean that's what's the value we put in there and so we're going to talk about it how do we choose a factor k k n algorithm is
based on feature similarity choosing the right value of k is a process called parameter tuning and is important for
better accuracy so at k equals three we can classify we have a question mark in the middle as either a as a square or
not is it a square or is it in this case a triangle and so if we set k equals to three we're going to look at the three
nearest neighbors we're going to say this is a square and if we put k equals a seven we classify as a triangle
depending on what the other data is around and you can see as the k changes depending on where that point is that
drastically changes your answer and we jump here we go how do we choose the factor of k you'll find this in all
machine learning choosing these factors that's the face you get he's like oh my gosh they choose the right k did i set
it right my values in whatever machine learning tool you're looking at so that you don't have a huge bias in one
direction or the other and in terms of k n n the number of k if you choose it too
low the bias is based on it's just two noises it's right next to a couple things and it's going to pick those
things and you might get a skewed answer and if your k is too big then it's going to take forever to process so you're
going to run into processing issues and resource issues so what we do the most common use and there's other options for
choosing k is to use the square root of n so it is a total number of values you
have you take the square root of it in most cases you also if it's an even number so if you're using uh like in
this case squares and triangles if it's even you want to make your k value odd that helps it select better so in other
words you're not going to have a balance between two different factors that are equal so usually take the square root of
n and if it's even you add one to it or subtract one from it and that's where you get the k value from that is the
most common use and it's pretty solid it works very well when do we use knn we
can use knn when data is labeled so you need a label on it we know we have a group of pictures with dogs dogs cats
cats data is noise free and so you can see here when we have a class and we
have like underweight 140 23 hello kitty normal that's pretty confusing we have a
high variety of data coming in so it's very noisy and that would cause an issue data set is small so we're usually
working with smaller data sets where you might get into a gig of data if it's really clean doesn't have a lot of noise
because k n is a lazy learner i.e it doesn't learn a discriminative function from the training set so it's very lazy
so if you have very complicated data and you have a large amount of it you're not going to use the knn but it's really
great to get a place to start even with large data you can sort out a small sample and get an idea of what that
looks like using the knn and also just using for smaller data sets k n works
really good how does a k n algorithm work consider a data set having two variables height in centimeters and
weight in kilograms and each point is classified as normal or underweight so
we see right here we have two variables you know true false they're either normal or they're not they're underweight on the basis of the given
data we have to classify the below set as normal or underweight using knn so if
we have new data coming in this says 57 kilograms and 177 centimeters is that
going to be normal or underweight to find the nearest neighbors we'll calculate the euclidean distance
according to the euclidean distance formula the distance between two points in the plane with the coordinates x y
and a b is given by distance d equals the square root of x minus a squared
plus y minus b squared and you can remember that from the two edges of a triangle we're computing the third edge
since we know the x side and the y side let's calculate it to understand clearly so we
have our unknown point and we placed it there in red and we have our other points where the data is scattered
around the distance d1 is the square root of 170 minus 167 squared plus 57
minus 51 squared which is about 6.7 and distance 2 is about 13. and distance 3
is about 13.4 similarly we will calculate the euclidean distance of unknown data point from all the points
in the data set and because we're dealing with small amount of data that's not that hard to do it's actually pretty quick for a computer and it's not a
really complicated mass you can just see how close is the data based on the euclidean distance hence we have
calculated the euclidean distance of unknown data point from all the points as shown where x1 and y1 equal 57 and
170 whose class we have to classify so now we're looking at that we're saying well here's the euclidean distance who's
going to be their closest neighbors now let's calculate the nearest neighbor at k equals three and we can see the three
closest neighbors puts them at normal and that's pretty self-evident when you look at this graph it's pretty easy to
say okay what we're just voting normal normal normal three votes for normal this is going to be a normal weight so
majority of neighbors are pointing towards normal hence as per k n algorithm the class of 57 170 should be
normal so a recap of knn positive integer k is specified along with a new
sample we select the k entries in our database which are closest to the new sample we find the most common
classification of these entries this is the classification we give to the new sample so as you can see it's pretty
straightforward we're just looking for the closest things that match what we got so let's take a look and see what that looks like in a use case in python
so let's dive into the predict diabetes use case so use case predict diabetes
the objective predict whether a person will be diagnosed with diabetes or not we have a data set of 768 people who
were or were not diagnosed with diabetes and let's go ahead and open that file and just take a look at that data and
this is in a simple spreadsheet format the data itself is comma separated very
common set of data and it's also a very common way to get the data and you can see here we have columns a through i
that's what one two three four five six seven eight eight columns with a particular
attribute and then the ninth column which is the outcome is whether they have diabetes as a data scientist the
first thing you should be looking at is insulin well you know if someone has insulin they have diabetes because
that's why they're taking it and that could cause issue in some of the machine learning packages but for a very basic
setup this works fine for doing the knn and the next thing you notice is it didn't take very much to open it up i
can scroll down to the bottom of the data there's 768. it's pretty much a small data set you
know at 769 i can easily fit this into my ram on my computer i can look at it i
can manipulate it and it's not going to really tax just a regular desktop computer you don't even need an enterprise version to run a lot of this
so let's start with importing all the tools we need and before that of course we need to discuss what ide i'm using
certainly can use any particular editor for python but i like to use for doing uh very basic visual stuff the anaconda
which is great for doing demos with the jupiter notebook and just a quick view of the anaconda navigator which is the
new release out there which is really nice you can see under home i can choose my application we're going to be using
python36 i have a couple different versions on this particular machine if i go under environments i can create a
unique environment for each one which is nice and there's even a little button there where i can install different packages so if i click on that button
and open the terminal i can then use a simple pip install to install different packages i'm working with let's go ahead
and go back under home and we're going to launch our notebook and i've already you know kind of like the old cooking shows i've already
prepared a lot of my stuff so we don't have to wait for it to launch because it takes a few minutes for it to open up a
browser window in this case i'm going to it's going to open up chrome because that's my default that i use and since
the script is pre-done you'll see i have a number of windows open up at the top the one we're working in and since we're
working on the n predict whether a person will have diabetes or not let's go and put that title in there and i'm
also going to go up here and click on cell actually we want to go ahead and first insert a cell below and then i'm
going to go back up to the top cell and i'm going to change the cell type to markdown that means this is not going to
run as python it's a markdown language so if i run this first one it comes up in nice big letters which is kind of
nice remind us what we're working on and by now you should be familiar with doing all of our imports we're going to import
the pandas as pd import numpy is np pandas is the pandas data frame and
numpy is a number array very powerful tools to use in here so we have our imports so we've brought in our pandas
our numpy our two general python tools and then you can see over here we have our trained test split by now you should
be familiar with splitting the data we want to split part of it for training our thing and then training our particular model and then we want to go
ahead and test the remaining data to see how good it is pre-processing a standard scalar preprocessor so we don't have a
bias of really large numbers remember in the data we had like number of pregnancies isn't going to get very
large where the amount of insulin they take can get up to 256 so 256 versus 6
that will skew results so we want to go ahead and change that so they're all uniform between minus 1 and 1. and then
the actual tool this is the k neighbors classifier we're going to use and finally the last three are three
tools to test all about testing our model how good is it we just put down test on there and we have our confusion
matrix our f1 score and our accuracy so we have our two general python modules
we're importing and then we have our six modules specific from the sk learn setup
and then we do need to go ahead and run this so these are actually imported there we go and then move on to the next
step and so in this set we're going to go ahead and load the database we're going to use pandas remember pandas is
pd and we'll take a look at the data in python we looked at it in a simple spreadsheet but usually i like to also
pull it up so that we can see what we're doing so here's our data set equals pd.read csv that's a pandas command and
the diabetes folder i just put in the same folder where my ipython script is if you put in a different folder you
need the full length on there we can also do a quick length of the data set that is a simple python command
len for length we might even let's go ahead and print that we'll go print and if you do it on its own line link that
data set the jupyter notebook it'll automatically print it but when you're in most of your different setups you
want to do the print in front of there and then we want to take a look at the actual data set and since we're in
pandas we can simply do data set head and again let's go ahead and add the print in there
if you put a bunch of these in a row you know the data set one head data set two head it only prints out the last one so
i usually always like to keep the print statement in there but because most projects only use one data frame panda's
data frame doing it this way doesn't really matter the other way works just fine and you can see when we hit the run
button we have the 768 lines which we knew and we have our pregnancies it's automatically given a label on the left
remember the head only shows the first five lines so we have zero through four
and just a quick look at the data you can see it matches what we looked at before we have pregnancy glucose blood
pressure all the way to age and then the outcome on the end and we're going to do a couple things in this next step we're
going to create a list of columns where we can't have zero there's no such thing as zero skin thickness or zero blood
pressure zero glucose uh any of those you'd be dead so not a really good factor if they don't if they have a zero
in there because they didn't have the data and we'll take a look at that because we're going to start replacing that information with a couple of
different things and let's see what that looks like so first we create a nice list as you can see we have the values
talked about glucose blood pressure skin thickness and this is a nice way when you're working with columns is to list the
columns you need to do some kind of transformation on a very common thing to do and then for this particular setup we
certainly could use the there's some panda tools that will do a lot of this where we can replace the n a but we're
going to go ahead and do it as a data set column equals data set column dot
replace this is this is still pandas you can do a direct there's also one that's that you look for your nan a lot of
different options in here but the n a n in num p n a n is what that stands for is none it doesn't exist so the first
thing we're doing here is we're replacing the zero with a numpy none
there's no data there that's what that says that's what this is saying right here so put the 0 in and we're going to
place zeros with no data so if it's a 0 that means the person's well hopefully not dead hope they just didn't get the
data the next thing we want to do is we're going to create the mean which is the integer from the data set from the
column dot mean where we skip n a's we can do that that is a pandas command
there the skip n a so we're going to figure out the mean of that data set and then we're going to take that data set
column and we're going to replace all the npn with the means why did we do
that we could have actually just taken this step and gone right down here and just replaced zero and skip anything
where except you could actually there's a way to skip zeros and then just replace all the zeros but in this case we want to go ahead and do it this way
so you could see that we're switching this to a non-existent value then we're going to create the mean well this is
the average person so if we don't know what it is if they did not get the data and the data
is missing one of the tricks is you replace it with the average what is the most common data for that this way you
can still use the rest of those values to do your computation and it kind of just brings that particular value of
those missing values out of the equation let's go ahead and take this and we'll go ahead and run it doesn't actually do
anything so we're still preparing our data if you want to see what that looks like we don't have anything in the first
few lines just not going to show up but we certainly could look at a row let's do that let's go into our data set with
printed data set and let's pick in this case let's just do glucose and if i run this this is going
to print all the different glucose levels going down and we thankfully don't see anything in here that looks
like missing data at least on the ones it shows you can see you skipped a bunch in the middle because that's what it does if you have too many lines in
jupiter notebook it'll skip a few and go on to the next in a data set let me go and remove this
and we'll just zero out that and of course before we do any processing before proceeding any further we need to
split the data set into our train and testing data that way we have something to train it with and something to test
it on and you're going to notice we did a little something here with the pandas database code there we go my drawing
tool we've added in this right here of the data set and what this says is that the first one in pandas this is
from the pd pandas it's going to say within the data set we want to look at the eye location and it is all rows
that's what that says so we're going to keep all the rows but we're only looking at 0 column 0 to 8. remember column 9
here it is right up here we put it in here is outcome well that's not part of the training data that's part of the answer yes column nine but it's listed
as eight number eight so zero to eight is nine columns so uh eight is the value and when you see it in here 0 this is
actually 0 to 7 it doesn't include the last one and then we go down here to y which is our answer and we want just the
last one just column 8 and you can do it this way with this particular notation and then if you remember we imported the
train test split that's part of the sk learn right there and we simply put in
our x and our y we're going to do random state equals zero you don't have to necessarily seat
it that's a seed number i think the default is one when you see it i have to look that up and then the test size test
size is 0.2 that simply means we're going to take 20 of the data and put it aside so that we can test it later
that's all that is and again we're going to run it not very exciting so far we haven't had any printout other than to
look at the data but that is a lot of this is prepping this data once you prep it the actual lines of code are quick
and easy and we're almost there with the actual writing of our knn we need to go ahead and do a scal the data if you
remember correctly we're fitting the data in a standard scalar which means instead of the data being from you know
5 to 303 in one column and the next column is 1 to 6 we're going to set that
all so that all the data is between -1 and one that's what that standard scalar does keeps it standardized and we only
want to fit the scalar with the training set but we want to make sure the testing
set is the x test going in is also transformed so it's processing it the
same so here we go with our standard scalar we're going to call it sc underscore x for the scalar and we're
going to import the standard scalar into this variable and then our x train equals sc underscore x dot fit transform
so we're creating the scalar on the x train variable and then our x test we're also going to transform it so
we've trained and transformed the x train and then the x test isn't part of that training it isn't
part of that of training the transformer it just gets transformed that's all it does and again we're going to run this
if you look at this we've now gone through these steps all three of them we've taken care of replacing our zeros
for key columns it shouldn't be zero and we replace that with the means
of those columns that way that they fit right in with our data models we've come down here we split the data so now we
have our test data and our training data and then we've taken and we scaled the
data so all of our data going in no no we don't tr we don't train the y part
the y train and y test that never has to be trained it's only the data going in
that's what we want to train in there then define the model using k neighbors classifier and fit the train data in the
model so we do all that data prep and you can see down here we're only going to have a couple lines of code where
we're actually building our model and training it that's one of the cool things about python and how far we've
come it's such an exciting time to be in machine learning because there's so many automated tools let's see before we do
this let's do a quick length of and let's do y we want let's just do length
of y and we get 768 and if we import math we do math dot square root let's do
y train there we go it's actually supposed to be x train before we do this let's go ahead and do
import math and do math square root length of y test and when i run that we get 12.409
i want to show you where this number comes from we're about to use 12 is an even number so if you know if you're
ever voting on things remember the neighbors all vote don't want to have an even number of neighbors voting so we
want to do something odd and let's just take one away we'll make it 11. let me delete this out of here this one the
reasons i love jupiter notebook is you can flip around and do all kinds of things on the fly so we'll go ahead and put in our classifier we're creating our
classifier now and it's going to be the k neighbors classifier n neighbors equal 11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors p equals 2 because we're looking for is it are the diabetic or not and we're using
the euclidean metric there are other means of measuring the distance you could do like square square means values
all kinds of measure this but the euclidean is the most common one and it works quite well it's important to
evaluate the model let's use the confusion matrix to do that and we're going to use the confusion matrix
wonderful tool and then we'll jump into the f1 score and finally accuracy score which is
probably the most commonly used quoted number when you go into a meeting or something like that so let's go ahead
and paste that in there and we'll set the cm equal to confusion matrix y test
y predict so those are the two values we're going to put in there and let me go ahead and run that and print it out
and the way you interpret this is you have the y predicted which would be your
title up here you could do let's just do p-r-e-d predict it across the top
and actual going down actual it's always hard to to write in here
actual that means that this column here down the middle that's the important column and it means that our prediction
said 94 and prediction in the actual agreed on 94 and 32. this number here
the 13 and the 15 those are what was wrong so you could have like three
different if you're looking at this across three different variables instead of just two you'd end up with the third row down here in the column going down
the middle so in the first case we have the the and i believe the zero is in 94
people who don't have diabetes the prediction said that 13 of those people did have diabetes and were at high risk
and the 32 that had diabetes it had correct but our prediction said another
15 out of that 15 it classified as incorrect so you can see where that
classification comes in and how that works on the confusion matrix then we're going to go ahead and print the f1 score
let me just run that and you see we get a 0.69 in our f1 score
the f1 takes into account both sides of the balance of false positives
where if we go ahead and just do the accuracy account and that's what most people think of is it looks at just how
many we got right out of how many we got wrong so a lot of people when you're a data scientist and you're talking to
other data scientists they're going to ask you what the f1 score the f score is if you're talking to the general public
or the decision makers in the business they're going to ask what the accuracy is and the accuracy is always better
than the f1 score but the f1 score is more telling it lets us know that there's more false positives than we
would like on here but 82 percent not too bad for a quick flash look at people's different statistics and
running an sk learn and running the knn the k nearest neighbor on it so we have
created a model using knn which can predict whether a person will have diabetes or not or at the very least
whether they should go get a checkup and have their glucose checked regularly or not the print accuracy score we got the
0.818 was pretty close to what we got and we can pretty much round that off and just say we have an accuracy of 80
percent tells us it is a pretty fair fit in the model the principal component
analysis we're going to cover dimensionality reduction principle component analysis what is it
important pca terminologies and you'll see it abbreviated normally as pca
principal component analysis pca properties pca example and then we'll
pull up some python code in our jupyter notebook and have some hands-on demo on the pca and how it's used dimensionality
reduction dimensionality reduction refers to the technique that reduces the number of input variables in a data set
and so you can see on the table on the right shows the orders made at an automobile parts retailer the retailer
sells different automobile parts from different companies and you can see we have company bpex isomax and they have
the item the tire the axle an order id a price number and a quantity in order to
predict the future cells we find out that using correlation analysis
that we just need three attributes therefore we have reduced the number of attributes from five to three
and clearly we don't really care about the part number i don't think the part number would have an effect on how many
tires are bought and even the store who's buying them probably does not have an effect on that
in this case that's what they've actually done is remove those and we just have the item the tire the price
and the quantity one of the things you should be taking away from this is in the scheme of things
we are in the descriptive phase we're describing the data and we're pre-processing the data what can we do
to clean it up why dimensionality reduction well number one less dimensions for a
given data set means less computation or training time that can be really important if you're
trying a number of different models and you're re-running them over and over again and even if you have seven
gigabytes of data that can start taking days to go through all those different models so this is huge this is probably the
hugest part as far as reducing our data set redundancy is removed after removing
similar entries from the data set again pre-processing some of our models
like a neural network if you put in two of the same data it might give them a higher weight than they would if it was
just once we want to get rid of that redundancy it also increases the processing time if
you have multiple data coming in space required to store the data is reduced
so if we're committing this into a big data pool we might not send the company that
bought it why would we want to store two whole extra columns when we added into that pool of data
makes the data easy for plotting in 2d and 3d plots this is my favorite part
very important you're in your shareholder meeting you want to be able to give them a really good
clear and simplified version you want to reduce it down to something people can take in
it helps to find out the most significant features and skip the rest which also comes in in post scribing
leads to better human interpretation that kind of goes with number four what makes data easy for plotting you have a
better interpretation when we're looking at it principle component analysis so what is it
principal component analysis is a technique for reducing the dimensionality of data sets increasing
interpretability but at the same time minimizing information loss so we take
some very complex data set with lots of variables we run it through the pca we reduce the variables we end up with a
reduced variable setup this is very confusing to look at because if you look at the end result
we have the different colors all lined up so what we're going to take a look at is let's say we have a picture here
let's say you are asked to take a picture of some toddlers and you are deciding which angle would be the best
to take the picture from so if we come up here we look at this we say okay this is you know one angle
we get the back of a lot of heads not many faces uh so we'll do it from here we might get
the one person up front smiling a lot of the people in the class are missing so we have a huge amount off to the right
of blank space maybe from up here again we have the back of someone's head
and it turns out that the best angle to click the picture from might be this bottom left angle you look at it you say
hey that makes sense it's a good configuration of all the people in the
picture now when we're talking about data it's not you really can't do it by what you think
is going to be the best we need to have some kind of mathematical formula so it's consistent and so it makes sense in
the back end one of the projects i worked on many years ago
has something similar to the iris if you've ever done the iris data sets probably one of the
most common ones out there where they have the flower and they're measuring the stamen uh in
the petals and they have width and they have length of the petal instead of putting through the width and
the length of the petal we could just as easily do the width-to-length ratio we
can divide the width by the length and you get a single number where you had two that's the kind of idea that's going on
into this in pre-processing and looking at what we can do to bring the data down the very simplified example on my
iris pedal example when we look at the similarity in pca we
find the best picture or projection of the data points and so we look down at from one angle
we've drawn a line down there we can see these data points based on in this case just two variables now keep in
mind we're usually talking about 36 40 variables almost all of your
business models usually have about 26 to 27 different variables they're looking
at uh same thing with like a bank loan model we're talking 26 to 36 different
variables they're looking at that are going in so we want to do is we want to find the best view in this case we're just
looking at the x y we look down at it and we have our second
idea pc2 and again we're looking at the x i this x y this time from a different direction
here for our ease we can consider that we get two principal components namely pc1 and pc2
comparing both the principal components we find the data points are sufficiently spaced in
pc1 if you look at what we got here we have pc1 you can see along the line how
the data points are spaced versus the spacing in pc2 and that's what they're coming up with what is going to give us
the best look for these data points when we combine them and we're looking at them from just a single angle
whereas in pc2 they are less spaced which makes the observation and further calculations much more difficult
therefore we accept the pc1 and not the pc2 as the data points are more spaced
now obviously the back end calculations are a little bit more complicated when we get into the math of how they decide
what is more valuable this gives you an idea though that when we're talking about this we're talking
about the perspective which would help in understanding how pca analysis works
we want to go ahead and do is dive into the important terminologies under pca
and important terminologies are views the perspective through which data points are observed
and so you'll hear that if someone's talking about a pca presentation and they're not taking the time to reduce it
to something that the average person shareholders can understand you might hear them refer to it as the different views what view are we taking
dimension number of columns in a data set are called the dimensions of that data set and we talked about you'll hear features
dimensions i was talking about features there's usually when you're running a business
you're talking 25 26 27 different features minimal and then you have the principal component new variables that
are constructed as linear combinations or mixtures of the initial variables
principal component is very important it's a combination if you remember my flower example it would be the
width over the length of the petal as opposed to putting both width and length in you just put in the
ratio instead which is a single number versus two separate numbers projections
the perpendicular distance between the principal component and the data points
and that goes to that line we had earlier it's that right angle line of where those point how those points fall
onto the line important properties important properties number of principal components is always less than or equal
to the number of attributes that just makes common sense uh you're not going to do 10 principal properties
with only three features you're trying to reduce them so it's just kind of goofy but it is important
to remember that people will throw weird code out there and just randomly do stuff with instead of really thinking
it through principal components are orthogonal and this is what we're talking about
that right angle from the line when we when we do pc1 we're looking at how
those points fall on to that line same thing with pc2 and we want to make
sure that pc1 does not equal pc2 we don't want to have the same two principal points
when we do two points the priority of principal components decreases as their numbers increase
this is important to understand if you're going to create uh one principle
component everything is summarized into that one component as we go to two components the
priority how much it holds value decreases as we
go down so if you have five different points each one of those points is going to have less value than just the one
point which has everything summarized in it how pca works
i said there was more in the back end we talked about the math this is what we're talking about is how does it actually
work so now we have understanding that you're looking at a perspective
now we want to see how that math side works pca performs the following operations in order to evaluate the
principal components for a given data set first we start with the standardization
then we have a covariance matrix computation and we use that to generate our i gene
vectors and i gene values which is the feature vector and if you
remember the i gene vector is like a translation for um moving the data
from x equals one to x equals two or whatever we're altering it and the igen value is the final value that we
generate when we talk about standardization the main aim of this step is to standardize
the range of the attributes so that each one of them lie within similar boundaries this process involves removal of the
mean from the variable values and scaling the data with respect to the standard deviation
and you can see here we have z equals the variable values minus the mean over the standard deviation
the covariance matrix computation covariance matrix is used to express the
correlation between any two or more attributes in multi-dimensional data set
the covariance matrix has the entries as the variance and the covariance of the tribute values the variance is denoted
by var and the covariance is denoted by cov on the right we can see the covariance
matrix for two attributes and their values when we do a hands-on and look at the
code we'll do a display of this so you can see what we're talking about and what that looks like
for now you can just notice that this is a matrix that we're generating with the variance and then the covariance of x to
y on the right side we can see the covariance table for more than two
attributes in a multi-dimensional data set this is what i was talking about we
usually are looking at not just one feature two features we're usually looking at 25 30 features
going on and so if we do a setup like this we should see all
those different features as the different variables covariance matrix tells us how the two
or more variables are related positive covariance indicate that the value of one variable is directly proportional to
the other variable negative covariance indicate that the value of one variable is inversely
proportional to the other variable that is always important to note whenever we're doing any of these matrixes that
we're going to be looking at that positive and negative whether it's inverted or not and then we have the iogene values and
the i gene vectors i gene values and hygiene vectors are the mathematical value
that are extracted from the covariance table they are responsible for the generation of a new set of variables
from the old set of variables which further lead to the construction of the principal components
igen vectors do not change directions after linear transformation i gene values are the scalars or the
magnitude of the i gene vectors and again this is just change transforming that data so we're going to
change the vector b to the b prime as denoted on the chart
and so when we have like multiple variables how do we calculate that new variable
and then we have feature vectors feature vectors is simply a matrix that has igen
vectors of the components that we decide to keep as the columns here we decide whether we must keep or
discard the less significant principal components that we have generated in the above steps
this becomes really important as we start looking at the back end of this
and we'll do this in the demo but one of the more important steps to understand
and so we have the pca example consider matrix x within rows or observations and
k columns are variables now for this matrix we would construct a variable space with as many dimensions as the
variable but for our simplicity let's consider this three dimensions for now
now each observation row of the matrix x is placed in the k-dimensional variable space such that the rows in the data
table form a swarm of points in this space now we find the mean of all the
observations and then place it along the data points on the plot
the first principal component is a line that best accounts for the shape of the point swarm it represents the maximum
variance direction in the data each observation may be projected onto
this line in order to get a coordinate value along the pc1 this value is known as a score
usually only one principal component is insufficient to model the systematic variation for a data set
thus a second principal axis is created the second principle component is
oriented such that it reflects the second largest source of variation in the data while being orthogonal to pc1
pc2 also passes through the average point let's go ahead and pull this up and just
see what that means inside our python scripting i'm going to use the anaconda navigator
and i will be in python 3.6 for this example
i believe there's even like a 3.9 out i tend to stay in 3.6 because a lot of
the models i use especially with the neural networks are stable in 3 6.
and then we open up our jupiter i'm in chrome and we go ahead and create a new
python 3. and for ease of use our team in the back
was nice enough to put this together for me and we'll go and start with the libraries the first thing i like to do
whenever i'm looking at any new setup well you know what let's do let's do the
libraries first we're going to do our basic libraries which is matplot library the plt from the matplot library
pandas our data frame pd numpy our numbers array np
seaborn for graphing sns that goes with the plot that actually sits on matplot library so the seaborn sits on there
and then we have our amber sign because we're in jupiter notebook map plot library in line the newer version
actually doesn't require that but i put it in there either anyway just because i'm so used to it
and then we want to go ahead and take a look at the data
and in this case we're going to pull in certainly you can have lots of fun with different data but we're going to use the cancer data set
and one of the reasons the cancer data set is it has like 36 35 different features so it's kind of fun to use that as our
base for this and we'll go ahead and run this and look at our keys
and the first thing we notice in our keys for the cancer data set is we have our data we have our target
our frame target names description feature names and file name
so what we're looking for in all this is let's take a look at the description
let's go in here and pull up the description on here
i'm not going to spend a huge amount of time on the description um because this is we don't want to get
into a medical domain we want to focus on our pca setup uh what's important is you start looking
at what the different attributes are what they mean if you were in the medical field you'd want to note all
these different things whether what they're measuring where it's coming from you can actually see the actual
different measurements they're taking no missing attributes
we page all the way to the bottom and you're going to have your data in this case our target
and if you dig deep enough to the target let's actually do this let's go ahead and print target names
real quick here i always like to just take a look and see what's on the other end of this
target names run that
yeah so the target name is is it malignant or is it b9
um so in other words is this uh dangerous growth or is it something we don't have to worry about that's the
bottom line with the cancer in this case and then we can go ahead and load our
data and you know let me go up a just a notch here for easy of
reading it's hard to get that just right that's all you have to do so let's go ahead and look at our data
uh our we're going to use our pandas and we're going to go ahead and do our data frame it's going to equal cancer
data columns equals cancer feature equals feature names so remember up here we already loaded the
names up of our of the features in there what is going to come out of this let me just see if we can get to that
it's at the top of target names that's just this list of names here in
the setup and we can go ahead and run this code
and it'll print the head and you can see here we have the mean radius the mean texture mean perimeter
i don't know about you this is a wonderful data set if you're playing with it because like many of the data that most the data
that comes in half the time we don't even know we're looking at we're just handed a bunch of stuff as a
data scientist going what the heck is this and so this is a good place to start because this has a number of
different features in there we have no idea what these feature means or where they come from we want to just look at
the data and figure that out and now we actually are getting into the
pca side of it as we've noticed before is difficult to visualize high dimensional data
we can use pca to find the first two principal components and visualize the
data this new two-dimensional space with a single scatter plot before we do this we need to go ahead
and scale our data now i haven't run this to see if you really
have to scale the data on this but as just a general
run time i almost do that as the first step of any modeling even if it's pre-modeling as we're doing here
in neural networks that is so important with pca visualization it's already going to scale it when we
do the means and deviation inside the pca but just in case it's always good to
scale it and then we're going to take our pca with the scikit learn uses very similar
process to other preprocessing functions that come with scikit-learn we instantiate a pca object find the
principal components using the fit method then apply the rotation and dimensionality reduction by calling
transform we can also specify how many components we want to keep when creating the pca object
and so the code for this oops getting a little bit ahead
let me go and run this code uh so the code for this is
from sklearn decomposition import pca pca equals pca in components equals two
and that's really important to note that because we're only going to want to look at two components
i would never go over four components especially if you're going to demo this with somebody else if you're showing
this to the shareholders the whole idea is to reduce it to something people can see
and then the pca fit we're going to is going to take the scaled data that we generated up here
and then you can see we've created our pca model with in components equals two
now whenever i use a new tool i like to go in there and actually see what i'm using so let's go to the scikit
webpage for the pca and you can see in here here's our call
statement it describes what all the different setups you have on there
probably the biggest one to look at would be well the biggest one is your components how many components do you want
which you have to put in there pretty much and then you also might look at the svd solver it's on auto right now but
you can override that and do different things with it it does a pretty good job as it is
and if we go down all the way down to
there we go to our methods if you notice we have fit
we have fit transform nowhere in here is predict because this
is not used for prediction it's used to look at the data again we're in the describe setup we're
fitting the data we're taking a look at it we've already looked at our minimum maximum we've already looked at
what's in each quarter we've done a full description of the data this is part of describing the data
that's the biggest thing i take away when i come zooming in here and of course i have examples of it down here
if you forget and the biggest one of course is the number of components
and then i mean the rest you can play with the actual solver whether you're doing a
full randomize there's different things it does pretty good on the auto
and now we can transform this data to its first two principal components
and so we have our xpca we're going to set that equal to pca
transform scaled data so there we go there's our first
transformation and let's just go ahead and print the scaled data shape and the xpca data
shape and the reason we want to do this is just to show us
what's going on here we've taken 30 features i think i said 36 or something like that but it's 30
and we've compressed it down to two features and we decided we wanted two features and that's where this comes
from we still have 569 data sets i mean data rows data sets we still have
569 rows of data but instead of computing 30 features we're now only doing our model on two features
so let's go ahead and uh plot these and take a look and see what's going on
and we're just going to use our plt figure
we'll set the figure size on here here's our scatter plot xpca
x underscore pca of of one these are two different
perceptions we're using and then you'll see right here c for color cancer
equals target and so remember we have zero we have one
and if i remember correctly zero was malignant one was b9 uh so everything in the zero column is
going to be one color and the other color is going to be one and then we're going to use the plasma map just kind of telling you what color it is add some
labels first principle component second principle component and we'll go ahead and run this
and you can see here instead of having a chart one of those heat maps with 30
different columns in it we can look at this and say hey this one actually did a pretty good job
of separating the data and a couple things when i'm looking at
this that i notice is first we have a very clear area where it's
clumped together where it's going to be benign and we have a huge area it's still
clumped together more spread out where it's going to be malignant or i think i had that backwards
and then in the middle because we're dealing with something in this particular case cancer
we would try to separate i would be exploring how to separate this middle group out
in other words there's an area where everything overlaps and we're not going to have a clear result on it
just because those are the people you want to go in there and have extra tests or treat it differently versus going in
and saying just cutting into the can into the cancer so the body absorbs it and it dissipates versus uh
actively going in there removing it testing it going through chemo and all the different things that's a big
difference you know as far as what's going to happen here and that middle line where the overlap is going to be
huge that's domain specific going back to the data we can see here
clearly by using these two components we can easily separate these two classes so the next step is what does that mean
interpreting the components unfortunately with this great power of dimensionality reduction comes the cost
of not being able to easily understand what these components represent i don't know what principle component
one licks work represents or second principle the components correspond to
combinations of original features the components themselves are stored as an attribute of the filtered pca object and
so we talk look at that we can go ahead and do look at the pca components this is in our model we built we've trained
it we can run that and you can see here's the actual components uh it's the
two components have each have their own array and within the array you can see the
what the scores are using and these actually give weight to what features are doing what
so in this numpy matrix array each row represents a principal component and each column relates back to the original
features what's really neat about this is we can now go in reverse
and drop this onto a heat map and start seeing uh what this means and
so let me go ahead and just put this down i've already got it down here
we'll go ahead and put this in here we're going to use our df comp data frame and we do our pca components
and i want you to notice how easy this is we're going to set our columns equal to
cancer feature names that just makes it really easy and we're dumping it into a data frame
what's neat about a data frame is when we get to seaborn it will pull that data frame apart and
and set it up for us we want and so we're just going to do the cn the seaborn heat map of our data frame
composition and we'll use the plasma coloring and it creates a nice little
color graph here you can see we have the mean radius and all the different features along the bottom on the right
we have a scale so we can see we have the dark colors all the way to the really light colors which are
what's really shining there this is like the primary stuff we want to look at
so this heat map and the color bar basically represent the correlation between the various features and the
principal component itself so you know very powerful map to look at
and then you can go in here and we might notice that the mean radius look how on the bottom of the map it is
um on some of this so you have some interesting correlations here that change the
variations on that and what means what this is more when you get to a post
scribe you can also use this to try to guess as what these things mean what you want to change to get a better result
regularization in machine learning so our agenda on this one is fitting the
data understanding linear regression bias and variance
what is overfitting what is underfitting and those are like the biggest things right now in data
science is overfitting and underfitting what does that mean and what is regularization and then
we'll do a quick hands-on demo to take a look at this so fitting the data let's start with
fitting the data and we talk about what is data fitting it's a process of
plotting a series of data points and drawing the best fit line to understand the relationship between the variables
and this is what we called data fitting and you can see here we have a couple of lines we've drawn on this graph we're
going to go in a little deeper on there so we might have in this case just the two dimensions we have an efficiency of
the car and we have the distance traveled in 1 000 kilometers
and so what is data fitting well it's a linear relationship and a linear relationship
very specifically linear means line the line used to represent the
relationship is a straight line that passes through the data points and the variables have linear relationship
linear regression so let's start with how linear regression works a linear regression
finds a line that best fits the data point and gives a relationship between the two variables
and so you can see here we have the efficiency of the car versus the distance traveled and you can
see this nice straight line drawn through there and when you talk about multiple variables all you're doing is putting
this instead of a line that now becomes a plane it gets a little more complicated with multiple variables but they all come
down to this linear kind of drawing a line through your data and finding what
fits the the data the best and so we can consider an example uh
let's say that we want to find the relationship between the temperature outside versus the sales of ice cream
and so we start looking at that we're looking at the how many ice cream cones we're selling or how much money we sold in ice cream and we're looking at how
warm it is outside which would hopefully draw a lot of people into the ice cream store and suppose we have two lines we're
going to draw l1 and l2 and we're going to kind of guess which one we think is the best fit
and which claim to describe the relationship between the variables
and so first we find the square of the distance between the line l1 and each data point and add them all and
find the mean distance and i want you to think about that when we
square something if it's a negative or positive number it no longer matters because a minus 2 squared is 4 2 squared
is 4. so we're removing what side of the line it's on and we're just looking for the error in this case the mean distance
of each of the little dotted lines you see here this way of calculating the square of
the distance adding them and then taking the mean is called mean square error or loss function
and we talk about loss how far off are we that's what we're really talking about what did we miss and we have a
positive distance and a negative distance and of course when we square it it is neither it just becomes a positive
error and so we take the mean square error and a lot of times you'll
see it referred to as mse if i look in the code and i'm going through my python code and i see mse
i know that's a mean squared error and we take all the dotted lines and we calculate this error we add them all
together and then we average it or find the means and in this case
they ran a demo on this and it was uh 1127
for our l1 line now we find the loss function for line l2 in a similar fashion and we get the
mean square error to be 6397
and it's computed the same way so maybe you put this line just way outside the data range and this is the error you get
by analyzing our results we find that the loss function or the mean square error is less for l1 than l2 hence l1 is
the best fit line this process describes a lot of machine
learning processes it was we're going to keep guessing and get as close as we can to find the right answer we have to have
some way to invite to calculate this and figure out which one's the best and the mean square error
is one of the better fits to doing for doing this and most commonly used
we really want to talk about bias and variance very important terms to know in machine
learning and with linear regression so bias
bias occurs when an algorithm has limited flexibility to learn from data
variance defines the algorithm's sensitivity to specific sets of data
let's start with bias and variance you can see here we have the two different setups
bias you can think is very generalized where variance is very specific
and so we talk about bias such models pay very little attention to the training data and over simplify the
model therefore the validation error or prediction error and training error follow similar trends
and uh with bias if you over simplify it so much you're going to miss
your local if if you have like a really good fit you're going to miss it you're
you're going to just kind of guess what the average is and that's what your answer is going to be
with variance a model with a high variance pays a lot of attention to training data and does not generalize
therefore the validation error or prediction error are far apart from each other
such models always lead to a high error on training and test data as a bias does
where variants such models usually perform very well on training data but have high error rates on test data
and i want you to think about this when we're talking about a bias the error is going to be high both when
you're training it and you're testing it why because we're just kind of
getting an average we're not really fitting it close with variance we're fitting it so close
that the test data does really good it's going to nail it every time if you're doing categorical testing that's a car
that's a truck that's a bicycle but with variants suddenly a
truck has to have certain features and it might have to be red because you had so many red pictures so
if it has if it's an 18 wheeler it has to be red if it's blue then it has to be a bicycle
that's the kind of variance we're talking about where it picks up on something and it cannot get the right
answer unless it gets a very specific data and we see that so that as you're testing it your models
and you programmed it you got to look for how i trained it what is coming out and if it's not if it's not looking good on
either bias or if it's not looking good on the training or on the test data
then your bias then you're biasing your data if it really looks good on the training
data then that's going to be your variance you've over fitted the data and those are very important things to
know when you are building your models in regression of any kind or any kind of
setup for predicting so in dark games if all the data found a
particular pointer this can be considered as a biased throw and the player aims for the particular score
for variance if all the darts fall on different pointers and no two darts fall on the same pointer then this can be
considered as a varied throw and the player aims for various scores again the bias sums everything up in one
point kind of averages it together where the variance really looks for the individual
predictions coming out so let's go ahead and talk about overfitting
when we talk about overfitting it's a scenario where the machine learning model tries to learn from the details
along with the noise and the data tries to fit each data point on the curve
uh you can see that um if you plug in your coordinates you're
just going to get the whatever is fitted every point on the data stream there's no average there's no
two points that might have that you know why might have two different answers because uh if the wind blows a certain
way in the efficiency of your car maybe you have a headwind so your car might alter
how efficient it is as it goes and so there's going to be this variance on here and this says no you can't have any
variance with you know the this is it's going to be exactly this it can't be any you can't be the same speed or the same
car and have a slightly different efficiency so as the model has very less
flexibility it fails to predict new data points and thus the model rejects every
new data point during the prediction uh so you'll get like a really high
error on here and so reasons for overfitting data used for training is not cleaned
and contains noise garbage values in it you can spend so much time cleaning your data and it's so important it's so
important that if you have if you have some kind of something wrong with the data coming in
it needs to be addressed whether it's a source of the data maybe they use in medical different measuring tools
so you now have to adjust for data that came in from hospital a versus hospital b or even off of machine a and machine b
that's testing something and those those numbers are coming in wrong
the model has a high variance again wind is a good example i was talking about that with the car
you may have a hundred tests but because the wind's blowing it's all over the place
size of training data used is not enough so a small amount of data is going to also cause this problem you only have a
few points and you try to plot everything the model is too complex
this comes up a lot we put too many pieces together and how they interact can't even be tracked
and so you have to go back back break it up and find out actually what correlates and what doesn't
so what is underfitting a scenario where machine learning models
can either learn the relationship between the data points nor predict
or classify a new data point and you can see here we have our efficiency of our car and our line drawn
and it's just going to be way off for both the training and the predicting data as the model doesn't fully learn the
patterns it accepts every new data point during the prediction so instead of looking for a general
pattern we just kind of accept everything data used for training is not cleaned
and contains noise garbage and values again under fitting and overfitting same issue you got to clean your data
the model has a high bias we've seen this in all kinds of things
from [Music] uh the mod the most common is the driving cars to facial identification or
whatever it is the model itself when they build it might have a bias towards one thing and this would be an
underfitted model would have that bias because it's averaged it out so if you have
five people from india and 10 people from africa and 20 people from the u.s you
created a bias because it's looking at the 20 people and you only have a small amount of data
to work with size of training data used is not enough
that goes with the size i was just talking about so we have a model with a high bias we have size of training data used it's not
enough the model is too simple again this is one straight line through
all the data when it needs a slight shift to it for other reasons
so what is a good fit a linear curve that best fits the data is neither overfitting or underfitting
models but is just right and of course we have the nice examples here where we have overfitting lines
going up and down every point is trying to be include gluted underfitting
the line really is off from where the data is and then a good fit is got to get rid of that minimize that
error coming through regularization is taking the guesswork out you're looking at this graph and you're going oh which
one is that really over fit or is that under fit that's pretty hard to tell so we talk about regularization
regularization techniques are used to calibrate the linear regression models
and to minimize the adjusted loss function and prevent overfitting or underfitting
so what that means in this case we're going to go ahead and take a look at a couple different things
we're going to look at regularization which we'll start with a linear model we'll look at the ridge regularization
and the lasso regularization and these models are just like just like we did the
mlp the multi-layered positron in the sk learn module you could bring in the
ridge module and you can bring in the lasso module so when we talk about
ridge regression it modifies the overfitted or underfitted models by adding the penalty equivalent to the sum
of the squares of the magnitude of the coefficients and so we have a cost function equals
loss equals lambda times the sum of w squared or the absolute value of w
depending on how you're doing it now remember we talked about error whether we either square it or we absolute value
it because that removes a plus or minus sign on there and there's reasons to do it either way but it is more common to
square the value and then we have our in this case the lambda is going to be
the penalty for the errors we've thrown in a greek character for you just to confuse everybody and w is the slope of the curve of the
line so uh we're going to look at this and we're going to draw a line this is going
to be like a linear regression model so if you had in sk learned you could import just a standard linear regression
model it would plot this line across whatever data we're working on
and we look at this of course we're just extrapolating this i know they use some specific data but you don't want to get
into the actual domain and so for a linear regression line let's consider two points that are on
the line and we'll go ahead and have a loss equals zero uh considering the two
points on the line we'll go and do lambda equals one we'll set our w is going to be one point
then the cost function equals 0 plus 1 times 1.4 squared which equals 1.96
so really don't get caught up too much in the math on this other than understanding that this is something
that's very easy for a computer to calculate and if you ever see the loss plus the plus the lambda times w the sum
of w squared and then let's say we have a ridge
regression line and it does this we go ahead and plot it and we do the calculations on the data
and for the ridge regression let's assume a loss equals 0.3 squared plus 0.2 squared equals 0.13 so when they put
all the calculations through of the two points we end up at the point 0.62
so we've now had a linear regression model we now had a ridge regression model and the ridge regression model plots a
little differently than the standard linear regression model
and comparing the two models with all the data points we can see that the ridge regression line fits the model
more accurately than the linear regression line and i find this true on a lot of data i
work with i'll end up using either the ridge regression model or the lasso mars regression model for fitting especially
dealing with a lot of like stock markets daily setup they come out slightly better you
get a slightly better fit and so we have our lasso we just talked
about lasso coming in here and the cost function equals
instead of doing a squared we're just going to do the absolute value and so if you remember this is where ridge
regression changes where's my ridge regression model we're squaring the value here
and if you look at this we're not squaring the value we're just finding the absolute value on here and so the loss of this of the squared individuals
and here is our lambda symbol again penalty for errors
and w equals the slope of the curve and comparing the two models with all
the data points we can see that the lasso regression line fits the model more accurately than the linear regression line
and this is like i said i use these two models a lot the ridge and this is important this is
this is kind of the meat of the matter how do you know which one to use some of it is you just do it a bunch of times
and then you figure it out uh ridge regularization is useful when we have many variables with relatively
smaller data samples the model does not encourage convergence towards zero but is likely to make them
closer to zero and prevent overfitting the lasser regularization model is
preferred when we are fitting a linear model with fewer variables so in the la in the iris thing we had
four or five variables as you measure the different leaf pieces you might be doing the measurements on
the cancer project which has 36 different variables
so as we get down to the iris with four variables lasso lar will probably uh work pretty
good where you might use the ridge regularization with more model with if you have something significant larger
and it encourages the coefficients of the variables to go towards zero because of the shape of the constraint which is
an absolute value and with any of this we want to go ahead and do a demo and lasso and ridge
regression so let's take a look and see what that looks like in our code and bring up our jupiter notebook
we'll start with our imports pandas is pd import numpy is np import matplot
library as plt um sklearn we're going to import our data sets it's kind of more generic
we usually just import one data set instead of all of them but you know quick and dirty when you're putting some of these together
we have our sklearn model selection we're going to import our train test split for splitting our data up
and then we'll bring in our linear regression model and we'll go ahead and run these just to
load them up and then load our data set we're just talking about that you could just
have imported the load boston and boston data set in there instead of loading all the data sets
and then once we've loaded our data set we want to go ahead and take a look at that data and
see what we got here let me just go ahead and pop that down there and
go and run it and so we've gone ahead and taken our boston
data we're going to look we put it into our pandas data frame the boston data set and then the boston
columns so we want to see what's going on with them we have our target we have the house price
etc and so our x equals boston um eye location
now remember in pandas the new updates to pandas they want eye location if
you're going to pull data we used to be able to leave this off but it does something different it creates a slice versus a direct
setup so make sure using that eye location and the i the output so this is all just bringing our data together
and we can see here if we do we print the boston panda's head we can see here all of our
different aspects we're looking for
and if you're following the x and the y the x is everything
except for the last column where y is uh all the it's that's what
this means all the rows except for the last column and then y is all the rows
but just the last column so y is our house price and the x is the
crimson industry chas knox and all these other different statistics they've collected for house
sales in boston there we go oops control
ah so we'll go ahead and split our data uh x train and our x
test y train y test equals the train test split which we imported
and we have our boston you could have easily used the x and y on here as opposed to boston
eye location and we'll create our test size we're going to take 25 of the data and put it
in as a test uh and then we'll go ahead and run this
i need an extra drink there uh so we have our train and test and then
of course the print the train data shape i love doing this kind of thing whenever
i'm working with this data print out the shape make sure everything looks correct uh so
that we have 127 by 13 and 127 by one 379 by 13 they should match
and if they're if the the data sets are not quite matching then you know something's wrong and
you're going to get all those errors i don't know how many times i've gone through here and it's dropped a row on one of them and
not on the other or something weird has happened when i'm cleaning the data this is pretty straightforward and simple because the data comes in a nice
pre-package is all clean for you so let's go ahead and apply apply the
multiple linear regression model and we'll call this l r e g l reg linear
regression and we're going to go ahead and fit that linear regression model to x train and y train
then we'll generate the prediction on the test set uh so here's our l reg y predict
with our x test going into the prediction and let's calculate that mean square
error mse i told you you'll see mse used a lot um people use it in variables and things
like that it's pretty common and we get our mean squared error equals this is just the basic formula we've
already been talking about what's the difference squared and then we look for the average of that
we'll go ahead and just run this and you can see when we get through the
end of this we have our mean square error on test we have our total and then we have each
column coming down and at this point unless you really know
the data you're working with it's not going to mean a whole lot so if it's in your domain you might be know what
you're looking at when you see these kinds of numbers coming up but if it's not it's just a bunch of numbers and that's okay
at least that's okay for this demo uh and then we're gonna go ahead and plot these so we can see what's going on
and this is always kind of fun it's always nice to have a nice visual of what you're looking at and you can see here
we plot the coefficient scores on here and we the guys in the back did a great job
putting some pretty colors together making it look nice and setting up the columns
you can see here uh your nox has like just a huge coefficient um
when i look at a table like this i look for what has very little
different coefficients they're not using a huge change and what has huge changes and that flags you for all kinds of
things as you're working with the data but it depends so much on the domain you're working with
these are great things though as just a quick look to see what's going on with your data and what you're looking for
and of course once we look at this now our motive is to reduce the coefficient score so now we want to take these and
and bring them down as much as we can and for that we're going to work with
the ridge regression on here so let's start by going we're going to import our ridge model
the rid regression from the sklearn library or the scikit and we're going to go ahead and train
the model so here's our ridge r equals alpha equals one and i
mentioned that earlier when i work with the ridge model you'll see alpha equals one if you set
alpha equal to zero that's a standard linear regression model so you have alpha equals one two three four and you
usually use one two or three four and a standard integer on there and we'll go ahead and fit the ridge model on there
with our x train and our y train data generate a prediction for that for our x
test and we'll calculate the mean square error just like we did before this should all
look familiar and we'll go ahead and print that out and we'll look at the ridge coefficients for our data and see
what that looks like now if i jump up and down between these two
you'll get a headache [Laughter] you'll still see the knox value let's just look at the knocks because that was
the biggest value is a minus 9 here and if we go back up here the nox value is a minus 18. so right off the bat i'm
seeing a huge change in the biggest coefficient there
uh so if we're going to do that nice setup we want to go ahead and just print it and see what that looks like
here we go and we've set up our plot subplots and again the team put together
some nice colors so it makes it look good and we're doing an x bar based on the columns
and our l regress coefficients color equals color x spine bottom and so forth
so just put together a nice little graph and you're starting to see one this when you compare this if you
put on the same graph as this one up here this is up here minus 18. this is at minus 9
and so this graph is half the size of the graph above the same thing with these values here
they might look the same but they're actually all almost half the value on here
and then finally you can do the same thing for the lasso regression this would all look
very similar as far as what we worked on before and i'm just going to print that on here
and run it and again let's go up to uh knox look where nox is it's all the way down
to zero and if we look at our next biggest coefficient it's minus 0.8 and really
here's our 22.73 let me go up here
16.7 and we go up here and we look at the same number
16.69 and so we look at this uh if i was
running this and doing a working on a project with this i would look at these numbers i start with the 16.69
come down here and compare it to 16.78 six nine is better than seven eight uh
so from the very beginning we might start looking at this first model for overall
predicting but there's other factors involved uh we
might know that uh the knox value is central and the other ones aren't quite as good and so we might start looking at
just certain setups like what is our what is this particular coefficient because it might
have a certain meaning to us and so forth and so you can look at all those different items in there again but the bottom
dollar is our first model did better than our other two models our mean square error on the test set
continues to come down on this we're going to cover reinforcement learning today and what's
in it for you we'll start with why reinforcement learning we'll look at what is
reinforcement learning we'll see what the different kinds of learning strategies are that are being used today in computer models under
supervised versus unsupervised versus reinforcement we'll cover important terms specific to
reinforcement learning we'll talk about markov's decision process and we'll take a look at a
reinforcement learning example but we'll teach a tic-tac-toe how to play why reinforcement learning training a
machine learning model requires a lot of data which might not always be available to us further the data provided might
not be reliable learning from a small subset of actions will not help expand the vast realm of solutions that may
work for a particular problem you can see here we have the robot learning to walk
very complicated setup when you're learning how to walk and you'll start asking questions like if i'm taking one
step forward and left what happens if i pick up a 50 pound object how does that change how a robot
would walk these things are very difficult to program because there's no actual information on it until it's
actually tried out learning from a small subset of actions will not help expand the vast realm of solutions that may
work for a particular problem and we'll see here learned how to walk this is going to slow the growth that
technology is capable of machines need to learn to perform actions by
themselves and not just learn off humans and you see the objective climb a
mountain a real interesting point here is that as human beings we can go into a
very unknown environment and we can adjust for it and kind of explore and
play with it most of the models the non-reinforcement models in computer
machine learning aren't able to do that very well there's a couple of them that can be used or integrated see how it
goes is what we're talking about with reinforcement learning so what is reinforcement learning
reinforcement learning is a sub-branch of machine learning that trains a model to return an optimum solution for a
problem by taking a sequence of decisions by itself consider a robot learning to go from one
place to another the robot is given a scenario must arrive at a solution by itself the robot
can take different paths to reach the destination it will know the best path by the time
taken on each path it might even come up with a unique solution all by itself
and that's really important is we're looking for unique solutions we want the best solution but you can't
find it unless you try it so we're looking at our different systems our different model we have
supervised versus unsupervised versus reinforcement learning and with the supervised learning that is probably the
most controlled environment we have a lot of different supervised learning models whether it's linear regression
neural networks there's all kinds of things in between decision trees the data provided is labeled data with
output values specified and this is important because we talk about supervised learning you already
know the answer for all this information you already know the picture has a motorcycle in it so you're supervised
learning you already know that the outcome for tomorrow for you know
going back a week you're looking at stock you can already have like the graph of what the next day looks like so
you have an answer for it and you have labeled data which is used you have an external supervision and
solves problems by mapping labeled input to known output so very controlled
unsupervised learning and unsupervised learning is really interesting because it's now taking part in many other
models they start with and you can actually insert an unsupervised learning model in almost either supervised or
reinforcement learning is part of the system which is really cool data provided is unlabeled data the
outputs are not specified machine makes its own predictions used to solve association with
clustering problems unlabeled data is used no supervision solves problems by
understanding patterns and discovering output so you can look at this and you can
think some of these things go with each other they belong together so it's looking for
what connects in different ways and there's a lot of different algorithms that look at this
when you start getting into those are some really cool images that come up of what unsupervised learning is how it
can pick out say the area of a donut one model will see the area of the donut
and the other one will divide it into three sections based on its location versus what's next to it so there's a
lot of stuff that goes in with unsupervised learning and then we're looking at reinforcement learning probably the biggest industry
in today's market uh in machine learning or growing market it's very it's very infant stage
as far as how it works and what it's going to be capable of the machine learns from its environment using rewards and errors used to solve
reward based problems no predefined data is used no supervision
follows trail and error problem solving approach so again we have a random at first you
start with a random i try this it works and this is my reward doesn't work
very well maybe or maybe doesn't even get you where you're trying to get it to do and you get your reward back and then
it looks at that and says well let's try something else and it starts to play with these different things finding the
best route so let's take a look at important terms in today's reinforcement model
and this has become pretty standardized over the last few years so these are really good to know we have
the agent agent is the model that is being trained via reinforcement learning so this is
your actual entity that has however you're doing it whether using a neural network or a
cue table or whatever combination thereof this is the actual agent that you're using this is the model
and you have your environment the training situation that the model must optimize to is called its
environment and you can see here i guess we have a robot who's trying to get a chest full of gyms or whatever and that's the
output and then you have your action this is all possible steps that can be taken by the model and it picks one action and
you can see here it's picked three different routes to get to the chest of diamonds and gems
we have a state the current position condition returned by the model
and you could look at this uh if you're playing like a video game this is the screen you're looking at so
when you go back here the environment is a whole game board so if you're playing one of those mobius
games you might have the whole game board going on uh but then you have your current position where are you on that
game board what's around that what's around you if you were talking about a robot
the environment might be moving around the yard where it is in the yard and what it can see what input it has in that location
that would be the current position condition returned by the model and then the reward to help the model move in the
right direction it is rewarded points are given to it to appraise some kind of action
so yeah you did good or didn't do as good trying to maximize the reward and have the best reward possible
and then policy policy determines how an agent will behave at any time it acts as
a mapping between action and present state this is part of the model what what is
your action that you're you're going to take what's the policy you're using to have an output from your agent
one of the reasons they separate uh policy as its own entity
is that you usually have a prediction um of a different options
and then the policy well how am i going to pick the best based on those predictions i'm going to guess at
different options and we'll actually weigh those options in and find the best option we think
will work so it's a little tricky but the policy thing is actually pretty cool how it works
let's go and take a look at a reinforcement learning example and just in looking at this we're going
to take a look consider what a dog that we want to train
so the dog would be like the agent so you have your your puppy or whatever and then your environment is going to be
the whole house or whatever it is where you're training them and then you have an action we want to teach the dog to fetch
so action equals fetching and then we have a little biscuit so we can get the dog to perform various
actions by offering incentives such as a dog biscuit as a reward the dog will follow a policy to maximize
this reward and hence will follow every command and might even learn new actions like begging by itself
uh so you have you know so we start off with fetching it goes oh i get a biscuit for that it tries something else you get
a handshake or begging or something like that and it goes oh this is also reward based and so it kind of explores things
to find out what will bring his biscuit and that's very much like how a reinforced model goes as it
looks for different rewards how do i find can i try different things and find a reward that works
the dog also will want to run around and play an explorers environment this quality of model is called
exploration so there's a little randomness going on in exploration
and explores new parts of the house climbing on the sofa doesn't get a reward in fact it usually gets kicked
off the sofa so let's talk a little bit about markov's decision process
markov's decision process is a reinforcement learning policy used to map a current state to an action
where the agent continuously interacts with the environment to produce new solutions and receive rewards
and you'll see here's all of our different uh vocabulary we just went over we have a reward our state or agent our
environment interaction and so even though the environment kind of contains
everything um that you really when you're actually writing the program your
environment is going to put out a reward in state that goes into the agent
uh the agent then looks at this state or it looks at the reward usually um first
and it says okay i got rewarded for whatever i just did or it didn't get rewarded and it looks at the state
and then it comes back and if you remember from policy the policy comes in
and then we have a reward the policy is that part that's connected at the bottom and so it looks at that policy and it
says hey what's a good action that will probably be similar to what i did or
sometimes are completely random but what's a good action that's going to bring me a different reward
so taking the time to just understand these different pieces as they go
is pretty important in most of the models today and so a lot of them actually have templates
based on this you can pull in and start using pretty straightforward as far as once
you start seeing how it works you can see your environment sends it says hey this is the agent did
this if you're a character in the game this happened and it shoots out a reward in a state the agent looks at the reward
looks at the new state and then takes a little guess and says i'm going to try this action and then that action goes
back into the environment it affects the environment the environment then changes
depending on what the action was and then it has a new state and a new reward that goes back to the agent
so in the diagram shown we need to find the shortest path between node a and d
each path has a reward associated with it and the path with a maximum reward is what we want to choose
the nodes a b c d denote the nodes to travel from node a to b
is an action reward is the cost of each path and policy is each path taken
and you can see here a can go uh to b or a can go to c right off the bat or
can go right to d and if explored all three of these you would find that a going to d was a zero reward
a going to c and d would generate a different reward or you could go a c b d there's a lot of
options here and so when we start looking at this diagram you start to realize
that even though today's reinforced learning models do really good at finding an answer they end up trying
almost all the different directions you see and so they take up a lot of work uh or
a lot of processing time for reinforcement learning they're right now in their infant stage and they're really
good at solving simple problems and we'll take a look at one of those in just a minute in the tic-tac-toe game
but you can see here once it's gone through these and it's explored it's going to find the a c d
is the best reward it gets a full 30 points for it so let's go ahead and take a look at a reinforcement learning
demo uh in this demo we're going to use reinforcement learning to make a tic-tac-toe game you'll be playing this
game against the machine learning model and we'll go ahead and we're doing it in python so let's go ahead and go through
i always not always actually have a lot of python tools let's go through anaconda which will open up a jupiter
notebook seems like a lot of steps but it's worth it to keep all my stuff separate and it's also has a nice
display when you're in the jupiter notebook for doing python so here's our anaconda navigator i open
up the notebook which is going to take me to a webpage and i've gone in here and created a new python folder in this case i've already
done it and enabled it to change the name to tic-tac-toe and then for this example we're going to go ahead and
import a couple things we're going to import numpy as np we'll go ahead and import pickle
numpy of course is our number array and then pickle is just a nice way sometimes for
storing uh different information different states that we're going to go through on here
and so we're going to create a class called state we're going to start with that
and there's a lot of lines of code to this class that we're going to put in here
don't let that scare you too much there's not as much here it looks like there's going to be a lie
here but there really is just a lot of setup going on in the in our class state
and so we have up here we're going to initialize it we have our board
it's a tic tac toe board so we're only dealing with nine spots on the board we
have player one player two uh is end we're going to create a board
hash we'll look at that in just a minute we're just going to store some information in there symbol player
equals one there's a few things going on as far as the initialization
uh then something simple we're just going to get the hash of the board you get the information
from the board on there which is columns and rows we want to know when a winner occurs so if you get three in a row
that's what this whole section here is for let me go ahead and scroll up a little bit
and you can get a copy of this code if you send a note over to simply learn we'll send you over
this particular file and you can play with it yourself and see how it's put together i don't want to spend a huge amount of
time on this because this is just some real general python coding
but you can see here we're just going through all the rows and you add them together and if it equals three three in a row same thing
with columns uh diagonal so you gotta check the diagonal that's what all this stuff does
here so it just goes through the different areas actually let me go ahead and put there we go
um and then it comes down here and we do our sum and it says true minus three just says did somebody win
or is it a tie so you gotta add up all the numbers on there anyway just in case they're all filled up
and next we also need to know available positions um these are ones that don't no one's ever used before this way when
you try something or the computer tries something uh it's not going to give it an illegal move that's what the
available positions is doing then we want to update our state and so you have your position going in
we're just sending in the position that you just chose and you'll see there's a little user interface we put in there
you we push pick the row and column in there
and again i mean this is a lot of code so really it's kind of a thing you'd
want to go through and play with a little bit and just read through it get a copy of it a great way to understand
how this works and here is a given reward so we're going to give a reward result
equal self winner this is one of the hearts of what's going on here is we have a result itself.winner
so if there's a winner then we have a result that the result equals one here's our feedback
if it doesn't equal one then it gets a zero so it only gets a reward in this particular case
if it wins and that's important to know because different
systems of reinforced learning do rewarding a lot differently depending on what you're trying to do this is a
very simple example with a three by three board imagine if you're playing a video game
certainly you only have so many actions but your environment is huge you have a
lot going on in the environment and suddenly a reward system like this is going to be just
it's going to have to change a little bit it's going to have to have different rewards and different setup and there's all kinds of advanced ways to do that as
far as weighing you add weights to it and so they can add the weights up depending on
where the reward comes in so it might be that you actually get a reward in this case you get the reward at the end of
the game and i'm spending just a little bit of time on this this is an important thing to note but there's different ways to
add up those rewards it might have like if you take a certain path the first reward
is going to be weighed a little bit less than the last reward because the last reward is actually winning the game or
scoring or whatever it is so this reward system gets really complicated in some of the more advanced
setups in this case though you can see right here that they give a
0.1 and a 0.5 reward just for getting picking the right value
and something that's actually valid instead of picking an invalid value so rewards again that's like key it's huge
how do you feed the rewards back in then we have a board reset that's pretty
straightforward it just goes back and resets the board to the beginning because it's going to try out all these different things while it's learning
it's going to do it by trial and error so you have to keep resetting it and then of course there's the play we want
to go ahead and play rounds equals 100 depends on what you want to do on here
you can set this different you can obviously set that to higher level but this is just going to go through and you'll see in here
that we have player one and player two this is this is the computer playing
itself uh one of the more powerful ways to learn to play a game
or even learn something that isn't a game is to have two of these models that
are basically trying to beat each other and so they always they keep finding explore new things this one works for
this one so this one tries new things it beats this we've seen this in chess i think with some big one where
they had the two players in chess with reinforcement learning uh it was one of the ways they trained one of the top
computer chess playing algorithms so this is just what this is it's going
to choose an action it's going to try something and the more i try stuff the more we're going to record the hash
we actually have a board hash where they self get the hash set up on here where it stores all the information
and then once you get to a win one of them wins it gets the reward then we go back and reset and try again
and then kind of the fun part we actually get down here is we're going to play with a human so we'll get a chance
to come in here and see what that looks like when you put your own information in and then it just comes in here and does
the same thing it did above it gives it a reward for its things or sees if it wins or ties
looks at available positions all that fun of fun stuff and then finally we want to show the
board so it's going to print the board out each time really
as an integration is not that exciting what's exciting in here is one looking at this reward
system whoops play one more up the reward system is really the heart of this how do you reward the different
setup and the other one is when it's playing it's got to take an action
and so what it chooses for an action is also the heart of reinforcement learning how
do we choose that action and those are really key to right now where reinforcement
learning is in today's technology is figuring this out
how do we reward it and how do we guess the next best action so we have our
environment and you can see the environment is we're going to be or the state which is kind of like what's going on
we're going to return the state depending on what happens and we want to go ahead and create our agent in this case our player so each
one is let me go and grab that until we look at a class player um
this is where a lot of the magic is really going on is what how is this player figuring out how to maneuver
around the board and then the board of course returns a state that it can look at and reward
so we want to take a look at this we have name self-state this is class player and when
you say class player we're not talking about a human player we're talking about just the computer players
and this is kind of interesting so remember i told you depending on what you're doing there's going to be a decay gamma
explore rate these are what i'm talking about is how do we train it
as you try different moves it gets to the end the first move is important but
it's not as important as the last one and so you could say that the last one has the heaviest weight and then as you
as you get there the first one let's see the first move gives you a five reward the second gives you a two reward and
the third one gives you a 10 reward because that's the final ending you got it the 10 is going to count more than the
first step uh and here's our we're going to get the board information coming in and
then choose an action this was the second part that i was talking about that was so important
so once you have your training going on we have to do a little randomness and you can see right here is our np random
uniform so it's picking out a random number take a random action
this is going to just pick which row and which column it is and so choosing the action
this one you can see we're just doing random states choice length of positions action
position and then it skips in there and takes a look at the board uh for p in positions
it's actually storing the different boards each time you go through so it has a record of what it did so it
can properly weigh the values and this simply just depends a hash date what's the last date append it to the
to our states on here here's our feedback rewards the reward comes in and it's
going to take a look at this and say is it none what is the reward and here is that
formula remember i was telling you about up here that was important because it has
decay gamma times the reward this is where as it goes through each
step this and this is really important this is this is kind of the heart of this of what i was talking about earlier uh you
have step one and this might have a reward of two you
have step two i should probably should have done abc this has a step three
uh step four and so on until you get to step in
and this might have a reward of ten so reward a 10
we're going to add that but we're not adding let's say this one right here
let's say this reward here right before 10 was let's say it's also 10 it just makes the
math easy so we had 10 and 10. we had 10 this is 10 and 10 in whatever
it is but it's time it's 0.9 so instead of putting a full 10 here
we only do 9 that's a 0.9 times
10. and so this formula
as far as the decay times the reward minus the cell state value uh it basically adds in it says here's
one or here's two i'm sorry i should have done this abc would have been easier so the first move goes in here and it
puts two in here then we have our self
set up on here you can see how this gets pretty complicated in the math but this is really the key is how do we train
our states and we want the the final state the win to get the most points if
you win you get most points and the first step gets the least amount of points
so you're really training this almost in reverse you're training you're training it from the last place where you have
like it says okay this is now i was need to sum up my rewards and i want to sum them up going in reverse and i want to
find the answer in reverse kind of an interesting play on the mind when you're trying to figure this stuff out
and of course we want to go ahead and reset the board down here uh save the policy load policy
these are the different things that are going in between the agent and the state to figure out what's going on let's go
ahead and load that up and then finally we want to go ahead and create a human player
and the human player is going to be a little different in that you choose an action row and column
here's your action uh if action is if action in positions meaning positions
that are available uh you return the action if not it just keeps asking you until
you get the action that actually works and then we're going to go ahead and append to the hash state which we don't
need to worry about because it returns the action up here and feed forward
again this is because it's a human at the end of the game bat propagate and
update state values this part isn't being done because it's not programming uh
the model the model is getting its own rewards so we've gone ahead and loaded this in
here so here's all our pieces and the first thing we want to do
is set up p1 player 1 p2 player 2 and then we're going to send our players
to our state so now it has p1 p2 and it's going to play and it's going to play 50 000 rounds
now we can probably do a lot less than this and it's not going to get the full results in fact you know what uh let's
go ahead and just do five just to play with it because i want to show you something here
oops somewhere in there i forgot to load something there we go
i must have forgot to run this run
oops forgot a reference there for the board rows and columns three by three
there is actually in the state it references that we just tack it on on the end it was supposed to be at the
beginning uh so now i've only set this up with
let's see where we go in here i've only set this up to train
five times and the reason i did that is we're going to
come in and actually play it and then i'm going to change that and we can see how it differs on there
there we go and then you make it through a run and we're going to go ahead and save the policy
so now we have our player 1 and our player 2 policy the way we set it up it has two separate
policies loaded up in there and then we're going to come in here and
we're going to do player 1 is going to be the computer experience rate 0 load policy 1 human player human and we're
going to go ahead and play this i remember i only went through it just one round of training in fact
minimal training and so it puts an x there and i'm going to go ahead and do row zero column one
you can see this is very uh basic on here and so i put in my zero and then i'm going
to go zero block it zero zero and you can see right here it let me win
uh just like that i was able to win 0 2
and human wins so i only trained it five times we're
going to run this again and this time instead of five let's do five thousand
or fifty thousand i think that's what the guys in the back had and this takes a while to train it
this is where reinforcement learning really falls apart look how simple this game is we're
talking about a three by three set of columns and so for me to train it
on this i could do a q table which would take which would go much quicker
you could build a quick cue table with almost all the different options on there and
you would probably get a the same result much quicker we're just using this as an example
so when we look at reinforcement learning you need to be very careful
what you apply it to it sounds like a good deal until you do like a large neural network
where you're doing you set the neural network to a learning increment of one so every time it goes through it learns
and then you do your actions you pick from the learning setup and you actually try actions on
the learning setup until you get what you think is going to be the best action so you actually feed what you think is
right back through the neural network there's a whole layer there which is really fun to play with
and then it has an output well think of all those processes i mean that is just a huge amount of work it's going to do
let's go ahead and skip ahead here give it a moment it's going to take a minute or two to go ahead and run
now to train it we went ahead and let it run and it took a while this this took
i got a pretty powerful processor and it took about five minutes plus to run it
and we'll go ahead and run our player setup on here oops
brought in the last whoops i brought in the last round so give me just a moment to re
do the policy save there we go i forgot to save the policy back in there
and then go ahead and run our player again so we've saved the policy then we want
to go ahead and load the policy for p1 as the computer and we can see the computer's gone in
the bottom right corner i'm going to go ahead and go 1-1 which is the center
and it's gone right up the top and if you ever played tic-tac-toe you know the computer has me but we'll go ahead and
play it out row zero column two there it is and then it's gone here and
so i'm going to go ahead and go row zero one two no zero one there we go and column zero
that's where i want it oh and it says okay you your action there we go boom uh so you can see here
we've got a didn't catch the win on this it said tie um kind of funny that didn't catch the win on there
but if we play this a bunch of times you'll find it's going to win more and more the more we train it the more the reinforcement happens
this lengthy training process is really the stopper on reinforcement
learning as this changes reinforcement learning will be one of the more powerful
packages evolving over the next decade or two in fact i would even go as far as to say it is the most important
machine learning tool and artificial intelligence tool out there as it learns not only a simple tic tac
toe board but we start learning environments and the environment would be like in language
if you're translating a language or something from one language to the other so much of it is lost if you don't know
the context it's in what the environments it's in and so being able to attach environment and context and
all those things together is going to require reinforcement learning to do
so again if you want to get a copy of the tic tac toe board it's kind of fun to play with run it you can test it out
you can do you know test it for different uh values you can switch from p1 computer
where we loaded the policy one to load the policy two and just see how it varies there's all kinds of things you
can do on there so what is q learning q learning is reinforcement learning policy which will fill the next best
action given a current state it chooses this action at random and aims to maximize the reward
and so you can see here's our standard reinforcement learning graph by now if you're doing any reinforcement
learning you should be familiar with this where you have your agent your agent takes an action
the action affects the environment and then the environment sends back the reward or the feedback and the state
it's the new state the agent's in where is it at on the chessboard where is it at in the video game
if your robots out there picking trash up off the side of the road where is it at on the road
consider an ad recommendation system usually when you look up a product online
you get ads which will suggest the same product over and over again
using q learning we can make an ad recommendation system which will suggest related products to
our previous purchase the reward will be if user clicks on the suggested product
and again you can see you might have a lot of products on your
web advertisement or your pages but it's still not a float number it's still a set number and that's something to be
aware of when you're using q learning and you can see here that if you have a hundred people clicking on ads
and you click on one of the ads it might go in there and say okay this person clicked on this ad
what is the best set of ads based on clicking on this ad or these two ads afterwards based on where they are
browsing so let's go ahead and look at some important terms we talk about q learning
we have states the state s represents the current position of an agent in an environment
the action the action a is the step taken by the agent when it is particular state
rewards for every action the agent will get a positive or negative reward
and again when we talk about states we're usually not with when you're using a q table
you're not usually talking about float variables you're talking about true false and we'll take a closer look at that in
a second and episodes when an agent ends up in a terminating state and can't take a new
action uh this might be if you're playing a video game your character stepped in and
is now dead or whatever q values used to determine how good an
action a taken at a particular state s is q a of s
and temporal difference a formula used to find the q value by using the value of the current state and action and
previous state in action and various i mean there's bellman's equation which basically is the equation
that kind of covers what we just looked at in all those different terms the bellman
equation is used to determine the values of a particular state and deduce how good it is to be in take that state the
optimal st the optimal state will give us the highest optimal value
factor influencing q values the current state and action that's your sa so your
current state and your action uh then you have your previous date in action which is your s
i guess prime i'm not sure how they reference that s prime a prime so this is what happened before
then you have a reward for action so you have your r reward and you have your maximum expected
future reward and you can see there's also a learning
rate put in there and a discount rate so we're looking at these just like any
other model we don't want to have an absolute final value on here we don't want it to
if you do absolute values instead of taking smaller steps you don't really have that approach to
the solution you just have a jump and then pretty soon if you jump one solution out that's what's going to be
the new solution whichever one jumps up really high first kind of ruining the whole idea of doing
a random selection i'll go into the random selection just a second steps in q learning
step one create an initial q table with all values initialized to zero
again we're looking at zero one so are you you know here's our action we
start we're an idle we took a wrong action we took a correct action and int
and then we have our actions fetching sitting and running of course we're just using the dog
example and choose an action and perform it update values in the table and of
course when we're choosing an action we're going to kind of do something random and just randomly pick one so you
start out and you sit and you have then then depending on that
action you took you can now update the value for sitting after you start from start to sitting get the value of the
reward and calculate the value the value q value using the bellman equation
and so now we attach a reward to sitting and we attach all those rewards we
continue the same until the table is filled with or an episode ends
and you and my membership is going to come back to the random side of this and there's a few different formulas i use for the random
setup to pick it i usually look whatever q model i'm using do their standard one because someone's usually gone in and
done the math for the optimal spread but you can look at this if i have
running has a reward of 10 sitting as a reward is seven fetching has a reward of
five um just kind of without doing like a a means to you know using the bell
curve for the means value and like i said there's some math you can put in there to pick so that you're more like
so that running has even a higher chance but even if you were just going to do an average on this
you could do an average a random number by adding them all together so you get 10 plus 7 plus 5 is
22 you could do 0 to 22 or 0 to 21 but 1 to 22
one to five would be fetching uh and so forth you know the last 10.
so you can just look at this as what percentage are you going to go for that particular option
and then that gets your random setup in there and then as you slowly increment these up uh you see that uh
if you're idle where's one here we go sitting at the end if you're at the end of wherever you're
at sitting gets a reward of one where's a good one on here oh wrong action running for a wrong action gets
almost no reward so that becomes very very less likely to happen but it still might happen it
still might have a percentage of coming up and that's where the random programming and q learning comes in the below table
gives us an idea of how many times an action has been taken and how positively correct action or negatively wrong
action it is going to affect the next state so let's go ahead and dive in and pull
up a little piece of code and see what this looks like in python
in this demo we'll use q learning to find the shortest path between two given points
if getting your learning started is half the battle what if you could do that for free visit scale up by simply learn
click on the link in the description to know more if you've seen my videos before
i like to do it in the anaconda jupiter notebook
setup just because it's really easy to see and it's a nice demo uh and so here's my anaconda this one i'm actually
using a python36 environment that i set up in here and we'll go ahead and launch the
jupiter notebook on this and once we're in our jupiter notebook uh which has the kernel loaded with
python3 we'll go ahead and create a new python3 folder in here
and we'll call this uh q learning
and to start this demo let's go ahead and import our numpy
array we'll just run that so it's imported and like a lot of these uh model
programs when you're building them you spend a lot of time putting it all together
and then you end up with this really short answer at the end uh and we'll we'll take a look at that
as we come into it so we go ahead and start with our location to state so we have
l1 l2 these are our nine locations one to nine and then of course the state is going to
be zero one two three four it's just a mapping of our location to a integer on there and then we have our actions our
actions are simply uh moving from um one location
to another so i can go to i can go to location zero i can go to location one two three four five six seven eight uh
so these are my actions i can choose these are the locations of our state
and if you remember earlier i mentioned that the limitation is that you you
don't want to put in a continually growing table because you can actually create a dynamic cue table
where you continually add in new values as they arise because
if you have float values this just becomes infinite and then you're remembering your computer's gone or you
know does it's not going to work at the same time you might think well that kind of really limits the the q uh
learning setup but there are ways to use it in conjunction with other systems
and so you might look at uh well i do i've been doing some work in stock
and one of the questions that comes out is to buy or sell the stock
and the state coming in might be you might take a concrete way called
buckets where anything that you predict is going to return more than a certain amount of
money the error for that stock that you've had in the past you put those in buckets and
suddenly you start putting creating these buckets you realize you do have a limited amount of information coming in
you no longer have a float number you now have bucket one two three and four
and then you can take those buckets put them through a q learning table
and come up with the best action which stock should i buy it's like gambling stock is pretty much gambling if you're
doing day trading you're not doing long term investments and so you can start looking
at it like that a lot of the current fees say that the best algorithms used for day traders redoing
it on your own is really to ask the question do i want to trade the stock yes or no and now you
have it in a q learning table and now you can take it to that next level and you can see where that can be a really powerful tool at the end of doing a
basic linear regression model or something what is the best investment and you start getting the best reward on there
and the so if we're going to have rewards these rewards we just create it says if basically if you're
this should match our q table because it's going to be you have your state and you have your
action across the top if you remember from the dog and so we have whatever state we're in
going down and then the next action and what the reward is for it and of course if you were actually doing
a something more connected your reward would be based on the actual environment it's in
and then we want to go ahead and create a state to location so we can map the indexes
so just like we defined our rewards uh we're going to go and do state to location
and you can see here it's a dictionary set up for location state and location to state with items
and we also need to define what we want for learning rates
uh you remember we had our two different rates as far as like learning from the past
and learning from the current so we'll go ahead and set those to 0.75 and the alpha set to 0.9 and we'll see that when
we do the formula and of course any of this code send a note to our simply learn team they'll
get you a copy of this code on here let's go ahead and pull
there we go on the next two sections
since we're going to keep it short and sweet here we go so let's go ahead and create
our agent um so our agent is going to have our initialization where we send it all the information uh
we'll define ourself gamma equals gamma we could have just set the gamma rate down here instead of submitting it
it's kind of nice to keep them separate because you can play with these numbers on our self alpha
then we have our location state we'll set that in here we have our choice of actions
we're going to go ahead and just embed the rewards right into the agent
so obviously this would be coming from somewhere else instead of from
self-generated and then a self state to location equals our state to location
dictionary and we go ahead and create a q learning table and i went ahead and just set the q learning table up to
zero to zero what what what the setup is location to state how many of them are there
and this just creates an array of zero to zero set up on there
and then the big part is the training we have our rewards new equals a copy of self.rewards
ending state equals the self-location state in location so this is whatever we end up
at rewards new equals ending state plus ending state equals 999. just kind of goes
to a dead end and we start going through iterations
and we'll go ahead um let's do this uh so this we're going to come back and
we're going to call call it on here let me just erase that switch it to an arrow
there we go so what we're doing is we're going to send in here to train it we're going to say hey
i want to iterate through this a thousand times and see what happens now this part would
actually be instead of iterating you might have your external environment and they're going
back and forth and you iterate through outside of here but just for ease of use our agent is
going to come in here and iterate through this sometimes i'll put this iteration in here
and i'll have it call the environment and say hey this is what i did what's the next state and the environment does
its thing right in here as i iterate through it and then we want to go ahead and pick a
random state to start with that's what's going on here you have to start somewhere
and then you have your playable actions we're going to start with just an empty thing for playable actions and we'll fill that up
so that's what choices i have and so we're going to iterate through the rewards matrix to get the states
directly reachable from the randomly chosen current state assign those states to a list named playable actions
and so you can see here we have a range nine i usually use length of whatever
i'm looking at which is our locations or states as they are we have a reward so we want to look at
the current the rewards uh the new reward is our
is in our chart here of rewards underscore new uh current state
plus j j being what is the next date we want to try and so we go ahead and do our playable
actions and we append j and so we're doing is we're randomly
trying different things in here to see what's going to generate a better reward and then of course we go ahead
and choose our next state so we have our random choice playable actions
and if you remember i mentioned on this let me just go ahead and let's do a free form
when we were talking about the next state this right here just does a random
selection instead of a random selection you might do something where
whatever the best selection is which might be option three
here and then so you can see that it might use a bell curve and then option two over here might have a bell curve
like this oops and we start looking at these averages and these spreads
or we can just add them all together and pick the one that kind of goes in all of those
so those are some of the options we have in here we just go with a random choice that's usually where you start play with
it and then we have our reward section down here and so we want to go ahead and find well
in this case the temporal difference uh so you have your rewards new plus the self gamma and this is the
formula we were looking at this is bellman's equation here so we have our current value our
learning rate our discount rate involved in there the reward system coming in for
that and we can add it all together this is of course our maximum expected future setup in here
so this is all of our our bellman's equation that we're looking at here and then we come up in here and we
update our q table that's all this is on this one and that's right here we have
self-cue current state next date and we add in our alpha because we don't want
to we don't want to train all of it at once in case there's slight differences coming in there we want to slowly
approach the answer and then we have our route equals the start location
and next location equals start location so we're just incrementing we took a step forward
and then finally remember i was telling you how we're going to do all this and just have some
simple thing at the end or just generates a simple path we're going to go ahead and and get the
optimal route we want to find the best route in here and so we've created a definition for
the optimal route down here scroll down for that and we get the optimal route we go ahead
and put the information in including the queue table self start location in location next location
route queue and it says while next location is not equal to in location so while we can still go
our start location equals self location to state start location so we already
have our best value for the start location the next state looks at the q table and
says hey what's uh the next one with the best value and then the next location we go ahead and pull that in and we just
append it that's what's going on down here and then our start location equals the
next location and we just go through all the steps and we'll go ahead and run this and now that we have our q table our
q agent loaded we're going to go ahead and take our q agent load them up with our alpha gamma that
we set up above along with the location step action reward state to location
and uh our goal is to plot a course between l9 and l1
and we're going to go through a hundred a thousand iterations on here and so when i run that it runs pretty quick
uh why is this so fast um if you've been running neural networks and you've been doing all these other
models you sit here and wait a long time well we're very small amount of data these are all integers these aren't
float values there's not a the math is not heavy on the on the processing end and this is where q tables are so
powerful if you have a small amount of information coming in you very quickly uh get an answer off of
this even though we went through it a thousand times to train it and you'll see here we have l nine eight five two
and one and that's based on our reward table we had set up on there and this is
the shortest path going between these different uh setups in here
and if you remember on our reward table you can see that if you start here you can go to here there's places you can't
go that's how this reward table was set up so i can only go to certain places
so kind of a little maze set up in there and you can play with it this is really fun setup to play with
and you can see how you can take this whole code and you can like i was saying earlier you can embed it into another
setup and model predictions where you put things into buckets and you're trying to guess the best investment the
best course of action long as you can take that course into action and
reduce it down to a yes no or if you're using text
you can use a one hot encoder which word is next there's all kinds of things you can do with a cue table
depending on just how much information you're putting in there so that wraps up our demo in this demo
we've found the shortest distance between two paths based on whatever rules or state rewards we have to get
from point a to point b and what available actions there are hello and welcome to this tutorial on deep
learning my name is mohan and in the next about one one and a half hours i will take you through what is deep
learning and into tensorflow environment to show you an example of deep learning now there
are several applications of deep learning really very interesting and innovative applications and one of them
is identifying the geographic location based on a picture and how does this
work the way it works is pretty much we train an artificial neural network with
millions of images which are tagged their geolocation is tagged and then
when we feed a new picture it will be able to identify the geolocation of this
new image for example you have all these images especially with maybe some
significant monuments or significant locations and you train with
millions of such images and then when you feed another image it need not be exactly one of those that you have
claimed it can be completely different that is the whole idea of training it will be able to recognize for example
that this is a picture from paris because it is able to recognize the eiffel tower so the way it works
internally if we have to look a little bit under the heart as these images are
nothing but this is digital information in the form of pixels so each image
could be a certain size it can be 256 by 256 pixel kind of a resolution and then
each pixel is either having a certain grade of color and all that is fed into
the neural network and it then gets trained in and it's able to based on
these pixels pixel information it is able to get trained and able to
recognize the features and extract the features and thereby it is able to
identify these images and the location of these images and then when you feed a
new image it kind of based on the training it will be able to figure out where this image is from so that's the
way a little bit under the hood how it works so what are we going to do in this tutorial we will see what is deep
learning and what do we need for deep learning and one of the main components
of deep learning is neural networks so we will see what is neural network what is a perceptron and how to implement
logic gates like and or nor and so on using perceptrons the different types of
neural networks and then applications of deep learning and we will also see how
neural networks works so how do we do the training of neural networks
and at the end we will end up with a small demo code
which will take you through in tensorflow now in order to implement
deep learning code there are multiple libraries or development environments that are available and tensorflow is one
of them so the focus at the end of this would be on how to use tensorflow to
write a piece of code using python as a programming language and we will take up
an example which is a very common one which is like the hello world of deep learning the handwriting number
recognition which is a mnist commonly known as mnist database so we will take
a look at mnist database and how we can train a neural network to recognize
handwritten numbers so that's what you will see in this particular
video so let's get started what is deep learning deep learning is like a subset
of what is known as a high level concept called artificial intelligence you must
be already familiar must have heard about this term artificial intelligence so artificial intelligence is like the
high level concept if you will and in order to implement artificial
intelligence applications we use what is known as machine learning and within machine
learning a subset of machine learning is deep learning machine learning is a little bit more generic concept and deep
learning is one type of machine learning if you will and we will see a little later in maybe the following slides a
little bit more in detail how deep learning is different from traditional machine learning but to start with we
can mention here that deep learning uses one of the differentiators between deep learning and traditional machine
learning is that deep learning uses neural networks and we will talk about what are neural networks and how we can
implement neural networks and so on and so forth as a part of this tutorial so a little deeper into deep learning deep
learning primarily involves working with complicated unstructured data compared
to traditional machine learning with where we normally use structured data in deep learning the
data would be primarily images or voice or maybe text files so
and it is large amount of data as well and deep learning can handle complex operations it involves complex
operations and the other difference between traditional machine learning and deep learning is that the feature
extraction happens pretty much automatically in traditional machine learning feature engineering is done
manually the data scientists we data scientists have to do feature engineering feature extraction but in
deep learning that happens automatically and of course deep learning for large
amounts of data complicated unstructured data deep learning gives very good performance now as i mentioned one of
the secret sources of deep learning is neural networks let's see what neural networks
is neural networks is based on our biological
neurons the whole concept of deep learning and artificial intelligence is
based on human brain and human brain consists of billions of tiny stuff
called neurons and this is how a biological neuron looks and this is how an
artificial neuron looks so neural networks is like a simulation of our
human brain human brain has billions of biological neurons and we are trying to
simulate the human brain using artificial neurons this is how a biological neuron looks it has dendrites
and the corresponding component with an artificial neural network is or an artificial neuron are the inputs they
receive the inputs through dendrites and then there is the cell nucleus which is basically the processing unit in a way
so in artificial neuron also there is a piece which is an equivalent of this
cell nucleus and based on the weights and biases we will see what exactly
weights in biases are as we move the input gets processed and that results in
an output in a biological neuron the output is sent through a synapse and in an artificial neuron there is an
equivalent of that in the form of an output and biological neurons are also interconnected so there are billions of
neurons which are interconnected in the same way artificial neurons are also interconnected so this output of this
neuron will be fed as an input to another neuron and so on now in neural
network one of the very basic units is a perceptron so what is a
perceptron a perceptron can be considered as one of the fundamental
units of neural networks it can consist at least one neuron but sometimes it can
be more than one neuron but you can create a perceptron with a single neuron
and it can be used to perform certain functions it can be used as a basic
binary classifier it can be trained to do some basic binary classification and
this is how a basic perceptron looks like and this is nothing but a neuron
you have inputs x 1 x 2 x 2 x n and there is a summation function and then
there is what is known as an activation function and based on this input what is
known as the weighted sum the activation function either gets gives an output
like a zero or a one so we say the neuron is either activated or not so that's the way it works so you get the
inputs these inputs are each of the inputs are multiplied by a weight and there is a bias that gets added and that
whole thing is fed to an activation function and then that results in an output and if the output is correct it
is accepted if it is wrong if there is an error then that error is fed back and
the neuron then adjusts the weights and biases to give a new output and so on
and so forth so that's what is known as the training process of a neuron or a
neural network there's a concept called perceptron learning so perceptron learning is again one of the very basic
learning processes the way it works is somewhat like this so you have all these
inputs like x1 to xn and each of these inputs is multiplied by a weight and
then that sum this is the formula of the equation so that sum w i x i
sigma of that which is a sum of all these product of x and w is added up and
then a bias is added to that the bias is not dependent on the input but or the
input values but the bias is common for one neuron however the bias value keeps
changing during the training process once the training is completed the values of these weights w1 w2 and so on
and the value of the bias gets fixed so that is basically the whole training
process and that is what is known as the perceptron training so the weights and
biases keep changing till you get the accurate output and the summation is of
course passed through the activation function as you see here this w i x i
summation plus b is passed through activation function and then the neuron gets either fired or
not and based on that there will be an output that output is compared with the
actual or expected value which is also known as labeled information so this is
the process of supervised learning so the output is already known and
that is compared and thereby we know if there is an error or not and if there is an error the error is fed back and the
weights and biases are updated accordingly till the error is reduced to
the minimum so this iterative process is known as perceptron learning or
perceptron learning rule and this error needs to be minimized so till the error
is minimized this iteratively the weights and biases keep changing and
that is what is the training process so the whole idea is to update the weights
and the bias of the perceptron till the error is minimized the error need not be
zero the error may not ever reach zero but the idea is to keep changing these
weights and bias so that the error is minimum the minimum possible that it can
have so this whole process is an iterative process and this is the iteration continues till either the
error is zero which is uh unlikely situation or it is the minimum possible
within these given conditions now in 1943 two scientists warren mcculloch and
walter pitts came up with an experiment where they were able to implement the
logical functions like and or and nor using neurons and that was a significant
breakthrough in a sense so they were able to come up with the most common
logical gates they were able to implement some of the most common logical gates which could take two
inputs like a and b and then give a corresponding result so for
example in case of an and gate a and b and then the output is a b in case
of an or gate it is a plus b and so on and so forth and they were able to do this using a single
layer perceptron now most of these gates it was possible to use single layer
perceptron except for xor and we will see why that is in a little bit so this
is how an and gate works the inputs a and b the output should be fired or the
neuron should be fired only when both the inputs are one so if you have zero zero the output should be zero for 0 1
it is again 0 1 0 again 0 and 1 1 the output should be 1. so how do we implement this with a
neuron so it was found that by changing the values of weights it is possible to
achieve this logic so for example if we have equal weights like 0.7 0.7 and then if we take
the sum of the weighted products so for example 0.7 into 0 and then 0.7 into 0
will give you 0 and so on and so forth and in the last case when both the inputs are 1 you get a value which is
greater than 1 which is the threshold so only in this case the neuron gets activated and the output is there is an
output in all the other cases there is no output because the threshold value is one so this is implementation of an and
gate using a single perceptron or a single neuron similarly an or gate in
order to implement an or gate in case of an or gate the output will be one if either of these inputs is one so for
example zero one will result in one or other in all the cases it is one except for zero zero so how do we implement
this using a perceptron once again if you have a perceptron with weights for example 1.2 now if you see here if in
the first case when both are 0 the output is 0 in the second case when it is 0 and 1 1.2 into 0 is 0 and then 1.2
into 1 is 1. and in the second case similarly the output is 1.2 in this last
case when both the inputs are 1 the output is 2.4 so during the cleaning
process these weights will keep changing and then at one point where the weights
are equal to w1 is equal to 1.2 and w2 is equal to 1.2 the system learns that
it gives the correct output so that is implementation of or gate using a single neuron or a single layer perceptron now
xor gate this was one of the challenging ones they tried to implement an xor gate
with a single level perceptron but it was not possible and therefore in order
to implement an xr so this was like a roadblock in the progress of
neural network however subsequently they realized that this can be implemented an
xor gate can be implemented using a multi-level perceptron or mlp so in
this case there are two layers instead of a single layer and this is how you can implement an xor gate so you will
see that x1 and x2 are the inputs and there is a hidden layer and that's why it is denoted as h3nh4 and then you take
the output of that and feed it to the output at o5 and provide a threshold
here so we will see here that this is the numerical calculation so the weights
are in this case for x1 it is 20 and minus 20 and once again 20 and minus 20.
so these inputs are fed into h3 and h4 so you'll see here for h3 the
input is 0 1 1 1 and for h4 it is 1 0 1 1 and if you now look at the output
final output where the threshold is taken as 1 if we use a sigmoid with the
threshold one you will see that in these two cases it is zero and in the last two
cases it is one so this is a implementation of xor in case of x or
only when one of the inputs is one you will get an output so that is what we are seeing here if we have either both
the inputs are one or both the inputs are zero then the output should be zero so that is what is an exclusive or gate
so it is exclusive because only one of the inputs should be one and then only you will get an output of one which is
satisfied by this condition so this is a special implementation xor gate is a special implementation of perceptron now
that we got a good idea about perceptron let's take a look at what is the neural network so we have seen what is a
perceptron we have seen what is a neuron so we will see what exactly is a neural
network so neural network is nothing but a network of these neurons and they are
different types of neural networks there are about five of them these are artificial neural network convolutional
neural network then recursive neural network or recurrent neural network deep
neural network and deep belief network so and each of these types of neural
networks have a special you know they can solve special kind of problems for
example convolutional neural networks are very good at performing image processing and image recognition and so
on whereas rnn are very good for speech recognition and also text analysis and
so on so each type has some special characteristics and they can they're good at performing certain special kind
of tasks what are some of the applications of deep learning deep learning is today used extensively in
gaming you must have heard about alphago which is a game created by a startup
called deepmind which got acquired by google and alphago is an ai which
defeated the human world champion lee sedol in this game of go so gaming is an
area where deep learning is being extensively used and a lot of research happens in the area of gaming
as well in addition to that nowadays there are neural networks a special type called generative adversarial networks
which can be used for synthesizing either images or music or text and so on
and they can be used to compose music so the neural network can be trained to
compose a certain kind of music and autonomous cars you must be familiar
with google google's self-driving car and today a lot of automotive companies
are investing in this space and deep learning is a core component of this
autonomous cars the cars are trained to recognize for example the road the
lane markings on the road signals any objects that are in front
any obstruction and so on and so forth so all this involves deep learning so that's another major application and
robots we have seen several robots including sofia you may be familiar with
sophia who was given a citizenship by saudi arabia and there are several such
robots which are very human like and the underlying technology in many of these
robots is deep learning medical diagnostics and health care is another
major area where deep learning is being used and within healthcare diagnostics
again there are multiple areas where deep learning and image recognition image processing can be used for example
for cancer detection as you may be aware if cancer is detected early on it can be
cured and one of the challenges is in the availability of specialists who can
diagnose cancer using these diagnostic images and various scans and and so on
and so forth so the idea is to train neural network to perform some of these
activities so that the lord on the cancer specialist doctors or oncologists
comes down and there is a lot of research happening here and there are already quite a few applications that
are claimed to be performing better than human beings in this space can be
lung cancer it could be breast cancer and so on and so forth so health care is a major area where deep learning is
being applied let's take a look at the inner working of a neural network so how does an artificial neural network let's
say identify can we train a neural network to identify the shapes like
squares and circles and triangles when these images are fed so this is how it
works any image is nothing but it is a digital information of the pixels so in
this particular case let's say this is an image of 28 by 28 pixel and this is
an image of a square there's a certain way in which the pixels are lit up and
so these pixels have a certain value maybe from 0 to 256 and 0 indicates that
it is black or it is dark and 256 indicates it is completely it is white or lit up so
that is like an indication or a measure of the how the pixels are lit up and so
this is an image is let's say consisting of information of 784 pixels so all the information what
is inside this image can be kind of compressed into this 784 pixels the way
each of these pixels is lit up provides information about what exactly is the
image so we can train neural networks to use that information and identify the images so
let's take a look how this works so each neuron the value if it is close to 1
that means it is white whereas if it is close to 0 that means it is black now
this is a an animation of how the salting works so these pixels one of the
ways of doing it is we can flatten this image and take this complete 784
pixels and feed that as input to our neural network the neural network can
consist of probably several layers there can be a few hidden layers and then
there is an input layer and an output layer now the input layer takes the 784 pixels as input the values of each of
these pixels and then you get an output which can be of three types or three classes one can
be a square a circle or a triangle now during the training process there will
be initially obviously you feed this image and it will probably say it's a circle or it will say it's a triangle so
as a part of the training process we then send that error back and the weights and
the biases of these neurons are adjusted till it correctly identifies that this
is a square that is the whole training mechanism that happens out here
now let's take a look at a circle same way so you feed these 784 pixels there is a
certain pattern in which the pixels are lit up and the neural network is trained to
identify that pattern and during the training process once again it would probably initially
identify it incorrectly saying this is a square or a triangle and then that error
is fed back and the weights and biases are adjusted finally till it finally gets the image
correct so that is the training process so now we will take a look at
same way a triangle so now if you feed another image which
is consisting of triangles so this is the training process now we have trained our neural network to classify these
images into a triangle or a circle and a square so now this neural network can
identify these three types of objects now if you feed another image and it
will be able to identify whether it's a square or a triangle or a circle now what is important to be observed is that
when you feed a new image it is not necessary that the image or the the triangle is exactly
in this position now the neural network actually identifies the patterns so even if the
triangle is let's say positioned here not exactly in the middle but maybe at the corner or in the side it would still
identify that it is a triangle and that is the whole idea behind pattern recognition so how does this
training process work this is a quick view of how the training process works
so we have seen that a neuron consists of inputs it receives inputs and then there is a weighted sum which is nothing
but this x i wi summation of that plus the bias and this is then fed to the
activation function and that in turn gives us a output now during the
training process initially obviously when you feed these images when you send maybe a square it will identify it as a
triangle and when you maybe feed a triangle it will identify as a square and so on so that error information is
fed back and initially these weights can be random maybe all of them have zero values and then it will slowly keep
changing so the as a part of the training process the values of these weights w1 w2 up to wn keep changing in
such a way that towards the end of the training process it should be able to
identify these images correctly so till then the weights are adjusted and that is known as the training process so and
these weights are numeric values it could be 0.5 0.25 0.35 and so on it
could be positive or it could be negative and the value that is coming here is the pixel value as we have seen
it can be anything between 0 to 1 you can scale it between 0 to 1 or 0 to 256
whichever way 0 being black and 256 being white and then all the other colors in between so that is the input
so these are numerical values this multiplication or the product w i x i is a numerical value and the bias is also a
numerical value we need to keep in mind that the bias is fixed for a neuron it doesn't change with the inputs whereas
the weights are one per input so that is one important point to be noted so but the bias also keeps changing initially
it will again have a random value but as a part of the training process the weights the values of the weights w1 w2
wn and the value of b will change and ultimately once the
training process is complete these values are fixed for this particular neuron w1 w2 up to wn and plus the value
of the b is also fixed for this particular neuron and in this way there will be multiple neurons and each there
may be multiple levels of neurons here and that's the way the training process
works so this is another example of multi-layer so there are two hidden layers in between and then you have the
input layer values coming from the input layer then it goes through multiple layers hidden layers and then there is
an output layer and as you can see there are weights and biases for each of these
neurons in each layer and all of them gets keeps changing during the training
process and at the end of the training process all these weights have a certain value and that is a trained model and
those values will be fixed once the training is completed all right then there is something known as activation
function neural networks consists of one of the components in neural networks is activation function and every neuron has
an activation function and there are different types of activation functions that are used it could be a relu it
could be sigmoid and so on and so forth and the activation function is what decides whether a neuron should be fired
or not so whether the output should be 0 or 1 is decided by the activation function and the activation function in
turn takes the input which is the weighted sum remember we talked about w
i x i plus b that weighted sum is fed as an input to the activation function and
then the output can be either a zero or a one and there are different types of activation functions which are covered
in an earlier video you might want to watch all right so as a part of the training process
we feed the inputs the label data or the training data and then it gives an
output which is the predicted output by the network which we indicate as y hat
and then there is a labeled data because we for supervised learning we already
know what should be the output so that is the actual output and in the initial process before the training is complete
obviously there will be error so that is measured by what is known as a cost function so the difference between the
predicted output and the actual output is the error and the cost function can be defined in
different ways there are different types of cost functions so in this case it is like the average of the squares of the
error so and then all the errors are added which can
sometimes be called as sum of squares sum of square errors or sse and that is
then fed as a feedback in what is known as backward propagation or back propagation and that helps in
the network adjusting the weights and biases and so the weights and biases get
updated till this value the error value or the cost function
is minimum now there is a optimization technique which is used here called
gradient descent optimization and this algorithm works in a way that the error which is
the cost function needs to be minimized so there's a lot of mathematics that goes behind this for
example they find the local minima the global minima using the differentiation
and so on and so forth but the idea is this so as a training process as the as
a part of training the whole idea is to bring down the error which is like let's
say this is the function the cost function at certain levels it is very high the
cost value of the cost function the output of the cost function is very high so
the weights have to be adjusted in such a way and also the bias of course that
the cost function is minimized so there is this optimization technique called
gradient descent that is used and this is known as the learning rate now gradient descent
you need to specify what should be the learning rate and the learning rate should be optimal because if you have a
very high learning rate then the optimization will not converge because
at some point it will cross over to the side on the other hand if you have very
low learning rate then it might take forever to convert so you need to come
up with the optimum value of the learning rate and once that is done
using the gradient descent optimization the error function is reduced and that's like the end of
the training process all right so this is another view of gradient descent so this is how it looks this is your cost
function the output of the cost function and that has to be minimized using gradient descent algorithm and these are
like the parameters and weight could be one of them so initially we start with certain random values so cost will be
high and then the weights keep changing and in such a way that the cost function
needs to come down and at some point it may reach the minimum value and then it
may increase so that is where the gradient descent algorithm decides that okay it has reached a minimum value and
it will kind of try to stay here this is known as the global minima now sometimes
these curves may not be just for explanation purpose this has been drawn in a nice way but sometimes these curves
can be pretty erratic there can be some local minima here and then there is a peak and then and so on so the whole
idea of gradient descent optimization is to identify the global minima and to
find the weights and the bias at that particular point so that's what is gradient descent and then this is
another example so you can have these multiple local minima so as you can see
at this point when it is coming down it may appear like this is a minimum value but then it is not this is actually the
global minimum value and the gradient is an algorithm will make an effort to reach this level and not get stuck at
this point so the algorithm is already there and it knows how to identify this
global minimum and that's what it does during the training process now in order to implement deep learning there are
multiple platforms and languages that are available but the most common platform nowadays is tensorflow and so
that's the reason we have uh this tutorial we've created this tutorial for
tensorflow so we will take you through a quick demo of how to write a tensorflow
code using python and tensorflow is an open source platform created by
google so let's just take a look at the details of tensorflow and so this is a a
library a python library so you can use python or any other languages it's also
supported in other languages like java and r and so on but python is the most common language that is used so
it is a library for developing deep learning applications especially
using neural networks and it consists of primarily two parts if you will so one
is the tensors and then the other is the graphs or the flow that's the way the name that's the reason for this kind of
a name called tensorflow so what are tensor sensors are like multi-dimensional
arrays if you will that's one way of looking at it so usually you have a one dimensional array
so first of all you can have what is known as a scalar which means a number and then you have a one dimensional
array something like this which means this like a set of numbers so that is a one dimension array then you can have a
two dimensional array which is like a matrix and beyond that sometimes it gets
difficult so this is a three dimensional array but tensorflow can handle many more dimensions so it can have
multi-dimensional arrays that is the strength of tensorflow and which makes computation
deep learning computation much faster and that's the reason why tensorflow is
used for developing deep learning applications so tensorflow is a deep learning tool and this is the way it
works so the data basically flows in the form of tensors and the way the
programming works as well is that you first create a graph of how to execute
it and then you actually execute that particular graph in the form of what is known as a session we
will see this in the tensorflow code as we move forward so all the data is managed or manipulated in tensors and
then the processing happens using these graphs there are certain terms called like for example ranks of a tensor the
rank of a tensor is like a dimensional dimensionality in a way so for example
if it is scalar so there is just a number just a one number the rank is
supposed to be zero and then it can be a one dimensional vector in which case the rank is
supposed to be one and then you can have a two-dimensional vector typically like
a matrix then in that case we say the rank is two and then if it is a
three-dimensional array then rank is three and so on so it can have more than three as well so it is possible that you
can store multi-dimensional arrays in the form of tensors so what are some of
the properties of tensorflow i think today it is one of the most popular
platform tensorflow is the most popular deep learning platform or library it is open source
it's developed by google developed and maintained by google but it is open source one of the most important things
about tensorflow is that it can run on cpus as well as gpus
gpu is a graphical processing unit just like cpu central processing unit now in
earlier days gpu was used for primarily for graphics and that's how the name has
come and one of the reasons is that it cannot perform generic activities very
efficiently like cpu but it can perform iterative actions or computations
extremely fast and much faster than a cpu so they are really good for
computational activities and in deep learning there is a lot of iterative computation that happens so in the form
of matrix multiplication and so on so gpus are very well suited for this kind
of computation and tensorflow supports both gpu as well as cpu and there's a certain way of writing code in
tensorflow we will see as we go into the code and of course tensorflow can be used for traditional machine learning as
well but then that would be an overkill but just for understanding it may be a good idea to start writing code for a
normal machine learning use case so that you get a hang of how tensorflow code
works and then you can move into neural networks so that is
just a suggestion but if you are already familiar with how tensorflow works then probably yeah you can go straight into
the neural networks part so in this tutorial we will take the use case of
recognizing handwritten digits this is like a hello world
of deep learning and this is a nice little mnist database is a nice little
database that has images of handwritten digits nicely formatted because very
often in deep learning and neural networks we end up spending a lot of time in preparing the data for training
and with mnist database we can avoid that you already have the data in the
right format which can be directly used for training and mnist also offers a
bunch of built-in utility functions that we can straight away use and call those
functions without worrying about writing our own functions and that's one of the reasons why mnist database is very
popular for training purposes initially when people want to learn about deep learning and tensorflow this is the
database that is used and it has a collection of 70 000 handwritten digits
and a large part of them are for training then you have test just
like in any machine learning process and then you have validation and all of them are labeled so you have the images and
their label and these images they look somewhat like this so they are hand written images collected from a lot of
individuals people have these are samples written by human beings they
have handwritten these numbers these numbers going from zero to nine so people have written these numbers and
then the images of those have been taken and formatted in such a way that it is
very easy to handle so that is mnis database and the way we are going to
implement this in our tensorflow is we will feed this data
especially the training data along with the label information and the data is basically these images are
stored in the form of the pixel information as we have seen in one of the previous slides all the images are
nothing but these are pixels so an image is nothing but an arrangement of pixels and the value of
the pixel either it is lit up or it is not or in somewhere in between that's how the images are stored and that is
how they are fed into the neural network and for training once the network is
trained when you provide a new image it will be able to identify within a
certain error of course and for this we will use one of the simpler
neural network configurations called softmax and for simplicity what we will
do is we will flatten these pixels so instead of taking them
in a two-dimensional arrangement we just flatten them out so for example it
starts from here it is a 28 by 28 so there are pixels so pixel number one starts here
it goes all the way up to 28 then 29 starts here and goes up to 56 and so on and the pixel number 784 is here so we
take all these pixels flatten them out and feed them like one single line into
our neural network and this is a what is known as a softmax layer what it does is
once it is trained it will be able to identify what digit this is so there are in this
output layer there are 10 neurons each signifying a digit and at
any given point of time when you feed an image only one of these 10 neurons gets
activated so for example if this is trained properly and
if you feed a number 9 like this then this particular neuron gets
activated so you get an output from this neuron let me just use a pen or a laser to show you here okay
so you're feeding the number nine let's say this has been trained and now if you're feeding a number nine this will
get activated now let's say you fade one to the trained
network then this neuron will get activated if you feed two this neuron
will get activated and so on i hope you get the idea so this is one type of a
neural network or an activation function known as soft max layer so that's what we will be using here this is one of the
simpler ones for quick and easy understanding so this is how the code would look we will
go into our lab environment in the cloud and we will show you there directly but very
quickly this is how the code looks and let me run you through briefly here and then we will go into
the jupiter notebook where the actual code is and we will run that as well so
as a first step first of all we are using python here and that's why the syntax of the language is python and the
first step is to import the tensorflow library so and we do this by using this line of
code saying import tensorflow as tf df is just for convenience so you can name give any name and
once you do this tf is tensorflow is available as an object in the name of tf and then you can run its methods and
accesses its attributes and so on and so forth and mnist database is actually an
integral part of tensorflow and that's again another reason why we as a first step we always use this example mnist
database example so you just simply import mnist database as well using this
line of code and you slightly modify this so that the labels
are in this format what is known as one hot true which means that the label
information is stored like an array and let me just use a pen to show what exactly it is so
when you do this one hot true what happens is each label is stored in the
form of an array of 10 digits and let's say the number is
8 okay so in this case all the remaining values there will be a bunch of zeros so
this is like array at position 0 this is at position 1 position 2 and so on and
so forth let's say this is position 7 then this is position 8 that will be 1
because our input is 8 and again position 9 will be 0. okay so one hot
encoding this one hot encoding true will kind of load the data in such a way that the
labels are in such a way that only one of the digits has a value of 1 and that
indicates so based on which digit is one we know what is the label
so in this case the eighth position is one therefore we know this sample data the value is eight similarly if you have
a two here let's say then the labeled information will be somewhat like this so you have your labels so you have this
as zero the zeroth position the first position is also zero the second
position is one because this indicates number two and then you have third as zero and so on okay so that is the
significance of this one hot true all right and then we can
check how the data is looking by displaying the the data and as i
mentioned earlier this is pretty much in the form of digital form like numbers so
all these are like pixel values so you will not really see an image in this format but there is a way to
visualize that image i will show you in a bit and this tells you how many images are there
in each set so the training there are 55 000 images in training and in the test
set there are 10 000 and then validation there are 5000 so all together there are
70 000 images all right so let's move on and we can
view the actual image by uh using the matplotlib library
and this is how you can view this is the code for viewing the images and you can
view them in color or you can view them in grayscale so the c map is what tells
in what way we want to view it and what are the maximum values and the minimum
values of the pixel values so these are the max and minimum values so of the pixel values so maximum is 1 because
this is a scaled value so 1 means it is white and 0 means it is black and in
between is it can be anywhere in between black and white and the way to train the model there is
a certain way in which you write your tensorflow code and the first step is to create some
placeholders and then you create a model in this case we will use the softmax model one of the simplest ones and
placeholders are primarily to get the data from outside into the neural network so this is a very common
mechanism that is used and then of course you will have variables which are your you remember these are your weights
and biases so for in our case there are 10 neurons and each neuron actually has
784 because each neuron takes all the inputs if we go back to our slide here
actually every neuron takes all the 784 inputs right this is the first neuron it
has it receives all the 784 this is the second neuron this also receives all the 700 so each of these inputs needs to be
multiplied with the weight and that's what we are talking about here so these are this is a
matrix of 784 values for each of the neurons and so it
is like a 10 by 784 matrix because there are 10 neurons
and similarly there are biases now remember i mentioned
bias is only one per neuron so it is not one per input unlike the weights so
therefore there are only 10 biases because there are only 10 neurons in this case so that is what we are
creating a variable for biases so this is something little new in tensorflow
you will see unlike our regular programming languages where everything is a variable here the variables can be
of three different types you have placeholders which are primarily used for feeding data you have variables
which can change during the course of computation and then a third type which is not shown here are constants so these
are like fixed numbers all right so in a regular programming language you may have everything as variables or at the
most variables and constants but in tensorflow you have three different types placeholders variables and
constants and then you create what is known as a graph so tensorflow programming consists of graphs and
tensors as i mentioned earlier so this can be considered ultimately as a tensor
and then the graph tells how to execute the whole implementation so that the
execution is stored in the form of a graph and in this case what we are doing is we are doing a multiplication tf you
remember this tf was created as a tensorflow object here one more level one more so tf is available here now
tensorflow has what is known as a matrix multiplication or matmal function so
that is what is being used here in this case so we are using the matrix multiplication of tensorflow so that you
multiply your input values x with w right this is what we were doing x w
plus b you're just adding b and this is in very similar to one of the earlier
slides where we saw sigma x i wi so that's what we are doing here matrix
multiplication is multiplying all the input values with the corresponding weights and then adding the bias so that
is the graph we created and then we need to define what is our loss function and
what is our optimizer so in this case we again use the tensorflow's apis so tf
dot nn soft max cross entropy with logits is the api that we will use and reduce mean is
what is like the mechanism whereby which says that you reduce
the error and optimizer for doing deduction of the error what optimizer are we using
so we are using gradient descent optimizer we discussed about this in a couple of slides
earlier and for that you need to specify the learning rate you remember we saw that there was a slide somewhat like
this and then you define what should be the learning rate how fast you need to come down that is the learning rate and
this again needs to be tested and tried and to find out the optimum level of
this learning rate it shouldn't be very high in which case it will not converge or shouldn't be very low because it will
in that case it will take very long so you define the optimizer and then you call the method
minimize for that optimizer and that will kickstart the training process
and so far we've been creating the graph and in order to actually execute that
graph we create what is known as a session and then we run that session and once the training is completed we
specify how many times how many iterations we want it to run so for example in this case we are saying
thousand steps so that is a exit strategy in a way so you specify the
exit condition so training will run for thousand iterations and once that is done we can then evaluate the model
using some of the techniques shown here so let us get into the code quickly and see how it
works so this is our cloud environment now you can install tensorflow on your
local machine as well i'm showing this demo on our existing cloud but you can
also install tensorflow on your local machine and there is a separate video on
how to set up your tensorflow environment you can watch that if you want to install your local environment
or you can go for other any cloud service like for example
google cloud amazon or cloud labs any of these you can use and
run and try the code okay so it has got started
we will login
all right so this is our deep learning tutorial code
and this is our tensorflow environment and so let's get started
the first we have seen a little bit of a code walk through in the slides as well now you will see
the actual code in action so the first thing we need to do is import tensorflow and then we will import the data and we
need to adjust the data in such a way that the one hot is encoding is set to true one
hot encoding right as i explained earlier so in this case the label values will be shown appropriately
and if we just check what is the type of the data so you can see that this is a
data sets python data sets and if we check the number of
images the way it looks so this is how it looks it is an array of type float32
similarly the number if you want to see what is the number of
training images there are 55 000 then there are test images 10 000 and then validation images 5 000. now
let's take a quick look at the data itself visualization so we will use
matplotlib for this and if we take a look at the shape now shape
gives us like the dimension of the tensors or or the arrays if you will so in this
case the training data set if we see the size of the training data set using the method shape it says there are 55 000
and 55 000 by 784 so remember the 784 is
nothing but the 28 by 28 28 into 28 so that is equal to 784 so that's what it
is uh showing now we can take just uh one image and just see what is the
the first image and see what is the shape so again size obviously it is only 784
similarly you can look at the image itself the data of the first image itself so this is how it it
shows so large part of it will probably be zeros because as you can imagine
in the image only certain areas are written rest is blank so that's why you will mostly see
zeros either it is black or white but then there are these values are so the
values are actually they are scaled so the values are between 0 and 1 okay so this is what you're seeing so certain
locations there are some values and then other locations there are zeros so
that is how the data is stored and loaded if we want to actually see what is the
value of the handwritten image if you want to view it this is how you view it so you
create like do this reshape and matplotlib has this
feature to show you these images so we will actually use the function called
i am show and then if you pass this parameters appropriately you will be
able to see the different images now i can change the values in this position
so which image we are looking at right so we can say if i want to see what is there in maybe
5 000 right so 5000 has 3
similarly you can just say 5 what is in 5 5 as 8
what is in 50 again 8 so basically by the way if
you're wondering uh how i'm executing this code shift enter in case you're not familiar with jupiter notebooks shift
enter is how you execute each cell individual cell and if you want to execute the entire program
you can go here and say run all so that is how this code gets executed
and here again we can check what is the maximum value and what is the minimum value of this pixel values as i
mentioned this is it is scaled so therefore it is between the values lie between 1 and 0. now this is
where we create our model the first thing is to create the required placeholders and variables and
that's what we are doing here as we have seen in the slides so we create one placeholder and we create two
variables which is for the weights and biases these two variables are actually
matrices so each variable has 784 by 10 actual values okay so
one for this 10 is for each neuron there are 10 neurons and 784 is for the pixel
values inputs that are given which is 28 into 28 and the biases as i mentioned
one for each neuron so there will be 10 biases they are stored in a variable by
the name b and this is the graph which is basically the multiplication of these matrix
multiplication of x into w and then the bias is added for each of the neurons
and the whole idea is to minimize the error so let me just execute i think
this code is executed then we define what is our the y value is basically the
label value so this is another placeholder we had x as one placeholder and white underscore
true as a second placeholder and this will have values in the form of uh 10
digit 10 digit arrays and since we said one hot encoded the
position which has a one value indicates what is the label for that particular number all
right then we have cross entropy which is nothing but the
loss loss function and we have the optimizer we have chosen
gradient descent as our optimizer then the training process itself so the training process is nothing but to
minimize the cross entropy which is again nothing but the loss function
so we define all of this in the form of a graph so up to here remember what we
have done is we have not exactly executed any tensorflow code till now we
are just preparing the graph the execution plan that's how the tensorflow
code works so the whole structure and format of this code will
be completely different from how we normally do programming so even with
people with programming experience may find this a little difficult to
understand it and it needs quite a bit of practice so you may want to view this
video also maybe a couple of times to understand this flow because the way tensorflow programming
is done is slightly different from the normal programming some of you who let's say have done
maybe spark programming to some extent will be able to easily understand this but even in spark the the programming
the code itself is pretty straightforward behind the scenes the execution happens slightly differently
but in tensorflow even the code has to be written in a completely different way
so the code doesn't get executed in the same way as you have written so that that's something you need to
understand and a little bit of practice is needed for this so so far what we have done up to here
is creating the variables and feeding the variables and rather not feeding but setting up the
variables and the graph that's all defining maybe the
what kind of a network you want to use for example we want to use a softmax and so on so you have created the variables
how to load the data loaded the data viewed the data and prepared everything but you have not
yet executed anything in tensorflow now the next step is the execution in
tensorflow so the first step for doing any execution in tensorflow is to
initialize the variables so anytime you have any variables defined in your code
you have to run this piece of code always so you need to basically create
what is known as a node for initializing so this is a node you still are not yet executing anything here you just created
a node for the initialization so let us go ahead and create that
and here onwards is where you will actually execute your code
in tensorflow and in order to execute the code what you will need is a session
tensorflow session so tf.session will give you a session and there are a
couple of different ways in which you can do this but one of the most common methods of doing this is with what is
known as a width loop so you have a with tf.session
as says and with a colon here and this is like a block
starting of the block and these indentations tell how far this block goes and
this session is valid till this block gets executed so that is the
purpose of creating this width block this is known as a width block so with tf dot session as says you say says dot
run init now says dot run will execute a node that is
specified here so for example here we are saying says dot run says is basically an instance of the session
right so here we are saying tf.session so an instance of the session gets created and we are calling that
says and then we run a node within that one of the nodes in
the graph so one of the nodes here is init so we say run that particular node and
that is when the initialization of the variables happens now what this does is
if you have any variables in your code in our case we have w
is a variable and b is a variable so any variables that we created you have to
run this code you have to run the initialization of these variables otherwise you will get an error okay so
that is the that's what this is doing then we within this with block we specify a
for loop and we are saying we want the system to iterate for thousand steps and perform
the training that's what this for loop does run training for thousand iterations
and what it is doing basically is it is fetching the data or these images remember there
are about 50 000 images but it cannot get all the images in one shot because
it will take up a lot of memory and performance issues will be there so this is a very common way of
performing deep learning training you always do in batches so we have maybe 50
000 images but you always do it in batches of 100 or maybe
500 depending on the size of your system and so on and so forth so in this case we are saying okay get me 100
images at a time and get me only the training images remember we use only the
training data for training purpose and then we use test data for test purpose you must be familiar with machine
learning so you must be aware of this but in case you are not in machine learning also not this is not specific
to deep learning but in machine learning in general you have what is known as training data set and test data set your
available data typically you will be splitting into two parts and using the
training data set for training purpose and then to see how well the model has been trained you use the test data set
to check or test the validity or the accuracy of the model so that's what we are doing here and you observe here that
we are actually calling an mnist function here so we are saying mnist train dot next patch
right so this is the advantage of using mnist database because they have provided some very nice helper functions
which are readily available otherwise this activity itself we would have had to write a piece of code to fetch this
data in batches that itself is a lengthy exercise so we can avoid all that if we are using mnist database and that's why
we use this for the initial learning phase okay so when we say fetch what it
will do is it will fetch the images into x and the labels into y and then you
use this batch of 100 images and you run the training so says dot run basically what
we are doing here is we are running the training mechanism which is nothing but it passes this through the neural
network passes the images through the neural network finds out what is the output and if the output obviously they
initially it will be wrong so all that feedback is given back to the neural
network and thereby all the w's and b's get updated
till it reaches thousand iterations in this case the exit criteria is thousand but you
can also specify probably accuracy rate or something like that for the as an exit criteria so here it is it just says
that okay this particular image was wrongly predicted so you need to update your weights and biases that's the
feedback given to each neuron and that is run for 1000 iterations and typically
by the end of this thousand iterations the model would have learned to recognize
these handwritten images obviously it will not be 100 accurate okay so once
that is done after so this happens for 1000 iterations once that is done
you then test the accuracy of these models by using the test data set right so this is
what we are trying to do here the code may appear a little complicated because if you're seeing this for the first time
you need to understand the various methods of tensorflow and so on but it is basically comparing the output with
what has been what is actually there that's all it is doing so you have your test data and you're trying to find out
what is the actual value and what is the predicted value and seeing whether they are equal or not tf.equal right and how
many of them are correct and so on and so forth and based on that the accuracy
is calculated as well so this is the accuracy and that is what we are trying
to see how accurate the model is in predicting these numbers or these digits
okay so let us run this this entire thing is in one cell so we will have to just run it in one shot it may take a
little while let us see and not bad so it has finished the thousand iterations and what we see here as an
output is the accuracy so we see that the accuracy of this model is around 91
okay now which is pretty good for such a short exercise within such a short time
we got 90 percent accuracy however in real life this is probably not
sufficient so there are other ways in to increase the accuracy we will see
probably in some of the later tutorials how to improve this accuracy how to change maybe the hyper parameters like
number of neurons or number of layers and so on and so forth and so that this accuracy can be increased
beyond 90 percent hello and welcome to the tensorflow object detection api
tutorial in this video i will walk you through the tensorflow code to perform
object detection in a video so let's get started this part is basically you're
importing all the libraries we need a lot of these libraries for example numpy
we need image io datetime and pill and so on and so forth and of course
matplotlib so we import all these libraries and then there are a bunch of variables which have some paths for the
files and folders so this is regular stuff let's keep moving then we import
the matplotlib and make it inline and a few more imports all right and then
these are some warnings we can just ignore them so if i run this code once again it could go away all right and
then here onwards we do the model preparation now what we're going to do is we're going to use an existing neural
network model so we are not going to chain a new one because that really will take a long time and it needs a lot of
computation resources and so on and it is really not required there are already models that have been trained and in
this case it is the ssd with mobile net that's the model that we are going to use and this model is trained to detect
objects and it is readily available as open source so we can actually use this
and if you want to use other models there are a few more models available so you can click on this
link here and let me just take you there there are a few more models but we have chosen this
particular one because this is faster it may not be very accurate but that is one
of the faster models but on this link you will see a lot of other models that are readily available these are trained
models some of them would take a little longer but they may be more accurate and
so on so you can probably play around with these other models okay so we will
be using that model so this piece of code this line is basically importing
that model and this is also known as frozen model the term we use is frozen
model so we import download and import that and then we will actually use that
model in our code all right so these two cells we have downloaded and import the model
and then once it is available locally we will then load this into our program all
right so we are loading this into memory and
the you need to perform a couple of additional steps which is basically we
need to map the numbers to text as you may be aware when we
actually build the model and when we run predictions the model will not give a
text the output of the model is usually a number so we need to map that to a
text so for example if the network predicts that the output is 5 we know
that 5 means it is an airplane things like that so this mapping is done in
this next cell all right so let's keep moving and then we have a helper code which will basically load the data or
load the images and transform into numpy arrays this is also used in doing object
detection in images so we are actually going to reuse because video is nothing but it consists of frames which in turn
are images so we are going to pretty much use reuse the same code which we
used for doing object detection in images so this is where the actual detection starts so here this is the
path for where the images are stored so this is here once again we are reusing
the code which we wrote for detecting objects in an image so this is the path
where the images were stored and this is the extension and this was done for about two or three images so we will
continue to use this and we go down i'll skip this section so
this is the cell where we are actually loading the video and converting it into frames and then using frame by frame we
are detecting the objects in the image so in this code what we are doing basically is there are a few lines of
code what they do is basically once they find an object a box will be drawn around those
each of those objects and the input file the name of the input video file is
traffic it is the extension is mp4 and we have this video reader it's a
excellent object which is basically part of this class called image i o so we can
read and write videos using that and the video that we are going to use is
traffic.mp4 you can use any mp4 file but in our case i picked up video which has
like car so let me just show you so this is in this object detection folder i
have this mp4 file i'll just quickly play this video it's a little slow yeah okay so here we go this is the video
it's a short one relatively small video so that for this particular demo and
what it will do is once we run our code it will detect each of these cars and it
will annotate them as cars so in this particular video we only have cars we
can later on see with another video i think i have cat here so we can also try
with that but let's first check with this traffic video so let me go back so we will be reading this frame by frame
and um you know actually we will be reading the video file but then we will be analyzing it frame by frame and we
will be reading them at 10 frames per second that is the rate we are mentioning here and analyzing it and
then annotating and then writing it back so you will see that we will have a
video file named something like this traffic underscore annotated and
we will see the annotated video so let's go back and run through this piece of code and then we will come back and see
the annotated video this might take a little while so i will pause the video
after running this particular cell and then come back to show you the results all
right so let's go ahead and run it so it is running now and it is also important
that at the end you close the video writer so that it is similar to a file
pointer when you open a file you should also make sure you close it so that it doesn't hog the resources so it's very
similar at the end of it the last piece or last line of code should be video underscore writer dot close all right so
i'll pause and then i'll come back okay so i will see you in a little bit all
right so now as you can see here the processing is done the hourglass has disappeared that means the video has
been processed so let's go back and check the annotated video go back to my file manager so this was the original
traffic.mp4 and now you have here traffic underscore annotated mp4 so
let's go and run this and see how it looks
you see here just got each of these cars are getting detected let me pause and
show you so we pause here it says car 70 percent let us allow it to go a little
further it detects something on top what is that truck okay so i think because of
the board on top it somehow thinks there is a truck let's play it some more and
see if it detects anything else so this is again a car looks like so let us yeah
so this is a car and it has confidence level of 69 percent
okay this is again a car all right so basically till the end it goes and detects each and every car that is
passing by now we can quickly repeat this process for another video let me just show you the other video which is a
cat again there is uh this cat is not really moving or anything but it is just
standing there staring and moving a little slowly and our application will our network will
detect that this is a cat and even when the cat moves a little bit in
the other direction it will continue to detect and show that it is a cat okay so yeah so this is how the original video
is let's go ahead and change our code to analyze this one and see if it detects
our network detects this cat close this here we go and i'll go back to my code
all we need to do is change this traffic to cat the extension it will automatically
pick up because it is given here and then it will run through so very quickly once again what it is doing is this
video reader video underscore reader has a neat little feature or interface whereby you can say for frame in video
underscore reader so it will basically provide frame by frame so you're in a loop frame by frame and then you take
that each frame that is given to you you take it and analyze it as if it is an image individual image so that's the way
it works so it is very easy to handle this all right so now let's once again
run just this cell the rest of the stuff remains the same so i will run this cell
again it will take a little while so the r glasses come back i will pause and then come back in a little while all
right so the processing is done let's go and check the annotated video go here so
we have get annotated dot mp4 let's play this all right so you can see here it is
detecting the cat and in the beginning you also saw it detected something else here there looks like it detected one
more object so let's just go back and see what it has detected here let's see yes so
what is it trying to show here it's too small not able to see but
it is trying to detect something i think it is saying it is a car i don't know all right okay so in this video there's
only pretty much only one object which is the cat and let's wait for some time and see if it
continues to detect it when the cat turns around and moves as well just
in a little bit that's going to happen and we will see there we go and in spite
of turning the other way i think our network is able to detect that it is a
cat so let me freeze and then show here it is actually still continues to detect
it as a cat all right so that's pretty much it i think that's the only object that it detects in this particular video
okay so close this so that's pretty much it thank you very much for watching this
video and you have a great day and in case you have any questions please put them below the video here and we will be
more than happy to get back to you and make sure you put your email id so that we can contact you in case you have any
questions thank you once again bye-bye today we're going to be covering the convolutional neural network tutorial
do you know how deep learning recognizes the objects in an image and really this particular neural network is how image
recognition works it's very central one of the biggest building blocks for image recognition it does it using convolution
neural network and we over here we have the basic picture of a hummingbird pixels of an image fed as input you have
your input layer coming in so it takes that graphic and puts it into the input layer you have all your hidden layers
and then you have your output layer and your output layer one of those is going to light up and say oh it's a bird we're
going to go into depth we're going to actually go back and forth on this a number of times today so if you're not catching all the image don't worry we're
going to get into the details so we have our input layer accepts the pixels of the image as input in the form of arrays
and you can see up here where they've actually labeled each block of the bird in different arrays so we'll dive into
deep as to how that looks like and how those matrixes are set up your hidden layer carry out feature extraction by
performing certain calculations and manipulation so this is the part that kind of reorganizes that picture
multiple ways until we get some data that's easy to read for the neural network this layer uses a matrix filter
and performs convolution operation to detect patterns in the image and if you
remember that convolution means to coil or to twist so we're going to twist the data around and alter it and use that
operation to detect a new pattern there are multiple hidden layers like convolution layer rel u is how that is
pronounced when that's the rectified linear unit that has to do with the activation function that's used
pooling layer also uses multiple filters to detect edges corners eyes feathers
beak etc and just like the term says pooling is pulling information together
and we'll look into that a lot closer here so if you're if it's a little confusing now we'll dig in deep and try
to get you squared away with that and then finally there is a fully connected layer that identifies the object in the
image so we have these different layers coming through in the hidden layers and they come into the final area and that's
where we have a one node or one neural network entity that lights up that says it's a bird what's in it for you we're
going to cover an introduction to the cnn what is convolution neural network how cnn recognizes images we're going to
dig deeper into that and really look at the individual layers in the convolutional neural network and finally
we do a use case implementation using the cnn we'll begin our introduction to the cnn by introducing the pioneer of
convolutional neural network jan le he was the director of facebook ai research group built the first
convolutional neural network called lynnette in 1988 so these have been
around for a while and have had a chance to mature over the years it was used for character recognition tasks like reading
zip code digits imagine processing mail and automating that process cnn is a feed forward neural network
that is generally used to analyze visual images by producing data with a grid-like topology a cnn is also known
as a convent and very key to this is we are looking at images that was what this
was designed for and you'll see the different layers as we dig in near some of the other some of them are actually
now used since we're using uh tensorflow and cross in our code later on you'll see that some of those layers appear in
a lot of your other neural network frameworks but in this case this is very central to processing images and doing
so in a variety that captures multiple images and really drills down into their different features in this example here
you see flowers are two varieties orchid and a rose i think the orchid is much more dainty and beautiful and the rose
smells quite beautiful of a couple rose bushes in my yard uh they go into the input layer that data is then sent to
all the different nodes in the next layer one of the hidden layers based on its different weights and its setup it
then comes out and gives those a new value those values then are multiplied by their weights and go to the next
hidden layer and so on and then you have the output layer and one of those nodes comes out and says it's an orchid and
the other one comes out and says it's a rose depending on how it was well it was trained what separates the cnn or the
convolutional neural network from other neural networks is a convolutional
operation forms the basis of any convolutional neural network in a cnn every image image is represented in the
form of arrays of pixel values so here we have a real image of the digit eight that then gets put onto
its pixel values represented in the form of an array in this case you have a two dimensional array and then you can see
in the final in form we transform the digit eight into its representational
form of pixels of zeros and one where the ones represent in this case the black part of the eight and the zeros
represent the white background to understand the convolution neural network or how that convolutional
operation works we're going to take a side step and look at matrixes in this case we're going to simplify it and
we're going to take two matrices a and b of one dimension now kind of separate this from your thinking as we learned
that you want to focus just on the matrix aspect of this and then we'll bring that back together and see what
that looks like when we put the pieces for the convolutional operation here we've set up two arrays we have uh in
this case are a single dimension matrix and we have a equals five three seven five nine seven and we have b equals one
two three so in the convolution as it comes in there's gonna look at these two and we're going to start by doing
multiplying them a times b and so we multiply the arrays element wise and we
get 5 6 6 where 5 is the 5 times 1 6 is 3 times
two and then the other six is two times three and since the two arrays aren't the same size they're not the same setup
we're gonna just truncate the first one and we're gonna look at the second array multiplied just by the first three
elements of the first array now that's going to be a little confusing remember a computer gets to repeat these processes hundreds of times so we're not
going to just forget those other numbers later on we'll see we'll bring those back in and then we have the sum of the
product in this case 5 plus 6 plus 6 equals 17. so in our a times b
our very first digit in that matrix of a times b is 17. and if you remember i said we're not going to forget the other
digits so we now have 3 2 5 we move one set over and we take 3 2 5 and we
multiply that times b and you'll see that 3 times 1 is 3 2 times 2 is 4 and
so on and so on we sum it up so now we have the second digit of our a times b product in the matrix and we continue on
with that same thing so on and so on so then we would go from three seven five to seven five nine to five nine seven
this short matrix that we have for a we've now covered all the different entities in a that match three different
levels of b now in a little bit we're going to cover where we use this math at
this multiplying of matrixes and how that works but it's important to understand that we're going through the
matrix and multiplying the different parts to it to match the smaller matrix with the larger matrix i know a lot of
people get lost at is you know what's going on here with these matrixes oh scary math not really that scary when
you break it down we're looking at a section of a and we're comparing it to b so when you break that down your mind
like that you realize okay so i'm i'm just taking these two matrixes and comparing them and i'm bringing the
value down into one matrix a times b we're reducing that information in a way
that will help the computer see different aspects let's go ahead and flip over again back to our images
here we are back to our images talking about going to the most basic two-dimensional image you can get to
consider the following two images the image for the symbol backslash when you press the backslash the above image is
processed and you can see there for the image for the forward slash is the opposite so we click the forward slash
button that flips uh very basic we have four pixels going in can't get any more basic than that here we have a little
bit more complicated picture we take a real image of a smiley face then we represent that in the form of
black and white pixels so if this was an image in the computer it's black and white and like we saw before we convert
this into the zeros and one so where the other one would have just been a matrix of just four dots now we have a
significantly larger image coming in so don't worry we're going to bring this all together here in just a little bit
layers in convolutional neural network we're looking at this we have our convolution layer and that really is the
central aspect of processing images in the convolutional neural network that's
why we have it and then that's going to be feeding in and you have your relu
layer which is you know as we talked about the rectified linear unit we'll talk about that a little bit later the
relu isn't how it act is how that layer is activated is the math behind it what makes the neurons fire you'll see that a
lot of other neural networks when you're using it just by itself it's for processing smaller amounts of data where
you use the atom activation feature for large data coming in now because we're processing small amounts of data in each
image the relu layer works great you have your pooling layer that's where
you're pulling the data together pooling is a neural network term it's very commonly used i like to use the term
reduce so if you're coming from the map and reduce side you'll see that we're
mapping all this data through all these networks and then we're going to reduce it we're going to pull it together and then finally we have the fully connected
layer that's where our output is going to come out so we have started to look at matrixes we've started to look at the
convolutional layer where it fits in and everything we've taken a look at images so we're going to focus more on the
convolution layer since this is a convolutional neural network a convolution layer has a number of
filters and perform convolution operation every image is considered as a matrix of pixel values consider the
following 5x5 image whose pixel values are only 0 and 1. now obviously when we're dealing with color there's all
kinds of things that come in on color processing but we want to keep it simple and just keep it black and white and so
we have our image pixels so we're sliding the filter matrix over the image and computing the dot product to detect
the patterns and right here you're going to ask where does this filter come from this is a bit confusing because the
filter is going to be derived later on we build the filters when we
program or train our model so you don't need to worry what the filter actually is but you do need to understand how a
convolution layer works is what is the filter doing filter and you'll have mini filters you don't have just one filter
you'll have lots of filters that are going to look for different aspects and so the filter might be looking for just
edges it might be looking for different parts we'll cover that a little bit more detail in a minute right now we're just focusing on how the filter works as a
matrix remember earlier we talked about multiplying matrixes together and here we have our two dimensional matrix and
you can see we take the filter and we multiply it in the upper left image and you can see right here one times one one
times zero one times one we multiply those all together then sum them and we end up with the convolved feature of
four we're going to take that and sliding the filter matrix over the image and computing the dot product to detect
patterns so we're just going to slide this so we're going to predict the first one and slide it over one notch predict
the second one and so on and so on all the way through until we have a new matrix and this matrix which is the same
size as the filter has reduced the image and whatever filter whatever that's filtering out is going to be looking at
just those features reduced down to a smaller matrix so once the feature maps
are extracted the next step is to move them to the relu layer so the relu layer
the next step first is going to perform an element-wise operation so each of those maps coming in if there's negative
pixels so it says all the negative pixels to zero and you can see this nice graph where it just zeros out the negatives and then
you have a value that goes from zero up to whatever value is coming out of the matrix this introduces non-linearity to
the network so up until now we have a we say linearity we're talking about the
fact that the feature has a value so it's a linear feature this feature um came up and has let's say the feature is
the edge of the beak you know it's like or the backslash that we saw he'll look at that and say okay this feature has a
value from negative 10 to 10 in this case if it was one it'd say yeah this
might be a beak it might not might be an edge right there a minus five means no we're not even going to look at it to
zero and so we end up with an output and the output takes all these features all these filtered features remember we're
not just running one filter on this we're running a number of filters on this image and so we end up with a
rectified feature map that is looking at just the features coming through and how they weigh in from our filters so here
we have an input of a looks like a toucan bird very exotic looking real image is
scanned in multiple convolution and the relu layers for locating features and
you can see up here is turn it into a black and white image and in this case we're looking in the upper right hand
corner for a feature and that box scans over a lot of times it doesn't scan one pixel at a time a lot of times it will
skip by two or three or four pixels to speed up the process that's one of the ways you can compensate if you don't
have enough resources on your computation for large images and it's not just one filter slowly goes across
the image you have multiple filters have been programmed in there so you're looking at a lot of different filters
going over the different aspects of the image and just sliding across there and forming a new matrix one more aspect to
note about the relu layer is we're not just having one value coming in
so not only do we have multiple features going through but we're generating multiple relu layers for locating the
features that's very important to note you know so we have a quite a bundle we have multiple filters multiple rail u
which brings us to the next step forward propagation now we're going to look at the pooling layer the rectified
feature map now goes through a pooling layer pooling is a down sampling operation that reduces the
dimensionality of the feature map that's all we're trying to do we're trying to take a huge amount of information and
reduce it down to a single answer this is a specific kind of bird this is an
iris this is a rose so you have a rectified feature map and you see here we have our rectified feature map coming
in we set the max pooling with a two by two filters and a stride of two and if you
remember correctly i talked about not going one pixel at a time uh well that's where the stride comes in we end up with
a two by two pooled feature map but instead of moving one over each time and looking at every possible combination we
skip a st we skip a few there we go by two we skip every other pixel and we just do every other one and this reduces
our rectified feature map which is you can see over here 16 by 16 to a 4x4 so
we're continually trying to filter and reduce our data so that we can get to something we can manage and over here
you see that we have the max three four one and two and in the max pooling we're
looking for the max value a little bit different than what we were looking at before so coming from the rectified
feature we're now finding the max value and then we're pulling those features together so instead of think of this as
image of the map think of this as how valuable is a feature in that area how much of a feature value do we have we
just want to find the best or the maximum feature for that area they might have that one piece of the filter of the
beak said oh i see a one in this beak in this image and then it skips over and says i see a three in this image and
says oh this one is rated as a four we don't want to sum it together because then you know you might have like five
ones and it'll say ah five but you might have uh four zeros and one ten and that
ten says well this is definitely a beak where the ones will say probably not a beak a little strange analogy since
we're looking at a bird but you can see how that pulled feature map comes down and we're just looking for the max value
in each one of those matrixes pooling layer uses different filters to identify different parts of the image like edges
corners body feathers eyes beak etc i know i focus mainly on the beak but
obviously each feature could be each a different part of the bird coming in so let's take a look at what that looks
like structure of a convolution neural network so far this is where we're at right now we have our input image coming
in and then we use our filters and there's multiple filters on there that are being developed to kind of twist and
change that data and so we multiply the matrixes we take that little filter maybe it's a two by two we multiply it
by each piece of the image and if we step two then it's every other piece of the image that generates multiple
convolution layers so we have a number of convolution layers we have set up in there just looking at that
data we then take those convolution layers we run them through the relu setup and then once we've done through
the release setup and we have multiple values going on multiple layers that are relu then we're going to take those
multiple layers and we're going to be pooling them so now we have the pooling layers or multiple poolings going on up
until this point we're dealing with sometimes multiple dimensions you can have three dimensions some strange data
setups that aren't doing images but looking at other things they can have four five six seven dimensions uh so
right now we're looking at 2d image dimensions coming in into the pooling layer so the next step is we want to
reduce those dimensions or flatten them so flattening flattening is a process of
converting all of the resultant two-dimensional arrays from pooled feature map into a single long
continuous linear vector so over here you see where we have a pooled feature map maybe that's the bird wing and it
has values 6847 and we want to just flatten this out and turn it into 6847
or a single linear vector and we find out that not only do we do each of the pooled feature maps we do all of them
into one long linear vector so now we've gone through our convolutional neural
network part and we have the input layer into the next setup all we've done is taken all those different pooling layers
and we flatten them out and combine them into a single linear vector going in so
after we've done the flattening we have just a quick recap because we've covered so much so it's important to go back and
take a look at each of the steps we've gone through the structure of the network so far we have our convolution where we twist it and we filter it and
multiply the matrixes we end up with our convolutional layer which uses the rel u
to figure out the values going out into the pooling as you have numerous convolution layers that then create
numerous pooling layers pulling that data together which is the max value which one we want to send forward we
want to send the best value and then we're going to take all of that from each of the pooling layers and we're going to flatten it and we're going to
combine them into a single input going into the final layer once you get to
that step you might be looking at that going boy that looks like the normal intuit to most neural network and you're
correct it is so once we have the flattened matrix from the pooling layer that becomes our input so the pulling
layer is fed as an input to the fully connected layer to classify the image and so you can see as our flattened
matrix comes in in this case we have the pixels from the flattened matrix fed as an input back to our toucan or whatever
that kind of bird that is i need one of these to identify what kind of bird that is it comes into our ford propagation
network and that will then have the different weights coming down across and then finally it selects that that's a bird
and that it's not a dog or a cat in this case even though it's not labeled the final
layer there in red is our output layer our final output layer that says bird cat or dog so quick recap of everything
we've covered so far we have our input image which is twisted and multiplied the filters are multiplied times the
matrix the two matrixes multiplied all the filters to create our convolution layer our convolution layers there's
multiple layers in there because it's all building multiple layers off the different filters then goes through the relu as this activation and that creates
our pooling and so once we get into the pooling layer we then and the pooling look for who's the best what's the max
value coming in from our convolution and we take that layer and we flatten it and then it goes into a fully connected
layer our fully connected neural network and then to the output and here we can see the entire process how the cnn
recognizes a bird this is kind of nice because it's showing the little pixels and where they're going you can see the filter is generating this convolution
network and that filter shows up in the bottom part of the convolution network and then based on that it uses the relu
for the pooling the pooling then find out which one's the best and so on all the way to the fully connected layer at
the end or the classification in the output layer so that'd be a classification neural network at the end
so we covered a lot of theory up till now and you can imagine each one of these steps has to be broken down in
code so putting that together can be a little complicated not that each step of
the process is overly complicated but because we have so many steps we have one two three four five different steps
going on here with sub steps in there we're going to break that down and walk through that in code so in our use case
implementation using the cnn we'll be using the cfar10 dataset from canadian
institute for advanced research for classifying images across 10 categories
unfortunately they don't let me know whether it's going to be a toucan or some other kind of bird but we do get to find out whether it can categorize
between a ship a frog deer bird airplane automobile cat dog horse truck so that's
a lot of fun and if you're looking anything in the news at all of our automated cars and everything else you
can see where this kind of processing is so important in today's world and very cutting edge as far as what's coming out
in the commercial deployment i mean this is really cool stuff we're starting to see this just about everywhere in
industry so great time to be playing with this and figuring it all out let's go ahead and dive into the code and see
what that looks like when we're actually writing our script before we go on let's do uh one more
quick look at what we have here let's just take a look at data batch one keys and remember jupiter notebook i can get
by with not doing the print statement if i put a variable down there it'll just display the variable and you can see in
our data batch one for the keys since this is a dictionary we have the batch one label data and file names so you can
actually see how it's broken up in our data set so for the next step or step
four as we're calling it uh we want to display the images using matte plot library there's many ways to display the
images you can even well there's other ways to drill into it but matplot library is really good for this and
we'll also look at our first reshape uh setup or shaping the data so you can have a little glimpse into what that
means uh so we're going to start by importing our map plot and of course since i am doing jupiter notebook i need
to do the matplot inline command so it shows up on my page so here we go we're going to import
matplot library.pipelot as plt and if you remember matplot library the pie
plot is like a canvas that we paint stuff onto and there's my percentage sign matplot library inline so it's
going to show up in my notebook and then of course we're going to import numpy as np for our numbers python array setup
and let's go ahead and set x equals to data batch 1. so this will
pull in all the data going into the x value and then because this is just a long stream of binary data we need to go
a little bit of reshaping so in here we have to go ahead and reshape the data we
have 10 000 images okay that looks correct and this is kind of an interesting thing it took me a little
bit to i had to go research this myself to figure out what's going on with this data and what it is is it's a 32 by 32
picture and let me do this let me go ahead and do a drawing pad on here uh so
we have 32 bits by 32 bits and it's in color so there's three bits of color now
i don't know why the data is particularly like this it probably has to do with how they originally encoded it but most pictures put the three
afterward so what we're doing here is we're going to take the shape we're going to take the data
which is just a long stream of information and we're going to break it up into 10 000 pieces and those 10 000
pieces then are broken into three pieces each and those three pieces then are 32 by 32. you can look at this like an
old-fashioned projector where they have the red screen or the red projector the blue projector and the green projector
and they add them all together and each one of those is a 32 by 32 bit so that's probably how this was originally
formatted was in that kind of ideal things have changed so we're going to transpose it and we're going to take the
three which was here and we're going to put it at the end so the first part is reshaping the data from a single line of
bit data or whatever format it is into 10 000 by 3 by 32 by 32 and then we're
going to transpose the color factor to the last place so it's the image then
the 32 by 32 in the middle that's this part right here and then finally we're
going to take this uh which is three bits of data and put it at the end so it's more like we do process images now
and then as type this is really important that we're going to use an integer eight you can come in here and
you'll see a lot of these they'll try to do this with a float or float 64. what you got to remember though is a float
uses a lot of memory so once you switch this into uh something that's not
integer eight which just goes up to 128 you are just going to the the amount of
ram let's just put that in here is going to go way up the amount of ram that it loads so you want to go ahead and use
this you can try the other ones and see what happens if you have a lot of ram on your computer but for this exercise this
will work just fine and let's go ahead and take that and run this so now our x variable is all loaded and it has all
the images in it from the batch one data batch one and just to show we were
talking about with the as type on there if we go ahead and take x0 and just look
for its max value let me go ahead and run that uh you'll see it doesn't oops i said 128
it's 255. uh you'll see it doesn't go over 255 because it's basically an ascii character is what we're keeping that
down to we're keeping those values down so they're only 255 0 to 255 versus a
float value which would bring this up um exponentially in size and since we're
using the matplot library we can do oops that's not what i wanted since
we're using the map plot library we can take our canvas and just do a plt dot im
for image show and let's just take a look at what x0 looks like and it comes in i'm not sure what that is but you can
see it's a very low grade image broken down to the minimal pixels on there and if we did the same thing oh
let's do uh let's see what one looks like hopefully it's a little easier to see run on there not enter let's hit the
run on that and we can see this is probably a semi truck that's a good guess on there and i can just go back up here instead of
typing the same line in over and over we'll look at three that looks like a dump truck unloading
uh and so on you can do any of the ten thousand images we can just jump to 55 looks like some kind of animal looking
at us there probably a dog and just for fun let's do just one more uh run on there and we can see a nice car
for our image number four so you can see we pace through all the different images it's very easy to look at them and
they've been reshaped to fit our view in what the matplot library uses for its format so
the next step is we're going to start creating some helper functions we'll start by a one hot encoder to help us
more processing the data remember that your labels they can't just be words they have to switch it and we use the
one hot encoder to do that and then we'll also create a class
cfar helper so it's going to have an init and a setup for the images and then finally we'll go ahead and run that code
so you can see what that looks like and then we get into the fun part where we're actually going to start creating our model our actual neural network
model so let's start by creating our one hot encoder we're going to create our own here uh and it's going to be an out
and we'll have our vector coming in and our values equal 10. what this means is that we have the 10 values the 10
possible labels and remember we don't look at the labels as a number because a
car isn't one more than a horse i'd be just kind of bizarre to have horse equals zero car equals one plane equals
two cat equals three so a cat plus a car equals what uh so instead we create a
numpy array of zeros and there's going to be 10 values so we have a 10 different values in there so you have
0 or 1. 1 means it's a cat 0 means it's not a cat in the next line it might be that 1
means it's a car 0 means it's not a car so instead of having one output with a value of 0 to 10 you have 10 outputs
with the values of 0 to 1. that's what the one hot encoder is doing here and we're going to utilize this in code in
just a minute so let's go ahead and take a look at the next helpers we have a few of these helper functions we're going to
build and when you're working with a very complicated python project dividing it up into separate definitions and
classes is very important otherwise it just becomes really ungainly to work with so let's go ahead and put in our
next helper which is a class and this is a lot in this class so we'll break it down here and let's just start uh oops
we can put a space right in there there we go that was a little bit more readable at a second space so we're
going to create our class the cipher helper and we'll start by initializing it now there's a lot going on in here so
let's start with the uh init part uh self dot i equals zero that'll come in a
little bit we'll come back to that in the lower part we want to initialize our training batches so when we went through
this there was like a meta batch we don't need the meta batch but we do need the data batch one two three four five
and we do not want the testing batch in here this is just the self all train batches so we're going to come make an
array of all those different images and then of course we left the test batch out so we have our self.test batch
we're going to initialize the training images and the training labels and also the test images and the test labels so
these are just this is just to initialize these variables in here then we create another definition down here
and this is going to set up the images let's just take a look and see what's going on in there now we could have all
just put this as part of the init part since this is all just helper stuff but
breaking it up again makes it easier to read it also makes it easier when we start executing the different pieces to
see what's going on so that way we have a nice print statement to say hey we're now running this and this is what's going on in here we're going to set up
the self training images at this point and that's going to go to a numpy array v stack and in there we're going to load
up in this case the data for d itself all train batches again that points right up
to here so we're going to go through each one of these uh files or each one of these data sets because they're not a
file anymore we've brought them in data batch one points to the actual data and so our self-training images is going to
stack them all into our into a numpy array and then it's always nice to get the training length and that's just a
total number of uh self-training images in there and then we're going to take the self training images let me switch
marker colors because i am getting a little too much on the markers up here oops there we go bring down our marker
change so we can see it a little better and at this point this should look familiar
where did we see this well when we wanted to look at this above and we want to look at the images in the matplot library we
had to reshape it so we're doing the same thing here we're taking our self training images and based on the
training length total number of images because we stacked them all together so now it's just one large file of images
we're going to take and look at it as our our three video cameras that are
each displaying a 32 by 32 we're going to switch that around so that now we have each of our images that stays the
same place and then we have our 32 by 32 and then by our three our last are three
different values for the color and of course we want to go ahead and they run
this where we say divide by 255 that was from earlier it just brings all the data into 0 to 1. that's what this is doing
so we're turning this into a 0 to 1 array which is all the pictures 32 by 32
by three and then we're going to take the self training labels and we're going to pump those through our one hot
encoder we just made and we're going to stack them together and again we're converting this into an
array that goes from uh instead of having horse equals one dog equals two and then horse plus dog would equal
three which would be cat now it's going to be you know an array of ten where each one is zero to one
then we want to go ahead and set up our test images and labels and uh when we're doing this you're going to see it's the
same thing we just did with the rest of them we just changed colors right here this is no different than what we're doing up here with our training set
we're going to stack the different images we're going to get the length of them so we know how many images are in
there you certainly could add them by hand but it's nice to let the computer do it especially if it ever changes on
the other end and you're using other data and again we reshape them and transpose them and we also do the one
hot encoder same thing we just did on our training images so now our test
images are in the same format so now we have a definition which sets up all our
images in there and then the next step is to go ahead and batch them or next batch and let's do another breakout here
for batches because this is really important to understand tends to throw me for a little loop when i'm working
with tensorflow or keras or a lot of these we have our data coming in if you remember we had like 10 000 photos let
me just put 10 000 down here we don't want to run all 10 000 at once so we want to break this up into batch sizes
and you also remember that we had the number of photos in this case length of test or whatever number is in there we
also have 32 by 32 by 3. so when we're looking at the batch
size we want to change this from 10 000 to a batch of in this case i think we're
going to do batches of a hundred so we want to look at just 100 the first hundred of the photos and if you
remember we set self i equal to zero uh so what we're looking at here is
we're going to create x we're going to get the next batch from the very initialize we've already initialized it for 0. so we're going to look at x from
0 to batch size which we've set to 100 so just the first 100 images and then
we're going to reshape that into and this is important to let the data know that we're looking at 100 by 32 by
32 by 3. now we've already formatted it to the 32 by 32 by 3. this just sets
everything up correctly so that x has the data in there in the correct order in the correct shape and then the y just
like the x uh is our labels so our training labels again they go from zero
to batch size in this case they do sell five plus batch size because the cell phi is going to keep changing and then
finally we increment the self i because we have zero so we so the next time we call it we're going to get the next
batch size and so basically we have x and y x being the photograph data coming
in and y being the label and that of course is labeled through one hot encoder so if you remember correctly if
it was say horse is equal to zero it would be um one for the zero position
since this is the horse and then everything else would be zero in here let me just put lines through there there we go there's our array
hard to see that array so let's go ahead and take that and uh we're going to finish loading it since this is our
class and now we're armed with all this um our setup over here let's go ahead and
load that up and so we're going to create a variable ch with the cfar helper in it and then we're going to do
ch dot setup images now we could have just put all the setup images under the init but by breaking
this up into two parts it makes it much more readable and also if you're doing other work there's reasons to do that as
far as the setup let's go ahead and run that and you can see where it says uh setting up training images and labels
setting up test images and that's one of the reasons we broke it up is so that if you're testing this out you can actually
have print statements in there telling you what's going on which is really nice uh they did a good job with this setup i like the way that it was broken up in
the back and then one quick note you want to remember that batch to set up the next batch as we have to run
batch equals ch next batch of 100 because we're going to use the 100 size
but we'll come back to that we're going to use that just remember that that's part of our code we're going to be using in a minute from the definition we just
made so now we're ready to create our model first thing we want to do is we want to import our tensorflow as tf i'll
just go ahead and run that so it's loaded up and you can see we got a warning here
that's because they're making some changes it's always growing and they're going to be depreciating one of the
values from float 64 to float type or is treated as an np float 64. uh nothing to
really worry about this doesn't even affect what we're working on because we've set all of our stuff to a 255
value or zero to one and do keep in mind that 0 to 1 value that we converted the
255 is still a float value but it'll easily work with either the numpy float
64 or the numpy d type float it doesn't matter which one it goes through so the
depreciation would not affect our code as we have it and in our tensorflow uh
we'll go ahead and just increase the size in there just a moment so you can get a better view of the um what we're
typing in uh we're going to set a couple placeholders here and so we have we're going to set x equals tf placeholder tf
float 32 we just talked about the float 64 versus the numpy float we're actually
just going to keep this at float32 more than a significant number of decimals
for what we're working with and since it's a placeholder we're going to set the shape equal to and we've set it equal to none
because at this point we're just holding the place on there we'll be setting up as we run the batches that's what the first value is and then 32 by 32 by 3
that's what we've reshaped our data to fit in and then we have our y true equals placeholder tf float 32 and the
shape equals none comma 10. 10 is the 10 different labels we have so it's an array of 10. and then let's create one
more placeholder we'll call this uh hold prob or hold probability and we're going to use this we don't have to have a
shape or anything for this this placeholder is for what we call drop out if you remember from our theory before
we drop out so many nodes is looking at or the different values going through
which helps decrease bias so we need to go ahead and put a placeholder for that also and we'll run this so it's all
loaded up in there so we have our three different placeholders and since we're in tensorflow when you use keras it does
some of this automatically but we're in tensorflow direct cross sits on tensorflow we're going to go ahead and create some more helper functions we're
going to create something to help us initialize the weights initialize our bias if you remember that each layer has
to have a bias going in we're going to go ahead and work on our conversional 2d
our max pool so we have our pooling layer our convolutional layer and then our normal full layer so we're going to
go ahead and put those all into definitions and let's see what that looks like in code and you can also grab some of these helper functions from the
mnist the uh nist setup let me just put that in there if you're under the tensorflow so a lot of these already in
there but we're gonna go ahead and do our own and we're gonna create our init weights and one of the reasons we're
doing this is so that you can actually start thinking about what's going on in the back end so even though there's ways
to do this with an automation sometimes these have to be tweaked and you have to put in your own setup in here now we're
not going to be doing that we're just going to recreate them for our code and let's take a look at this we have our weights and so what comes in is going to
be the shape and what comes out is going to be random numbers so we're going to go ahead and just admit some random numbers
based on the shape with a standard deviation of 0.1 kind of a fun way to do that and then the tf variable
init random distribution so we're just creating a random distribution on there that's all that is for the weights now
you might change that you might have a higher standard deviation in some cases you actually load preset weights that's
pretty rare usually you're testing that against another model or something like that and you want to see how those
weights configure with each other now remember we have our bias so we need to go ahead and initialize the bias with a
constant in this case we're using 0.1 a lot of times the bias is just put in as one and
then you have your weights to add on to that but we're going to set this as 0.1 so we want to return a convolutional 2d
in this case a neural network this is uh would be a layer on here what's going on with the con 2d is we're taking our data
coming in we're going to filter it strides if you remember correctly strides came from
here's our image and then we only look at this picture here and then maybe we have a stride of one so we look at this
picture here and we continue to look at the different filters going on there the other thing this does is that we have
our data coming in as 32 by 32
by 3 and we want to change this so that it's just this is three dimensions and
it's going to reformat this as just two dimensions so it's going to take this number here and combine it with the 32
by 32 so this is a very important layer here because it's reducing our data down using different means and it connects
down i'm just going to jump down one here it goes with the convolutional layer so you have your your kind of your
preformatting and the setup and then you have your actual convolution layer that goes through on there and you can see
here we have init weights by the shape a knit bias shape of three because we have the three different here's our three
again and then we return the tfnn relu with the convention 2d so this
convolutional has this feeding into it right there it's using that as part of it and of course the input is the x y
plus b the bias so that's quite a mouthful but these two are the are the keys here to creating the convolutional
layers there the convolutional 2d coming in and then the convolutional layer which then steps through and creates all
those filters we saw then of course we have our pooling uh so after each time we run it through the convectional layer
we want to pull the data if you remember correctly on the on the pool side and let me just get rid of all my marks it's
getting a little crazy there and in fact let's go ahead and jump back to that slide let's just take a look at that slide over here uh so we have our image
coming in we create our convolutional layer with all the filters remember the filters go um you know the filter is
coming in here and it looks at these four boxes and then if it's a step let's say step two it then goes to these four
boxes and then the next step and so on uh so we have our convolutional layer that we generate or convolutional layers
they use the uh relu function there's other functions out there for this though the relu is the
most the one that works the best at least so far i'm sure that will change then we have our pooling now if you
remember correctly the pooling was max uh so if we had the filter coming in and they did the multiplication on there and
we have a one and maybe a two here and another one here and a three here three
is the max and so out of all of these you then create an array that would be
three and if the max is over here two or whatever it is that's what goes into the pooling of what's going on in our
pooling uh so again we're reducing that data down reducing it down as small as
we can and then finally we're going to flatten it out into a single array and
that goes into our fully connected layer and you can see that here in the code right here we're going to create our
normal full layer um so at some point we're going to take from our pooling layer this will go into some kind of
flattening process and then that will be fed into the full the different layers
going in down here and so we have our input size you'll see our input layer get shape which is just going to get the
shape for whatever's coming in uh and then input size initial weights is also based on the input layer coming in and
the input size down here is based on the input layer shape so we're just going to already use the shape and already have
our size coming in and of course you have to make sure you knit the bias always put your bias on there and we'll
do that based on the size so this will return tf.matimol
input layer w plus b this is just a normal full layer that's what this means right down here that's what we're going
to return so that was a lot of steps we went through let's go ahead and run that so those are all loaded in there and
let's go ahead and create the layers let's see what that looks like now that we've done all the heavy
lifting and everything uh we get to do all the easy part let's go ahead and create our layers we'll create a
convolution layer one and two two different convolutional layers and then we'll take that and we'll flatten that
out and create a reshape pooling in there for our reshape and then we'll have our full uh layer at the end so
let's start by creating our first convolutional layer then we come in here and let me just run that real quick and
i want you to notice on here the 3 and the 32 this is important because
coming into this convolutional layer we have three different channels and 32 pixels each so that has to be in there
the 4 and 4 you can play with this is your filter size so if you remember you
have a filter and you have your image and the filter slowly steps over and
filters out this image depending on what your step is for this particular setup 4
4 is just fine that should work pretty good for what we're doing for the size of the image and then of course at the
end once you have your convolutional layer set up you also need to pull it and you'll see that the pooling is
automatically set up so that it would see the different shape based on what's coming in so here we have max toolbar
2x2 and we put in the convolutional one that we just created the convolutional layer we just created goes right back
into it and that right up here as you can see is the x that's coming in from here so it knows to look at the first
model and set the the data accordingly set that up so matches and we went ahead
and ran this already i think i read let me go and run it again and if we're going to do one layer let's go ahead and do a second layer down here and it's uh
we'll call it convo 2. it's also a convolutional layer on this and you'll see that we're feeding
convolutional 1 in the pooling so it goes from convolutional one into
convolutional one pooling from convolutional one pooling into convolutional two and then from convolutional two into convolutional to
pooling and we'll go ahead and take this and run this so these variables are all loaded into memory and for our flattened
layer uh let's go ahead and we'll do uh since we have 64 coming out of here and
we have a four by four going in let's do eight by eight by 64. so let's do
4096. this is going to be the flat layer so that's how many bits are coming through on the flat layer and we'll
reshape this so we'll reshape our convo 2 pooling and that will feed into
here the combo 2 pulling and then we're going to set it up as a single layer that's 4096 in size that's what that
means there we'll go ahead and run this so we've now created this variable the convo 2 flat and then we have our first
full layer this is the final neural network where the flat layer going in and we're going to again use
the relu for our setup on there on a neural network for evaluation and you'll
notice that we're going to create our first full layer our normal full layer that's our definition so we created that
that's creating the normal full layer and our input for the data comes right here from the this goes right into it uh
the convo to flat so this tells it how big the data is and we're going to have it come out it's going to have 10 24
that's how big the layer is coming out we'll go ahead and run this so now we have our full layer one and with the
full layer one we want to also define the full one dropout to go with that so
our full layer one comes in uh keep probability equals whole probability remember we created that earlier and the
full layer one is what's coming into it and this is going backwards and training the data we're not training every weight
we're only training a percentage of them each time which helps get rid of the bias so let me go ahead and run that and
finally we'll go ahead and create a y predict which is going to equal the normal full 1 dropout and 10 because we
have 10 labels in there now in this neural network we could have added additional layers that would be another
option to play with you can also play with instead of 10 24 you can use other numbers for the way that sets up and
what's coming out going into the next one we're only going to do just the one layer and the one layer drop out and you
can see if we did another layer it'd be really easy just to feed in the full one dropout into full layer two and then
full layer two dropout would have full layer two feed into it and then you'd switch that here for the y prediction
for right now this is great this particular data set is tried and true and we know that this will work on it
and if we just type in y predict and we run that we'll see that this is a tensor object
shape question mark 10 d type 32 a quick way to double check what we're working on so now we've got all of our we've
done a setup all the way to the y predict which we just did we want to go ahead and apply the loss function and
make sure that's set up in there create the optimizer and then trainer optimizer and create a variable
to initialize all the global tf variables so before we dive in to the
loss function let me point out one quick thing or just kind of a rehab over a couple things and that is when we're
playing with this these setups we pointed out up here we can change the 4 4 and use different numbers there they
change your outcome so depending on what numbers you use here will have a huge impact on how well your model fits and
that's the same here of the 1024 also this is also another number that if you continue to raise that number you'll get
possibly a better fit you might overfit and if you lower that number you'll use less resources and generally you want to
use this in the exponential growth an exponential being 2 4 8 16 and in this case the next
one down would be 512. you can use any number there but those would be the ideal numbers uh when you look at this
data so the next step in all this is we need to also create a way of tracking
how good our model is and we're going to call this a loss function and so we're going to create a cross entropy loss
function and so before we discuss exactly what that is let's take a look and see what we're feeding it
we're going to feed it our labels and we have our true labels and our prediction labels so coming in here is where the
two different variables we're sending in or the two different probability distributions is one that we know is
true and what we think it's going to be now this function right here when they talk about cross entropy
in information theory the cross entropy between two probability distributions over the same underlying set of events
measures the average number of bits needed to identify an event drawn from the set that's a mouthful uh really
we're just looking at the amount of error in here how many of these are correct and how many of these um are
incorrect so how much of it matches and we're going to look at that we're just going to look at the average that's what the mean the reduced to the mean means
here so we're looking at the average error on this and so the next step is
we're going to take the error we want to know our cross entropy or our loss function how much loss we have that's
going to be part of how we train the model so when you know what the loss is and we're training it we feed that back
into the back propagation setup and so we want to go ahead and optimize that here's our optimizer we're going to
create the optimizer using an atom optimizer remember there's a lot of different ways of optimizing the data
atoms the most popular used so our optimizer is going to equal the tf train atom optimizer if you don't
remember what the learning rate is let me just pop this back into here here's our learning rate when you have your
weights you have all your weights and your different nodes that are coming out here's our node coming out
and it has all its weights and then the error is being prop sent back through in
reverse on our neural network so we take this error we adjust these weights based on the different formulas in this case
the atom formula is what we're using we don't want to just adjust them completely we don't want to change this
weight so it exactly fits the data coming through because if we made that kind of adjustment it's going to be biased to whatever the last data we sent
through is instead we're going to multiply that by .001 and make a very small shift in this weight so our delta
w is only 0.001 of the actual delta w of the full change we're going to compute
from the atom and then we want to go ahead and train it so our training or set up a training
variable or function and this is going to equal our optimizer minimize cross entropy and we make sure we go ahead and
run this so it's loaded in there and then we're almost ready to train our model but before we do that we need to create one
more variable in here and we're going to create a variable to initialize all the global tf variables and when we look at
this the tf global variable initializer this is a tensorflow
object it goes through there and it looks at all our different setup that we have going under our tensor flow and
then initializes those variables so it's kind of like a magic wand because it's all hidden in the back end
of tensorflow all you need to know about this is that you have to have the initialization on there which is an
operation and you have to run that once you have your setup going so we'll go ahead and run this piece of code and then we're
going to go ahead and train our data so let me run this let's load it up there and so now we're going to go ahead and
run the model by creating a graph session graph session is a tensorflow
term so you'll see that coming up it's one of the things that throws me because i always think of graphics and spark and
graph as just general graphing uh but they talk about a graph session so we're going to go ahead and run the model and
let's go ahead and walk through this uh what's going on here and let's paste this data in here and here we go so
we're going to start off with it with a tf session as cess so that's our actual tf session we've created uh so we're
right here with the tf session our session we're creating we're going to run tf global variable
initializer so right off the bat we're initializing our variables here and then we have for i in range 500. so what's
going on here remember 500 we're going to break the date up and we're going to batch it in at 500 points each we've
created our session run so we're going to do with tf session as session right
here we've created our variable session uh and then we're going to run we're going to go ahead and initialize it so
we have our tf global variables initializer that we created that initializes our session in here the
next thing we're going to do is we're going to go for i in range of 500 batch equals ch.nextbatch
so if you remember correctly this is loading up 100 pictures at a time and
this is going to loop through that 500 times so we are literally doing uh what
is that 500 times 100 is 50 000. so that's 50 000 pictures we're
going to process right there and the first process is we're going to do a session run we're going to take our train we created our train variable or
optimizer in there we're going to feed it the dictionary we had our feed dictionary that we created and we have x
equals batch 0 coming in y true batch 1 hold the probability 0.5 and then just
so that we can keep track of what's going on we're going to every 100 steps we're going to run a print so currently
on step format accuracy is and we're going to look at matches
equals tf.equal tf argument y prediction 1 tf.arg max y true comma 1. so we're
going to look at this as how many matches it has and here our acc all we're doing here is we're going to
take the matches how many matches they have it creates it generates a chart we're going to convert that to float
that's what the tfcast does and then we just want to know the average we just want to the average of the accuracy and
then we'll go ahead and print that out print session run accuracy feed dictionary so it takes all this and it
prints out our accuracy on there so let's go ahead and take this oops screen's there let's go ahead and take
this and let's run it and this is going to take a little bit to run uh so let's see what happens on my old laptop and
we'll see here that we have our current uh we're currently on step zero it takes a little bit to get through the accuracy
and this will take just a moment to run we can see that on our step zero it has an accuracy of 0.1 or 0.1028
and as it's running we'll go ahead you don't need to watch it run all the way but this accuracy is going to change a little bit up and down so we've actually
lost some accuracy during our step two but we'll see how that comes out let's come back after we run it all the way
through and see how the different steps come out it's actually reading that backwards uh the way this works is the
closer we get to one the more accuracy we have so you can see here we've gone from a point one to a point three nine
um and we'll go ahead and pause this and come back and see what happens when we're done with the full run all right
now that we've prepared the meal got it in the oven and pulled out my finished dish here if
you've ever watched any of the old cooking shows let's discuss a little bit about this accuracy going on here and
how do you interpret that we've done a couple things first we've defined
accuracy the reason i got it backwards before is you have loss or accuracy and with loss you'll
get a graph that looks like this it goes oops that's an s by the way there we go you get a graph that curves down like
this and with accuracy you get a graph that curves up this is how good it's doing now in this case uh one is
supposed to be really good accuracy that means it gets close to one but it never crosses one so if you have an accuracy
of one that is phenomenal in fact that's pretty much you know unheard of and the same thing with loss if you have a loss of zero
that's also unheard of the zero's actually on this this axis right here as we go in there so how do we interpret
that because you know if i was looking at this and i go oh 0.51 that's uh 51
you're doing 50 50. no this is not percentage let me just put that in there
it is not percentage uh this is log rhythmic what that means is that 0.2 is
twice as good as 0.1 and when we see 0.4 that's twice as good as 0.2 real way to
convert this into a percentage you really can't say this is is a direct percentage conversion what you can do
though is in your head if we were to give this a percentage uh we might look at this as uh 50
we're just guessing equals 0.1 and if 50 roughly equals 0.1 that's we started up
here at the top remember at the top here here's our 0.1028 the accuracy of 50
then 75 percent is about 0.2 and so on and so on don't quote those numbers
because that doesn't work that way they say that if you have 0.95 five that's pretty much saying a
hundred percent and if you have uh anywhere between you'd have to go look this up let me go and remove all my
drawings there uh so the magic number is 0.5 we really want to be over a 0.5 in
this whole thing and we have uh both 0.504 remember this is accuracy if we
were looking at loss then we would be looking the other way but 0.05 you know instead of how high it is we want how
low it is uh but with accuracy being over 0.5 is pretty valid that means this is pretty solid and if you get to a 0.95
then it's a direct correlation that's what we're looking for here in these numbers you can see we finished with
this model at 0.5135 so still good um and if we look at uh when they ran this in the other
end remember there's a lot of randomness that goes into it when we see the weights uh they got .5251 so a little better than ours but
that's fine you'll find your own uh comes up a little bit better or worse depending on just that randomness and so
we've gone through the whole model we've created we've trained the model and we've also gone through on every 100th
run to test the model to see how accurate it is welcome to the rnn
tutorial that's the recurrent neural network so we talk about a feed forward
neural network in a feed forward neural network information flows only in the forward direction from the input nodes
through the hidden layers if any into the output nodes there are no cycles or loops in the network and so you can see
here we have our input layer i was talking about how it just goes straight forward into the hidden layers so each one of those connects and then connects
to the next hidden layer connects to the output layer and of course we have a nice simplified version where it has a predicted output the refer to the input
is x a lot of times and the output as y decisions are based on current input no
memory about the past no future scope why recurrent neural network issues in
feed forward neural network so one of the biggest issues is because it doesn't have a scope of memory or time a
feed-forward neural network doesn't know how to handle sequential data it only considers only the current input so if
you have a series of things and because three points back affects what's happening now and what your output
affects what's happening that's very important so whatever i put as an output is going to affect the next one um a feed forward doesn't look at any of that
it just looks at this is what's coming in and it cannot memorize previous inputs so it doesn't have that list of
inputs coming in solution to feed forward neural network you'll see here where it says recurrent neural network
and we have our x on the bottom going to h going to y that's your feed forward but right in the middle it has a value c
so there's a whole another process so it's memorizing what's going on in the hidden layers and the hidden layers as
they produce data feed into the next one so your hidden layer might have an output that goes off to y
but that output goes back into the next prediction coming in what this does is this allows it to handle sequential data
it considers the current input and also the previously received inputs and if we're going to look at general drawings
and solutions we should also look at applications of the rnn image captioning
rnn is used to caption an image by analyzing the activities present in it a dog catching a ball in midair that's
very tough i mean you know we have a lot of stuff that analyzes images of a dog and the image of a ball but it's able to
add one more feature in there that's actually catching the ball in midair time series prediction any time series
problem like predicting the prices of stocks in a particular month can be solved using rnn and we'll dive into
that in our use case and actually take a look at some stock one of the things you should know about analyzing stock today
is that it is very difficult and if you're analyzing the whole stock the stock market at the new york stock
exchange in the u.s produces somewhere in the neighborhood if you count all the individual trades in fluctuations by the
second it's like three terabytes a day of data so we're going to look at one stock just
analyzing one stock is really tricky in here we'll give you a little jump on that so that's exciting but don't expect
to get rich off of it immediately another application of the rnn is natural language processing text mining
and sentiment analysis can be carried out using rnn for natural language processing and you can see right here
the term natural language processing when you stream those three words together is very different than ice if i
said processing language natural leap so the time series is very important when we're analyzing sentiments it can change
the whole value of a sentence just by switching the words around or if you're just counting the words you may get one
sentiment where if you actually look at the order they're in you get a completely different sentiment when it rains look for rainbows when it's dark
look for stars both of these are positive sentiments and they're based upon the order of which the sentence is
going in machine translation given an input in one language rnn can be used to translate the input into a different
languages as output i myself very linguistically challenged but if you
study languages and you're good with languages you know right away that if you're speaking english you would say
big cat and if you're speaking spanish you would say cat big so that translation is really important to get
the right order to get there's all kinds of parts of speech that are important to know by the order of the words here this
person is speaking in english and getting translated and you can see here a person is speaking in english in this
little diagram i guess that's denoted by the flags i have a flag i own it no um
but they're speaking in english and it's getting translated into chinese italian french german and
spanish languages some of the tools coming out are just so cool so somebody
like myself who's very linguistically challenged i can now travel into worlds i would never think of because i can
have something translate my english back and forth readily and i'm not stuck with a communication gap so let's dive into
what is a recurrent neural network recurrent neural network works on the principle of saving the output of a
layer and feeding this back to the input in order to predict the output of the layer sounds a little confusing when we
start breaking it down it'll make more sense and usually we have our propagation forward neural network with
the input layers the hidden layers the output layer with the recurrent neural network we turn that on its side so here
it is and now our x comes up from the bottom into the hidden layers into y and they usually draw very simplified x to h
with c as a loop a to y where a b and c are the perimeters a lot of times you'll
see this kind of drawing in here digging closer and closer into the h and how it works going from left to right you'll
see that the c goes in and then the x goes in so the x is going upward bound and c is going to the right a is going
out and c is also going out that's where it gets a little confusing so here we have xn
cn and then we have y out and c out and c is based on h t minus one so our value
is based on the y and the h value are connected to each other they're not necessarily the same value because h can
be its own thing and usually we draw this or we represent it as a function h of t equals a function of c where h of t
minus 1 that's the last h output and x of t going in so it's the last output of
h combined with the new input of x where h t is the new state fc is a function
with the parameters c that's a common way of denoting it h t minus 1 is the old state coming out
and then x of t is an input vector at time of step t well we need to cover types of recurrent
neural networks and so the first one is the most common one which is a one-to-one single output
one-to-one neural network is usually known as a vanilla neural network used for regular machine learning problems
why because vanilla is usually considered kind of a just a real basic flavor but because it's very basic a lot
of times they'll call it the vanilla neural network which is not the common term but it is you know kind of a slang
term people will know what you're talking about usually if you say that then we run one to many so you have a single input and you might have a
multiple outputs in this case uh image captioning as we looked at earlier where we have not just looking at it as a dog
but a dog catching a ball in the air and then you have mini to one network takes in a sequence of inputs examples
sentiment analysis where a given sentence can be classified as expressing positive or negative sentiments and we
looked at that as we were discussing if it rains look for a rainbow so positive sentiment where rain might be a negative
sentiment if you're just adding up the words in there and then the course if you're going to do a one-to-one mini to
one one to many there's many to many networks takes in a sequence of inputs and generates a sequence of outputs
example machine translation so we have a lengthy sentence coming in english and then going out in all the different
languages you know just a wonderful tool very complicated set of computations if
you're a translator you realize just how difficult it is to translate into different languages one of the biggest
things you need to understand when we're working with this neural network is what's called the vanishing gradient
problem while training an rnn your slope can be either too small or very large
and this makes training difficult when the slope is too small the problem is known as vanishing gradient and you'll
see here they have a nice image loss of information through time so if you're pushing not enough
information forward that information is lost and then when you go to train it you start losing the third word in the
sentence or something like that or doesn't quite follow the full logic of what you're working on exploding gradient problem oh this is
one that runs into everybody when you're working with this particular neural network when the slope tends to grow
exponentially instead of decaying this problem is called exploding gradient issues in gradient problem long training
time poor performance bad accuracy and i'll add one more in there uh your
computer if you're on a lower end computer testing out a model will lock up and give you the memory error
explaining gradient problem consider the following two examples to understand what should be the next word in the
sequence the person who took my bike and blank a thief the students who got into
engineering with blank from asia and you can see in here we have our x value
going in we have the previous value going forward and then you back propagate the error like you do with any
neural network and as we're looking for that missing word maybe we'll have the person took my bike and blank was a
thief and the student who got into engineering with a blank were from asia consider the following example the
person who took the bike so we'll go back to the person who took the bike was blank a thief in order to understand
what would be the next word in the sequence the rnn must memorize the previous context whether the subject was
singular noun or a plural noun so was a thief as singular the student who got
into engineering well in order to understand what would be the next word in the sequence the rnn must memorize
the previous context whether the subject was singular noun or a plural noun and so you can see here the students who got
into engineering with blank were from asia it might be sometimes difficult for
the air to back propagate to the beginning of the sequence to predict what should be the output so when you run into the gradient problem we need a
solution the solution to the gradient problem first we're going to look at exploding gradient where we have three
different solutions depending on what's going on one is identity initialization
so the first thing we want to do is see if we can find a way to minimize the identities coming in instead of having
it identify everything just the important information we're looking at next is to truncate the back propagation
so instead of having whatever information it's sending to the next series we can truncate what it's sending
we can lower that particular set of layers make those smaller and finally is
a gradient clipping so when we're training it we can clip what that gradient looks like and narrow the
training model that we're using when you have a vanishing gradient the option problem we can take a look at weight
initialization very similar to the identity but we're going to add more weights in there so it can identify
different aspects of what's coming in better choosing the right activation function that's huge so we might be
activating based on one thing and we need to limit that we haven't talked too much about activation function so we'll
look at that just minimally there's a lot of choices out there and then finally there's long short term memory
networks the lstms and we can make adjustments to that so just like we can
clip the gradient as it comes out we can also expand on that we can increase the
memory network the size of it so it handles more information and one of the most common problems in today's
setup is what they call long-term dependencies suppose we try to predict the last word in the text the clouds are
in the and you probably said sky here we do not need any further context it's pretty clear that the last word is going
to be sky suppose we try to predict the last word in the text i have been staying in spain for the last 10 years i
can speak fluent maybe you said portuguese or french no he probably said spanish the word we predict will depend
on the previous few words in context here we need the context of spain to predict the last word in the text it's
possible that the gap between the relevant information and the point where it is needed to become very large
lstms help us solve this problem so the lstms are a special kind of recurrent
neural network capable of learning long-term dependencies remembering information for long periods of time is
their default behavior all recurrent neural networks have the form of a chain of repeating modules of neural network
connections in standard rnns this repeating module will have a very simple structure such as a single tangent h
layer lstms's also have a chain-like structure but the repeating module has a different
structure instead of having a single neural network layer there are four interacting layers communicating in a
very special way lstms are a special kind of recurrent neural network capable
of learning long-term dependencies remembering information for long periods of time is their default behavior ls
tmss also have a chain like structure but the repeating module has a different structure instead of having a single
neural network layer there are four interacting layers communicating in a very special way as you can see the
deeper we dig into this the more complicated the graphs get in here i want you to note that you have x a t
minus one coming in you have x a t coming in and you have x a t plus one
and you have h of t minus one and h of t coming in and h of t plus one going out
and of course on the other side is the output a in the middle we have our tangent h but
it occurs in two different places so not only when we're computing the x of t
plus one are we getting the tangent h from x to t but we're also getting that value coming in from the x a t minus one
so the short of it is as you look at these layers not only does it does the propagate through the first layer goes
into the second layer back into itself but it's also going into the third layer so now we're kind of stacking those up
and this can get very complicated as you grow that in size it also grows in memory too and in the amount of
resources it takes but it's a very powerful tool to help us address a problem of complicated long sequential
information coming in like we were just looking at in the sentence and when we're looking at our long short term
memory network uh there's three steps of processing sensing in the lstms that we look at the first one is we want to
forget irrelevant parts of the previous state you know a lot of times like you know is as in a unless we're trying to
look at whether it's a plural noun or not they don't really play a huge part in the language so we want to get rid of
them then selectively update cell state values so we only want to update the cell state values that reflect what
we're working on and finally we want to put only output certain parts of the cell state so whatever is coming out we
want to limit what's going out too and let's dig a little deeper into this let's just see what this really looks
like so step one decides how much of the past it should remember first step in the
lstm is to decide which information to be omitted in from the cell in that
particular time step it is decided by the sigmoid function it looks at the previous state h to t minus 1 and the
current input x of t and computes the function so you can see over here we have a function of t
equals the sigmoid function of the weight of f the h at t minus one and
then x a t plus of course you have a bias in there with any of our neural networks so we have a bias function so f
of t equals forget gate decides which information to delete that is not important from the previous time step
considering an stm is fed with the following inputs from the previous and present time step alice is good in
physics john on the other hand is good in chemistry so previous output john plays football well he told me yesterday
over the phone that he had served as a captain of his college football team that's our current input so as we look
at this the first step is the forget gate realizes there might be a change in context after encounting the first full
stop compares with the current input sentence of x a t so we're looking at
that full stop and then compares it with the input of the new sentence the next sentence talks about john so the
information on alice is deleted okay that's important to know so we have this input coming in and if we're going to
continue on with john then that's going to be the primary information we're looking at the position of the subject
is vacated and is assigned to john and so in this one we've seen that we've weeded out a whole bunch of information
and we're only passing information on john since that's now the new topic so step two is then to decide how much
should this unit add to the current state in the second layer there are two parts one is a sigmoid function and the
other is the tangent h in the sigmoid function it decides which values to let through zero or one tangent h function
gives a weightage to the values which are passed deciding their level of importance minus one to one and you can
see the two formulas that come up the i of t equals the sigmoid of the weight of
i h to t minus 1 x of t plus the bias of i and the c of t equals the tangent of h
of the weight of c of h of t minus 1 x of t plus the bias of c so our i of t
equals the input gate determines which information to let through based on its significance in the current time step if
this seems a little complicated don't worry because a lot of the programming is already done when we get to the case
study understanding though that this is part of the program is important when you're trying to figure out these what
to set your settings at you should also note when you're looking at this it should have some semblance to your forward propagation neural networks
where we have a value assigned to a weight plus a bias very important steps
than any of the neural network layers whether we're propagating into them the information from one to the next or
we're just doing a straightforward neural network propagation let's take a quick look at this what it looks like
from the human standpoint as i step out of my suit again consider the current input at x of t john plays football well
he told me yesterday over the phone that he had served as a captain of his college football team that's our input
input gate analysis the important information john plays football and he was a captain of his college team is
important he told me over the phone yesterday is less important hence it is forgotten this process of adding some
new information can be done via the input gate now this example is as a human form and we'll look at training
this stuff in just a minute but as a human being if i wanted to get this information from a conversation
maybe it's a google voice listening in on you or something like that um how do we weed out the information that he was
talking to me on the phone yesterday well i don't want to memorize that he talked to me on the phone yesterday or maybe that is important but in this case
it's not i want to know that he was the captain of the football team i want to know that he served i want to know that
john plays football and he was the captain of the college football team those are the two things that i want to
take away as a human being again we measure a lot of this from the human viewpoint and that's also how we try to
train them so we can understand these neural networks finally we get to step three decides what part of the current
cell state makes it to the output the third step is to decide what will be our output first we run a sigmoid layer
which decides what parts of the cell state make it to the output then we put the cell state through the tangent h to
push the values to be between -1 and 1 and multiply it by the output of the sigmoid gate so when we talk about the
output of t we set that equal to the sigmoid of the weight of zero of the h
of t minus one and back one step in time by the x of t plus of course the bias
the h of t equals the outer t times the tangent of the tangent h of c of t so
our o t equals the output gate allows the passed in information to impact the output in the current time step let's
consider the example to predicting the next word in the sentence john played tremendously well against the opponent
and one for his team for his contributions brave blank was awarded player of the match there could be a lot
of choices for the empty space current input brave is an adjective adjectives
describe a noun john could be the best output after brave thumbs up for john awarded player of the match and if you
were to pull just the nouns out of the sentence team doesn't look right because that's not really the subject we're
talking about contributions you know brave contributions or brave team brave player brave match
so you look at this and you can start to train this these this neural network so starts looking at and goes oh no john is
what we're talking about so brave is an adjective john's going to be the best output and
we give john a big thumbs up and then of course we jump into my favorite part a
case study use case implementation of lstm let's predict the prices of stocks
using the lstm network based on the stock price data between 2012 2016.
we're going to try to predict the stock prices of 2017. and this will be a narrow set of data
we're not going to do the whole stock market it turns out that the new york stock exchange generates roughly three
terabytes of data per day that's all the different trades up and down of all the different stocks going on in each
individual one second to second or nanosecond to nanosecond but we're going to limit that to just some very basic fundamental
information so don't think you're going to get rich off this today but you at least you can give an eye you can give a
step forward in how to start processing something like stock prices a very valid
use for machine learning in today's markets use case implementation of lstm let's
dive in we're going to import our libraries we're going to import the training set and get the scaling going
now if you watch any of our other tutorials a lot of these pieces just start to look very familiar because it's
very similar setup let's take a look at that and just reminder we're going to be using anaconda the jupiter notebook so
here i have my anaconda navigator when we go under environments i've actually set up a cross python 36 i'm in python36
and nice thing about anaconda especially the newer version i remember a year ago messing with anaconda in different
versions of python in different environments anaconda now has a nice interface
and i have this installed both on a ubuntu linux machine and on windows so it works fine on there you can go in
here and open a terminal window and then in here once you're in the terminal window this is where you're going to start
installing using pip to install your different modules and everything now we've already pre-installed them so we
don't need to do that in here but if you don't have them installed in your particular environment you'll need to do that and of course you don't need to use
the anaconda or the jupiter you can use whatever favorite python id you like i'm just a big fan of this because it keeps
all my stuff separate you can see on this machine i have specifically installed one for cross since we're
going to be working with cross under tensorflow we go back to home i've gone up here to application and that's the
environment i've loaded on here and then we'll click on the launch jupiter notebook now i've already in my jupiter
notebook have set up a lot of stuff so that we're ready to go kind of like martha stewart's in the old cooking show so we
want to make sure we have all our tools for you so you're not waiting for them to load and if we go up here to where it
says new you can see where you can create a new python 3. that's what we
did here underneath the setup so it already has all the modules installed on it and i'm actually renamed this if you
go under file you can rename it we've i'm calling it rnn stock and let's just take a look and start diving into the
code let's get into the exciting part now we've looked at the tool and of course you might be using a different tool which is fine let's start putting
that code in there and seeing what those imports and uploading everything looks like now first half is kind of boring
when we hit the run button because we're going to be importing numpy as np that's uh the number python which is your numpy
array and the matplot library because we're going to do some plotting at the end and our pandas for our data set our
pandas as pd and when i hit run it really doesn't do anything except for load those modules just a quick note let
me just do a quick draw here oops shift alt there we go you'll notice when we're doing this setup if i was to divide this
up oops i'm going to actually let's overlap these here we go
this first part that we're going to do is our data
prep a lot of prepping involved in fact depending on what your system is
since we're using keras i put an overlap here uh but you'll find that almost maybe even half of the code we do is all
about the data prep and the reason i overlapped this with uh cross let me just put that down because that's what
we're working in uh is because cross has like their own preset stuff so it's already pre-built in which is really
nice so there's a couple steps a lot of times that are in the keras setup we'll take a look at that to see what comes up
in our code as we go through and look at stock and the last part is to evaluate
and if you're working with shareholders or classroom whatever it is you're working
with the evaluate is the next biggest piece so the actual code here crosses a little
bit more but when you're working with some of the other packages you might have like three lines that might be it
all your stuff is in your pre-processing and your data since cross has is is cutting edge and you load the individual
layers you'll see that there's a few more lines here and crosses a little bit more robust and then you spend a lot of
times like i said with the evaluate you want to have something you present to everybody else and say hey this is what i did this is what it looks like so
let's go through those steps this is like a kind of just general overview and let's just take a look and see what the
next set of code looks like and in here we have a data set train and it's going to be read using the pd or pandas dot
read csv and it's a google stock price train dot csv and so under this we have
training set equals data set train dot i location and we've kind of sorted out
part of that so what's going on here let's just take a look at let's look at the actual file and see what's going on there now if we look at this
ignore all the extra files on this i already have a train and a test set
where it's sorted out this is important to notice because a lot of times we do that as part of the pre-processing of
the data we take 20 percent of the data out so we can test it and then we train the rest of it
that's what we use to create our neural network that way we can find out how good it is uh but let's go ahead and just take a look and see what that looks
like as far as the file itself and i went ahead and just opened this up in a basic word pad and text editor just so
we can take a look at it certainly you can open up an excel or any other kind of spreadsheet
and we note that this is a comma separated variables we have a date
open high low close volume this is the standard stuff that we import into our stock or the most basic set of
information you can look at in stock it's all free to download in this case we downloaded it from google that's why
we call it the google stock price and this specifically is google this is the google stock values from as you can
see here we started off at 1 3 2012. so when we look at this first setup up
here we have a data set train equals pd underscore csv and if you noticed on the
original frame let me just go back there they had it set to home ubuntu downloads
google stock price train i went ahead and changed that because we're in the same file where i'm running the code so
i've saved this particular python code and i don't need to go through any special paths or have the full path on
there and then of course we want to take out certain values in here and you're going to notice that we're using
our data set and we're now in pandas so pandas basically it looks like a
spreadsheet and in this case we're going to do i location which is going to get specific
locations the first value is going to show us that we're pulling all the rows
in the data and the second one is we're only going to look at columns one and two and if you remember here from our
data as we switch back on over columns we saw we start with zero which is the date
and we're going to be looking at open and high which would be one and two
we'll just label that right there so you can see now when you go back and do this you
certainly can extrapolate and do this on all the columns but for the example let's just limit a
little bit here so that we can focus on just some key aspects of stock
and then we'll go up here and run the code and again i said the first half is very boring whenever we hit the run
button it doesn't do anything because we're still just loading the data and setting it up
now that we've loaded our data we want to go ahead and scale it and we want to do what they call feature scaling and in
here we're going to pull it up from the sk learn or the sk kit pre-processing
import min max scaler and when you look at this you got to remember that biases
in our data we want to get rid of that so if you have something that's like a really high value let's just draw a
quick graph and i have something here like the maybe the stock has a value one stock has a
value of a hundred and another stock has a value of five
you start to get a bias between different stocks and so when we do this we go ahead and say okay 100 is going to
be the max and 5 is going to be the min and then everything else goes and then
we change this so we just squish it down i like the word squish so it's between 1
and 0. so 100 equals 1 or 1 equals 100 and 0 equals 5. and you can just
multiply it's usually just a simple multiplication we're using multiplication so it's going to be minus
5 and then 100 divided or 95 divided by 1. so or whatever value is is divided by
95. and once we've actually created our scale we've telling is going to be from
0 to 1 we want to take our training set and we're going to create a training set scaled and we're going to use our scalar
sc we're going to fit we're going to fit and transform the training set so we can now use the sc this this
particular object we'll use it later on our testing set because remember we have to also scale that when we go to test
our model and see how it works and we'll go ahead and click on the run again it's not going to have any output yet because
we're just setting up all the variables right so we pasted the data in here and
we're going to create the data structure with the 60 time steps and output
first note we're running 60 time steps and that is where this value here also
comes in so the first thing we do is we create our x train and y train variables
and we set them to an empty python array very important to remember what kind of array we're in what we're working with
and then we're going to come in here we're going to go for i in range 60 to 1258 there's our 60 60 time steps and
the reason we want to do this is as we're adding the data in there there's nothing below the 60. so if we're going
to use 60 time steps we have to start at point 60 because it includes everything underneath of it otherwise you'll get a
pointer error and then we're going to take our x train and we're going to append training set scaled this is a
scaled value between 0 and 1 and then as i is equal to 60 this value is going to
be 60 minus 60 is 0. so this actually is 0 to i so it's going to be 0 60 1 to 61.
let me just circle this part right here 1 to 61 2 to 62 and so on and so on and if you
remember i said 0 to 60 that's incorrect because it does not count remember it starts at 0 so this is a count of 60 so
it's actually 59. important to remember that as we're looking at this and then the second part of this that we're
looking at so if you remember correctly here we go we go from uh 0 to 59 of i
and then we have a comma a 0 right here and so finally we're just going to look at the open value now i know we did put
it in there for one to two if you remember quickly it doesn't count the second one so it's just the open
value we're looking at just open and then finally we have y train dot
append training set i to 0 and if you remember correctly i 2 or i comma 0. if
you remember correctly this is 0 to 59 so there's 60 values in it so we do i
down here this is number 60. so we're going to do this is we're creating an array and we have 0
to 59 and over here we have number 60 which is
going into the y train it's being appended on there and then this just goes all the way up so this is down here
is a 0 to 59 and we'll call it 60 since that's the value over here and it goes
all the way up to 12 58. that's where this value here comes
in that's the length of the data we're loading so we've loaded two arrays we've loaded one array that has uh which is filled
with arrays from 0 to 59 and we loaded one array which is just the value and what we're looking at you want to think
about this as a time sequence here's my open open open open open open what's the next one in the series so
we're looking at the google stock and each time it opens we want to know what the next one 0 through 59 what's 60 1
through 60 what's 61 2 through 62 what's 62 and so on and so on going up and then
once we've loaded those in our for loop we go ahead and take x train and y train equals
np.arrayxtrain.npray train we're just converting this back into a numpy array that way we can use all the cool tools
that we get with numpy array including reshaping so if we take a look and see what's going on here we're going to take
our x train we're going to reshape it wow what the heck does reshape mean uh
that means we have an array if you remember correctly um so many numbers by 60.
that's how wide it is and so we're when you when you do x train dot shape that gets one of the
shapes and you get um x train dot shape of one gets the other shape and we're
just making sure the data is formatted correctly and so you use this to pull the fact that it's 60 by um
in this case where's that value 60 pi 1199 1258 minus 60 11.99 and we're
making sure that that is shaped correctly so the data is grouped into 11.99 by 60 different arrays and then
the one on the end just means at the end because this when you're dealing with shapes and numpy they look at this as
layers and so the in layer needs to be one value that's like the leaf of a tree
where this is the branch and then it branches out some more and then you get the leaf np dot reshape
comes from and using the existing shapes to form it we'll go ahead and run this
piece of code again there's no real output and then we'll import our different cross modules that we need so
from cross models we're going to import the sequential model dealing with sequential data we have our dense layers
we have actually three layers we're going to bring in our dents our lstm which is what we're focusing on and our
dropout and we'll discuss these three layers more in just a moment but you do need the with the lstm you do need the
dropout and then the final layer will be the dents but let's go ahead and run this and they'll bring port our modules
and you'll see we get an error on here and if you read it closer it's not actually an error it's a warning what
does this warning mean these things come up all the time when you're working with such cutting edge modules are complete
being updated all the time we're not going to worry too much about the warning all it's saying is that the h5py
module which is part of cross is going to be updated at some point and if you're running new stuff on cross and
you start updating your cross system you better make sure that your h5 pi is updated too otherwise you're going to
have an error later on and you can actually just run an update on the h5 pi now if you wanted to not a big deal
we're not going to worry about that today and i said we were going to jump in and start looking at what those layers mean i meant that and we're going
to start off with initializing the rnn and then we'll start adding those layers in and you'll see that we have the lstm
and then the dropout lstm then dropout lstm then dropout what the heck is that
doing so let's explore that we'll start by initializing the rnn regressor equals
sequential because we're using the sequential model and we'll run that and load that up and then we're going to start adding our lstm layer and some
dropout regularization and right there should be the q dropout regularization
and if we go back here and remember our exploding gradient well that's what we're talking about the dropout drops
out unnecessary data so we're not just shifting huge amounts of data through the network so and so we go in here
let's just go ahead and add this in i'll go ahead and run this and we had three of them so let me go ahead and put all
three of them in and then we can go back over them there's the second one and let's put one more in let's put that in
and we'll go ahead and put two more in i mean i said one more in but it's actually two more in and then let's add one more after that and as you
can see each time i run these they don't actually have an output so let's take a closer look and see what's going on here
so we're going to add our first lstm layer in here we're going to have units 50. the units is the positive integer
and it's the dimensionality of the output space this is what's going out into the next layer so we might have 60
coming in but we have 50 going out we have a return sequence because it is a sequence data so we want to keep that
true and then you have to tell it what shape it's in well we already know the shape by just going in here and looking
at x train shape so input shape equals the x train shape of one comma one makes
it really easy you don't have to remember all the numbers that put in 60 or whatever else is in there you just
let it tell the regressor what model to use and so we follow our stm with a
dropout layer now understanding the dropout layer is kind of exciting because one of the things that happens
is we can over train our network that means that our neural network will memorize such specific data that it has
trouble predicting anything that's not in that specific realm to fix for that each time we run through the training
mode we're going to take point two or twenty percent of our neurons and just turn them off so we're only gonna train
on the other ones and it's gonna be random that way each time we pass through this we don't over train these
nodes come back in in the next training cycle we randomly pick a different 20. and finally they see a big difference as
we go from the first to the second and third and fourth the first thing is we
don't have to input the shape because the shapes already the output units is 50 here this item the next step
automatically knows this layer is putting out 50 and because it's the next layer it automatically sets that and
says so 50 is coming out from our last layer that's coming up you know goes into the regressor and of course we have
our dropout and that's what's coming into this one and so on and so on and so the next three layers we don't have to
let it know what the shape is it automatically understands that and we're going to keep the units the same we're
still going to do 50 units it's still a sequence coming through 50 units and a sequence now the next piece of code is
what brings it all together let's go ahead and take a look at that and we come in here we put the output layer the
dense layer and if you remember up here we had the three layers we had lstm dropout and dents dense just says we're
going to bring this all down into one output instead of putting out a sequence we just know i want to know the answer
at this point and let's go ahead and run that and so in here you notice all we're doing is setting things up one step at a
time so far we've brought in our way up here we brought in our data we brought in our different modules we formatted
the data for training it we set it up you know we have our y x train and our y train we have our source of data and the
answers where we know so far that we're going to put in there we've reshaped that we've come in and built our cross
we've imported our different layers and we have in here if you look we have what five total layers now cross is a little
different than a lot of other systems because a lot of other systems put this all in one line and do it automatic but
they don't give you the options of how those layers interface and they don't give you the options of how the data
comes in cross is cutting edge for this reason so even though there's a lot of extra steps in building the model this
has a huge impact on the output and what we can do with this these new models from cross so we brought in our dents we
have our full model put together a regressor so we need to go ahead and compile it and then we're going to go
ahead and fit the data we're going to compile the pieces so they all come together and then we're going to run our
training data on there and actually recreate our regressor so it's ready to be used so let's go ahead and compile
that and i go ahead and run that and if you've been looking at any of our other tutorials on neural networks you'll see
we're going to use the optimizer atom atom is optimized for big data there's a couple other optimizers out there beyond
the scope of this tutorial but certainly atom will work pretty good for this and loss equals mean squared value so when
we're training it this is what we want to base the loss on how bad is our error we're going to use the mean squared value for our error and the atom
optimizer for its differential equations you don't have to know the math behind them but certainly it helps to know what
they're doing and where they fit into the bigger models and then finally we're going to do our fit fitting the rn into
the training set we have the regressor.fit xtrain y train epochs and
batch size so we know where this is this is our data coming in for the x train our y train is the answer we're looking
for of our data our sequential input epics is how many times we're going to go over the whole data set we created a
whole data set of x trains so this is each each of those rows which includes a time sequence of 60. and bad size
another one of those things where cross really shines is if you were pulling this save from a large file instead of
trying to load it all into ram it can now pick smaller batches up and load those indirectly we're not worried about
pulling them off a file today because this isn't big enough to cause a computer too much of a problem to run
not too straining on the resources but as we run this you can imagine what happened if i was doing a lot more than
just one column in one set of stock in this case google stock imagine if i was
doing this across all the stocks and i had instead of just the open i had open close high low and you can actually find
yourself with about 13 different variables times 60 because there's a time sequence suddenly you find yourself
with a gig of memory you're loading into your ram which will just completely you know if it's just if you're not on
multiple computers or cluster you can start running into resource problems but for this we don't have to worry about
that so let's go ahead and run this and this will actually take a little bit on my computer because it's an older laptop
and give it a second to kick in there there we go all right so we have epic so this is going to tell me it's running
the first run through all the data and as it's going through it's batching them in 32 pieces so 32 lines each time and
there's 1198 i think i said 11.99 earlier but it's 11.98 i was off by one and each one of these is 13 seconds so
you can imagine this is roughly 20 to 30 minutes run time on this computer like i said it's an older laptop running at 0.9
gigahertz on a dual processor and that's fine what we'll do is i'll go ahead and stop go get a drink of coffee and come
back and let's see what happens at the end and where this takes us and like any good cooking show
i've kind of gotten my latte i also have some other stuff running in the background so you'll see these numbers jumped up to like 19 seconds 15 seconds
but you can scroll through you can see we've run it through 100 steps or 100 epics so the question is what does all
this mean one of the first things you'll notice is that our loss can is over here it kind of stopped at 0.0014 but you can
see it kind of goes down until we hit about .014 three times in a row so we guessed our epic pretty close since our
losses remain the same on there so to find out we're looking at we're going to go ahead and load up our test data the
test data that we didn't process yet and a real stock price data set test eye location this is the same thing we did
when we prepped the data in the first place so let's go ahead and go through this code and we can see we've labeled
it part three making the predictions and visualizing the results so the first thing that we need to do is go ahead and
read the data in from our test csv you see i've changed the path on it for my computer and then we'll call it the real
stock price and again we're doing just the one column here and the values from
ilocation so it's all the rows and just the values from these that one location that's the open stock open let's go
ahead and run that so that's loaded in there and then let's go ahead and create we have our inputs we're going to create
inputs here and this should all look familiar because this is the same thing we did before we're going to take our data set total we're going to do a
little panda concat from the datastate train now remember the end of the data set train is part of the data going in
let's just visualize that just a little bit here's our train data let me just put tr for train and it went up to this
value here but each one of these values generated a bunch of columns it was 60
across and this value here equals this one and this value here equals this one and this value here equals this one and
so we need these top 60 to go into our new data so to find out what we're looking at we're going to go ahead and
load up our test data the test data that we didn't process yet and a real stock
price data set test eye location this is the same thing we did when we prep the data in the first place so let's go
ahead and go through this code and we can see we've labeled it part three making the predictions and visualizing
the results so the first thing we need to do is go ahead and read the data in from our test csv you see i've changed
the path on it for my computer and then we'll call it the real stock price and
again we're doing just the one column here and the values from i location so
it's all the rows and just the values from these that one location that's the open stock open let's go ahead and run
that so that's loaded in there and then let's go ahead and create we have our inputs we're going to create inputs here
and this should all look familiar because this is the same thing we did before we're going to take our data set total we're going to do a little panda
concat from the data state train now remember the end of the data set train
is part of the data going in let's just visualize that just a little bit here's our train data let me just put tr for
train and it went up to this value here but each one of these values generated a
bunch of columns it was 60 across and this value here equals this one and this value here equals this one and this
value here equals this one and so we need these top 60 to go into our new
data because that's part of the next data or it's actually the top 59. so that's what this first setup is over
here is we're going in we're doing the real stock price and we're going to just take the data set test and we're going
to load that in and then the real stock price is our data test.test location so we're just looking at that first column
the open price and then our data set total we're going to take pandas and we're going to concat and we're going to
take our data set train for the open and our data set test open and this is one way you can reference these columns
we've referenced them a couple different ways we've referenced them up here with the one two but we know it's labeled as a panda set as open so pandas is great
that way lots of versatility there and we'll go ahead and go back up here and run this there we go and you'll notice
this is the same as what we did before we have our open data set where pended our two different or concatenated our
two data sets together we have our inputs equals data set total length data set total minus length data set minus
test minus 60 values so we're going to run this over all of them and you'll see why this works because normally when
you're running your test set versus your training set you run them completely separate but when we graph this you'll
see that we're just going to be we'll be looking at the part that we didn't train it with to see how well it graphs and we
have our inputs equals inputs dot reshapes or reshaping like we did before we're transforming our inputs so if you
remember from the transform between 0 and 1 and finally we want to go ahead and take our x test and we're going to
create that x test and for i in range 60 to 80. so here's our x test and we're
appending our inputs i to 60 which remember is 0 to 59 and i comma 0 on the
other side so it's just the first column which is our open column and once again we take our x test we convert it to a
numpy array we do the same reshape we did before and then we get down to the final two lines and here we have
something new right here on these last two lines let me just highlight those or or mark them predicted stock price
equals regressor dot predicts x test so we're predicting all the stock including both the training and the testing model
here and then we want to take this prediction and we want to inverse the transform so remember we put them
between zero and one well that's not going to mean very much to me to look at a float number between 0 1 i want the
dollar amount so i want to know what the cash value is and we'll go ahead and run this and you'll see it runs much quicker
than the training that's what's so wonderful about these neural networks once you put them together it takes just a second to run the same neural network
that took us what a half hour to train ahead and plot the data we're going to plot what we think it's going to be and
we're going to plot it against the real data what the google stock actually did so let's go ahead and take a look at
that in code and let's uh pull this code up so we have our plt that's our oh if
you remember from the very beginning let me just go back up to the top we have our matplot library.pi plot as plt
that's where that comes in and we come down here we're going to plot let me get my drawing thing out again we're going
to go ahead and plt is basically kind of like an object it's one of the things that always threw me when i'm doing
graphs in python because i always think you have to create an object and then it loads that class in there well in this
case plt is like a canvas you're putting stuff on so if you've done html5 you'll
have the canvas object this is the canvas so we're going to plot the real stock price that's what it actually is
and we're going to give that color red so it's going to be in bright red we're going to label it real google stock
price and then we're gonna do our predicted stock and we're gonna do it in blue and it's gonna be labeled predicted
and we'll give it a title cause it's always nice to give a title to your uh graph especially if you're gonna present this to somebody you know to your
shareholders in the office and the x label is going to be time because it's a time series and we didn't actually put
the actual date and times on here but that's fine we just know they're incremented by time and then of course the y label is the actual stock price
plt.legend tells us to build the legend on here so that the color red and real
google stock price show up on there and then the plot shows us that actual graph so let's go ahead and run this and see
what that looks like and you can see here we have a nice graph and let's talk just a little bit about this graph
before we wrap it up here's our legend i was telling you about that's why we have the legend to show the prices we have
our title and everything and you'll notice on the bottom we have a time sequence we didn't put the actual time
in here now we could have we could have gone ahead and plotted the x since we know what the
dates are and plotted this to dates but we also know that it's only the last piece of data that we're looking at so
last piece of data which in somewhere probably around here on the graph i
think it's like about twenty percent of the data probably less than that we have the google price and the google price
has this little up jump and then down and you'll see that the actual google instead of uh a turn down here just
didn't go up as high and didn't go down so our prediction has the same pattern but the overall value is pretty
far off as far as um stock but then again we're only looking at one column we're only looking at the open price
we're not looking at how many volumes were traded like i was pointing out earlier we talk about stock just right
off the bat there's six columns there's open high low close volume then there's
weather i mean volume shares then there's the adjusted open adjusted high
adjusted low adjusted close they have a special formula to predict exactly what it would really be worth based on the
value of the stock and then from there there's all kinds of other stuff you can put in here so we're only looking at one small aspect the opening price of the
stock and as you can see here we did a pretty good job this curve follows the curve pretty well it has like a little
jumps on it bins they don't quite match up so this bin here does not quite match
up with that bin there but it's pretty darn close we have the basic shape of it and the prediction isn't too far off and
you can imagine that as we add more data in and look at different aspects in the specific domain of stock we should be
able to get a better representation each time we drill in deeper of course this took a half hour for my program my
computer to train so you can imagine that if i was running it across all those different variables it might take
a little bit longer to train the data not so good for doing a quick tutorial like this so we're going to dive right
into what is cross we'll also go all the way through this into a couple of tutorials because that's where you
really learn a lot is when you roll up your sleeves so we talk about what is cross cross is a high level deep
learning api written in python for easy implement implementation of neural
networks it uses deep learning frameworks such as tensorflow pytorch etc is back in to make computation
faster and this is really nice because as a programmer there is so much stuff out
there and it's evolving so fast it can get confusing and having some kind of high level order in there we can
actually view it and easily program these different neural networks is really powerful it's really powerful
to have something out really quick and also be able to start testing your models and seeing where you're going
so crossworks by using complex deep learning frameworks such as tensorflow pytorch
ml played etc as a backend for fast computation while providing a user friendly and easy
to learn front-end and you can see here we have the cross api specifications and under that you'd
have like tf cross for tensorflow thano cross and so on and then you have your
tensorflow workflow that this is all sitting on top of and this is like i said it organizes
everything the heavy lifting is still done by tensorflow or whatever you know
underlying package you put in there and this is really nice because you don't have to
dig as deeply into the heavy end stuff while still having a very robust package
you can get up and running rather quickly and it doesn't distract from the processing time because all the heavy
lifting is done by packages like tensorflow this is the organization on top of it
so the working principle of cross uh the working principle of keras is cross uses computational graphs to
express and evaluate mathematical expressions you can see here we put them in blue
they have the expression expressing complex problems as a combination of simple mathematical
operators where we have like the percentage or in this case in python that's usually your
left your remainder or multiplication you might have the operator of x to the
power of 0.3 and it uses useful for calculating derivatives by using
back propagation so if we're doing with neural networks we send the error back up to figure out how to change it
this makes it really easy to do that without really having not banging your head and having to hand write everything
it's easier to implement distributed computation and for solving complex problems specify
input and outputs and make sure all nodes are connected and so this is really nice as you come
in through is that as your layers are going in there you can get some very complicated
different setups nowadays which we'll look at in just a second and this just makes it really easy to
start spinning this stuff up and trying out the different models so we look across models across model we
have a sequential model sequential model is a linear stack of layers where the previous layer leads
into the next layer and this if you've done anything else even like the sk learn with their neural
networks and propagation and any of these setups this should look familiar you should have your input layer it goes
into your layer 1 layer 2 and then to the output layer and it's useful for
simple classifier decoder models and you can see down here we have the model equals across sequential this is
the actual code you can see how easy it is we have a layer that's dense your layer
one as an activation now they're using the relu in this particular example and then you have your name layer one layer dense
relu name layer two and so forth and they just feed right into each other so it's really easy just to stack them
as you can see here and it automatically takes care of everything else for you and then there's a functional model and
this is really where things are at this is new make sure you update your cross or you'll find yourself running this
doing the functional model you'll run into an error code because this is a fairly new release
and he uses multi-input and multi-output model the complex model which forks into
two or more branches and you can see here we have our image inputs equals your cross input shape
equals 32 by 32 by 3. you have your dense layers dense 64
activation relu this should look similar to what you already saw before but if you look at the graph on the
right it's going to be a lot easier to see what's going on you have two different inputs
and one way you could think of this is maybe one of those is a small image and one of those is a full sized image
and that feedback goes into you might feed both of them into one note because it's
looking for one thing and then only into one note for the other one and so you can start to get
kind of an idea that there's a lot of use for this kind of split and this kind of setup we have multiple information
coming in but the information is very different even though it overlaps and you don't want to send it through the
same neural network and they're finding that this trains faster and is also has a better result
depending on how you split the data and how you fork the models coming down
and so in here we do have the two complex models coming in we have our image inputs which is a 32
by 32 by three or three channels or four if you're having an alpha channel uh you have your dense your layers dense is 64
activation using the relu very common x equals dense inputs x layers dense x64
activation equals relu x outputs equals layers dense 10 x model equals cross model inputs equals
inputs outputs equals outputs name equals minced model uh so we add a little name on there and
again this is this kind of split here this is setting us up to have the input go into different areas
so if you're already looking at cross you probably already have this answer what are neural networks
but it's always good to get on the same page and for those people who don't fully understand neural networks to dive into them a little bit or do a quick
overview neural networks are deep learning algorithms modeled after the human brain
they use multiple neurons which are mathematical operations to break down and solve complex mathical problems
and so just like the neuron one neuron fires in and it fires out to all these other neurons or nodes as we call them
and eventually they all come down to your output layer and you can see here we have the really standard graph input layer a hidden
layer and an output layer one of the biggest parts of any data
processing is your data pre-processing so we always have to touch base on that
with a neural network like many of these models they're kind of uh when you first start
using them they're like a black box you put your data in you train it and you test it and see how
good it was and you have to pre-process that data because bad data in is
bad outputs so in data pre-processing we will create our own data examples set
with keras the data consists of a clinical trial conducted on 2100
patients ranging from ages 13 to 100 with a the patients under 65 and the
other half over 65 years of age we want to find the possibility of a patient experiencing side effects due to
their age and you can think of this in today's world with covid uh what's going to happen on there and
we're going to go ahead and do an example of that in our live hands on like i said most of this
you really need to have hands-on to understand so let's go ahead and bring up our anaconda and
open that up and open up a jupyter notebook for doing the python code in now if you're not familiar with those
you can use pretty much any of your setups i just like those for doing demos
and showing people especially shareholders it really helps because it's a nice visual so let me go and flip
over to our anacon and the anaconda has a lot of cool two tools they just added data lore and ibm watson studio cloud
into the anaconda framework but we'll be in the jupiter lab or jupiter notebook i'm going to do a
jupiter notebook for this because i use the lab for like large projects with multiple pieces because it has multiple
tabs where the notebook will work fine for what we're doing and this opens up in our browser window
because that's how jupiter helps our jupiter notebook is set to run
and we'll go under new create a new python 3 and it creates an untitled python we'll go ahead and give
this a title and we'll just call this a cross
tutorial and let's change that to capital there
we go i'm going to just rename that and the first thing we want to go ahead and do is
get some pre-processing tools involved and so we need to go ahead and import some stuff for that like our numpy do
some random number generation i mentioned sklearn or your site kit if
you're installing sklearn the sklearn stuff it's a scikit you want to look up
that should be a tool of anybody who is doing data science if you're not if
you're not familiar with the sklearn toolkit it's huge but there's so many things in
there that we always go back to and we want to go ahead and create some train labels and train samples for
training our data and then
just a note of what we're we're actually doing in here let me go ahead and change this this is kind of a fun thing you can
do we can change the code to markdown and then markdown code is nice for doing
examples once you've already built this our example data we're going to do experimental
there we go experimental drug was tested on 2100 individuals between 13 to 100 years of
age half the participants are under 65 and 95 of participants are under 65
experienced no side effects well 95 of participants over 65 experience side effects
so that's kind of where we're starting at and this is just a real quick example because we're going to do another one
with a little bit more complicated information and so we want to go ahead and generate
our setup uh so we want to do for i and range and we want to go ahead and create if you
look here we have random integers trading the labels append so we're just creating some random data
let me go ahead and just run that and so once we've created our random data and if you if i mean you can
certainly ask for a copy of the code from simplylearn they'll send you a copy of this or you can zoom in on the video and see how we went
ahead and did our train samples of pin [Music] and we're just using this i do this kind
of stuff all the time i was running a thing on that had to do with errors following a bell-shaped curve on
a standard distribution error and so what do i do i generate the data on a standard distribution error to see
what it looks like and how my code processes it since that was the baseline i was looking for in this we're just
doing generating random data for our setup on here and we could actually go in
print some of the data up let's just do this print we'll do train
samples and we'll just do the first five pieces of data in there to see what
that looks like and you can see the first five pieces of data in our train samples is 49 85 41 68
19 just random numbers generated in there that's all that is and we generated
significantly more than that um let's see 50 up here one thousand yeah so there's one thousand here one
thousand numbers we generated and we could also if we wanted to find that out we could do a quick uh print the length
of it and
so or you could do a shape kind of thing and if you're using numpy although the link for this is just fine
and there we go it's actually 2100 like we said in the demo setup in there
and then we want to go ahead and take our labels oh that was our train labels
we also did samples didn't we so we could also print
do the same thing labels
and let's change this to labels
and labels and run that just to double check and
sure enough we have 2100 and they're labeled one zero one zero one zero
i guess that's if they have symptoms or not one symptoms zero none and so we wanna go ahead and take our train labels
and we'll convert it into a numpy array and the same thing with our samples and
let's go ahead and run that and we also shuffle this is just a neat feature you can do
in numpy right here put my drawing thing on which i didn't have on earlier
i can take the data and i can shuffle it uh so we have our so it just randomizes
it that's all that's doing um we've already randomized it so it's kind of an overkill it's not really
necessary but if you're doing a larger package
where the data is coming in and a lot of times it's organized somehow and you want to randomize it just to make sure
that that you know the input doesn't follow a certain pattern that might create a bias in your model
and we go ahead and create a scalar the scalar range minimum max scalar feature range zero to one
then we go ahead and scale the uh scaled train samples so we're gonna go ahead and fit and transform the data uh so
it's nice and scaled and that is the age uh so you can see up here we have 49 85 41. we're just moving
that so it's going to be between zero and one and so this is true with any of your neural networks
you really want to convert the data to zero and one otherwise you create a bias
so if you have like a hundred creates a bias versus the math behind it gets really
complicated if you actually start multiplying stuff there's a lot of multiplication addition
going on in there that higher end value will eventually multiply down and it will have a huge
bias as to how the model fits it and then it will not fit as well and then one of the fun things we can do
in jupiter notebook is that if you have a variable you're not doing anything with it it's the last one on the line
it will automatically print and we're just going to look at the first five samples on here and so it's
going to print the first five samples and you can see here we go 0.9195
0.791 so everything's between 0 and 1. and that just shows us that we scaled it
properly and it looks good it really helps a lot to do these kind of print ups halfway through you never
know what's going to go on there i don't know how many times i've gotten down and found out that the data sent to
me that i thought was scaled was not and then i have to go back and track it down and figure it out on there
so let's go ahead and create our artificial neural network and for doing that this is where we
start diving into tensorflow and cross tensorflow
if you don't know the history of tensorflow it helps to jump into we'll just use
wikipedia be careful don't quote wikipedia on these things because you get in trouble
but it's a good place to start back in 2011 google brain built disbelief as a proprietary machine learning setup
tensorflow became the open source for it so tensorflow was a google product
and then it became uh open sourced and now it's just become
probably one of the defactos when it comes for neural networks as far as where we're at
so when you see the tensorflow setup it it's got like a huge following there
are some other setups like the scikit under the sk learn has their own little neural network but the
tensorflow is the most robust one out there right now and cross sitting on top of it makes it
a very powerful tool so we can leverage both the cross easiness in which we can build a
sequential setup on top of tensorflow and so in here we're going to go ahead
and do our input of tensorflow uh and then we have the rest of this is
all cross here from number two down uh we're gonna import from tensorflow the
cross connection and then you have your tensorflow cross models import sequential it's a specific
kind of model we'll look at that in just a second if you remember from the files that means it goes from
one layer to the next layer to the next layer there's no funky splits or anything like that
and then we have from tensorflow cross layers we're going to import our activation and our dense layer
and we have our optimizer atom um this is a big thing to be aware of how
you optimize uh your data when you first do it atoms as good as any atom is
usually there's a number of optimizer out there there's about there's a couple main ones but atom is usually assigned
to bigger data it works fine usually the lower data does it just fine but atom is probably
the mostly used but there are some more out there and depending on what you're doing with your layers your different layers might have different activations
on them and then finally down here you'll see our setup where we want to go ahead and
use the metrics and we're going to use the tensorflow cross metrics
for categorical cross entropy so we can see how everything performs when we're done that's all that is a lot
of times you'll see us go back and forth between tensorflow and then scikit has a lot of really good
metrics also for measuring these things again it's the end of the day you know at the end of the story how good does
your model do and we'll go ahead and load all that and then comes the fun
part i actually like to spend hours messing with these things
and four lines of code you're like oh you're gonna spend hours on four lines of code
um no we don't spend hours on four lines of code that's not what we're talking about when i say spend hours on four lines of code uh what we have here i'm
going to explain that in just a second we have a model and it's a sequential model if you remember correctly we
mentioned the sequential up here where it goes from one layer to the next and our first layer is going to be your
input it's going to be what they call dense which is
usually just dense and then you have your input and your activation how many units are coming in we have 16
what's the shape what's the activation and this is where it gets interesting
because we have in here uh relu on two of these and softmax activation
on one of these there are so many different options
for what these mean and how they function how does the relu how does the soft max function
and they do a lot of different things we're not going to go into the activations in here that is what really
you spend hours doing is looking at these different activations and just
some of it is just almost like you're playing with it like an artist you start getting a fill for
like a inverse tangent activation or the 10h activation
takes up a huge processing amount so you don't see it a lot
it comes up with a better solution especially when you're doing uh when you're analyzing word documents and
you're tokenizing the words and so you'll see this shift from one to the other because you're both trying to
build a better model and if you're working on a huge data set
you'll crash the system it'll just take too long to process and then you see things like soft max
softmax generates an interesting um setup
where a lot of these when you talk about rayleigh it lets me do this rayleigh there we go relu has
a setup where if it's less than zero it's zero and then it goes up
and then you might have what they call lazy setup where it has a slight negative to it so that the errors can translate
better same thing with softmax it has a slight laziness to it so that errors translate better all these little
details make a huge difference on your model so one of the really cool things about
data science that i like is you build your what they call you build to fail and
it's an interesting design setup oops i forgot the end of my code here
the concept of build a file is you want the model as a whole to work so you can
test your model out so that you can do uh
you can get to the end and you can do your let's see where was it over shot down here
you can test your test out the quality of your setup on there and
see where did i do my tensorflow oh here we go it was right above me there we go we start doing your cross entropy and
stuff like that is you need a full functional set of code so that when you run
it you can then test your model out and say hey it's either this model works
better than this model and this is why and then you can start swapping in these models and so when i say spend a huge
amount of time on pre-processing data is probably 80 percent of your programming time
well between those two it's like 80 20. you'll spend a lot of time on the models
once you get the model down once you get the whole code and the flow down set
depending on your data your models get more and more robust as you start experimenting with different inputs
different data streams and all kinds of things and we can do a simple model summary here
uh here's our sequential here's a layer output a parameter
this is one of the nice things about cross is you just you can see right here here's our sequential one model boom
boom boom boom everything's set and clear and easy to read so once we have our model built the next
thing we're going to want to do is we're going to go ahead and train that model
and so the next step is of course model training and when we come in here
this a lot of times it's just paired with the model because it's so straightforward it's nice to
print out the model setup so you can have a tracking but here's our model
the keyword in cross is compile optimizer atom learning rate another
term right there that we're just skipping right over that really becomes the meat of um
the setup is your learning rate uh so whoops i forgot that i had an arrow but i'll just underline it
a lot of times the learning rate is set to 0.00 set to 0.01
depending on what you're doing this learning rate can over fit and underfit
so you'd want to look up i know we have a number of tutorials out on overfitting and under fitting that are really worth reading once you get to
that point and understanding and we have our loss sparse categorical cross entropy so this
is going to tell kuros how far to go until it stops and then we're looking for metrics of
accuracy so we'll go ahead and run that and now that we've compiled our model
we want to go ahead and run it fit it so here's our model fit
we have our scaled train samples our train labels our validation split
in this case we're going to use 10 percent of the data for validation uh batch size another number you kind of
play with not a huge difference as far as how it works but it does affect how long it takes to
run and it can also affect the bias a little bit most of the time though a batch size is between 10 to 100
depending on just how much data you're processing in there we want to go ahead and shuffle it we're going to go through
30 epics and put a verb rose of two let me just go
ahead and run this and you can see right here here's our epic here's our training um here's our loss now if you remember
correctly up here we set the loss let's see where was it compiled our data
there we go loss uh so it's looking at the sparse categorical cross entropy this tells us that as it goes how how
how much how much does the error go down
is the best way to look at that and you can see here the lower the number the better it just keeps going down
and vice versa accuracy we want let's see where's my accuracy
value accuracy at the end and you can see 0.619 0.69 0.74 it's
going up we want the accuracy would be ideal if it made it all the way to one but we also the loss is more important
because it's a balance you can have 100 accuracy and your model
doesn't work because it's over fitted again you won't look up overfitting and
under fitting models and we went ahead and went through uh 30
epics it's always fun to kind of watch your code going um to be honest i
usually uh the first time i run it i'm like oh that's cool i get to see what it does
and after the second time of running it i'm like i'd like to just not see that and you can repress those of course in
your code repress the warnings in the printing
and so the next step is going to be building a test set and predicting it now so here we go we want to go ahead and
build our test set and we have just like we did our training set
a lot of times you just split your your initial set up but we'll go ahead and do a separate set on here and this is just what we did
above there's no difference as far as
the randomness that we're using to build this set on here the only difference is that
we already did our scalar up here well it doesn't matter because the the data is going to
be across the same thing but this should just be just transformed down here instead of fit transform
because you don't want to refit your data on your testing data
there we go now we're just transforming it because you never want to transform the test data
easy to mistake to make especially on an example like this where we're not doing
um you know we're randomizing the data anyway so it doesn't matter too much because we're not expecting something
weird and then we went ahead and do our predictions the whole reason we built
the model is we take our model we predict and we're going to do here's our x scale data batch size 10 verbose
and now we have our predictions in here and we could go ahead and do a
oh we'll print predictions
and then i guess i could just put down predictions in five so we can look at the first five of the predictions
and what we have here is we have our age and uh the prediction on this age versus
what is what we think it's going to be but what we think is going to they're going to have symptoms or not
and the first thing we notice is that's hard to read because we really want a yes no answer
so we'll go ahead and just round off the predictions using the arg max
the numpy arg max for prediction so it just goes to 0 1
if you remember this is a jupiter notebook so i don't have to put the print i can just put in
rounded predictions and we'll just do the first five and you can see here zero one zero zero
zero so that's what the predictions are that we have coming out of this is no symptoms symptoms no symptoms
symptoms no symptoms and just as we were talking about at the beginning we want to go ahead and
take a look at this there we go confusion matrix is for accuracy check um
most important part when you get down to the end of the story how accurate is your model before
you go and play with the model and see if you can get a better accuracy out of it and for this we'll go ahead and use
the scikit the sk learn metrics site kit being where that comes from
import confusion matrix some iteration tools and of course a nice matte plot library
that makes a big difference so it's always nice to have a nice graph to look at a
picture is worth a thousand words and then we'll go ahead and call it cm
for confusion matrix y true equals test labels why predict rounded predictions
and we'll go ahead and load in our cm and i'm not going to spend too much time
on the plotting going over the different plotting code
you can spend like whole we have whole tutorials on how to do your different plotting on
there but we do have here is we're going to do a plot confusion matrix there's our cm
our classes normalize false title confusion matrix c map is going to be in blues
and you can see here we have uh to the nearest c map titles all the different pieces whether you put tick marks or not
the marks the classes the color bar so a lot of different information on here as far as how we're doing the
printing of the of the confusion matrix you can also just dump the confusion matrix into a seaborn and real quick get
an output it's worth knowing how to do all this when you're doing a presentation to the
shareholders you don't want to do this on the fly you want to take the time to make it look really nice like our guys in the back
did and let's go ahead and do this forgot to put together our cm plot labels
we'll go ahead and run that and then we'll go ahead and call the little the
definition for our mapping
and you can see here plot confusion matrix that's our the little script we just wrote and we're going to dump our
data into it so our confusion matrix our classes title confusion matrix and let's just go
ahead and run that and you can see here we have our basic setup
no side effects 195 had side effects 200
no side effects that had side effects so we predicted the 10 of them who actually had side effects and that's
pretty good i mean i don't know about you but you know that's five percent error on this and this is because
there's 200 here that's where i get five percent is uh divide these both by by two and you get five out of a hundred uh you can do
the same kind of math up here not as quick on the flags it's 15 and 195 not an easily rounded number but you
can see here where they have 15 people who predicted to have no
with the no side effects but had side effects kind of set up on there and these
confusion matrix are so important at the end of the day this is really where where you show
whatever you're working on comes up and you can actually show them hey this is how good we are or not how messed up it
is so this was a i spent a lot of time on some of the
parts but you can see here is really simple we did the random generation of data but
when we actually built the model coming up here here's our model summary
and we just have the layers on here that we built with our model on this and then we went ahead and trained it and ran the
prediction now we can get a lot more complicated let me flip back on over here because we're going to do another uh demo
so that was our basic introduction to it we talked about the uh oops there we go
okay so implementing a neural network with keras after creating our samples and labels we need to create our cross
neural network model we will be working with a sequential model which has three layers and this is what we did we had
our input layer our hidden layers and our output layers and you can see the input layer coming in
was the age factor we had our hidden layer and then we had the output are you going to have symptoms or not
so we're going to go ahead and go with something a little bit more complicated training our model is a two-step process
we first compile our model and then we train it in our training data set so we have compiling compiling converts
the code into a form of understandable by machine we use the atom in the last example a
gradient descent algorithm to optimize a model and then we trained our model which means it let it
learn on training data and i actually had a little backwards
there but this is what we just did is we if you remember from our code we just had let me go back here
here's our model that we created summarized
we come down here and we compile it so it tells it hey we're ready to build
this model and use it uh and then we train it this is the part where we go ahead and fit our model and
and put that information in here and it goes through the training on there and of course we scaled the data which
was really important to do and then you saw we did the creating a confusion matrix with keras
as we are performing classifications on our data we need a confusion matrix to check the results a confusion matrix
breaks down the various misclassifications as well as correct classifications to get the accuracy
and so you can see here this is what we did with the true positive false positive true negative false negative
and that is what we went over let me just scroll down here on the end we printed it out and you can
see we have a nice printout of our confusion matrix with the true positive false positive
false negative true negative and so the blue ones we want those to be the biggest numbers
because those are the better side and then we have our false predictions on here
as far as this one so had no side effects but we predicted let's see no side effects predicting
side effects and vice versa if getting your learning started is half the battle what if you could do that for free visit
skill up by simply learn click on the link in the description to know more now
saving and loading models with cross we're going to dive into a more complicated demo
and you're going to say oh that was a lot of complication before well if you broke it down we randomized some data we
created the cross setup we compiled it we trained it we predicted and we ran our matrix
uh so we're going to dive into something a lot a little bit more fun is we're going to do a face mask detection with cross so we're going to build a cross
model to check if a person is wearing a mask or not in real time and this might be important if you're at the front of a
store this is something today which is might be very useful as far as some of
our you know making sure people are safe and so we're going to look at mask and no mask and let's start with a little
bit on the data and so in my data i have with a mask you can see they just have a number of
images showing the people in masks and again if you want some of this information
contact simply learn and they can send you some of the information as far as people with and without masks so you can
try it on your own and this is just such a wonderful example of this setup on here
so before i dive into the mass detection talk about being in the current with
covid and seeing if people are wearing masks this particular example i had to go
ahead and update to a python 3.8 version it might run in a 3.7 i'm not sure i
haven't i kind of skipped three seven and installed three eight uh so i'll be running in a three python
38 and then you also want to make sure your tensorflow is up to date because the
they call functional layers with that's where they split if you remember correctly from back
let's take a look at this remember from here the functional model and a functional layer allows us to feed
in the different layers into different you know different nodes into different layers and split them a very powerful
tool very popular right now in the edge of where things are with neural networks
and creating a better model so i've upgraded to python 3.8 and let's go ahead and open that up and
go through our next example which includes multiple layers
programming it to recognize whether someone wears a mask or not and then saving that model so we can use it in
real time so we're actually almost a full um end-to-end development of a product here uh of course this is a very
simplified version and it'd be a lot more to it you'd also have to do like recognizing whether it's someone's face
or not all kinds of other things go into this so let's go ahead and jump into that code and we'll open up a new python
three oops python three it's working on it there we go
and then we want to go ahead and train our mask we'll just call this train mask
and we want to go ahead and train mask and save it so it's it's uh
save mask train mask detection not to be confused with masking data a
little bit different we're actually talking about a physical mask on your face
and then from the cross standpoint we got a lot of imports to do here
and i'm not going to dig too deep on the imports we're just going to go ahead and notice a few of them
so we have in here alt d there we go have something to draw with a little bit
here we have our image processing and the image processing right here let
me underline that deals with how do we bring images in because most images are like a
square grid and then each value in there has three values for the three different colors
cross and tensorflow do a really good job of working with that so you don't have to
do all the heavy listening and figuring out what's going to go on uh and we have the mobile net average
pooling 2d this again is how do we deal with the images and
pulling them uh dropout's a cool thing worth looking up if you haven't when you as you get more
and more into cross and tensorflow uh it'll auto drop out certain nodes
that way you'll get a better the notes just kind of die and they find that they actually create more of a bias
and help and they also add processing time so they remove them
and then we have our flatten that's where you take that huge array with the three different colors and you find a
way to flatten it so it's just a one dimensional array instead of a two by two by three
uh dense input we did that in the other one so that should look a little familiar oops there we go our input our model
again these are things we had on the last one here's our optimizer with our atom
we have some pre-processing on the input that goes along with bringing the data in
more pre-processing with image to array loading the image this stuff is so nice it looks like a
lot of work you have to import all these different modules in here but the truth is is it does everything for you you're
not doing a lot of pre-processing you're letting the software do the pre-processing
and we're going to be working with the setting something to categorical again that's just a conversion from a
number to a category 0 1 doesn't really mean anything it's like true false
label binarizer the same thing uh we're changing our labels around and then
there's our train test split classification report our
im utilities let me just go ahead and scroll down here a notch for these
this is something a little different going on down here this is not part of the tensorflow or the sklearn this is the
site kit set up in tensorflow above the path this is part of
opencv and we'll actually have another tutorial going out with the opencv so if you want to know more about opencv you'll get a
glance on it in this software especially the second piece when we reload up the data and
hook it up to a video camera we're going to do that on this round but this is part of the opencv thing
you'll see cv2 is usually how that's referenced but the im utilities has to do with how
do you rotate pictures around and stuff like that and resize them and then the matplot
library for plotting because it's nice to have a graph tells us how good we're doing and then of course our numpy numbers array and just a straight os
access wow so that was a lot of imports uh like i said i'm not going to spend i spent a little time going through them
but we didn't want to go too much into them and then i'm going to create
some variables that we need to go ahead initialize we have the learning rate number of
epics to train for and the batch size and if you remember correctly we talked about the learning rate
to the negative 4.0001 a lot of times it's 0.001 or 0.001
usually it's in that variation depending on what you're doing and how many epochs and they kind of play with the epics
the epics is how many times we're going to go through all the data now i have it as two
the actual setup is for 20 and 20 works great the reason i have it for two is it
takes a long time to process one of the downsides of jupiter
is that jupiter isolates it to a single kernel so even though i'm on an eight core processor
with 16 dedicated threads only one thread is running on this no matter what
so it doesn't matter so it takes a lot longer to run even though tensorflow really scales up nicely and
the batch size is how many pictures do we load at once in process again those are numbers you have to
learn to play with depending on your data and what's coming in and the last thing we want to go ahead and do is there's a directory with the data set
we're going to run and this just has images of mass and not
masks and if we go in here you'll see data set
and then you have pictures with mass they're just images of people with mass on their face
uh and then we have the opposite let me go back up here without masks so it's pretty
straightforward they look kind of askew because they tried to format them into very similar uh setup on there so
they're they're mostly squares you'll see some that are slightly different on here and that's kind of important thing to do
on a lot of these data sets get them as close as you can to each other and we'll we actually will run in
the in this processing of images up here and the cross layers and importing and
dealing with images it does such a wonderful job of converting these that a lot of it we don't have to do a whole lot with
so you have a couple things going on there and so we're now going to be this is now loading the
images and let me see and we'll go ahead and create data and
labels here's our here's the features going in which is going to be our pictures and our labels
going out and then for categories in our list directory directory and if you remember
i just flashed that at you it had face mask or or no face mask those are
the two options and we're just going to load into that we're going to append the image itself and the labels so we'll
just create a huge array and you can see right now this could be an issue if you had more data at some
point thankfully i have a gig hard drive or
ram even that you could do with a lot less of that probably under 16 or even eight gigs
would easily load all this stuff and there's a conversion going on in here i told you about how we are going
to convert the size of the image so it resizes all the images that way our data is all
identical the way it comes in and you can see here with our labels we
have without mask without mask without mask the other one would be with mask those
are the two that we have going in there and then we need to change it to the one
not hot encoding and this is going to take our
up here we had was it labels and data we want the labels to be categorical so
we're going to take labels and change it to categorical and our labels then equal a categorical list
we'll run that and again if we do uh labels and we just do the last or the
first 10 let's do the last 10 just because minus 10 to the end there we go
just so we can see where the other side looks like we now have one that means they have a mask one zero one zero so on
uh one being they have a mask and zero no mask and if we did this in reverse
i just realized that this might not make sense if you've never done this before let me run this
zero one so zero is uh do they have a mask on zero do
they not have a mask on one so this is the same as what we saw up here without mask one equals
um the second value is without mass so with mass without mask
and that's just a with any of your data processing we can't really zero if you have a zero
one output uh it causes issues as far as training and
setting it up so we always want to use a one hot encoder if the values are not actual
linear value or regression values they're not actual numbers if they represent a thing
and so now we need to go ahead and do our train x test x train y test y
train split test data and we'll go ahead and make sure it's going to be random and we'll take 20 of
it for testing and the rest for setting it up as far as training their
model this is something that's become so cool when they're training these set they realize we can augment the data
what does augment mean well if i rotate the data around and i zoom in i zoom out
i rotate it share it a little bit flip it horizontally
fill mode as they do all these different things to the data it is able to it's kind of like increasing
the number of samples i have so if i have all these perfect samples
what happens when we only have part of the face or the face is tilted sideways or
all those little shifts cause a problem if you're doing just a standard set of data so we're going to create an augment
in our image data generator which is going to rotate zoom and do all kinds of cool thing and this is worth
looking up this image data generator and all the different features it has a lot of times i'll the first time
through my models i'll leave that out because i want to make sure there's a thing we call build to fail which is
just cool to know you build the whole process and then you start adding these different things in
uh so that you can better train your model and so we go and run this and then we're gonna load
and then we need to go ahead and you probably would have gotten an error if you hadn't put this piece in right here
i haven't run it myself because the guys in the back did this we take our base model and one of the
things we want to do is we want to do a mobile net v2 and this we this is a big thing right
here include the top equals false a lot of data comes in with a label on
the top row so we want to make sure that that is not the case
and then the construction of the head of the model that will be placed on the top of the base model
we want to go ahead and set that up
and you'll see a warning here i'm kind of ignoring the warning because it has to do with the
size of the pictures and the weights for input shape so they'll switch things to defaults to
saying hey we're going to auto shape some of this stuff for you you should be aware that with this kind of imaging
we're already augmenting it by moving it around and flipping it and doing all kinds of things to it
so that's not a bad thing in this but another data it might be if you're working in a different domain
and so we're going to go back here and we're going to have we have our base model we're going to do our head model equals our base model output
and what we got here is we have an average pooling 2d pool size 77 head model
head model flatten so we're flattening the data uh so this is all
processing and flattening the images and the pooling has to do
with some of the ways it can process some of the data we'll look at that a little bit when we get down to the lower levels processing it
and then we have our dents we've already talked a little bit about a dense just what you think about and then the head model has a drop out
of 0.5 what we can do with a drop out
the dropout says that we're going to drop out a certain amount of nodes while training
so when you actually use the model it will use all the nodes but this drops certain ones out and it helps
stop biases from performing so it's really a cool feature in here they discovered this a while back
we have another dense mode this time we're using soft max activation lots of different activation options
here soft is a real popular one for a lot of things and so was the relu
and you know there's we could do a whole talk on activation formulas and why what their different uses are
and how they work when you first start out you'll you'll use mostly the relu and the softmax for
a lot of them uh just because they're some of the basic setups it's a good place to start
and then we have our model equals model inputs equals base model dot input outputs equals head model so again we're
still building our model here we'll go ahead and run that and then we're going to loop over all
the layers in the base model and freeze them so they will not be updated during the first training process
so for layer and base model layers layers dot trainable equals false a lot of times when you go through your
data you want to kind of jump in part way through i
i'm not sure why in the back they did this for this particular example but i do this a lot when i'm working
with series and specifically in stock data i wanted to iterate through the first set of 30 data
before it does anything i would have to look deeper to see why they froze it on this particular one
and then we're going to compile our model so compiling the model atom
init layer decay initial learning rate over epics
and we go ahead and compile our loss is going to be the binary cross entropy which we'll have that print out
optimizer for opt metrics is accuracy same thing we had before not a huge jump
as far as the previous code
and then we go ahead and we've gone through all this and now we need to go ahead and fit our model
so train the head of the network print info training head run
now i skipped a little time because you'll see the run time here is at 80 seconds per epic takes a couple minutes
for it to get through on a single kernel one of the things i want you to notice
on here while we're well it's finishing the processing is that we have up here our augment
going on so anytime the train x and trading y go in there's some randomness going on there
and it's jiggling it around what's going into our setup uh of course we're batch sizing it so
it's going through whatever we set for the batch values how many we process at a time and then we have the steps per
epic the train x the batch size validation data here's our
test x and test y where we're sending that in and this again it's validation
one of the important things to know about validation is our when both our training data and our test
data have about the same accuracy that's when you want to stop that means that
our model isn't biased if you have a higher accuracy on your
testing you know you've trained it and your accuracy is higher on your actual test data then something in there is probably
uh has a bias and it's overfitted so that's what this is really about right here with the validation data and
validation steps so it looks like it's let me go ahead and see if it's done processing looks
like we've gone ahead and gone through two epics again you could run this through about 20
with this amount of data and it would give you a nice refined model at the end
we're going to stop at 2 because i really don't want to sit around all afternoon and i'm running this on a single thread
so now that we've done this we're going to need to evaluate our model and see how good it is and to
do that we need to go ahead and make our predictions these are predictions on our test x
to see what it thinks are going to be so now it's going to be evaluating the network and then we'll go ahead and go
down here and we will need to turn the index in because remember it's
it's either 0 or 1 it's a 0 1 0 1 so you have two outputs not
wearing wearing a mask not wearing a mask and so we need to go ahead and take that argument at the end and change
those predictions to a zero or one coming out
and then to finish that off we want to go ahead and let me just put this right in here
and do it all in one shot we want to show a nicely formatted classification report so we can see what that looks
like on here and there we have it we have our precision uh it's 97 with a mask
there's our f1 score support without a mass 97 percent
um so that's pretty high set up on there you know you three people are going to sneak into the store
who are without a mask and thinks they have a mask and there's going to be three people with a mask
that's going to flag the person at the front to go oh hey look at this person you might not have a mask that's if i guess it's just set up in
front of a store um so there you have it and of course one of the other cool things about this
is if someone's walking in to the store and you take multiple pictures of them um
you know this is just an it would be a way of flagging and then you can take that average of those pictures and make
sure they match or don't match if you're on the back end and this is an important step because we're gonna this is just
cool i love doing this stuff uh so we're gonna go ahead and take our model and we're gonna save it
so model save massdetector.model we're going to give it a name we're going to save the format
in this case we're going to use the h5 format and so this model we just programmed has
just been saved so now i can load it up into say another program what's cool about this is let's
say i want to have somebody work on the other part of the program well i just saved the model they upload the model
now they can use it for whatever and then if i get more information and we start working with that at some
point i might want to update this model and make a better model and this is true of
so many things where i take this model and maybe i'm running a prediction on
making money for a company and as my model gets better i want to keep updating it and then it's
really easy just to push that out to the actual end user uh here we have a nice graph you can see
the training loss and accuracy as we go through the epics we only did the you know only shows just
the one epic coming in here but you can see right here as the value loss train accuracy and value
accuracy starts switching and they start converging and you'll hear converging
this is a convergence they're talking about when they say you're you're i know when i work in the
psy kit with sk learn neural networks this is what they're talking about a
convergence is our loss and our accuracy come together and also up here and this is why i'd run it
more than just two epochs as you can see they still haven't converged all the way
so that would be a cue for me to keep going but what we want to do is we want to go
ahead and create a new python3 program
and we just did our train mask so now we're going to go ahead and import that and use it and show you in a live action
get a view of both myself in the afternoon along with my background of an
office which is in the middle still of reconstruction for another month
and we'll call this a mask detector
and then we're going to grab a bunch of a few items coming in uh we have our
mobilenet v2 import pre-processing input so we're still going to need that
we still have our tensorflow image to array we have our load model that's where most of stuff is going on
this is our cv2 or opencv again i'm not going to dig too deep into that we're
going to flash a little opencv code at you and we actually have a tutorial on that coming out
our numpy array our im utilities which is part of the opencv or cv2 setup
uh and then we have of course time and just our operating system so those are the things we're going to go ahead and set up on here and then we're going to
create this takes just a moment
our module here which is going to do all the heavy lifting so we're going to detect and predict the
mask we have frame face net mass net these are going to be generated by our
open cv we have our frame coming in and then we want to go ahead and create a mask around the face it's going to try
to detect the face and then set that up so we know what we're going to be processing through our model
and then there's a frame shape here this is just our height versus width that's all hw stands for
they've called it blob which is a cv2 dnn blob form image frame so this is reformatting this frame
that's going to be coming in literally from my camera and we'll show you that in a minute that little piece of code
that shoots that in here and we're going to pass the blob through the network and obtain the face
detections so face net dot set import blob detections face net forward
print detections shape uh so these is this is what's going on
here this is that model we just created we're going to send that in there and i'll show you in a second where that is
but it's going to be under face net and then we go ahead and initialize our list of faces their corresponding
locations and the list of predictions from our face mask network
we're going to loop over the detections and this is a little bit more work than you think
as far as looking for different faces what happens if you have a crowd of faces
so we're looping through the detections and the shapes going through here and probability associated with the
detection here's our confidence of detections we're going to filter out weak detection
by ensuring the confidence is greater than the minimum confidence so we've said it remember zero to one so
0.5 would be our minimum confidence probably is pretty good [Music]
and then we're going to put in compute bounding boxes for the object if i'm zipping through this it's because we're
going to do an open cv and i really want to stick to just the cross part
and so i'm just kind of jumping through all this code you can get a copy of this code from simplylearn and take it apart
or look for the opencv coming out and we'll create a box the box sets it
around the image ensure the bounding boxes fall within the dimensions of the frame
so we create a box around what's going to what we hope is going to be the face extract the face roi convert it from bgr
to rgb channel again this is an open cv issue not really an issue but it has to do with
the order i don't know how many times i've forgotten to check the order colors when working with opencv
because there's all kinds of fun things when red becomes blue and blue becomes red uh and we're gonna
go ahead and resize it process it frame it uh face frame setup again the face the cbt color we're
gonna convert it we're gonna resize it image to array pre-process the input
uh pin the face locate face start x dot y and x boy that was just a huge amount and i
skipped over a ton of it but the bottom line is we're building a box around the face and that box because the open cv
does a decent job of finding the face and that box is going to go in there and see hey does this person have a mask on
it and so that's what that's what all this is doing on here and then finally we get
down to this where it says predictions equals mass net dot predict faces batch size 32
so these different images where we're guessing where the face is are then going to go through and
generate an array of faces if you will and we're going to look through and say does this face have a mask on it and
that's what's going right here is our prediction that's the big thing that we're working for
and then we return the locations and the predictions the location just tells where on the picture it is
and then the prediction tells us what it is is it a mask or is it not a mask
all right so we've loaded that all up so we're going to load our serialized
face detector model from disk and we have our the path that it was
saved in obviously you're going to put it in a different path depending on where you have it or however you want to do it and how you
saved it on the last one where we trained it and then we have our weights path
and so finally our face net here it is equals cb2.dnn.readnet
prototext path weights path and we're going to load that up on here so let me go ahead and run that
and then we also need to i'll just put it right down here i always hate separating these things in there
and then we're going to load the actual mass detector model from disk this is the the model that we saved so let's go
ahead and run that on there also so this is pulling on all the different pieces we need for our model
and then the next part is we're going to create open up our video
and this is just kind of fun because it's all part of the opencv the video setup
and we just put this all in as one there we go so we're going to go ahead and open up
our video we're going to start it and we're going to run it until we're done and this is where we get some real
like kind of live action stuff which is fun this is what i like working about with images and videos
is that when you start working with images and videos it's all like right there in front of you it's visual and
you can see what's going on so we're going to start our video streaming this is grabbing our video
stream source zero start uh that means it's grabbing my main camera i have hooked up um
and then you know starting video you're gonna print it out here's our video source equals zero start
loop over the frames from the video stream oops a little redundancy there um let me
close i'll just leave it that's how they headed in the code so uh so while true
we're going to grab the frame from the threaded video stream and resize it to have the maximum width of 400 pixels so
here's our frame we're going to read it from our visual stream
we're going to resize it and then we have we're returning remember we returned from the our
procedure the location and the prediction so detect and predict mask we're sending it the frame we're sending
it the face net and the mast net so we're sending all the different pieces that say this is what's going through on
here and then it returns our location and predictions and then for our box
and predictions in the location and predictions and the boxes is again this is an open
cv set that says hey this is a box coming in from the location because you have the two different
points on there and then we're going to unpack the box in predictions and we're going to go
ahead and do mask without a mask equals prediction we're going to create our label
no mask and create color if the label equals mask l0 225 and you know it's
going to make a lot more sense when i hit the run button here but we have the probability of the label
we're going to display the label and bounding box rectangle on the output frame and then we're going to go ahead and
show the output from the frame cv2 i am show frame frame and then the key equals cp2 weight key one we're just going to
wait till the next one comes through from our feed and we're going to do this until
we hit the stop button pretty much so are you ready for this to see if it works we've distributed our
our model we've loaded it up into our distributed uh code here we've got it hooked into our camera and we're going
to go ahead and run it and there it goes it's going to be running and we can see the data coming down here and we're waiting for the
pop-up and there i am in my office with my
funky headset on uh and you can see in the background my unfinished wall and it says up here no
mask oh no i don't have a mask on i wonder if i cover my mouth
what would happen you can see my no mask
goes down a little bit i wish i brought a mask into my office it's up at the house but you can see here that this
says you know there's a 95 98 chance that i don't have a mask on and it's true i don't have a mask on right
now and this could be distributed this is actually an excellent little piece of script that you can start you know you
install somewhere on a video feed on a on a security camera or something and then you have this really neat uh setup
saying hey do you have a mask on when you enter a store or a public transportation or whatever it is where
they're required to wear a mask let me go ahead and stop that
now if you want a copy of this code definitely give us a hauler we will be going into opencv in another one so i
skipped a lot of the opencv code in here as far as going into detail
really focusing on the cross saving the model uploading the model and
then processing a streaming video through it so you can see that the model works we actually have this working
model that hooks into the video camera which is just pretty cool and a lot of
fun so i told you we're going to dive in and really roll up our sleeve and do a lot
of coding today we did the basic uh demo up above for just pulling in across
and then we went into a cross model where we pulled in data to see whether someone was wearing a mask or not so
very useful in today's world as far as a fully running application
and we're going to take a look at image classification using cross and the basic setup and we'll actually look at two
different demos on here what's in it for you today
what is image classification intel image classification data
creating neural networks with keras and the vgg16 model
what is image classification the process of image classification
refers to assigning classes to an entire image images can be classified based on different categories like weather
it is a night time or daytime shot what the image represents etc you can see here we have mountains looking for
mountains why should we doing some pictures of scenery and stuff like that
in deep learning we perform image classification by using neural networks to extract features from images and
classifies them based on these features and you can see here where it says like
what computer sees and it says oh yeah we see mostly forests maybe a little bit of mountains because the way the image
is and this is really where one of the areas that neural networks
really shines if you try to run this stuff through more like a linear regression model
you'll still get results but the results kind of miss a lot of things as the
neural networks get better and better at what they do with different tools we have out there
so intel image classification data the data being used is the intel image
classification data set which consists of images of six types of land areas and so we have forest building glaciers
and mountains sea and street and you can see here there's a couple of the images out of there as a setup in
the intel image classification data that they use
and then we're going to go into creating a neural networks with keras
the convolutional neural network that we are creating from scratch looks as shown below
you'll see here we have our input layer um they haven't listed max pooling so you
have as you're coming in with the input layer and this the input layer is actually
before this but the first layer that it's going to go into is going to be a convolutional neural network
then you have a max pooling that pulls those the the convolutional neural networks returns
in this case they have two of those that is very standard with convolutional neural networks uh one of the ones that
i was looking at earlier that was standard being used by um one of the larger companies i can't
remember which one for doing a large amount of identification had two convolutional neural networks each with
their max pooling and then about 17 dense layers after it we're not going to
do that heavy duty of a of a code but we'll get you head in the right direction and that gives you an idea of
what you're actually going to be looking at when you look at the flattened part and then the dents we're talking like 17
dense layers afterwards i find that a lot of the stuff i've been working on i end up maxing it out right
around nine dense layers it really depends on what you have going in and what you're working with
and the vgg16 model vgg16 is a pre-trained cnn model which
is used for image classification it is trained on a large varied data set
and fine-tuned to fit image classification data sets with ease
and you can see down here we have the input coming in the convolutional neural network one to
one one to two and then pooling and then we do two to one two to two convolutional network then pooling three
to two and you can see there's just this huge layering of convolutional neural networks and in this case they have five
such layers going in and then three dents going out or uh more
now when they took this setup this actually won an award back in 2019 for this particular
setup and it does it does really good except that again
we only show the three dense layers here and as you find out depending on your data going in what you
have set up that really isn't enough on one of these setups and i'm going to show you why we
restricted it because it does take up a lot of processing power in some of these things so let's go ahead and roll up our
sleeves and we're going to look at both the setups we're going to start with the the first classification and then we'll go into
the vgg16 and show you how that's set up now i'm going to be using anaconda and
let me flip over to my anaconda so you can see what that looks like now i'm running in the anaconda here
you'll see that i've set up a main python 3 8. i always put that in there this is
where i'm doing like most of my kind of playing around this is done in python version 3.8 we're
not going to dig too much into versions at this point you should already have cross installed on there usually cross
takes a number of extra steps and then our usual
setup is the numpy the pandas your sk your scikit which is going to be
the sk learn your seaborn and i'll show you those in just a minute and then i'm just going to be in the
jupiter lab where i've created a new notebook in here
and let's flip on over there to my blank notebook
now there's a couple of cool things to note in here is that one i use the the
anaconda jupiter notebook setup because it keeps everything separate except for cross
cross is actually running separately in the back i believe it's a c program
what's nice about that is that it utilizes the multiprocessors on the computer and i'll mention that just in a
little bit when we actually get down to running the code and when we look in here a couple things
to note is here's our uh oops i thought i'd grab the other
drawing thing uh but here's our numpy and our pandas right here and our operating system this is our sai kit you
always import it as sklearn for the classification report uh we're going to be using well usually
import like seaborn brings in all of your pie plot library also
kind of nice to throw that in there i can't remember if we're actually using seaborne if they just the people in the back just threw that together
and then we have the sklearn shuffle for shuffling data here's our matplot library that the seaborn is pretty much
built on cv2 if you're not familiar with that that is
our image module for importing the image and then of course we have our
tensorflow down here which is what we're really working with and then the last thing is just for
visual effect while we're running this if you're doing a demo and you're working with the partners or the
shareholders this tqdm is really kind of cool it's an extensible progress bar for python and
i'll show you that too remember data science is not i mean muscle's code when i'm looking through
this code i'm not going to show half of this stuff to the shareholders or anybody i'm working with they don't
really care about pandas and all that we do because we want to understand how it works
uh so we need to go ahead and import those different setup on there and then the next thing
is we're going to go ahead and set up our classes now we remember if we had mountain
street glacier building c and forest those were the different images that we have coming in
and we're going to go ahead and just do class name labels and we're going to kind of match that class name of i for i
class name equals the class names so our labels are going to match the names up here
and then we have the number of classes and the print the class names and the
labels and we'll go ahead and set the image size this is important that we resize everything because if you
remember with neural networks they take one size data coming in and so
when you're working with images you really want to make sure they're all resized to the same setup it might
squish them it might stretch them that generally does not cause a problem in these uh in some of the other tricks you
can do with if you if you need more data and this is one that's used regularly we're not going to do it in here is you
can also take these images and not only resize them but you can tilt them one way or the other crop parts of them
so they process slightly differently and they'll actually increase your accuracy of some of these predictions
and so you see here we have mountain equals zero that's what this class name label is street equals 1 glacier equals
2 buildings equals 3 c4 4s equals 5. now we did this as an enumerator so each
one is 0 through 5. a lot of times we do this instead as
0 1 0 1 0 1 so you have five outputs and each one's a zero or a one coming out
so the next thing we really want to do is we want to go ahead and load the data up and just put a label in there loading
data just just so you know what we're doing i'm going to put in the loading data down here
make sure it's well labeled and we'll create a definition for this and this is all part of your
pre-processing at this point you could replace this with all kinds of different things
depending on what you're working on and if you once you download you can go download this data set uh send a note to
the simply learn team here in youtube um and they'll be happy to direct you in the right direction and make sure you
get this path here um so you have the right whatever wherever you saved it a lot of times i'll just abbreviate the
path or put it as a sub thing and just get rid of the directory but again double check your paths
we're going to separate this into a segment for training and a segment for testing and that's
actually how it is in the folder let me just show you what that looks like
so when i have my lengthy path here where i keep all my programming simply learn this particular setup we're
working on image classification and image classification clearly you probably wouldn't have that
lengthy of a list and when we go in here you'll see sequence train sequence test
they've already split this up this is what we're going to train the data in and again you can see buildings 4th glacier mountain c street
and if we double click let's go under forest you can see all these different forest images and there's a lot of variety here
i mean we have winter time we have summer time so it's kind of interesting you know here's like a fallen tree
versus a road going down the middle that's really hard to train and if you
look at the buildings a lot of these buildings are looking up a skyscraper we're looking down the
setup here's some trees with one and i want to highlight this one it has trees in it ah
let me just open that up so you can see it a little closer
the reason i want to highlight this is i want you to think about this we have trees growing is this the city
or a forest so this kind of imagery makes it really hard for a classifier and if you start
looking at these you'll see a lot of these images do have trees and other things in the foreground weird angles
really a hard thing for a computer to sort out and figure out whether it's
going to be a forest or a city
and so in our loading of data uh one we have to have the path the directory
we're going to come in here we have our images and our labels so we're going to load the images in one
section the labels in another and if you look through here it just
goes through the different folders in fact let me do this let me
there we go uh as we look at this we're just going to loop through the three to the six different folders that have the
different landscapes and then we're going to go through and pull each file out
and each label so we set the label we set the folder for file and list
here's our image path join the paths this is all kind of general stuff
so i'm kind of skipping through it really quick and here's our image setup if you
remember we're talking about the images we have our cv2 reader so it reads the
image in it's going to go ahead and take the image and convert it to from
blue green red to red green green blue this is a cv2 thing almost all the time
it imports it and instead of importing it as a standard that's used just about everywhere it imports it with the bgr
versus rgb rgb is pretty much a standard in here you have to remember that with cv2
and then we're going to go ahead and resize it this is the important part right here we've said it we've decided
what the size is and we want to make sure all the images have the same size on them
and then we just take our images and we're just going to pin the image pin the label and then the images it's going to turn
into a numpy array this just makes it easier to process and manipulate
and then the labels is also a numpy array and then we just return the output append images and labels and we return
the output down here
so we've loaded these all into memory uh we haven't talked too much there'd be a different setup in there
because there is ways to feed the files directly into your cross model but we want to go ahead and just load
them all because really for today's processing and what our
computers can handle that's not a big deal and then we go ahead and set the train
images train labels test images test labels and that's going to be returned in our output append and you can see
here we did images and labels set up in there and it just loads them in there so we'll have
these four different categories let me just go ahead and run that
so now we've gone ahead and loaded everything on there
and then if you remember from before uh we imported let me just go back up there
shuffle here we go here's our sk learn utilities import shuffle and so we want to take these labels and
shuffle them around a little bit just mix them up so it's not having the same if you run the same process over
and over uh then you might run into some problems on there
and just real quick let's go ahead and do a plot so we can just you know we've
looked at them as far as from outside of our code we pulled up the files and i showed you
what that was going on we can go ahead and just display them here too and i tell you when you're working with
different people this should be highlighted right here this thing is like when i'm working on
code and i'm looking at this data and i'm trying to figure out what i'm doing i skip this process
the second i get into a meeting and i'm showing what's going on to other people
i skip everything we just did so and go right to here where we want to go
ahead and display some images and take a look at it
and in this display i've taken them and i've resized the images to 20 by 20.
that's pretty small so we're going to lose just a massive amount of detail and you can see here these nice
pixelated images i might even just stick with the folder showing them what images
were processing again this is yeah be a little careful this
maybe resizing it was a bad idea in fact let me try it without resizing and see what happens oops so i took out
the image size and then we put this straight in here one of the things again this is
put the d there we go one of the things again that we want to know whenever we're working on these things
uh is the cv2 there are so many different uh image classification setups it's
really a powerful package when you're doing images but you do need to switch it around so that it works with the pie
plot and so make sure you take your numpy array and change it to a u integer
8 format because it comes in as a float otherwise you'll get some weird images down there
and so this is just basically we've split up our we've created a plot we went ahead and did the plot 20 by 20
um or the plot figure size is 20 by 20
and then we're doing 25 so a 5x5 subplot nothing really going on here too
exciting but you can see here where we get the images and really when you're showing people what's
going on this is what they want to see so you skip over all the code and you have your meeting you say okay here's
our images of the building don't get caught up in how much work you do get caught up in what they want to
see so if you want to work in data science that's really important to know
and this is where we're going to start having fun here's our model this is where it gets
exciting when you're digging into these models and you have here
let me get there we go when you have here if you look here
here's our convolutional neural network 2d and
2d is an image you have two different dimensions x y and even though there's three colors it's still considered 2d
if you're running a video you'd be convolutional neural network 3d if you're doing a series going across
a time series it might be 1d and on these you need to go ahead and
have your convolutional neural network if you look here there's a lot of really cool settings going on
to dig into we have our input shape so everything's been set to 150 by 150
and it has of course three different color schemes in it that's important to notice
activation default is relu this is
small amounts of data are being processed on a bunch of little
neural networks and right here is the 32 that's how many
of these convolutional null networks are being strung up on here and then the three by three
uh when it's doing its steps it's actually looking at a little three by three square on each image
and so that's what's going on here and with convolutional neural networks the
window floats across and adds up all these numbers going across on this data and then eventually
it comes up with 30 in this case 32 different feature options that it's looking for
and of course you can change that 32 you can change the three by three so you might have a larger setup you know if
you're going across 150 by 150 that's a lot of steps so we might run this as 15 by 15.
there's all kinds of different things you can do here we're just putting this together again that would be something you would play
with to find out which ones are going to work better on this setup and there's a lot of play involved
that's really where it becomes an art form is guessing at what that's going to be the second part i mentioned earlier and
i can only begin to highlight this when you get to these dense layers
one is the activation is a relu they use a relu and a soft max here
it's a whole a whole setup just explaining why these are different
and how they're different because there's also an exponential there's a tangent in fact there's just a ton of these and you can
build your own custom activations depending on what you're doing a lot of different things go into these
activations there are two or three major thoughts on these activations and
rayleigh and soft max are well relu you're really looking at just the number
you're adding all the numbers together and you're looking at euclidean geometry ax plus b
x 2 plus c x 3 plus a bias
with soft max this belongs to the party of um it's activated or it's not except it's
they call it soft max because when you get the the to zero instead of it just being zero
uh it's actually slightly a little bit less than zero so that when it trains it doesn't get lost
there's a whole series of these activations another activation is the tangent
um where it just drops off and you have like a very narrow area where you have
from minus one to one or exponential which is zero to one so there's a lot of different ways to do
the activation again we can do that would be a whole separate lesson on here
we're looking at the convolutional neural network and we're doing the two pools this is so common you'll see two two
convolutional neural networks stacked on top of each other each with its own max pool underneath
let's go ahead and run that so we built our model there and then we need to go ahead and
compile the model so let's go ahead and do that
uh we're gonna use the atom uh optimizer the bigger the data the atom fits better
on there there's some other optimizer but i think atom is a default um i don't really play with the optimizer too much that's like the if
once you get a model that works really good you might try some different optimizers atoms usually the most and
then we're looking at loss pretty standard we want to minimize our lot we want
to maximize the loss of error and then we're going to look at accuracy
everybody likes to say accuracy i'm going to tell you right now i start talking to people and i'm like
okay what's what's the loss on this and that and as a data science yeah i want to know how the lot what's going on with
that we'll show you why in a minute but everybody wants to see accuracy we want to know how accurate this is
and then we're going to run the fit and i wanted to do this just so i can show you
even though we're in a python setup in here where jupyter notebook is
using only a single processor i'm going to bring over my little cpu tool
this is eight cores on 16 dedicated threats so it shows up as 16 processors
and actually i got to run this and then move it over so we're going to run this
and hopefully it doesn't destroy my mic and as it comes in you can see it's starting to do go through the epics and
we said i set it for five epics and then this is really nice because cross uses all the different uh threads
available so it does a really good job of doing that this is going to take a while if you
look at here it's uh eta 2 minutes and 25 seconds 24 seconds so
this is roughly two and a half minutes per epic and we're doing five epics
so this is going to be done in roughly 15 minutes
i don't know about you but i don't think you want to sit here for 15 minutes watching the green bars go across so
we'll go ahead and let that run and there we go there was our 15 minutes it's actually
less than that because i did when i went in here realized that where was it
here we go here's our model compile here's our model flip uh fit and here's our epics uh so i did four
epics so a little bit better a little more like 10 to 11 minutes instead of uh
doing the full 15. and when we look at this here's our model we
did we talked about the compiler uh here's our history we're going to click history equals the model fit
we'll go into that in just a minute and what we're looking at is we have our epics
here's our validation split so as we train it we're weighing the accuracy versus
you kind of pull some data off to the side while you're training it and the reason we do that is that
you don't want to overfit and i'll we'll look at that chart in just a minute
uh here's batch size this is just how many images you're sending through at a time
the larger the batch it actually increases the processing speed um and
there's reasons to go up or down on the batch size because of the the the smaller the batch
there's a certain point where you get too large of a batch and it's trying to fit everything at once uh
so i yeah 128 is kind of big depends on the computer you're on what it can handle
and then of course we have our train images and our train labels going in telling it what we're going to train on
and then we look at our four epochs here here's our accuracy we want the accuracy
to go up and we get all the way up to 0.83 or 83 percent now this is actual
percentage based pretty much and we can see over here our loss we want our loss to go down really
fluctuates uh 55 1.2.77.48
uh so we have a lot of things going on there let's go ahead and graph those
turn that up and our team in the back did a wonderful job of putting together um this basic
plot set up um here's our subplot coming in we're going to be looking at um
uh from the history we're going to send it the accuracy and the value accuracy
labels are set up on there and we're going to also look at loss and value loss so you can see what this
looks like what's really interesting about this setup and let me let me just go ahead and show you because without
actually seeing the plots it doesn't make a whole lot of sense it's just basic plotting of
the data using the pi plot library and i want you to look at this this is really interesting
when i ran this the first time i had very different results and they vary greatly and you can see
here our accuracy continues to climb
there's a cross over here put it in here right here's our crossover
and i point that out because as we get to the right of that crossover where our
accuracy and we're like oh yeah i got point eight percent we're starting to get an overfit here
that's what this this switch over means um as our value uh as a training set versus
the value um accuracy stays the same and so that this is the one we're actually
really want to be aware of and where it crosses is kind of where you want to stop at
and we can see that also with the train loss versus the value loss right here we did one epic and look how it just
flatlines right there with our loss so really one epic
is probably enough and you're going to say wow okay 0.8 percent
certainly if i was working with the shareholders um telling them that it has an 80 accuracy
isn't quite true and we'll look at that a little deeper it really comes out here that the accuracy
of our actual values is closer to 0.41 percent right here
even after running it this number of times and so you really want to stop right here at that crossover
one epic would have been enough so the data is a little overfitted on this when we do four epics
and uh whoops there we are okay my drawing won't go away um let's see if
i can get there we go for some reason i've killed my drawing
ability and my recorder all right took a couple extra clicks
uh so let's go ahead and take a look at our actual test loss um so you see where
it crosses over that's where i'm looking at that's where we start overfitting the model
and this is where if we were going to go back and continually upgrade the model we would
start taking a look at the images and start rotating them we might start playing with the
convolutional neural network instead of doing the three by three window we might expand that or you know find
different things that might make a big difference as far as the way it processes these things um so let's go ahead and take a look at
our our test loss now remember we had our training data now we're going to look at our test images and our test labels
for our test loss here and this is just model evaluate uh just like we did fit up here
where was it one more model fit with our training data going in now we're going to evaluate it on the
and and this data has not been touched yet so this model's never seen this data
this is on completely new information as far as the model is concerned of course we
already know what it is from the labels we have
and this is what i was talking about here's the actual accuracy right here point four eight
uh or four eight four seven so this forty nine percent of the time
guesses what the image is uh and i mean really that's the bottom dollar
does this work for what you're needing does 49 work do we need to upgrade the model more
um in some cases this might be oh what was i doing i was working on
stock evaluations and we were looking at what stocks were the
top performers well if i get that 50 percent correct on top performers
i'm good with that that's actually pretty good for stock evaluation in fact the number i had for stock was
more like 30 something percent as far as being a top performer stock
much harder to predict but at that point you're like well you'll make money off of that so again
this number right here depends a lot on the domain you're working on
and then we want to go ahead and bring this home a little bit more uh as far as looking at the different setup in here
and one of the from sk learn if you remember actually let's go back to the top
uh we had the classification report and this came in from our sklearn or scikit
setup and that's right here you can see it right here on the um see there we go
classification report right here uh sklearn metrics import classification report this we're going to look at next
a lot of this stuff depends on who you're working with so when we start looking at
precision you know this is like for each value i can't remember what one one one was
probably mountains so if 44 is not good enough if you're doing like
um you're in the medical department and you're doing cancer is it is this cancerous or not and i'm only 44
accurate not a good deal you know i would not go with that um
so it depends on what you're working with on the different labels and what they're used for facebook yeah you know
44 i'm guessing the right person i hope it does a little bit better than that
but here's our main accuracy this is what almost everybody looks at they say oh 48 that's what's important
again it depends on what domain you're in and what you're working with
and now we're going to do the same model oops somehow i got my there it goes i thought i was going to get stuck on
there again uh this time we're going to be using the vgg16
and remember this one is all those layers going into it so it's basically a bunch
of convolutional neural networks getting smaller and smaller on here and so we need to go ahead and
import all our different stuff from cross we're importing the main one is the v g 16 set up on there
just aim that there we go there's kind of a pre-processing images
applications pre-process input this is all part of the vg g16 setup on there
and once we have all those we need to go ahead and create our model
and we're just going to create a vgg16 model in here inputs model inputs outputs model inputs
i'm not going to spend as much time as they did on the other one we're going to go through it really quickly one of the first things i would do
is if you remember in cross you can create treat a model like you would a layer
and so at this point i would probably add a lot of dense layers on after the
vgg16 model and create a new model with all those things in there and we'll go ahead and run this
because here's our model coming in and our setup it'll take it just a moment to compile that
what's funny about this is i'm waiting for it to download the package since i pre-ran this um it takes it a couple
minutes to download the vgg16 model into here and so we want to go ahead and train
features for the model we're going to predict that we're going to predict the train images and we're going to test
features on the predict test images on here and then i told you it's going to create
another model too and the people in the back did not disappoint me they went ahead and did just that
and this is really an important part um this is worth stopping for i told you i
was going to go through this really quick so here's our uh
we we have our model 2 um coming in and we've created a model up
here with the vgg16 model equals model inputs model inputs and so we have our vgg16 this has
already been pre-programmed uh and then we come down here i want you
to notice on this right here layer model two layers minus four to one
x layer x we're basically taking this model and
we're adding stuff on to it and so we've taken we've just basically
duplicated this model we could have done the same thing by using
model up here as a layer we could have the input go to this model
and then have that go down here so we've added on this whole setup this whole block of code from 13 to 17
has been added on to our vggg16 model and we have a new model
with the layer ma input and x down here let's go ahead and run that and compile
it and that was a lot to go through right there when you're building these models this is the part that gets so
complicated that you get stuck playing in and yet it's so fun uh it's like a puzzle how
can i loop these models together and in this case you can see right here
that the layers we're just copying layers over and adding each layer in
this is one way to build a new model and we'll go ahead and run that
like i said the other way is you can actually use the model as a layer i've had a little trouble playing with it
sometimes when you're using the straight model over you run into issues
it seems like it's going to work and then you mess up on the input and the output layers there's all kinds of things that come up
let's go ahead and do the new model we're going to compile it again here's our metrics accuracy sparse categorical
loss pretty straightforward just like we did before you got to compile the model
and just like before we're going to take our create a history the history is going to be
a new model fit train 128 and just like before if you remember
when we started running this stuff we're going to have to go ahead and it's going to light up our setup on here and
this is going to take a little bit to get us all set up it's not going to just happen in a couple minutes so let me go
ahead and pause the video and run it and then we'll talk about what happened okay now when i ran that these actually
took about six minutes each so it's a good thing i put it on whole
we did four epics i actually had to stop it says at 10 and switch it to forks i didn't want to wait an hour
and you can see here our accuracy and our loss numbers going down
and just at a glance it actually performed if you look at the
accuracy 0.2658 so our accuracy is going down or you
know 26 um 34 35 percent and you can see here at some point it
just kind of kicks the bucket again this is overfitting that's always an issue when you're
running on programming these different neural networks
and then we're going to go ahead and plot the accuracy history we built that nice little subroutine up above so we might as well
use it and you can see it right here
there's that crossover again and if you look at this look how the how
the uh the red shifts up how the uh our loss functions and everything crosses over we're overfitting after one
epic um we're clearly not helping the problem or doing better
we're just going to it's just going to baseline this one actually shows with the training versus the loss
value loss maybe second epic so on here we're now talking more between the first
and the second epic and that also shows kind of here so somewhere in here it starts overfitting
and right about now you should be saying uh-oh uh something went wrong there i thought that
we went up here and ran this look at this we have the accuracy up here it's hitting that 48
and we're down here um you look at the score down here that
looks closer to 20 percent not nearly anywhere in the ballpark of what we're
looking for and we'll go ahead and run it through the the actual test features here and there
it is um we actually run this on the unseen data and everything point uh one eight or eighteen percent
um i don't know about you but i wouldn't want you know at 18 this did a lot worse than the other one
i thought this was supposed to be the supermodel the model that beats all models the vgg-16
that won the awards and everything well the reason is is that
one we're not pre-processing the data so it needs to be more there needs to be more
um as far as like rotating the data at you know 45 degree angle taking partials
of it so you can create a lot more data to go through here and that would actually greatly change
the outcome on here and then we went up here we only added a couple dense layers
we added a couple convolutional neural networks this huge pre-trained setup is looking
for a lot more information coming in as far as how it's going to train and so
this is one of those things where i thought it would have done better and i had to go back and research it and look at it and say why didn't this work why
am i getting only 18 here instead of 44 or better
and that would be wise it doesn't have enough training data coming in uh and again you can make your own
training data so it's not that we have a shortage of data it's that that some of that has to be switched around and moved
around a little bit and this is interesting right here too if you look at the precision
we're getting it on number two and yet we had zero on everything else so for some reason it's not seeing
the different variables in here so it'd be something else to look in and try to track down
and that probably has to do with the input but you can see right here we have a really good solid 0.48 up here and
that's where i'd really go with is starting with this model and then we look at this model and find out why are
these numbers not coming up better is it the data coming in where's the setup on there
and that is the art of data science right there is finding out which models work better and why
so sequential models so what makes this a sequential model sequential models are linear stacks of
layers where one layer leads to the next it is simple and easy to implement and you just have to make sure that the
previous layer is the input to the next layer so you have used for plain stack of
layers where each layer has one input and one output tensor and this is what tensorflow is
named after is each one of these layers is like a tensor each node is a tensor and then the layer
is also considered a tensor of values and it's used for simple classifier
declassifier models you can it's also used for regression models too so it's not just about uh this is something this
is a teapot this is a cat this is a dog it's also used for generating um
regret the actual values you know this is worth ten dollars that's worth thirty dollars uh the weather is going to be 90
degrees out or whatever it is so you can use it for both classifier and declassifier models
and one more note when we talk about sequential models the term sequential is
used a lot and it's used in different areas and different notations when you're in data science so when we talk
about time series we'll talk about sequential that is something very different sequential in this case means
it goes from the input to layer 1 to layer 2 to the output so it's very directional
it's important to note this because if you have a sequential model can you have a non-sequential model and the answer is
yes if you master the basics of a sequential model you can just as easily have
another model that shares layers you can have another model where the you have an input coming in and it splits
and then you have one set that's doing one set of nodes maybe they're doing a
yes no kind of node where it's either putting out a zero or a one a classifier and the other one might be regression
it's just processing numbers and then you recombine them for the output that's what they call across the cross
api so there's a lot of different availabilities in here and all kinds of cool things you can do as far as
encoding and decoding and all kinds of things and you can share layers and things like that
we're just focusing on the basic cross model with the sequential model
so let's dive into the meat of the matter let's do and do a demo on here today's demo in this demo we'll be
performing flower classification using sequential model and cross and we'll use our model to classify between five
different types of flowers
now for this demo and you can do this demo on whatever platform you want or whatever
user interface for developing python i'm actually using anaconda and then i'm
using jupyter notebooks to develop in and if you're not familiar with this you
can go under environment once you've created environment you can come in here to open a terminal window
and if you don't have the different modules in here you can do your conda install whatever module it is
just happened that this particular setup didn't have a seaborn in it which i already installed
so here's our anaconda and then i'm going to go back and start up my jupiter notebook
where i already created a new python project python 3 i'm in python
3.8 on this particular one sequential model for flowers
so lots of fun there so we're going to jump right into this the first thing is to make sure you have
all your modules installed so if you don't have numpy pandas
matplot library and seaborn in the cross an sk learn or site kit it's not
actually sklearn you'll need to go ahead and install all of those now having done this for years and
having switched environments and doing different things i get all my imports done and then we
just run it and if we get an error we know we have to go back and install something right off the bat though we have numpy
pandas matplot library seaborn these are built on top of each other panda is a data frame and built on top of numpy the
data array and then we bring in our sklearn or scikit this is the scikit setup sci
kit even though you use sklearn to bring it in it's a scikit and then our cross we
have our pre-processing the images image data generator our model this is our basic model our
sequential model and then we bring in from cross layers import dents
optimizers these optimizers a lot of them already come in these are your different
optimizers and it's almost a lot of this is so automatic now atom
is the a lot of times the default because you're dealing with a large data
and then we get our sgd which is smaller data does better on smaller pieces of data
and i'm not going to go into all of these different optimizers we didn't even use these in the actual demo you just have
to be aware that they are different optimizers and the digger the more you dig into these models
you'll hit a point where you do need to play with these a little bit but for the most part leave it at the default when you're first starting out
and we're doing just the sequential you'll see here layers dense and then if we come down a little bit
more when they put this together and they're running the dense layers you'll also see they have dropout they have
flattened they have activation they have the convolutional
layer 2d max pooling 2d batch normalization
what are all these layers and when we get to the model we're going to talk about them a lot of times when you're just starting
you can just import cross dot layers and then you have your drop out your flattened uh
your convolutional neural network 2d and we'll we'll cover what these do
in the actual example when we get down there what i want you to take from here though is you need to run your imports
and load your different aspects of this and of course your tensorflow tf because
this is all built on tensorflow and then finally import random is rn just for random
generation and then we get down here we have our cv2
that is your open image or your opencv they call it for processing images that's what the
cvd 2 is we have our tqdm
the tqdm is for is a progress bar just a fancy way
of adding when you're running a process you can view the bar going across in the jupiter
setup not really necessary but it's kind of fun to have we want to be able to shuffle some files
again these are all different things pill is another image processor it goes with the cv2 a
lot of times you'll see both of those and so we run those we've got to bring them all in
and the next thing is to set up our directories and so when we come into the directories
there's an important thing to note on here other than we're looking at a lot of flowers which is fun
as we get down here we have our directory archive flowers that just happens to be where the different
files for different flowers are put in we're denoting an x and a z
and the x is the data of the image and the z is the tag for it what kind of
flower is this and the image size is really important because we have to re-size everything
if you have a neural network and if you remember from our neural networks let me flip back to that slide
we look at this light we have two input nodes here uh with an image you have an input node
depending on how you set it up for each pixel and that pixel has three different
color schemes usually in it sometimes four so if you have a picture that's 150 by 150
uh you multiply 150 times 150 times three that's how many nodes input layers
coming in i mean so this is a massive input a lot of times you think oh yeah it's just a small amount of data or
something like that no it's a full image coming in and then you have your hidden layers a lot of times they match what the image size is
coming in so each one of those is also just as big and then we get down to just a single output
so that's kind of a thing to note in here what's going on behind the scenes and of course each one of these layers
has a lot of processes and stuff going on and then we have our different
directories on here let me go and run that so i'm just setting the directories that's all this is
archive flowers daisy sunflower tulip dandelion rose just our different directories that
we're going to be looking at
uh and then we want to go ahead and we need to assign labels remember we defined x and z
so we're just going to create a definition here
and the first thing is a return flower type okay just returns it what kind of flower it
is i guess assigned label to it but we're going to go ahead and make our train data and when you look at this there's a
couple things to take away from here the first one is we're just appending right onto our numpy array the image
we're going to let numpy handle all that different aspects as far as 150 by 150 by three
we just dump it right into the numpy which makes it really easy we don't have to do anything funky on the processing
and we want to leave it like that and i'm going to talk about that in a minute and then of course we have to have the
string append the label on there and i want you to notice right here we're going to read the image in
and then we're going to size it and this is important because we're just changing this to 150 by 150 we're resizing the
image so it's uniform every image comes in identical to the other ones
this is something that's so important is when you're resizing or reformatting your data you really have to be aware of
what's going on with images it's not a big deal because with an image you just resize it so it looks
squishy or spread out or stretched the neural network picks up on that and
it doesn't really change how it processes it so let's go ahead and run that
and now we've got our definition set up on there and then we want to go ahead and make
our training data so make the train data
daisy flower daisy directory uh print length of x so here we go let's go and run that
and we're just loading up the flower daisy so this is going all in there and it's setting
um it's adding it in to the our setup on there to our x and z setup
and we see we have 769 um and then of course you can see this nice bar here this is the bar going
across is that little added code in there that just makes it really cool for doing demos
not necessarily when you're building your own model or something like that but if you're going to display this to other people adding that little what was
it called tqdm i can never remember that but the
tqdm module in there is really nice and we'll go ahead and do sunflowers and of course you could have just
created an array of these but this has an interesting problem that's going to come up and i want to show you something
it doesn't matter how good the people in the back are or how good you are programming errors are going to come up and you got
to figure out how to handle them and so when we get all the way down to
the where is it dandelion here's our dandelion directory we're going to build
jupiter has some cool things it does which makes this really easy to deal with
but at the same time you want to go back in there depending on how many times you rerun this how many times you pull this
so when you're finding errors going in here there's a couple things you can do and we're just going to hope
it wasn't there it is there's our error i knew there was an error this processed
1062 out of 1065. now i can do a couple things one i could
go back into our definition and i can just put in here try and so if it has a bad conversion this is where
the error is coming from just skip it that's one way to do it
when you're doing a lot of work in data science and you look at something like this where you're losing three points of
data at the end you just say okay i lost three points who cares or you can go in there and try to delete
it it really doesn't matter for this particular demo and so we're just going to leave that
error right along and skip over because it's already added all the other files in there and this is a wonderful thing
about jupiter notebook is that i can just continue on there and the x and z which we're creating
is still running and we'll just go right into the next flower rows so all these flowers are in there
that's just a cool thing about jupiter notebook
and then we can go ahead and just take a quick look and see what we're dealing with and this is of
course really when you're dealing with a other people and showing them stuff this is just kind of fun where we can display
it on the plot library here and we're just going to go through and
see what we got here uh looks like we're gonna do like five of each of them
i think is that how they set this up um plot library five by two okay oh i see
how they did it okay so two each so we have five by two set up on our axes and we're just going to go in and look at a
couple of these flowers it's always a good thing to look at some of your data
no matter what you're doing we've reformatted this to 150 by 150
you can see how it really blurs this one up here on the tulip that is that resize to 150 by 150
and these are what's actually going in these are all 150 by 150 images you can check the dimensions on the side
and you can see just a quick sampling of the flowers we're actually going to process on here
and again like i said at the beginning most of your work in data science is reprocessing
this different information so we need to go ahead and take our labels uh and run a label encoder on there and
then we're just gonna ellie is a label encoder one of the things we imported
and then we always use the fit to categorical y comma five x here's our
array um x so if you look at this here's our fit we're gonna transform z
that's our z array we created um and then we have y which equals that
and then we go ahead and do uh to categorical we want five different categories and then we create our x uh np array of
x x equals x over 255. so what's going on here there's two
different transformations one we've turned our categories into 0 1 2 3 4 5 as the output
and we have taken our x array and remember the x array is three values
of your different colors this is so important to understand when we do this across a numpy array this
takes every one of those three colors so we have 150 by 150 pixels out of those 150 by 150 pixels they each
have three color arrays and those color arrays range from 0 to 250.
so when we take the x equals x over 255 i'm sorry range from 0 to 255.
this converts all those pixels to a number between 0 and 1.
and you really want to do that when you're working with neural networks now if you do a linear regression model
it doesn't affect it as much so you don't have to do that conversion if you're doing straight numbers but when you're running neural networks if you
don't do this you're going to create a huge bias and that means they'll do really good on predicting one or two things and they'll
just totally die on a lot of other predictions so now we have our
x and y values x being the data n y being our known output
and with any good setup we want to divide this data into our training so we have x train
we have our x test this is the data we're not going to program the model with and of course your y train corresponds
to your x train and your y test corresponds to your x test the outputs and this is uh when we do the train test
split this was from the site kit sklearn we imported train test split and we're
just going to go ahead and do the test size at about a quarter of the data 0.25 and of course random is always good
this is such a good tool i mean certainly you can do your own division um you know you could just take the first
you know 0.25 of the data or whatever do the length of the data not real hard to do but this is randomized so if you're
running this test a few times you can kind of get an idea whether it's going to work or not sometimes what i will do
is i'll just split the data into three parts and then i'll test it on two with one
being the or train it on two of those parts with one being the test and i rotate it so i come up with three different answers
which is a good way of finding out just how good your model is but for setting up let's stick with the x train x test and the sk learn package
and then we're going to go ahead and do a random seed now a lot of times the cross actually
does this automatically but we're going to go ahead and set it up on here and you can see we did an np random seed
from 42 and we get a nice rn number and then we do tf random we set the seed
so you can set your randomness at the beginning of your tensorflow and that's what the
tf.random.set is
so that's a lot of prep all this prep and then we finally get to the exciting part
this is where you probably spend once you have the data prepped and you have your pipeline going
and you have everything set up on there this is the part that's exciting is building these models
and so we look at this model one we're going to designate a sequential um they have the api which is across the cross
tensorflow api versus sequential sequential means we're going one layer to the next so we're not going to split
the layer and bring it back together it looks almost the same with the exception of
bringing it back together so it's not a huge step to go from this to an api
and the first thing we're going to look at is our convolutional neural network in 2d
so what's going on here there's a lot of stuff that's going on here the default
for well let's start with the beginning what is a convolutional 2d network
well convolutional 2d network creates a number of small windows and those small
windows float over the picture and each one of them is their own neural network and this basically
becomes like a categorization and then it looks at that and it says oh if we add these numbers
up a certain way we can find out whether this is the right flower based on this this little
window floating around which looks at different things and we have filters 32 so this is
actually creating 32 windows is what that's doing
and the kernel size is 5x5 so we're looking at a five by five square
remember it's 150 by 150 so this narrows it down to a five by five it's a two d
so it has your x y coordinates um and we look at this five by five remember
each one of these is is actually looking at five by five by three so we're actually looking at fifteen by
fifteen different pixels and padding is just um
usually just ignore that activation by default is relu we went
ahead and put the relu in there there's a lot of different activations
relu is for your smaller uh when you remember i mentioned atom when you have a lot of datum data use an atom
kind of activation or using atom processing we're using the relu here
it kind of gives you a yes or no but it it doesn't give you a full yes or no it
has a zero and then it kind of shoots off at an angle very common it's the most common one and
then of course here's our input shape 150 by 150 by three pixels
and then we have to pull it so whenever you have a two convolutional 2d layer
we have to bring this back together and pull this into a neural network and then
we're going to go ahead and repeat this so we're going to add another network
here one of the cool things if you look at this is that it as it comes in it just kind of automatically assumes you're
going down to the next layer and so we have another convolutional null network 2d
here's our max pooling again we're going to do that again max pooling and we're just going to filter on down
now one of the things they did on this one is they changed the kernel size they changed the number of filters
and so each one of these steps kind of looks at the data a little bit differently
and that's kind of cool because then you get a little added filtering on there this is where you start playing with the
model you might be looking at a convolutional neural network which is great for image classifications
we get down to here one of the things we see is flattened so we added we just flatten it remember this is 150 by 150
by three well and actually the pool size changes so it's actually smaller than that flattened just puts
that into a 1d array so instead of being you know a tensor of this really
complexity with the pixels and everything it's just flat and then the dents
is just another activation on there by default it is probably relu
as far as its activation and then oh yeah here we go in sequential they actually added the
activation as relu so this just because this is sequential this activation is attached to the dents
and there's a lot of different activations but relu is the most common one and then we also see a soft max
softmax is similar but it has its own kind of variation and one of the cool things you know what
let me bring this up because if we if you don't know about these activations this doesn't make sense
and i just did a quick google search on images of tensorflow activations
um i should probably look at which set website this is but this is the output of the values
so as your x as it adds in all those uh weighted x values going into the node
it's going to activate it a certain way that's a sigmoid activation and you can see it goes between 0 and one and has a
nice curve there this also shows the derivatives um and if we come down the seven popular activation functions
non-linear activations there's a lot of different options on this let me see if i can find the
oops make sure we can find the specific to relu so this is a leaky relu
and you can see instead of it just being zero and then a value between going up it has a little leaky there otherwise
your railu loses some nodes they just become inactive um but you can see there's a lot of different options here
here's a good one right here with the rayleigh you can see the rayleigh function on the upper on the upper left here and then the leaky rayleigh over
here on the right which is very commonly used also one of the things i use with processing
language is the sig as the exponential one or the tangent h the hyperbolic tangent
because they have that nice funky curve that comes in that has a whole different meaning and captures word use better
again these are very specific to domain and you can spend a lot of time playing with different models for our
basic model we'll stick to the relu and the softmax on here and we'll go ahead and run and build this model
so now that we've had fun playing with all these different models that we can add in there we need to go ahead and have a batch
size on here uh 128 epix10
this means that we're going to send 128 rows of data or flowers at a time to be
processed and the epics 10 that's how many times we're going to loop through all the data
and then there's all kinds of stuff you can do again this is now built into a lot of cross models already by default
so there's different ways to reduce the values and verbose verbose equals one means that
we're going to show what's going on value monitor what we're monitoring
we'll see that as we actually train the model this is what's what's going to come out of there if you set the verbose
equal to 0 you don't have to watch it train the model although it is kind of nice to actually know
what's going on sometimes and since we're still working on
bringing the data in here's our batch side here's our epics we need to go ahead and create a data generator
this is our image data generator and it has all the different settings in
here almost all of these are defaults so if you're looking at this going oh my gosh this is confusing
most of the time you can actually just ignore most of this vertical flip so you can randomly flip
pictures you can randomly horizontally flick them you can shift the picture around this
kind of helps gives you multiple data off of them zooming rotation there's all kinds of different things you can do with images
most of these we're just going to leave as false we don't really need to do all that
setup because we already have a huge amount of data if you're short data you can start flipping like a horizontal picture and
it will generate it's like doubling your data almost so the upside is you double your data
the downside is that if you already have a bias in your data you already have um
5 000 sunflowers and only two roses that's a huge bias it's also going to
double that bias that is the downside of that
and so we have our model compile and this you're going to see in all the cross we're going to take this model
here we're going to take all this information as far as how we want it to go and we're going to compile it
this actually builds the model and so we're going to run that and i want you to notice uh
learning rate very important this is the default zero zero one
there's there you really don't this is how slowly it adjusts to find the right answer
and the more data you have you might actually make this a smaller number with larger with you have a very small
sample of data you might go even larger than that and then we're going to look at the loss categorically categorical
cross entropy most commonly used and this is uh how how much it improves
the model is improving is what this number means or yeah that's that's important on there and then the accuracy
we want to know just how good our model is on the accuracy and then
one of the cool things to do is if you're in a group of people who are studying the model if you're in
shareholders you don't want to do this as you can run the model summary i do this by default and you can see the
different layers that you built into this model just a quick summary on there so we went ahead and we're going to go
ahead and create a we'll call it history but we want to do
a model fit generator and so what this history is doing is
this is tracking what's going on as while it fits the model now there's a lot of new setups in here
where they just use fit and then you put the generator in here we're going to leave it like this even
though the new default is a little different on that
doesn't really matter it does the same thing and we'll go ahead and just run that
and you can see while it's running right here we're going through the epics we have one of ten now we're going through
six to twenty five here's our loss we're printing that out so you can see how it's improving and
our accuracy the accuracy gets better and better and this is 6 out of 25. this
is going to take a couple minutes to process because we are training 150 by 150 by 3
pixels across six layers or eight layers whatever it was
that is a huge amount of processing so this will take a few minutes to process this is when we talk about the hardware
and the problems that come up in data science and why it's only now just exploding being able to do neural
networks this is why this process takes a long time
now you should have seen a jump on the screen here because i did pause the recorder to let this go ahead and run
all the way through its epics let's go ahead and take a look and see what these epics are and if you set the
verbose to zero instead of one it won't show what's going on in the behind the scenes as
it's training it so we look at this epic 10 epic so we went through all the data 10 times
if i remember correctly there's roughly a gig of data there so that's a lot of data the first thing you're going to notice
is the 270 seconds that's how much each of those epics took
to run and so if you divide 60 in there you roughly get about five minutes worth of
each epic so if i have 10 epics that's 50 minutes almost an hour of run time
that's a big deal we talk about processing uh in on this particular computer
i actually have what is it eight cores with 16 dedicated threads so it runs like a 16 core computer it
alternates the threads going in and it still takes it five minutes for each one of these epics so you start to
see that if you have a lot of data this is going to be a problem if you have a number of models you want to find
out how good the models are doing and what model to use and so each of those models could take all night to run in fact i have a model
i'm running now that takes over uh takes about a day and a half to test each model
it takes four days to do with the whole data so what i do is i actually take a small piece of the data
test it out to find out get an idea of how the different setups are going to do
and then i increase that size of the data and then increase it again and i can just take that that curve and kind
of say okay if the data is doing this then i need to add in more dense layers
or whatever uh so you can do a small chunks of data and then figure out what it costs to do a large set of data and what kind of
model you want the loss as we see here continues to go
down this is the error this is how much error is in there it really isn't a
user-friendly number other than the more it trends down the better and so if you continue to see the loss going
down eventually get to the point where it stops going down and it goes up and down and kind of wavers a little bit at
that point you know you've run too many epics you're starting to get a bias in there and it's not going to give you a
good model fit the accuracy just turns us into something that
we can use and so the accuracy is what percentage of guesses in this case is categorical
so this is the percentage of guesses are correct um value loss is similar you know it's a
minus a value loss and then you have the value accuracy and you'll see the value accuracy is pretty similar to the accuracy
just rounds it off basically and so a lot of times you come down here and you go okay we're doing 0.5.6
0.7 and that is 70 accuracy or in this case 68.59
accuracy that's a very usable number and it's very important to have if you're identifying flowers that's
probably good enough if you can get within a close distance in knowing what flower you're identifying if you're trying to figure out whether
someone's going to die from a heart attack or not you might want to rethink it a little bit or re-key how you're
building your model so if i'm working with a a group of clients
shareholders in a company or something like that you don't really want to show them this
you don't want to show them hey you know this is what's going on with the accuracy these are just numbers and so
we want to go and put the finishing touches just like when you are building a house and you put in the frame and the
trim on the house it's nice to have something a nice view of what's going on and so we'll go ahead and do a pie plot
and we'll just plot the history of the loss the history of the value loss
over here epics train and test and so we're just going to compute these this is really important and what i want
you to notice right here is when we get to about oh five epochs a little more than five six
epics you see a cross over here and it starts crossing as far as the
value loss and what's going on here is you have the loss in your actual model and your actual data and you have the
value loss where it's testing it against the the test data the data wasn't used
to program your model wasn't used to train your model on and so when we see this crossing over
this is where the bias is coming in this is becoming over fitted and so when you put these two together
right around five and six you start to see how it does this this switch over
here and that's really where you need to stop right around five yeah six um
it's always hard to guess because at this point the model is kind of a black box see but you know that right around here
if you're saving your model after each run you want to use the one that's right around five epics because that's the one
that's going to have the least amount of bias so this is really important as far as guessing what's going on with your
model and its accuracy and when to stop it also is you know i don't show people
this mess up here i show somebody this kind of model and i say this is where the training and the testing comes in on this model
it just makes it easier to see and people can understand what's going on
so that completes our demo and you can see we did what we were set out to do we took our flowers and we were able to
classify them within about you know 68 70 accuracy whether it's going to be a
dahlia sunflower cherry blossom rose a lot of other things you can do with your output as far as a
different tables to see where the errors are coming from and what problems are coming up
and that brings us to the end of this video on artificial intelligence 2022 full course by simply learn i hope you
liked it if you have any queries then please feel free to put them in the comment section of the video thank you for watching