
1)Introduction
hello and welcome to the course introduction to data analytics my nameis nandan sudashanam i'm a faculty here at iit madras department of management studies and i'm ravindran a faculty in the computer science and engineering department here at iit madras we are really excited to be bringing you this course the focus of this course is to introduce you to the tools and techniques that are used currently for understanding data and to derive useful knowledge from it so the course itself follows the broad contours of the different types of data analytics that are there the early part of the course is going to focus more on data analysis through statistical methods and in that we will be covering descriptive statistics which is the idea of how do you describe data how do you represent or summarize data and in that we will be covering lots of visualization techniques and so on and then we move on to inferential statistics the context here is that you see the data as a sample as a sample from a broader universe that's generating this data and the idea is at this point you're not content with just summarizing the data you really want to say something about this broader phenomena that is generating this data so can i make some kind of generalization or some kind of inference about this data so this leads us to the main part of the course which is machine learning and data mining so here we'll be diving deep into the art and science of predictive analytics right in predictive analytics we are looking to build algorithms that build that learn models from historic data that relate one or more variables okay so note that the emphasis here is on automatically learning these kinds of models from the data and therefore we'll be focusing more on the algorithms that learn these models rather than the models themselves so the models these algorithms learn should be such that they are able to predict values about different variables given only a subset of the variables that describe the data in addition to learning these models we will also be looking at learning interesting patterns in the data as well as any hidden structure that is present in the data we will finish the course with the final section on prescriptive analytics this is what you might have heard as data-driven decision making the idea here is that you in the earlier part of the course you've learned to make predictions you've learned to mine for interesting patterns in the data you've learned to make inferences but how do you use this information to make concrete decisions i hope this has given you a broad overview of what we seek to cover in this course and we really look forward to having you join us thank you see you soon

2)Course Overview
Hello, and welcome to our first lecture for the course Introduction to Data Analytics. In this lecture and perhaps the next few lectures, I am going to be providing you with the Course Overview and giving you a detailed description of what we will be covering during this course. Let us start off with a little bit of logistics associated with the course. First thing is, there is a good amount of information available on the course website. So, I just wanted to make sure that everybody can avail of the course outline, the syllabus, the reference books and so on, it is all uploaded on the NPTEL website and you should access it, so you have the appropriate information. The second thing is that the forums in the course are a great resource for answering many of the questions that you might have or that you might come across during the course. The most of the cases, the professors myself and Professor Ravindran, we will try to answer some of your questions, at times we will have teaching assistants help us with answering questions and most importantly, many of these questions can be answered by your own pears. So, I really encourage you to use that resource and also contribute to it in any way that you deem fit. If for instance we feel like a particular question is not been answered appropriately, clear it keeps coming up; we will definitely join in and get in on the discussion, so definitely try to use that as a resource. Before, I started there are already a lot of questions on the forums, so I just wanted to address one or two of them, they pertain to the course at large. So, the first question relates to the course style. For instance, is this course going to be very theoretical, is it going to be a very case study base, so on and so forth. So, to give you a feel for what this course is going to cover, the course is not going to be only theory or highly theoretical, we are not going to emphasize extensively on the pure math of the course. They might be some amount of math and some amount of programming, but we are not going to go in depth and derivation and so on. The course is going to be heavily conceptual and it is going to try and have as many applications as possible. So, we are going to give you the whats and whys of data analytics are ranging from the statistics to the machine learning, what techniques and tools will you apply, where, why do some methods work in certain situations and not others, how do these algorithms work, what is the background logic behind it and we are going to give you lots of applications. We will be assisting you with little bit of how. So, how do you implement this algorithm or how do you, what kind of software could you potentially use and so on, but the course itself is not going to be a tutorial style course. So, it is not, you know anything that you can essentially get from the help file of package that you using for data analytics, we are not going to be repeating that here. So, that we believe that we are adding more value by spending our time and giving you really the core concepts. Syntax, you can learn from one software to the next. Which brings me to the next core area, which is the software and programming area. Many of you have expressed, concern or have questions about, what software we would be using, what programming languages, how much do I need to know. So, the main languages that we are going to be using in this course are R and Matlab and occasionally, we will show you, how it can also be applied in Microsoft excel. But, again it is not going to be tutorial style, we will not teach you the nuts and bolts of how to use R and the code itself that we would be expecting or teaching would be very fairly basic and more like command line instructions. So, it would be fairly easy to pick up, even if you did not know the software per say, but you had some comfort level with basic programming. So, without I have covered some forum questions and basic logistics, so let us dive into our course overview. So, in our course overview, I just wanted to give you the basic module list. Now, this is available on the course website and the basic module list just gives you the topics that we are going to be covering in this course. So, we will be having descriptive statistics, an inferential statistics that is the first two broad areas. Within descriptive statistics, we would also be covering some amount of probability and probability distributions. We then move on to more advance concepts in inferential statistics, namely something called the ANOVA or analysis of variance. We will be talking about what, where and why we apply that. We will introduce regression, something that you might have heard of. And then, we move on to what we see as the main focus of the course, which is machine learning. Both an introduction to it, core concepts, how do they tools and what are the tools and techniques are there, how do they work and within that, we would be talking about both two classes called supervised and unsupervised learning. And finally, we come to this module on, what do we do when you do not have data. How do you go about, what is data analytics when you do not already have the data? So, there is a lot of interesting work there. So, do not worry if at this stage you do not understand some of the words that I have used, that is what this overview is for to give you some idea of what it is that we are going to be covering. I am now going to go step by step, talk about each of these modules and place them and give you some idea of, what is the core concept behind them. Now, again the purpose of this is not to replace the actual class. So, in actual class for instance, I will be giving you far more thorough treatment and so Professor Ravindran, but this is really to help you get a first class view of what it is that you will learn at the end of this course. So, great, let us start with our first module, which is descriptive statistics and exploratory data analysis. Here we are really talking about, how do you describe data. So, we would be talked the, one of the main things that would be introduced here is data visualization techniques. So, this primarily concerns different forms of graphs, how do you represent a single variable graphically, how do you represent relationships between two variables graphically and typically, we are dealing with a data set. So, what kinds of graphs and what kinds of tables are best suited, especially also given that different variables are of different types and for different variable types are there, different ways of representing this data. So, visualization is one part of the descriptive statistics and we will spend a few short classes there. We then move on to summarizing data. So, this is part of descriptive statistics, which is you have a data set, how do I you know summarize the data set, how do I present it to you in a single statistic or a set of statistics. Out here for instance, we will be mostly concern with something like measures of central tendency. What do we mean by that? You might have heard of the words means, median and mode and even if you not, I am sure you all colloquially used the words saying, you have this data set, but what is the average. So, if it is a single variable especially, what is the average or what is a typical value in that data set, what is a value in between and you know these are all very colloquial ways of talking about it. But, there are various different measures of central tendency and different measures express different properties associated with data set and we would be going into that, another area is also measures of dispersion. So, you might have heard of the words variance or standard deviation and essentially, what that captures is an inherent variability. So, we spoke about this concept of mean or measures of central tendency, but how does the data itself vary around that average and that is what we will be talking about there. We will focus on measures of dispersion and measures of central tendency. But, beyond summarizing data through these measures, we can go even further and probability distributions are the richest way of expressing a data. And they do so, because you do not just stop with saying this is the mean or this is the average or this is the standard deviation. You almost literally express it as you know, 20 percent of the data is below this value, 30 percent of the data is below this value and so on. So, when you do that you not only get an idea of the dispersion and the centrality, but you know the entire characterization of the data set. So, you will be talking about probability distributions as well there. So, I think in that is and that gives you some sense of, what descriptive statistics really seeks to accomplish and that is, what we will be covering in that module. We move on to the next part of the course, which is inferential statistics. The idea here is, the idea is one of populations and samples and I am going to explain to you, what these words really mean. The idea here is that, the data that you have is essentially some sample from a broader universe; that is that could be creating this data. And this broader universe is what we call the population and the population can be like a finite, very large data set and you do not have that entire data set, which is why you need a sample from the data set. But, it could also be something more conceptual, essentially this data does not physically exist anywhere. The population data does not physically exist, but it is the concept of this really large universe of potential data that you can get, but you essentially have only, you can only get a sample of the data set. So, perhaps I mean, just to give you a stronger intuition for it, let us just talk about one or two examples. If you took for instance the height of all IIT Madras students, let us say the height of all IIT Madras students. Here is the case, where you actually have a finite population, you define your state space, you define that is space as the data is ultimately the height of all IIT Madras students as of today, let us say as of 2015, that is a very large number and you might not have the data. So, you might choose to take a sample from the data, so you might choose 20, 30 students and go actually measure their heights and that is your sample and here the population was, population would have been the height of every single IIT Madras student as of 2015. I give you another example, where it might not be a finite population. So, you might say well I have this manufacturing process, that makes a certain product and I am interested in the dimension of the product. Now, let us say I go and change that manufacturing process and I am interested in measuring, so let us say it is the width of a particular piece of metal that gets that comes out of this machine. Now, the population here would be essentially infinite in size, it could potentially be the width of infinite number of metal pieces that come out of this particular machine and you know, you are not interested in actually getting that entire population you could not. But, you might, you could still have a sample, you might have just collected about 20 pieces of such metal and you have the sample. Now, what is the point of all these? Why is this an important concept? It is a very important concept, because in many instances, we find that we are not satisfied with just describing or giving you statistics about the sample. You are far more interested in saying something about the population. So, for instance I might be interested in saying something like the average height of IIT students is less than 130 centimeters or some such number or 150 centimeters. The average height of IIT students is less than 150 centimeters. Here note that you making a statement about the population, you making a statement about the average height of IIT students and we said the height of IIT students is the entire population. But, you taken only a sample of 20, 30 students, you do not have data associated with the population. So, is there something you can do with this statistic that is, you can do with the sample of 30 or 40 and say something about the population. Another example, for instance institute could be something like let us say I have a tooth paste company and my job is to put certain amount of fluoride in the tooth paste. And let us say, I am interested in putting about 1000 parts per million of fluoride in my tooth paste use. Will every single tooth paste you have exactly 1000? Probably not and what is my population here, it is potentially every single tooth paste tube that I have sent out to the market or I could be sending out to the market. But, I can do one thing; I can go, take a sample of about 10 or 20 tubes or 30 tubes, measure the amount of fluoride in that and see, if the average amount of fluoride in my tooth paste tubes is equal to 1000 or less than 1000 or greater than 1000, this is with the full recognition that not every single tube is going to have that exact amount, because the world is not a perfect place the world is a noisy place. But, is my average equal to the value I think it is or is my average less than or greater than the value I think it is. So, the whole idea again just to kind of recap, what inferential statistics tries to capture is that, you now have split the word into a population and a sample. A population could be finite or it need not even be finite, but it is essentially you can think of it is very large universe of data and all you are getting is a sample, all you can measure is from the sample. But, is there something I can do with the sample to make a statement about the population and that is, those of the tools and the techniques that we will be discussing there. And it goes beyond for instance the two examples that I gave, in for instance the two examples that I gave we got a sample and we compare that to some number we had in our head. We said, is the average IIT student height less than 130 centimeters or is the amount of fluoride equal to some number x. But, you could also be comparing two samples and there by essentially comparing their populations. Is the average amount of fluoride in tooth paste brand A equal to the average amount of fluoride in tooth paste brand B and extend that even further, A versus B versus C and so on. So, the core idea again is that you not are confined to one sample, you could go to many samples and the idea of going to many sample is what is get captured in a technique called ANOVA or analysis of variance. We are going to have a separate module on that as well, but again stepping back are we living in a world, where we get samples of data whether is a bigger phenomena, but can I use these samples to make statements about the broader phenomena. So, that is what we will be covering in the inferential statistics part of the course. We then, move on to something that many of you might have heard or encountered, probably has bend the motivation for you to even may be take this course and I am talking about regression. When I say the word regression here, I am going to actually start using the word regression analysis, because the word regression itself can be used to cover many highly relative, but slightly different concepts. But, we are going to talk about regression analysis and regression analysis is a great segue, it is a great step going after inferential statistics and going before our machine learning. Because, regression analysis uses inferential statistics, it is you cannot say regression analysis is only inferential statistics, it uses inferential statistics, but it also uses many others tools. It uses optimization, it uses some concepts from descriptive statistics and so on and it has a lot of overlap with machine learning. Regression analysis per say might not in many courses or depending on, who you ask might not come under necessarily the umbrella of machine learning, because regression analysis has been there, been around far before, say machine learning analysis. But, in a sense it tries to tackle the same problem statement or a very similar problem statement, that some of the more advance techniques in machine learning try to do. So, it is a great thing to learn and understand right of the back and so we would be introducing a regression to you in this course. With the regression itself, we are going to start with the simplest form, simple linear regression and we going to use fairly simple techniques of doing it called the ordinary least squares techniques. And right now, let me just give you again a very basic intro to what a regression is again like I have mentioned before a this is not to replace the class, that we will have on regression all of these things and the talking about today we are going to go into it in great detail as we go through the course the purpose out here is to let you know what you getting into in the course at this stage. So, let me just spend few minutes and give you very brief in true to what regression analysis tries to do. The regression analysis is essentially about creating a relationship between the dependent variable, you can also think of that as the output variable and one or more independent variables and you can think of the independent variables as input variables. We definitely going to talk about some examples here, but the core idea is to create to this relationship and one of the simplest ways of the creating this relationship is to fit a line through the data, what do you mean by fit a line through the data. Take a simplest example you have the independent variable and may be your best suited with some examples here, but let is say you are independent variable is the amount of rain fall that a particular region in India receives particular rural region and this is collected over the course of yours, so it is the annual rain fall that a particular region in India collects. And the output variable is the amount of crop yield, so amount of rice that grow the amount of wheat that grow that particular year. The core idea is the independent variable is the rain fall the dependent variable or the output variable is the crop yield, why because we might suspect, that how much rain fall it is, depending on how much rain fall there is that is going to influence the crop yield, The amount of crop yield is the output depends on the amount of rain fall may be we are right may be we are not, but what we are essentially doing is taking pairs of data from different years. So, year one how much did it rained, what was the crop yield how much wheat or rice of crop did we get. Year two how much did it rained how much not. So, you have this data set right of inputs and outputs independent variables and dependent variables. The core idea is can I create a relationship between these two variables and the simplest way is to create fit a line through this data. And you can see in the slide in front of you that I have just created an example graph where the x axis is essentially the independent variable how much did it rained the y axis is how much what was the crop yield and the idea is if you can fit a line through this data that line in some way represents that relationship between these two variables. You can think of many more examples like this for instance may be your independent variable is exercise and your dependent variable could be something like weight loss in this particular case the line; however, will look different the more you exercise the greater the weight loss, but if you can if you thought of it as if you thought of the loss it depends on how you think of the loss right if your y axis the dependent variable for instance is your overall weight and not the weight loss. Then, you would have a line that started from the top left corner and came down to the bottom right corner, but that is fine that is still a regression and one way or the other as long as you are trying to fit a line through the data that is what you are trying to do. There are many more examples that even come from engineering some of the early experiments at then, in trying to understand voltage as the independent variable and current as the dependent variable the frequency to the inductive impedance was another one that is tried like this. Another great example from mechanical engineering is a essentially speed of the vehicle to the mileage that you get from the vehicle. So, at different speeds are you getting different mileages is there some relationship between these two variables and how do I quantify that. But, the examples can be endless, but the core idea with such an exercise typically tends to be one of two things or either both of them one is to capture that functional relationship. So, you understand the system you are dealing with right and this requirement is prevalent and it is not mutually exclusive from this other requirement, which is given the independent variable can I predict the dependent variable. So, the first one the more obvious one is I need to know one I need to understand my system. So, by doing this data analysis I understand, how rain fall effects crop yield, but there might be another goal another not unrelated goal, but another goal which is given some amount of rain fall can I predict what my crop yield would be. Now, this has some critical advantages this has many business applications and to give you an example keeping with the same example. Now, given the amount of rain fall there is if I can predict, what the crop yield would be, then as a government can I institute certain policies for relief performers given the amount of rain fall there is if I for instance providing insurance for farmers can I come up with the prediction of what their crop yield should have been. So, at least I am not in the dark about, what it should be now with each of the other example we spoke about we can also think of cases, where being able to predict what would happen or what the case would be for a given instance has as a lot of advantages for us. And many of the machine learning techniques for instance that we are going to be focusing on really emphasis this prediction part, how accurately can I guess, what is going to be and some amount of it might be that its really helpful to guess that dependent variable the output variable, because it is not possible for me to always measure the dependent variable. I have measured that you for some amount of time and given you some data. So, that you can go and build models based of it, but it is not possible for me in real time continuously keep monitoring that variable, so prediction becomes an advantage there. Another advantage could be that the dependent variable actually takes place at some point of time; however, small, after the independent variable takes place and an example that is often sighted is ambient temperature and number of heat strokes in a hospital. So, that if I just knew if clearly these two variables could be related that the dependent variable, which is the number of heat strokes appearing coming into a hospital is some function of the temperature outside if the temperature outside is really cool and the sun is not out you are not going to that many heat strokes I mean that is reasonable right. But now, because there might be a time lag between when temperature speak and when people actually physically arrive at the hospital can I be better prepared I use some historic data between temperatures and heat strokes and build my and fit my line and build this regression model. Tomorrow, I am just going to use this regression model and say the temperature is x at a temperature x can I use my regression line and predict how many heat strokes are going to arrive at my hospital and therefore, I will be better prepared at the hospital. So, I hope this gives you some insight into what regression is what it seeks to achieve and with that we conclude our first session of the course overview. In the next session, we will be continuing with giving continuing with giving you an overview of the course and introduce the more detailed sessions on machine learning and so on. So, look forward to having you join us then. Thank you for participating today.

3) Course Overview (cont'd)
Welcome to the second lecture of the course Introduction to Data Analytics. In this session, we are going to continue from our previous session, where we presented to you a brief overview of what, we are going to be covering in this course. And we started of talking about in the previous session we spoke about descriptive statistics, inferential statistics, the use of ANOVA in inferential statistics and finally, we spoke about regression and regression analysis and how we would be using that and we would be talking about that in this course. We now move on to the next session of the course, which is machine learning. Again, just to remind all of you there, this is not the introduction to machine learning part of the course. This is the part, where I am just giving you an overview of everything that we are going to be covering in this course. Obviously, with each session we are going to separately introduce the topic and go over it in great detail during that particular session. But, this is just again to give you an idea of what it is that we are going to be covering in this course. So, let us talk about machine learning. Machine learning is what we feel is, a primary focus in this course and having covered concepts in probability, statistics and also in with regression analysis should set you up fairly robustly for understanding machine learning. So, many of you might have heard of the word machine learning, come across it in some form or the other or you might have also come across machine learning through one of it is related topics. So, you might have come across data mining, you might have heard of the terms data mining, you might have heard of the term pattern recognition or statistical learning in some cases. Now, all of these are highly related topics, but they are not necessarily the same. For instance, the focus on the machine learning is more focus on the algorithm themselves that are going to be used to convert data into usable knowledge. So, and that is also going to be the focus of this particular course. So, let us talk broadly about machine learning, topic of machine learning is itself broadly divided into two areas, one of supervised learning and unsupervised learning. And now, I am going to give you a brief idea of what we seek to achieve in both these topics separately. So, let us for instance take supervised learning. Before we jump into a definitional understanding of supervised learning, you already saw the first glimpses of what supervised learning tries to do and you saw that when you cover the module on regression analysis. Now, to jump into the definition, the core idea of supervised learning is essentially a task of creating a function or a relationship from training data. So, based of historic data, which has at least one explicit output variable, traditionally this is also indicated as data that is labeled, so that is coming from the computer science camp, where people say the data is labeled. But, what that essentially means if you are not familiar with the terminology, is there is this clear single variable, which I can call as the output variable and I am primarily interested in create a functional or algorithmic mapping between this output variable and one or more input variables that I might have. So, that is supervised learning and we can take the same examples that we were looking at when we were speaking about regression analysis as examples of supervised learning. So, you might have data, where one of your inputs is something like the rainfall and your output is the crop yield or you might have data, which says that your input is a square footage of the house and your output for instance could be the price of the house. Again, there are many, many, many examples we can think of, but a supervised learning is this idea that we have an output variable and your primary focus is to either predict the output variable or create a functional relationship between the inputs and outputs, which can be used or it is useful for the future. Now, within supervised learning itself there are two broad classes of problems. Now, this classification of problems does not mean the algorithms themselves are really different. So, essentially you are, the idea here is that your supervised learning problems can be classified into two broad classes and they called classification problems and regression problems. The word regression means something quite differentiate, it does not mean the exact same thing as a regression analysis, but once I explained this division you will understand better. Classification problems are essentially problems, where you still trying to do what supervised learning tries to do, which is create a relationship between the inputs to the output. But, here your output variable is a discrete categorical variable and more often, the not is a nominal categorical variable, meaning there is no explicit ordering of the classes. So, an example of this could be something like your output variable is either male or female. So, you are trying to predict something and the output variable is not something like the previous example, where we said how much, what the crop yield was. So, how much crop did I get is a continuous variable, meaning 20 kgs or whatever per hectare is a very, is exactly twice 10 kgs per hectare. So, that is a continuous variable, you can get any value between 0 to infinity or negative infinity to infinity. But, with classification problems you are trying to predict based on the inputs as to which class the output variable should belong to and that just means that the output variable is discretized and in all likelihood, it is a categorical variable and it is typically nominal. Now, move on to the class of problems which are called regression problems within supervised learning, that just again quite simply means the output variable is a continuous quantitative variable, such as the crop yield given some amount of rainfall, how much crop are you going to get given the rainfall. The methods themselves are just marginally different and many of the supervised learning tools and techniques are perfectly capable of being deployed in classification scenarios as well as regression scenarios. And, but at the same time there are techniques, which are just suited for one of the two and you need to make some modifications to the technique for it applied to the other. But, we will be discussing this dichotomy as we go through the course also and even as we go through the techniques themselves, we talk about them a little bit. We now move on to unsupervised learning and I am, let me just give you a brief idea of what we will be covering in unsupervised learning. An unsupervised learning is the task of creating patterns from data, which have no explicit measure or signal guiding us. In other words, there is no single variable, which we can call as our output variable. Again, here people say the data is unlabeled, but ultimately if you are familiar with the terminology great, so if you think of it is labeled data for supervised learning unlabeled data for unsupervised, I find it easier to think of it as with supervised learning there is an explicit output variable, with unsupervised learning there is not one or two you know variables that I can just point to once, so say these are the output variables these are the input variables with unsupervised learning you just have the variables. Now, that we have basic definitional understanding of supervised and unsupervised learning. I am just going to give you some idea of what are the tools and techniques that we are going to cover in them. I might not, I am not again, because we are not in the supervised learning class, I am just giving you an overview I am not going to go into what each of these techniques are, but this is more to just familiarize you with the names of these techniques and in some cases, you might have come across or heard of these names somewhere, so I just want to make sure that you have familiar with that. So, with supervised learning we are going to be looking, we would have finished our module on regression analysis. We would be looking a little bit at more advanced methods in regression, modification setup available with regression analysis approaches. We will be looking at logistic regression, which is used for problems of classification, regression styled approach for not predicting continuous output variables, but categorical output variables. We will talk about an algorithmic approach called K NN methods. You might also come across this module on Classification and Regression Trees. It is also called CART, we will be talking about that. Other methods that you will come across are Support Vector Machines or SVMs, Linear and Quadratic Discriminant Analysis LDAs and QDAs, Artificial Neural Networks or ANNs and there are breed of methods called ensemble methods, which kind of use multiple predictors together, so that is also something that we will be covering this course. We do not stop at only tools and techniques, because just knowing the tools and techniques and sometime, some of these tend to be buzzwords, is good in that you know what you might be talking about. But, you also want to understand some of the concepts that go behind, creating some of these techniques and these concepts can be critical to fine tuning some of the parameters that are there in the techniques. So, we will be talking about some common supervised learning concepts called regularization, dimensionality reduction or cross validation and so on and at this point, if you do not understand some of these words, that is fine. The purpose of this is to just give you an idea of, what it is that we will be covering, fine. We then move on to unsupervised learning and in unsupervised learning, there are two major areas that we would be covering. The first is the concept of clustering. You might have heard of the word clustering and that is a topic that we are going to cover in unsupervised learning. And the next, the other topic that we will be covering in unsupervised learning is called association rule mining. So, let me just briefly give you an idea of what clustering is and what association rule mining is. This way you also get a slightly better idea of unsupervised learning. See with supervised learning, you had the example of the regression very concrete example it is easy to imagine. But, what is it mean to do machine learning, where there is no output variable, what is that feel like, perhaps talking through these two will give you some idea. With clustering, the core goal is that it is a task of grouping a set of objects into clusters or you can think of them as group into groups based on their similarities. How are these similarities defined? It is defined across a common set of attributes or features that each of these objects have and again, the easiest way to digest what I just said I will repeat it, is that clustering is the task of grouping a set of objects into clusters or groups based on their similarities and the similarities are defined based on a common set of attributes or features that these objects posses. So, let us take a couple of examples. Now, we understand a definition version of what clustering is, but let us take a concrete set of examples and may be, what we mean by objects and what we mean by features becomes more obvious. The easiest example to think of our customers for a business, so let us say that I am an online retailer or let us say I am a taxi company, take whichever business is close to your heart and let us say I had a database of my potential customers or may be my current customers, either database. The objects here would be the customers; the features are attributes, our features and attributes associated with the customers. So, a simple feature could be is my customer male or female, another feature could be, what is the age of my customer, another feature could be is this returning customer or is this a new customer, another feature could be the actual amount of rupees per transaction spent by this customer each time they come to me. So, these are all some attributes and features associated with the customers, the customers are objects. So, what are we doing in a clustering, what we are doing is we grouping these customers and why would we want to do that, for various business reasons. If I can group these customers, so nobody is coming and telling me, what is the right answer wrong answer, there is no output variable. But, I have taken these customers and now, I have created two or three groups and that could help me in a variety of ways. If I understand that there are only two or three types or groups of customers that come to me, knowing which group a particular customer belongs to. It might help me behave differently potentially to the customer or it might institute certain policies in my business environment based on the groups that get formed in amongst my customers. So, again there are many, many examples. We just spoke about businesses incoming customers for a business, this is been quite prevalently used in biology for instance, where the object here are different genes and the different genes performed different functions. So, these functions tend to be features or attributes. So, can I group genes based on the functions that they performs, so that is one application. There are also many applications in medicines. So, you have a whole plethora of disease and the disease form the objects, but are there certain set of symptoms or are there certain set of responses to treatments that these different disease have and so, can I group these disease based on the attributes, which could be symptoms or their responses to different kinds of treatments. And for instance, a grouping like that might help establish wings in hospitals or medical treatment facilities, where disease of a certain kind get grouped together and people are sent there. This is just thinking allowed, it might be a good idea, it might not be a good idea, but point is clustering can enable you to create these kinds of groups and how a business or an engineering application uses it is more domain specific in that sense. Some of the techniques in clustering that we are going to covering include K NN, so you might have heard it as K means clustering. We are going to be talking about hierarchical clustering, graph based clustering and also density clustering. So, this is just to give you some idea of different types of clustering techniques that we are going to be covering in this course. Let us now talk a little bit about the other major unsupervised learning technique that the course is going to focus on. And this is essentially association rule mining. Association rule mining is essentially this task of identifying relationships between features across a set of objects. So, keep the same object and feature definition that we created with the clustering. With clustering, your goal was to use the features and thereby group objects. With association rule mining you want to use these objects, you want to use these data essentially to create relationships between features. So, let me give you a concrete example and this is a seminal example that introduced in many ways association rule mining and it is called the market basket application. In fact, association rule mining was you know, times also called like a market basket analysis and so on. The idea here is that, let us say you are a point of sales system, you are a super market and your rows or your objects are essentially customers and these customers are come in and they buy some sub set of the products that you stock in the super market. And the super market now represents this whole transaction, where each row is a sale that a customer makes and the columns or the features or these different products that the super market stocks. So, a particular sale will have a stream of zeros and ones, where if I did not take product A I get marked as 0 for that product, if I do take product B, then that is a 1. So, each sale you can think of this stable, where each sale is a row in that table, each column is a product that the super market stocks. So, if a particular sale includes a certain product, then that is marked as a binary, it is binary systems gets marked 1 and if a particular sale does not have that product, it gets marked 0. So, let us say you have this table now, this table could potentially enable you to answer questions of the nature such as people, who buy coffee and coffee would represent a particular column in the table. You could say something like people, who buy milk tend to buy sugar. Because, typically when there is a 1 in my data set, it looks like under the category milk there also tends to be a one under the category sugar in my same data set. So, and this can be extracted to go beyond a one to one mapping say I give you an example, where people, who tend to buy milk tend to buy sugar, but you have lots of other combinations you can say things like people, who buy coffee and milk tend to buy sugar or people, who buy milk almost never buy milk substitute. So, why is this exciting well now, a super market knows, where to keep which product in its setup. So, you can lay things out where if coffee and milk bought, bought together can coffee and milk been next to each other and so on and so forth. But, association rule mining again need not be confined to a market basket context as long as you can again break down the data into simple thing of objects and features that will enable you to perhaps considered association rule mining. But, the important thing is again look there is no one variable that you are targeting. So, it is this is not a classification exercise it is not a prediction exercise of you know, who is most likely to buy sugar, but it is that any variable can be can become the relationship variable. So, there is no strict output variable within association rule mining there are a couple of challenges and we will be talking about that as well in this course. So, the idea of you know creating many complex rules becomes computationally very hard, so what do you do with that. And how do you how do you say a particular rule and when I say the rule of here something like people who buy coffee and milk tend to buy sugar how do you evaluate how good particular rule is in that is some of the core challenges in association rule mining. Finally, we come to the last module in this course overview and this is a module on creating data and we feel fairly strong about this module, because we see that this is a problem that is often faced with many organizations, where their interested in data analytics their interested they see this buzz word floating around sometimes big data and so on. And there a little unsure about, how that applies to them when they just is no data available or they have not really captured it. And we feel like a set of topics in here should help companies or organizations understand this part of the data analytics process better. So, there are three major topics that we are going to be covering here the first topic is on design of experiments. So, you have no data, but you want to take a data centric approach this whole idea of the data driven decision making you want the data to tell you what is the right thing to do. Perhaps the best thing for you to do is to conduct an experiment you try certain options and essentially you explicitly change that input variable in different settings and different points of time and then see, what happens and you use that data that just generated from the experiment to essentially do something like the regression or a supervised learning technique and thereby make decisions. So, design of experiments is would be one way of going forward there this is other really exciting area called active learning, which is the part of the machine learning machine learning words active learning comes about when you might have some data or very little data broader umbrella of machine learning and the idea here is also one of you can think of it as one of experimenting you could think of is one of sequentially quarrying the system. In other and its fairly expensive to gather this data. So, instead of just doing a blind approach of fixing senses all over the place which might be an expensive proposition and therefore, you do not have this data. Is there something we can to do the partial knowledge and can we sequentially quarry this system in what we mean by quarrying the system here is can we sequentially put senses can we sequentially see, which data we want to gather. Because, we do not have lot of data in we do not have enough to make conclusions. And at the same time we cannot just say let us start lets commission data gathering exercise only because it is hard to do that. So, given that we have fixed resources towards gathering data active learning is an area, where you sequentially go and gather data, but you only gather the critical data that you need in order to mine it or in order to process it for coming up with insides. Now, the third area that we will be covering in this section, which is creating data for data analytics is the area of reinforcement learning and this is also reinforcement learning is also subset of the larger machine learning umbrella and most specifically in the reinforcement learning re now, we are going to focusing on series of problems called the bandit problems and the context here is that you do not start with you do not have to start with any data or you might have some partial data. But, you just cannot go about experimenting to create data for a verity of reasons one may be cannot create this kind of lab setting which might be needed to conduct the experiments in create data. But, more importantly it is also possible that you cannot experiment only, because it effects the end user in some way you cannot essentially go off line and do your experiments. So, here you are do not have any data you want to try something is right because you do not know of what you doing is the best thing. But, you cannot just commission in experimentation exercise to create the data, which then gets analyzed, and then tells you what is the best, because when your experimenting you might be doing some horrible things and those horrible things might affect an end user. So, the whole idea behind banded problems is you can think of it one way of think it it is a form of experimentation and learning in an online setting. So, you do not only care about how much you are learning from the data, but you also care about how value of performing. And in fact, the experimenting itself becomes consequents because your grand objective in the banded problems tends to be one of performing as well as you can over sometime horizon. So, because you need to perform as well as you can, which is defined by some notion of how you do cumulatively you wind up a trying a few things out just, so that over time you are not continuously doing something that is not in your best interest. So, these are three these could be fairly useful techniques or tools to use when you are in an online setting or when you are in a setting were you do not have much data. Finally, I just wanted to mentioned that in addition to some of these topics that we have discussed we will also going to have some a couple of modules on major challenges for big data analytics and what I guess big data analytics its of means in the world today. And we are also going to be talking about some of the more popular techniques contemporary techniques like deep learning especially when we cover concepts and artificial neural networks and so on. So, with that we conclude this second session of the course overview and starting next session we would be directly diving into the content itself and I mean we cover the content today. But, again the spirit of it was to give you an idea of what it is that we are going to be covering in this course and I hope you found it interesting and I look forward to having you join us in the next session. Thank you.

4) Descriptive Statistics - Graphical Approaches
Hello, and welcome to the third lecture of the course Introduction to Data Analytics. My name is Professor Nandan Sudarsanam and today, we are going to be talking about Descriptive Statistics and more specifically about Graphical Approaches used in descriptive statistics. Now, before we jump into the content into descriptive statistics in the different types, it make sense for us to take a step back and talk more generally about data, what is data and most critically, what are the different types of data that you will encounter. And it is important to do this, because the descriptive statistics that you use in the approaches that you use vary according to the different types of variables or different types of data that you will encounter and this is a recurring theme in many other aspects of this course. So, make sense for us to talk about that for a few minutes right now. So, data is essentially numbers, now it can also be texts, symbols, but more often the not, you will encounter numbers, which represents some kind of information. And therefore, it is a kind of helps to think of data as values, because values is broad enough to cover numbers, text, symbols. So, you can think of data as values of quantitative and qualitative variables. Now, the variables themselves can be of different types and that is, what we are going to talk about right now. In statistics, you have various classifications of variables and so different depending on, who you ask, you find different classifications in different text books as well. But, one broad classification that make sense and that is very useful for us is to differentiate variables as quantitative and qualitative variables. So, quantitative variables are also called as numerical variables and these variables are essentially, you know the best way to think of them is that they have meaning as a measurement, such as a personÕs height or weight or IQ or they can be some kind of count, such as the number of something number of days it is rained and so on and so forth. But, a very intuitive way for me to, that is always been useful for me is to think of quantitative variables as variables, where some form of basic arithmetic like either adding or averaging or subtracting kind of make sense. So, I mean a definite requirement is that the quantitative variable is numerical, but some time you can also have numbers being used as symbols not as the actual numbers. So, a really simple rule that kind of helps me identify quantitative variables is to say that it has to be a numerical variable, that the values that the variables takes on are numerical and that, some basic forms of arithmetic kind of make sense on such a variables. Now, within the quantitative variables you have continuous and discrete variables. Continuous variables are essentially one square within a certain interval and this is an interval that the variable could, where the variables could take on values. Within this interval any value is possible, if any value within this interval is possible, then this variable is said to be a continuous variable. So, let me give one example. Let us say that the variable we are interested in is the height of students, who are registered for this course. And so, let us say I go take a sample of a 1000 students and write down and their heights and so this is a data set that I have. The data set have 1000 values and let say an interval and the interval can, you can may be get the interval from taking the highest value to the lowest value or you can just, as long as it is a real interval that covers this data. The question is, is any value possible within this interval and the answer is yes. So, let say my interval was 120 centimeter to 140 centimeters, is it possible that someone, who taken this course could have a height of 135 centimeters. Absolutely, there is nothing about heights that inherently stops people from having that particular height. Now, I can go even further, I can say is it possible for someone to have the height of a 135.156 centimeters and the answer is still yes. I mean I might not have a measuring scale that goes to a certain accuracy, but that is the measurement problem. There is nothing about heights that prohibits this value from existing in this data set. So, continuous data is essentially one, where any value between certain interval and the interval is sometimes formally defined as the highest value in the data set to the lowest value. So, any value within this interval is potentially possible, then you have a continuous data set. The second kind is the discrete, so the discrete quantitative data is one, where this condition is not true essentially and it again helps to think of, what kind of a data set would be such that a value not all possible values are there and I will give you another example for that. So, let say that I was interested in looking at the number of people, who enter IIT Madras every day, so the number of people, whoÉ Let us make it interesting, the number of unique people, who enter IIT Madras every day. So, if you come in and go out, come in go out and times, I do not care, you are still one person. So, number of unique people, who enter IIT Madras on a given day is my variable and the actual values of this variable I get from doing this kind of a survey or a study for 1 year. So, I have 365 data points, one data point for each day, which says the number of people who enter IIT Madras. Now, clearly this variable is discrete, because let say there is a lower bond, which is may be 0 people, nobody enter the IIT Madras highly unlikely, but on a given days. So, zero is the lower bond and the upper bond is some, you know 100,000, 50,000 something. Now, within this range, can I have is every value possible, no. You could for instance never have a day, where two and half people entered or let say, you know 133.2 people entered. So, here is discrete, because it is discrete in the sense that only the integer values are possible, all the more; no, where you can never have values that are non integers. So, that is an example of a discrete value and in this particular case, it happens to be one of being discrete at the integers, but that is just, because of the example I came up. You can come up with the other examples, where the variable is numerical and it is I mean its quantitative, but the values you can take up are discrete. We move on to the other class of variables, which are qualitative variables, these are also known as categorical variables and categorical variables essentially represents some characteristics, some characteristics, which can be categorized, which can be grouped. So, examples of that are things like a person gender, so that is the variable and the values of variable can take a male and female. And then, you might have something like marital status or more interesting, one might be home town of or state within India. Let say, let us take all the people who registered for this course from within India, which should be bulk of them. And the variable we are interested in is, which state are you from, so that is the variable the state that you are from and the values that this variable can take up are the different states of India. Now, the categorical variables again, because of their definition and their nature are always discrete, so that should be obvious. But, within these categorical discrete variables there are two classes again, there is nominal and the ordinal. With nominal, there is essentiallyÉ The big difference is that, with nominal there is no order. So, the great example of that would be this home state, which state are you from, variable. There is no order, which says that Madhya Pradesh is greater than or lesser than another state and so on and so forth. So, these are all, the values that these variables can take up cannot be ordered in a sensible way you know, whereas with ordinal data by definition of the variable, there is an order. Let me give an example of that, let say that we created a variable for, which is the color for terror alert, so some countries have this, the terror alert color signify something. And let say the possible values this can take is green, yellow, orange and red. So, the variable stated terror alert color green, yellow, orange and red are the four values that this particular variable can take, where green represents low risk and red represents very high risk. See, so there again it is a categorical variables, because it is not like you can do arithmetic operation on green, yellow, red. The variable itself is a qualitative variable, but yet there is some order. Because, you can say things like if orange is worst than yellow and yellow is worst than green, that must mean orange is worst than green. So, you can get an idea also, that is an ordered categorical variables, whereas with a nominal variable you could not say if Madhya Pradesh is greaterÉ First of all, you could never say greater or less than, so creating more complex relationships becomes impossible. So, that is just to give you a very quick idea about the different variable types. So, now, let us jump into descriptive statistics. So, descriptive statistics is the idea of quantitatively describing data and you can do that through various means, you can do that through visualization techniques like graphical representation, tabular representation, but you can also do that through summary statistics. The idea here is that, you crunch the data, you work with the data and come up with 1 or 2 or 3 or 4 different numbers that summarized the data for you. In this class we are going to be focusing more on the graphical and the tabular representation and the next module is going to be on the summary statistics, so that is the idea. Now, this is a very good time for us to just quickly review, you know in our overview classes we spoke about descriptive versus inferential statistics and this is the good point to just bring that up again and to kind of have a very quick idea, what descriptive statistics are really means. The core idea in this dichotomy is that descriptive statistics focus or is the way to say something meaningful for the data that you have at hand. So, you have some data at hand, whether you call it sample of population or whatever, if you are making the statement based of that data about that data derived from that data, you are dealing with descriptive statistics. Descriptive statistics do not; however, allow us to make conclusions beyond the data we have. So, you cannot look at the data, do something with the data and make and based of that make the generalization about potentially the source the data that, the data came from, you would need inferential statistics for that. Now, having said this; however, descriptive statistics is still very important, because you cannot just simply present raw data, it would be very hard to visualize, especially when the data is a lot. When you have a lot of data, you cannot just show the data, you need to present the data in a more meaningful way, which allows for some kind of simpler interpretation and that can be through the graph or through numbers, great. So, and a final point I just want to make is that, descriptive statistics is not just confined to a single variable, it can be about multiple variables and when you are dealing with multiple variables, our topic of interest is relationships. So, how does one variables change with respect to another variable. So, in essence you will be doing two things which is summarizing each variable or describing each variable, but you are also interested in showing interrelationship between variables. So, let us go ahead and now, that we have an understanding of different variable types, let us talk about some graphical representation techniques. If one is dealing with a single variable and let us say it is a categorical variable, a great way of representing data could be through a bar graph. So, that is the graph that you see on your left hand side here. So, here for instance let us say the example is one, where we sent out a survey and ask people, what their highest level of education is and highest level of education be the variable, the possible values that takes up our high school, bachelorÕs, masterÕs and doctorate. Hence, therefore, this is a categorical variable, there are only four possible qualitative states, that this particular variable can take up. And, what we plotting is the number that we, number of responses or the number of observations we counted in having this values. So, you sent the survey out let say it about 50000 people, may be 60000 from the local way. So, and about 15000 of them said that their highest level of education was high school. So, the height represents the frequency about of occurrences of this particular value of this variable. So, this kind of a representation can quickly summarize, which is more which is less and so on. But, an interesting thing to note out here is that this categorical variable is actually ordinal, meaning there is an order of going high school, bachelors, masters, doctorate. You could have flip the whole graph around, but you would still have the order that is a sense that a doctorate is a more years, I guess than masters and which is more than a bachelors, which is more than a high school or whatever. So, in some sense the variable itself has an intrinsic ordering and the good thing is something like a bar graph, I love for that. Just, because of the fact that there is this concept of a x axis, makes it very convenient to represent ordinal variables, which are categorical. Another way of representing categorical variables could be something like a pie chart, this is an example, where let say we looked at the number of students, who were in different engineering departments. And your different engineering departments here are mechanical, civil, electrical and computer science. These are just some random departments I chose and again the frequency of occurrence is more represented as a percentage of the whole. So, this percentage of this full circle is computer science students and thus the idea behind using the pie chart. And clearly a pie chart is not very suited for ordinal variables, which is more suited for nominal variables. Because, there is no order, one that the fact that computer science shares the wall with mechanical and civil is just coincidental, that is not what a pie chart seeks to capture. Sometimes people will keep similar things together, but that is not a requirement. One important thing about pie charts is that, usually you want to represent all the values. So, if there are some engineering departments that are not being represented, usually a pie chart need not be the best way, because there is an impression that this is all the departments together. So, if there are more engineering departments, but you wanted to only show 3 or 4, may be you could use a bar graph rather than a pie chart. Now, we move on to quantitative variables and with quantitative variables, you have a couple of different ways of representing a variable. One example is a box plot. So, with quantitative variables, remember that is numerical data and, so you might be interested in representing things like, what is the average, what is the variance and in our class on summary statistics, we are going to go to a great detail about it. But, for purposes of this, a box plot is essentially something that captures central tendency, which is this red line that is there in, typically that tends to be the median of the data set. And you have the two bounce of the data set, so the top and the bottom of the box itself and that tense to capture in some way the variability in the data and the way a box plot does that by representing the lower quartile and the upper quartile. Now, the lower quartile and the upper quartile in really simple words is just 25th percentile and the 75th percentile and, so that kind of that range a gets captured there and the whiskers themselves take on different meanings depending on the, which version of box plot is using, but more often they are not it tends to be lowest value and the highest value in the data set. And typically red dots like this represent outliers in the data. The box plot is itself something that will make more sense to you and we will talk about summary statistics, because you will understand, what exactly a median means you will understand, what a different way of representing spread an outliers and so on. But, it helps you at this stage to kind of say that this is one simple way to take a data set, which is full of numbers. So, let say this data set had you know 500 or 1000 numbers and it looks like these numbers are pretty much within the range of like 25 to 33 or, so and to represent all of these numbers in a single graphical representation, so great. Another way of representing quantitative data is through a histogram the histogram is what you see on the right hand side and histogram is arguably ah the richest representation of numerical quantitative data, because histogram essentially says how many data points do you have with in this range. So, the x axis out here represents the different possible values that this data set has. So, if you take this as 8 to 10 this should be something like 8 to 8.66 and this should be till like 9.33 and this should possibly be 10. So, the question is in your data set how many data points do you have that have values greater than 8 and values less out here. So, another way of showing that I am going to try highlight it is, so here interested in this range right here this range. So, this is 8 and this is 8, let say 8.66 is from reading the graph right, how many data points do you have. Because, this is a numerical quantitative data set that have values greater than 8 and less than 8.66 and the answer to that question is it looks like 6 data points right approximately may be 7 may be if I am reading graph correct. So, you answer that question for each of this bins this are all called bins each of this columns are called bins for each of this column you answer that questions and what you get is histogram and the histogram is the first step towards empirically constructing, what you will we will later learn is it distribution. So, once you capture this, this entire picture out here you have a very clear representation of the entire data set. So, again just keep in mind that we are going to be talking about distribution we are going to talking about medians means and variances, but keep in mind also that this is the graphical way of representing these things. So, now we move on to the multiples variables and the last section in graphical representation and there are three major of forms of representing ha this data and they are the following. The first is scatter plots this are very useful when you have two quantitative variables that is you know. So, two variables, which are both numerical you can you can very easily represent using scatter plots and, so in the key thing you should notice in this scatter plots is that it really helps you understand the relationship it does not do a very good job of understanding each individuals variable. So, may be if you done distribution of x and distribution of y separately you would understood those two variables, but what it does a good job is of capturing the relationship between x and y. And this particular case the fact that in general when x increases it look like y also increased or y also high or you know you can always flip it the other way around in general when y is high x is a high when y is low x is also low. So, that relationship gets captured for that reason this is the great graphical representation of two variables usually you can extend box plots if you feel like one of your variables is categorical and the other is quantitative. So, you are not just interested you are interested, let say one variable, which is country and the other variable, which is some indicator let us say of the economy or crime or whatever I have just called it values here because it does not matter. But, this variable is continuous right its mean I should not say quantitative I do not know if it is continuous it could be continuous or discreet, but this variable is quantitative, where as this variable is qualitative. So, one great way of look comparing different qualitative variables, which have data set that are on the quantitative scale is to perhaps use multiples box plots on the same graph and that gives you not just an idea of how on average his country is different, but there are also different in terms of their variability and their out so on, so forth. Finally, we come to the use of contingency table out here on the extremely right and the idea here is that when you have two categorical variables and what you are interested in representing is the frequency of occurrence. So, the frequency of occurrence is the theme of the data set. Then, contingency tables are great ways to do that, so an example that I have come up with out here is how many people let say you go to a company and you take a survey of all the managers working in the company may be interested in asking how many of them have MBA. So, y represents yes they have an MBA n represents no clearly this is categorical variables right it has only two states. Similarly, you could say before this people join the company did they have work experiences before they joined as managers and answer could be again be yes or no, so that is also categorical variables. So, here is an example why you have two categorical variables and what you are interested in his how many people belong to each combination. So, how many people have MBAÕs and had work experience before joining how many people had MBAÕs did not have work experience before joining. So, this can be complex data set where it very neatly summarized in the contingency tables and that is something that could be quiet useful. So, I think that is about it for graphical approaches to representing data in the modules descriptive statistics and in the next class we will be looking more at summary statistics as a means of as the sub modules in descriptive statistics. And then, we will move on to inferential statistics great, thank you for joining me and look forward to seeing you in the next lecture.

5) Descriptive Statistics - Measures of Central Tendency
Hello and welcome to our next module in the course, Introduction to Data Analytics. In this module, we continue our previous work on Descriptive Statistics and weÉ In our last session, we spoke about descriptive statistics through the use of various graphical and virtualization techniques. In this module, we start with the use of summary statistics or the idea that you can describe data with numbers, with numbers that summaries the data and most specifically, we are going to be talking about measures of central tendency in this lecture. So, just to jog your memories we spoke about the idea that there could be a data set and a data set essentially would be representing, could potentially be representing a particular variable and so, I provided for you a simple example of a data set. And this data set is what is been captured in this histogram. The histogram is a visualization tool that we spoke about in our last lecture. Now, the histogram essentially is a very rich representation, because it not only captures just some parameters associated with this data set, but it captures various new answers associated with it. So, just to give you a quick reminder on how this works, essentially the entire x axis breaks down possible values that the data sets could take. So, for instance this bend is the series of values between 10 and from the looks of it 10.66 on this side. Now, depending on the number of data points that you see here, that fall within that range of 10 and 10.66 that would get counted here and out here it looks like that is about 15 points. So, that is essentially how a histogram is calculated and the idea is that, using a histogram you could then fit something called distribution and the distribution is this red line that is shown on top of the histogram. And, so in some sense the histogram and the distribution that some time follows, tells us the full story associated with the data. And usually this red line is represented through some kind of a formula and we just call that, let say f of x for now. But, the basic idea behind summary statistics is that you do not even need to go this deep, I gave you this full picture just to tell you what the richest or the most detail story could be and our next sessionÕs, next modules are going to be about distributions. But, now let us take a step back. Is there something simpler that we could do? Is there something simpler without even fitting this distribution or even creating this histogram that could tell us a part of the story? And the answer is yes and there are these various summary statistics that do exactly that and I am just going to talk about a few of them now. The first and the most common one are these measures of central tendency and what they mean is that, you have this data set and for now, let us just occupy ourselves with the histogram not the distribution that is fit on the top of the histogram. But, essentially with this histogram, what is a fairly central value. So, it is clear that the values of this distribution go from here to about here, but what is something in the center and how do you define that. So, one idea is often to say, well one measure of central tendency is to see the minimum value going all the way to the maximum value and take something that is in between, so that is one way. Another way could be to say, at what point in this histogram am I really covering about 50 percent of the area. So, this histogram is defined by these blue bars and at what point am I covering about 50 percent of that area, so that could be one way of saying, what is central. There are other innovative ways, one of the most common one is to say think of this as a balance, as a sea saw essentially, this x axis line and all these blue bars are weights on top of it. So, the idea is, where would you want to put a fulcrum, such that this whole thing balances. This does not, one does not tip off the sea saw is essentially in balance. So, that is the core idea behind measures of central tendency, what is a central value. Now, you then have measures of dispersion and the idea behind measures of dispersion are, that so you might have something that is central value. But, how a data point actually dispersed around the central value? Are they are very far away from them or are they very close to it and so on. And these are the two major forms of summary statistics that we will cover in this course and that you are likely to encounter. But, you might have also heard of the concepts of skew and kurtosis and so, it just briefly what mentioning it, skew concerns the shape of this distribution itself and the like, the fact that sometimes distributions lean to one side versus the other and this is the fairly colloquial way of saying it. But, instead this distribution the red line being the way it is could you have a distribution that looked like that, which means it is leaning, the whole thing is leaning to one side. Again, I am just giving you conceptual feel for it, it is not a formal definition, we and then, in the same line kurtosis is the idea that how fat are the tails of the distribution and what we mean by that is, you know having a similar distribution that looks like this. So, that tails themselves are fatter than the other and that property gets captured with kurtosis, so great. So, this is, this should I have given you some a brief overview of the different, the over hatching idea of summarizing statistics through mean, describing statistics through summary statistics or numbers as a means of describing distributions. We now, go into the major subject of this lecture which is measures of central tendency. So, the best way to do this is through a concrete example and we, that is what we will do with this step. So, there are three major measures of central tendency and they are the mean, median and mode. With the mean, the core idea and many of these you might have encountered, you might have come across before. So, bear with me if you already heard this, the idea behind mean is just that it is the concept of average. If you have a data set and I am just given you a sample data set out here and, so it is the numbers 3, 4, 3, 1 and I have kept the data sets smalls, so that I can illustrate the concept typically might be dealing with much larger data sets, but the idea remains the same. So, for this data set the mean is nothing but, the sum of all the numbers divided by the total number of a numbers there are. So, we take each numbers 3, 4, 3 and we add them all up and the 12 that you get out here is the actual number of numbers that there are in this list. So, once you divide it and that is the concept of mean, which can also be represented mathematically in this form and I have just shown that, you use of that, when you see that it is, you not surprised by it. So, great and incidentally the mean is the concept that I was speaking about in this histogram of balancing the seesaw, where would you place the fulcrum; such that this seesaw gets balanced, that is the same concept of mean. We now move on to the next measure of central tendency, which is called the median. The median is calculated by arranging all the numbers in order. So, you had this data set and it was 3 comma 4 comma 3 that is what you had out of here, but when you bring it to median, you basically takes the smallest number put it first and then, in some order ascending or descending you arrange the numbers. Once you do that, you choose the central number and that is your median. Now, choosing that central number is quite easy when you have an odd number of numbers, so if you had 9 numbers the 5th number would be the central number, which you have 4 numbers before and you have 4 numbers after words and that is your central number, but when you have an even number of numbersÉ So, in this particular case we have 12 numbers, the central number is really not 1 number it is 2 numbers. So, at here it winds up at being a 6th and the 7th number. So, typically there you choose the 6th and the 7th number and take the average. In a particular case that is not a problem, because it happens to be the same number and quite easily we say that the answer is 4. The mode, which is a third measure of central tendency and there might be a few others, but these are the three most common ones that you will encounter, the mode essentially says what is the most common value. So, if you look at this data set, the number 3 appears 3 times, the number 4 appears twice and then, all the other numbers just appear once. And, so out here it is fairly clear that the number 3 is the most common one and hence 3 is the answer if you, if the question is, what is the mode. It is the most common number, but and that kind of make sense, if you have a data set, where there are only few numbers that are recurring, but the concept is again generalizable . So, even if you had a data set that look like this, where numbers you know, it does not make sense to ask the question, which is the most common number in fact, no number might repeat itself. But, out here the more essentially is based on the range itself. So, you would say this is the most common range and so this is the mode and so the mode out here would have been something like if 9.5 to 9.6, so great. So, we now understand how mean, median and mode are calculated. Now, let us take a step and see, where do we want to use which measure of central tendency. So, how do we choose between mean, median and mode? The main concept really comes between mean and median. So, let us talk about that for a minute and becomes kind of obvious, where mode is more useful, because it has a very different property associated with this central tendency, so great. If you have to choose between mean and median, much of the debate usually comes down to outliers. The idea of the outliers is that, it is a number or a value that is not really within that set of most of the other numbers that you see. Now, that can be, because of quite of few reasons. When this come about because of an error in the data, so the data set itself could have an error, then it is easy to say that is around you, so this is a bad outlier. But, sometimes this state can be that outlier is very much not an error and it tells an important part of the story. In really simple words, the median is not influenced much by the outlier, whereas the mean is greatly influenced by the outlier. For that reason, the median is often kind of expressed as been a more robust metric to outliers. But, that we need to take a step back, just a second about saying you know outliers can either be good or they need not be good and so, it really depends on what we think about the outliers, ask to whether we choose to go with the mean or median. Obviously, when we think the outlier is a bad think, it should not be there or it is not contributing towards a story that we want to tell. Then, we call it a bad outlier, we prefer the median in that case. So, to actually give you some insight as to, how the outlier affects the mean and not the median. Let us just go back to the previous example. Let us say that in our data set instead of 8 we had 800. So, that is clearly a mistake. Someone by mistake type two 0Õs next to 8 and, so it is 800 and let us assume it is a mistake, it is not obvious. Now, in the case of the median, this would have a huge impact instead of 8 being here you would put an 800 and that would greatly change this number. So, your mean is largely affected, it probably send the number into the 100Õs. Now, it will have no impact on the median, this 8 gets listed, it becomes 800, which means that it is not in it is place, it comes after 9. So, make some space here, so the 800 comes here, but the central two numbers still remain these two 4s, I mean these two 4s, so your answer really did not change. So, in many ways the outlier has like this huge impact on the mean, it has no impact on the median. Now, clearly if what you have faced with this kind of an error, you like using the median, because the mean is susceptible to this problem. There might be other situations, where you want to use the median and again it pertains outliers, but here we are not as much scared about errors, but you are scared that there is this one a typical case, which is just skewing a story that I want to tell. So, a classic example of where medians are used is, when looking at salaries of people, where the idea is that salaries having some sense of exponential. Many people earn a consistent salary and then, there is these few peoples who just earn these catastrophically high amounts and so something like mean, where and here the idea is the catastrophically high amounts are these outliers. And here talking about a mean will not give you the typical salary that a person earns, because of these one or two people who earn very high salaries. So, it is not just errors there might be other situations, where you have outliers and you feel like the presence of these outliers is moving you away from talking really about, what is a typical value, now having said that there are many situations again, where you are dealing with outliers, but these outliers are of very important part of the story. So, let me give you an example of this. So, let us say you have this data set, where you were looking at a particular financial strategy. And in this financial strategy you are looking at how much money you made on a daily basis and, so you have taken some historic data and you want to see you want to see, what is a typical scenario of the strategy you want to evaluate the strategy based on based on this data set. So, let us see this financial strategy actually made you lose 1 rupee every day on 99 percent of the days, but on 1 percent of the days this strategy gave you 10 crores, so large enough number. So, this strategy made you lose money on 99 percent of the days and on 1 percent of the days gave you 10 crores is this strategy you would like to take the very straight forward answer is if you like making money you really like this strategy, because despite the fact that you lose just 1 rupee on 99 percent on the days as long you as you can play this game or you can trade on this strategy in a stock market for long enough period of time here bound to in the long run make good amount of money. Because, 10 crores more than compensates for the 99 days or during, which you lost the 1 rupee. Now, let us see how mean and median would have represented this data right if you got a sufficiently large enough data set of having actually play the strategy. So, let us say you go and you actually collect your data set of the strategy over a 1000 days or less than 10, 1000 days, what would be the median of this strategy the answer is said the median would have been the minus 1 that you that the 1 rupee that you lost. So, quite simply, how is that on that data set for this would look something like this. It would look like this minus 1 comma 1, 1 comma minus 1 comma dot, dot, dot, dot, with the lot of minus 1Õs 99 percent of them are minus 1Õs and then, the odd time you are going to find this are really large number I am not even going to talk about how many 0Õs lots of 0Õs dot, dot, dot for the 0Õs. So, you put this in ascending order and you choose the middle value that is going to be a minus 1, so the median gives you a minus 1. However, you put these numbers you add all of these numbers up together include put your 10 crores in there with lots of 0Õs and then, divided by the total number of data points, which is you know 1000 or 10000 or something whatever the number of data point you have and you going to get a very large positive number positive and large great science. So, here is a story where yes youÕre not you have an outlier you got this huge outlier which is 10 crores. But, the story was in the outlier that is as much real money that you made or lost as the 1 rupee is that you lost. So, here is the case where the mean is probably a great measure to go by if you had to choose whether to play this strategy or not. So, I have that gives you some idea between mean and median, now let us talk a little bit about mode is an interesting one, because it just blank it says I am going to take, the value, which is the most popular and that works fine for you know distributions, which are fairly symmetric. But, in many cases that that people do not find that too meaningful the one big advantage; however, that the mode has is that you can even use a nominal variables. So, you might have a situation, where you are just counting the number of you are coming up with the count associated with a categorical variable an example could be in the number of reds the number of greens the number of yellows and all the mode is going to do is say lets pick the one which has the most number of it and it can also be fairly useful in multi modal distributions. Let me give you an example of where a multi modal distribution and multi moral just means that there are many peeks to the distributions. So, if you go back to this slide and let me just erase that for you. So, the red line is the distribution and we going by the we going talk a lot more about distributions multi modal distribution is one that might look like this, so there are like two peeks to this distribution. So, let me give you an example a real life example of where, you could have a multi modal distribution and, where you might want to use the mode. So, let us say you we looked on this street and this street was a 100 meters long. So, one end of this street is 0 meters and then, there are markers on this street. So, this 1 meter, 2 meter, 3 meter and the street goes all the way to 100 meters. So, if someone said this 75 meter you immediately knew, which point of the street or road we are talking about. So, all the residence of this street need to make a decision, on where to place a garbage can a trash can, which lets say for whatever reason people do not people have strong opinions of that. So, people are all going to go or we going to take we are going to take a survey of all the residence and each ones going to come up with the number. So, person one says I want the garbage can in the 25 meter mark another person says I have want it in the 50 meter mark so on and so forth. Now, let us say we collected all these data and we found that 40 percent of the residence said they want the garbage can in the 25th meter mark. Let us say another 40 percent or let us say 45 percent said they wanted the trash can on the 75th meter mark. So, just to recap 40 percent of the people say they want the trash can on the 25th meter mark you know 45 percent of the residency they wanted in the 75th meter mark and the remaining 15 percent they just its all over between its like uniform somewhere between 0 to a 100. Now, the problem is both mean and median might windup saying the average preference is to keep the trash can somewhere in the 51 52 meter mark, because that is bound to be a central value. Now, that might be something that nobody wanted, where as something like a mode would just categorically say keep it in the 75th meter, because that is the most populist preference. So, in case is, where kind of taking two extremes and averaging them out and in some sense median also does that as long as there are enough data point does not work and in those cases the mode could be fairly useful application. I have just gives you an idea of the difference measures of central tendency. In the next lecture we will take up measures of dispersion.

6) Descriptive Statistics - Measures of Dispersion
Descriptive Statistics: Summary Statistics: Measures of Dispersion Hello and welcome to our course Introduction to Data Analytics. In this lecture, we continue our work in Descriptive Statistics to give you a timeline of where we are. We have discussed within descriptive statistics, the various graphical and visualization techniques and the second half of the descriptive statistics deals with Summary Statistics, The use of numbers to describe and summarize data. Within summary statistics, in our previous lecture we spoke about Measures of Central Tendency. In this lecture, we will be concluding the use of Summary Statistics with discussion on Measures of Dispersion. So, we should be at this point fairly familiar with the use of this data set, essentially the data set is just a sample, which talks about different data points over a certain range and the histogram that you see to the right hand side of this is a histogram that is generated from this data set. So, we covered histograms during our lecture on graphical techniques and we use them extensively in our discussions of measures of central tendency. Again to be very clear, measures of central tendency and measures of dispersion and the specific matrix that we would discussed in them. So, for instance in measures of central tendency we spoke about mean, median mode and today we going to be speaking about some matrix associated with dispersion. All of these matrices don’t in any way shape or form need this histogram. They directly operate on this data set and in some sense, you might say why even talk about the histogram. And the idea is that you are absolutely right, we do not need this histogram, but it really helps to explain the concepts and it is also healthy way to start thinking about these matrices and start thinking about these distributions. It will help us also in the long run, when we cover concepts and probability distributions. So, now, having said that let us talk about what measures of dispersion seek to capture. We spoke about in the last lecture, how measures of central tendency try to capture in some sense a central value; some central value within this range of values that this data set takes up. So, the range of values of this data set takes up is shown here and in some sense, the histogram captures their likelihood in this axis. So, the range is here in the x axis and the y axis is in some sense, the likelihood of seeing that data and we spoke about, how measures of central tendency try to captures, what appears to be like a central value and we spoke about different matrix that do that. Measures of dispersion talk really about, how the data is dispersed around this value, how does the data deviate from this value. For instance, if every single data point and let us for now assume that 10 is our measure of central tendency. One measure of central tendency is the mean, so for now let us just say we are using the mean. So, if 10 is the mean, then if every single data point in this data set was equal to 10 and it is not, what is here, but it is every value is equal to 10. Then, there would be no deviation of data from 10, you would just see a single tall line in this histogram and none of the sides, none of these would exists all together, but that is not the case. Typically, most data sets, the values are going to be different and you might have some measure of central tendency, but there is going to be some amount of deviation of the data points on either side to the central value. Now, measures of dispersion try to capture that, do the values deviate a lot from the center or do they deviate very little from this center and that is what measures of dispersion seek to capture. To understand the different measures of dispersion, let us go back to the data set that we were using, when we were speaking about measures of central tendency. So, I have used the same data set out here and the simplest measure of dispersion is range and the range is quite simply nothing but, the highest value minus the lowest value. So, in this particular case the highest value is 9, the lowest value is 1, so quite simply 9 minus 1 is 8 and that should be intuited for you. The given that the mean of this data set is about 4.5, 4.6, a measure of dispersion is just the max minus min. Now, if for instance if there was very low dispersion, then the highest value would be close to 4.6 and the lowest value would be close to 4.6 and so that, range between max minus min could have been smaller. At the same time, if this dispersion is very high, on the high side and on the low side your max and min value is going to deviate a lot from the 4.6 and so, you would have high dispersion. So, that is the simple one. This second one is the Inter Quartile Range and the idea here is highly related to the concept of median, where you would arrange the data points and you would kind of take a central data point. Another way of saying that, we discussed that procedure of median during measures of central tendency, but another way of thinking of it is that, you are taking the 50th percentile point. With the Inter Quartile Range, what you are doing is you are taking the 75th percentile point or the 3rd quartile and subtracting from at the 25th percentile point. The idea being that within this data set if there is a high level of dispersion, then that range between the 75th percentile point to the 25th percentile point also be high and if the dispersion is low, then this range would be low and it is really noteworthy that this is the concept that gets captured in box plots, which we discussed in a graphical techniques. We spoke about, how in the box plots the upper line of the box plot and the lower line of the box plot, correspond usually to the third quartile and the first quartile of your data set. So, and this is also known as the Inter Quartile Range. So, it might be abbreviated to IQR in some text books, but this is also a measure of dispersion. We then come to, what is a fairly popular measure of dispersion and the idea behind this is to essentially look it, how much each data point deviates from the mean that you just calculated. So, x_i represents each data point, because i goes from the first value to the n_th value, when in a particular example n is 12. And, so we wanted to take each data point, see how much it deviates from the mean. In a particular case for this data set, the mean is 4.58. So, we will take the first data point which is 3, so the data point 3 and we subtract them from 4.58 and square that value. We would take the second data point to the same thing and we would keep adding up these squares and once you add up these squares, you take something that kind of looks like an average and it is not an exact average, because you have this minus 1 and we will talk about that in a minute. But, in concept you essentially are trying to get an average of the square deviation and you will ultimately take a square root of this. Now, when you take the square root, what you get is the standard deviation and when you do not take a square root, you get this measure called variance and variance is also a measure of dispersion. In concept, the only difference between standard deviation and variance is that, 1 is the square root of the other. Again, now that you understand how a standard deviation is calculated. Let us go through some questions that might have come up, when we discuss standard deviations. Given that the other two methods that we have discussed are of fairly straight forward and clear. So, here is some questions that always go with standard deviation. Why do we use this square function on the deviations and what are it is implications? So, what we are referring to here is the fact that, we actually take the square of the deviation. So, why, what is the purpose? If you want to calculate, see if you want to get some measure of average deviation, why not just take the deviation and take the average of it and the answer is fairly straight forward to that. The answer is that just by definition, because you are looking at the deviation from the mean, there are going to be some points that deviate from the mean on a positive side, there going to be some points that deviates from the mean on the negative side. So, 3 minus 4.58 would lead to a negative number, whereas 9 minus 4.58 would have been a positive number and again by definition, because of how you calculate a mean and the math for this is fairly straight forward. You will find that if you just took the deviations, some positive numbers and some negative numbers and you added them up, you would always get zero and that is because of, how the mean is calculated, because the mean is nothing but, the sum of all the numbers divided by the total number of such numbers. So, by definition just taking the deviation would result in some positive numbers and some negative numbers, which should cancel each other out and give you zero. So, what you really trying to capture is an average deviation, but you do not want the signs. So, what is one great thing you can do is to square it all. So, when you square a number, whether it is negative or positive, you always get a positive number and the other really interesting thing is the, only thing that matters is the magnitude. So, minus 3 square is 9, which is also the same as plus 3 square. So, the idea is that the square function is symmetric on the plus minus side and always gives you a positive number. So, for that reason we use the square function. Now, are there some implications of that and the answer is, yes there are some implications. The implications is, the effect that squaring has. So, let us say you had two separate deviations of one unit each, so let us say you had two data points that signified that there was a deviation of one unit. So, given the average is 4.58, let us say you had a 3.58 deviation. So, 3.58 and you had another data point, which was 5.58, so both of these would have a deviation of minus 1 and plus 1 and when you square these two numbers, so you square these two numbers, the answer comes out to be 2. So, that is what happens when you do this entire squaring process. Now, what happens when in one case? So we had two… We just focused on two data points. Now, what happens in one case when you just… In one case, you are right on the mean. So, you are right on the mean, in the other case you are deviating by two points essentially or whatever the unit you are using, a deviation is two units. So, here the deviation, because you are comparing it to the mean, you are essentially just replacing this 4 point, you are replacing this 3 with this 4.58 and we are looking at what would happen. In case, because you are doing that your deviation is zero. Here your deviation is 2 and because it gets squared, that becomes 4 and so, your cumulative deviation in some sense is 4, whereas in the previous case your cumulative deviation was only calculated as 2. In both cases, you deviated by two units from your mean across the two data points. In one case, you deviated by one unit in the first data point and one unit in the second data point, but the sum of the squares led you to a number 2. In the second case, you deviated from the mean by zero data points in the first, by zero units in the first data point and again two units in the second data point. So, in both cases if you just look at the actual deviation from the mean, in both cases you have deviated by only two points, but in the second instance, in this instance you would be recording a square deviation of four units, which is twice as much as the square deviation of the first case, which is two units. Now, many people like that and there are many contexts, where that makes a lot of sense. There are some contexts, where this justice not make sense, but that is one of the implications of squaring the deviations. So, second question is, why do we work on standard deviation and not the variance? So, the idea is, why do we take this square root. Why not just report the variance, why do we report mean, because they both the same function and the answer again is fairly straight forward. You have a data set and some units, that is 3, 4, 3, 1 could have some units and these units could be things like, simple things like rupees or kilometers per hour, whatever it is that you know. You might have then collecting data own and when you report a deviation from the mean, the units would then be in squared if you are using variance. So, if you use variance you will have to report a value that is in square and, so what is it mean to say rupees square. So, what is it mean to say a dispersion is a 500 rupees square and you know, rupees squared is not something that we can understand, it is far more intuitive. When it, truly it is form a meaningful to say a deviation is 23 rupees from the mean, you can make decisions based off of that and you can gather some insights based off of that. So, third question and often a very interesting question is why do we average by dividing by n minus 1 and not n. So, the idea here is that the sum of the deviations is always zero and so the last deviation, because you are essentially doing a series of deviations. Now, the last deviation you can be found, once we know the other n minus 1 deviations. So, we are not really averaging n unrelated numbers you are really averaging only n minus 1, a squared deviations. In some sense, it is almost like only the n minus 1 square deviations can vary freely and we average by dividing the total, essentially by n minus 1. This is also the concept of degrees of freedom, which is how many of the values can actually move freely with, can move freely and still maintain the final statistic and in this case, the final statistic is the mean, because you subtracting each number from the sample mean. Now, the important thing is, this mean which is 4.58 in our case is something that was calculated from this data. So, from the same data, which we are using to calculate the standard deviation, you calculated the mean and that is the reason essentially that you are using the n minus 1. If instead you are not using this mean, but someone came and told you, what the true mean of this data was. Someone said, here is the data set and by the way, the mean of this data set is 5. So, they just told you the data set or you knew the data set from past experience or you are able to compute that, then you would not have to do the n minus 1 and you would do the n, but also out here you would not substitute 4.58, you would be substituting 5. So, in each of these places you would be substituting 5, which is the true mean. We call that the true mean and we call 4.58 the sample mean, because 4.58 you calculated from this data, whereas 5 is something that you knew on principle or you are able to use some other source to know, what the true mean was. Now, another way the people like to describe this is also to say for instance that, if this 3, 4, 3, 1 this data is ultimately a sample from some other population, then you need to essentially do what we just discussed, now which is to use this n minus 1 and take the sample mean. So, again we are talking about the case, where nobody comes and tells you what the true mean is. So, your only hope is to calculate a mean from the data and you calculated 4.58 and because this data set is a sample from something else that generating this data, the right way to do it is the way the standard deviation formula right now is shown. But, if in some sense this data is the population, it is not a sample from some universe, but it is the real deal. Then, again the idea would be to use n and not n minus 1, because this is the true mean and again out here, you would be substituting the 4.58, but then this should be called a population standard deviation. So, it is POP, population standard deviation. But, more often than not in terms of the more realistic situation that you will encounter in life, I think it is fairly safe to say that, if you are taking the sample and you are calculating the mean, use n minus 1. If you given a sample data set, but you already know the mean, that is you are not calculating it from this data set you already know the true mean. In that case, you can just go ahead and use n instead of n minus 1 and that would be the right standard deviation. So, that is as far as standard deviation goes, but before we conclude on measures of dispersion, it is worth mentioning that there are some other measures of dispersion out there and these are called mean absolute deviation and there are many variance to it. But, the core idea is that, with mean absolute deviations you replace what you use in standard deviation, which is the deviation of each point from it is mean and squaring it, you replace that with an actual deviation. So, the deviation and this sign which is the two vertical lines on either side, what the essentially mean is that the negative symbol just goes away. So, a 3 minus 3.58 minus 4.58 which would result in minus 1 would just be written down as a 1 and so would a 5.58 minus of 4.58. So, negative signs are just taken off and then you do and the other operation of the same. The good thing with mean absolute deviation is that, it has lot of variance, so it is like what is the average deviation from the mean, that is the typical case and that is what I have written down here, but you can also replace this x bar with the median of the x’s. So, the mean absolute deviation from the median is another case and you also have cases like, what is the median absolute deviation from the mean, the median absolute deviation from the median. Obviously, our previous lecture on understanding the pros and cons of means and medians would play an important role in making such a selection. So, that should conclude a lecture on measures of dispersion. In the next lecture, we will continue with descriptive statistics, but focusing more on distributions. Thank you. English - NPTEL Official

7) Random Variables and Probability Distributions
Hello and welcome to the next lecture in our course Introduction to Data Analytics. In this lecture we are going to be talking about Random Variables and Probability Distributions and this would be the first lecture of the series that cover this topic. Just a recap on what we have completed so far, we finished looking at Descriptive Statistics and the use of various use of various graphical and visualization techniques in descriptive statistics as well as the use of summary statistics. Within summary statistics, we looked at measures of centrality and measures of dispersion. So, jumping into this topic, quick question is why do we need to talk about probability distributions. What does it have to do with data? It is just like a mathematical concept, why, what does it have to do with data. And the quick answer to that question is, if you go back to the use of the histogram we express that as a way of describing data. Essentially the histogram is, if you look at this picture on the slide, the histogram is those vertical gray, grayish blue bars that you see on this graph and that describes the data that summarizes the data in some way. But, you might be of the belief that if you redo this exercise, if you collect a new sample you will get bars that looks slightly different and the question is, is it really coming from a probability distribution, is it coming from some other mathematical function that closely approximates this histogram that you are seeing. And that red line that you see on this is the attempt to fit this mathematical function and the core idea here is that, this data is being generated by this probability distribution function, which is that red line and the histogram is, what you see in terms of data. Because, not every time you going to get data that looks exactly like the red line, so it is in this context that you can think of a probability distribution also as a way of just describing your data. But, I would say describing and not summarizing, because it is fairly comprehensive, it just does not give you one number or one thing. It gives the full form and shape of that data and you can think of it as an exercise also in modeling your data. So, you are not just describing it, you modeling it. So, in that context probability distributions are very important and we will also see how in various other things, not just describing data, but even in terms of doing more advanced analytics in the machine learning parts in the statistical inference parts, the use of probability distributions is critical. The think of a data set has random numbers that are being generated in accordance to some mathematical function is the whole idea behind, the use of probability distributions with respect to data. To do this, we kind of have to understand some basic concepts, which is and the first basic bond is to understand what random variables are. Random variable is essentially a variable whose value is subject to variations due to randomness as a post to variations due to some other phenomena. So, we are all familiar with the concept of constant, which just means it is a fixed number and variable. Here I am talking about the variable that you probably learnt in algebra in high school, non random variables and there you learnt that the variable is essentially something that can take on many possible numbers or any possible number. But, the distinguishing factor between a variable and a random variable is that, with a regular variable once you fix all the externalities, then the variable takes on a specific value. So, let me give you an example. So, you take something simple like, force is equal to mass times acceleration. So, force is equal to mass into acceleration and you might say, all these are variables and they are and that is true. So, force can be any possible value, given what I have just told you, force can take on various numbers as it is value with some units. But, once you fix mass and you fix acceleration, force will take on a very specific value. As opposed to a random variable, where even if you fix all the externalities, the best way to describe the random variable would be to say that it can still take on a set of possible values and that set could be a very large set, it could be infinitely large set. But, the variable itself can take on many possible values and each of those values have a specific probability associated with it and beyond that, you are not going to, you cannot reduce the variable beyond that by definition. Even after you fixed everything around this variable, you still have to describe the variable with a probability state space. So, let us, the best way to again get this even deeper, to understand this even better is to talk about somewhere you specific probability distributions and that is what we are going to do in the next part of this class. But, before I proceed I just wanted to tell you that, I am broadly breaking up the idea of probability distributions into discrete probability distributions and continuous distributions and that is just to give you some structure into it. Those words might not make immediate sense to you right away, but what we are going to do right now is to look at discrete probability distributions. You might also notice that I am using the word probability density functions and you might not know over that word means yet, but very soon we are going to be talking about that is well. So, great, so we are going to look at the most simplest discrete probability distribution and this, it is really simple, because I think we all used it in some sense in our daily life. So, we are here, so let us look at the first example, I will give these numbers just show that going forward, it is clear. So, we are looking at number 1. In number 1, matches the closest with a colloquial use of probability, chance, likelihood and so on and we are saying something simple, which is that the probability something happens is x. So, you might say the probability that rains today is 10 percent, is the 10 percent chance it is going to rain today. What goes unsaid is that, there is therefore, a 90 percent chance that it does not rain today and that is what is captured in this graph. So, we more used to saying, this is 30 percent chance there it is going to rain, this is 20 percent chance that there will be an accident, there is a 10 percent chance that the product that I am manufacturing is not fit to be shift. But, essentially we are talking about these kind of binary events, where one of the possible outcomes is x and therefore, by definition the remaining possible is just 1 minus x or if you thinking of it in terms of percentage, this is the 100 minus x. Again a very simple example of this could also be something like this, there is a 50 percent chance that, if I toss this coin I am going to get a heads and what goes without saying is therefore, that there is a 50 percent chance that you would not get heads. In this case, that is called tails, so great. So, that is one very simple conception of probability and this is a probability distribution, it is called Bernoulli distribution, but we can move to multiple outcomes. So, if you look at number 2, what we have there is, what you get when you role a dice. So, you role a dice and a dice has six faces and on each face you have a dot. So, you have, if you role a dice you either get a 1 or a 2 or a 3 or a 4 or a 5 or a 6 and the idea is that the probability of each of these is one sixth, if it is a fair dice and so, that is a different kind of a probability distribution. We will soon learn in our next class that is called discrete uniformed distribution, because they are all the same probability, but the possible outcomes going back to our definitions is set. So, the possible outcomes possible values are 1, 2, 3, 4, 5, 6; the probability associated with each of those possible values is one sixth and one thing that you might have noticed by now is, if you take all the possible values and you take each probability and you add the mole up, you always get 1. So, in the case of 1, we saw that if the probability of it raining was let us say 30 percent or let us say the probability it rains today is 30 percent and the probability it does not rain therefore, becomes 70 percent. You add those two up you get a 100 percent or you get 1. Similarly, you have six possible outcomes when you role a dice and there is a one sixth chance of each of them happening and six times 1 by 6 is 1 and the intuition for this should also be obvious, that if you role a dice or if the day passes means something has to have happened. So, you have had to have gotten one of those six numbers or you know it either rained or it did not rain, but as long as you have comprehensively covered the universe of possibilities, then something needs to have definitely happened within that universe. So, therefore, that should also been intuition us to why, that the probability distribution sum to 1. What we have in number 3 is the idea that, again it ties to this notion of probability not just being a theoretical exercise and you might actually have some data and you might choose to define the probability distribution based off of what you see in the data. So, if somebody came to you and said look, I do not want you to assume that, so I wanted to take this coin and I wanted to describe this random variable, which is the probability of getting a heads or a tails and that is the random variable and I do not want you to assume, there is a 50, 50 chance. So, you might say fine, I have nothing I cannot assume anything and you might toss the coin a few times. So, you do a data collection exercise, where you toss the coin 30 times and you notice that you get 14 heads and you get 16 tails and for whatever reason, if you do not want to assume anything about the distribution and let us say, you also do not want to do any statistical inference, again a topic that we will cover soon. You might just be contained and saying, I am going to describe this random variable with the actual data that I see. So, I am going to actually say that a 14 out of 30, there is a 14 by 30, because you actually got 14 heads when you toss the coin 30 times. So, I am going to actually say 14 by 30 is the probability of getting a heads and 16 by 30 is the probability of getting a tails. So, there is nothing wrong with doing something like that, we would have to see if that was actually made sense do, but nevertheless if you said I just wanted to take data and I wanted to describe a probability distribution with the data that I see, then you can definitely define a discrete distribution in this way. And now, we go on to something that is a little more complicated, which is continuous distributions. In the previous case, in the discrete distributions and let me erase this, all the ink. So, in the discrete distributions, what made as discrete, were that the possible outcomes would discrete. So, it was either an event or a non event, so that is discrete, there is no half event. So, there is no half event, not there. Same way are here, you either get a 1 or a 2 or a 3 or a 4, this set of possibilities that is the here x axis essentially has some countable number of possible states and so, you cannot get a 1.5 when you roll a dice and similarly, you cannot get a half head, half tails when you toss a coin. So, that is essentially the concept of it being a discrete distribution and with continuous distributions; however, that is not really true. The idea is that the x axis are here, so it is the same idea which is the possible outcomes are on the x axis, same thing that we saw on discrete and the probabilities are on the y axis. Case of this is the same core concept of describing the distribution, but here the x axis is not discrete. So, what is that mean? What it means is, you take something like the probability of a certain height. A height can be a 130 centimeters, so that could be one number out here, but it could also be 130.001 centimeters, it could also be a 129.999 centimeters. So, there is no inherent discretization. You might turn around and say, look what if I had a measuring scale that could only measure in 5 centimeter intervals. So, the way I mean or I can only measure up to a centimeter, I cannot measure less than a centimeter, because the scale that I have does not have more resolution and that is fine. You know, if your resolution for measurement is still accurate, meaning that anything between 130 and 131 gets called a 130, because of the inherent resolution of the scale. That is fine, you can create a discretized version of it, but the idea is this nothing about height or any, essentially the measurement of space. There is nothing about height that is inherently discretized, like the measurement of a dice is inherently discretized. You cannot possibly roll the dice and get a two and half, whereas if you had a five and half measure of height, you could get any possible value within the certain range. So, you might have the lower end of this being 20 centimeters and the upper end of this being a 180 centimeters. But, essentially any value between it is possible and we kind of spoke about the same concept when we spoke about discrete and continuous variables. The same concept of discrete and continuous variables applies to discrete and continuous distributions. Now, again another important thing to note is just like in the discrete distributions, where we said the sum of all the probabilities for each possibility should add up to a 100 percent or should add up to 1. Here you cannot have a countable number of possibilities, so you cannot take each possibility. Take the probability of that and added to 1, just because there are infinite number of them. So, you basically what you do is, you take the entire interval and sum the probability within that interval and the best way to do that is to look at this area as a whole. So, this way when you look at this area, it is like you are taking all the possibilities within that area and summing all the probabilities for each possibility and when you do that, you will be getting 1 or a 100 percent. So, now let us now that we understand both continuous and discrete random variables. Let’s just briefly talk about the use of probability density functions and cumulative density functions. So, far what I have been graphically showing you, have all bend probability density functions. For each probability density function, there exist a cumulative density function and so, I will describe it in the continuous case and that is the easiest and I separately do that for the discrete. The idea with the PDF is that, like we said for, because there are infinite number of possible states, you really does not really make sense to ask the question, what is the probability at a given point. It turns out that the answer to that question is that the probability of that given point is zero, although there is some y, there is some height for that given point. Because, there are infinite such points out here, because there are technically infinite such points. As a result, you can only say what is the probability of a given area, so you can say I want to look at this area and you can get the answer to that question, you can get the probability of this given area by just measuring the area under this curve and that is what I have done with the gray line on this graph as well. But, the idea behind PDFs to CDF is that, the CDF describes the cumulative probability up to a certain point. So, if I would ask the question, what is the probability within this area, you could answer it from me by looking at the area under the curve. If I ask the probability density function at this given point, you can use the function to figure out what this value is, but the probability itself is zero. But, the cumulative describes the probability from zero, from the lower end of this axis and that can be zero or that could be something like minus infinity or it could be some other value. You know this starting point essentially, from that starting point all the way up to the point of interest. So, this is x, the PDF describes the height out here for x, the CDF describes… So, this is your point x, the PDF describes the height of this curve at x and which is y, this entire curve out here this curve that is here on the left hand side is the PDF, whereas the CDF describes the area to the left of this point x and that area is the CDF. So, this graph is nothing but, the same as the graph to the left, where at each point x you are looking at what is the area to the left of x on the PDF and that is what you are plotting here and that should logically be equal to 1, when you complete and the reason for it is the following. We already discussed in the previous section is to how the overall area under this curve. The overall area under this curve is equal to 1, correct. We discussed that if you take all the possible states and add the probabilities of all the possible states, as to how you are getting up, you would get a probability of a 100 percent or 1 and. So, the CDF is nothing but, this description of the area to the left of the curve and when you reached your entire set of possible heights or in this particular case heights, but it can be anything else, then the CDF hits at the one mark when it ends. And it is, it would be helpful for you to know that therefore, the easiest way of getting from a PDF to CDF and a CDF to PDF is, from a PDF to CDF you would essentially want to integrate. For those of you, who used integration in high school or if you heard of the term integration, that is that symbol that looks like this and the idea is that an integration covers this core concept of area under the curve. So, if you integrate up to a point x, so let us say just for as an example that this started at zero, but you could change that. It could start y, it could started minus infinity, but you essentially if you integrate the area under this curve to get the CDF. You would just want to integrate from 0 to x, some f of x and the f of x here is your PDF function and that will give you the CDF and the idea of going from CDF to PDF is exactly the opposite, which is to differentiate this CDF and that will give you the PDF. So, we will conclude this lecture with that note and starting from the next lecture, we will be talking about some actual probability distributions and we will go through at least the most popular ones in that lecture. Thank you. English - NPTEL Official

8) Probability Distributions(contd)
 Hello and welcome to our second lecture in this series on Random Variables and Probability Distributions for a course Introduction to Data Analytics. In the previous lecture we saw, we made a more broad introduction to the concept of random variables and distributions, we spoke about the concept of having continuous distributions and discrete distribution, we presented some examples. And we also spoke about the idea of a probability density function versus cumulative density function. In todayÕs lecture, we are going to pick a 5 or 6 very common distributions and discuss them one at a time. And the first one that and these distributions are going to be both, some of them are going to be discrete, some of them are going to continuous and some of them could be both. So, we start with the most common distribution, which is the uniform distribution. The uniform distribution has a discrete version and a continuous version. Now, we will start with the most simple one, which is the discrete version. You already seen examples of this. So, for instance the example that we saw on the last class of the six sided dice, where we were quantifying the probability of getting any particular face values. So, the face value of the dice is essentially, you throw the dice you get something on top, so you see the 1 or 2 or 3 or 4 or 5 or 6. So, the probabilities associated with these 6 possible outcomes, assuming a fair dice is one sixth for each and, so that is example of discrete uniform distribution. We also saw the case of the coin toss, where here you have only two possible outcomes and as long as they are both equal, it is still uniform. So, the formula in terms of the PDF the Probability Density Function, which we discussed in the last class, it is fairly straight forward, it is just 1 by k when there are k outcomes. So, if this is 6 sided dice it is 1 by 6 for each of those six possible outcomes. If it is coin toss, which is two possible outcomes it is going to be 1 by 2 and the ideas that for each of those k possibilities, it is 1 by k and for the rest of the universe, it is 0. So, the probability of getting a 7 when you role a 6, a single 6 sided dice is 0, so you cannot get 7 and you cannot get minus 45 either, so that is what this formula says. With the continuous version, here again you are looking at a uniform distribution, so the probability is a uniform, but like we discussed the variable that we are quantifying is continuous. So, the variable in the six sided dice was, what is the number that shows up and that number is either 1, 2, 3, 4, 5, or 6 and that 6 discrete possibilities, but you might have many things that are not discrete and this goes back all the way to our discussions and quantitative variables, which can be continuous or discrete. So, examples of this and the truth is the uniform distribution, while it is theoretically very intuitive, could be quite convenient in some cases. There are not a lot of examples, real world things that tend to be uniforms, some of them are something like number of seconds pass the minute. So, if you were to randomly, if you have to have random process, which just looked at the clock over the course of date during some random intervals, the number of the seconds pass the last minute could be uniform and that is essentially a space, where you can have 0 seconds pass the minute all the way to 60 seconds pass the minute. So, your interval is from 0 to 60 and where you find yourself in that interval is described potentially by uniform distribution. And again the idea that it is continuous, just means that you not discretized the seconds as 1 or 2 or 3 all the way to 60, you could be in that in 4.5432 seconds pass the minute. So, it is continuous, the variable time is being monitored continuous as a continuous variable. Another example could be the exact age of the randomly selected person between the ages of 50 and 60 perhaps in a certain country. Now, again you know you have to come up with fairly specific examples, because something simple like the age of the randomly selected person is not likely to be uniform. You are not likely to find as many people between 80 and 90, as you are between 70 and 80. And even if you stop the clock at a certain point with typically with things like age, you see that the number of people or the probability of a person in that window decreasing as sometimes age increases and it depends on your state space and it depends on your countries, is the population increasing decreasing, so on and so forth. But, sometimes over a short enough interval, even if the overall distribution is not uniform you can create an interval and say that is my universe. So, my universe is people between the age 30 to 40, within that interval perhaps the distribution of the exact age of people. So, imagine I take the universe of all people between the ages of 30 to 40 in India and then, I try to create a distribution of the exact age of a randomly selected person from this bucket and that could potentially be a uniform. The uniform is also a great distribution, think of when you want to make absolutely no assumptions. So, if I want to make the assumption that the countryÕs population is and some of these need not even be assumptions, so it could be a fact. So, something simple like the countryÕs population is increased and so on, then I would be hard pressed to come up with a uniform distribution and I might find richer distribution to represent my data. But, this uniform distribution can be thought of us like, this ground 0 I do not make any assumptions, the probability of everything occurring within a certain interval is equal and, so I could potentially use it. I have also come across the usage of this a little bit in different aspects like physics and chemistry and so on, for instance the probability of certain types of molecules be in certain locations over certain space could be uniformly distributed and so on. So, we have spoken about, so the formula for the uniform distribution, we spoke about the discrete uniform PDF and we said, the PDF is essentially 1 by k. So, with the continuous, the PDF is essentially 1 by b minus a. So, the idea here is that, this is essentially a and this is essentially b and as you can see in the graph, the probability between a and b is uniform and just like in the discreet case, if this distance b minus a, then the probability is 1 by b minus a. And another way, another quiet simple way of thinking about it is, between this interval, over this entire interval from the lower limit to the upper limit to the area under this curve should be equal to 1 and if you look at the simple math of it, 1 by b minus a times b minus a, which should be, so the height is 1 by b minus a, the length of this rectangle is b minus a. So, 1 by b minus a times b minus a, would give you 1 and, so you can essentially think of it that ways well. And; obviously, like we discussed it has to be 0 for if you are less than a if you are greater than b. So, what is this CDF of this distribution? Again, it is x minus a by b minus a, we will not be exactly deriving it out. We will give you formulas at the end of this lecture to give you an idea of, how to get to the CDF, how to get to the mean, how to get to the variance. But, the core idea here is that, essentially you take some point and let us call this point x, so this is x, there are better x. And the whole idea is that we know this CDF is essentially the area under the curve to the left of x. So, the question could be if you knew that this height is 1 by b minus a, how do you go about writing out this area, how do you calculate this area and it can be a function of x. So, that general formula would be the CDF and the idea is fairly simple. If this is x and you are looking at the CDF is a ratio, it is essentially the area, we know that the total area is 1. So, this total area of the blue rectangle is 1 that we have discussed. So, what percentage of that rectangle have you essentially covered and the idea is, because it is uniform, because this height is constant. If this is x and this is a and this is b, then of the 1, you covered x minus a is this area that you have covered and b minus a would be the full area that you could be possibly covers. If you think of it as a ratio of, how much have covered and how much I can potentially cover, where this b minus a kind of represents the 1 in some sense of the full area. Then, I have covered x minus a of the b minus a that I could cover and therefore, I covered the percentage in some sense, where; obviously, if x was equal to b, then I would have covered the 100 percent and that would be equal to 1, so that is the over all idea behind getting this formula. Again the formula for the mean should be fairly intuitive, if this is a and you know, you have this is b, this central point in some sense is b by you know half of between b and a essentially. And again with variance we will not derive it, accept to tell you that the core concept is to say how much do we deviate on average from the mean. So, we will talk through some of this formula some of it is to give you an intuition, some of it is to actually give you these problems in the assignment. So, you get a feel for actually figuring out what the mean of variance exactly is. Let us move on to the next distribution. The next distribution we are going to talk about is the binomial distribution. So, the binomial distribution is also another distribution, where you have a lot of toy problems associated with it, but by nature of it in the real world sometimes it is more useful to approximate it with another distribution and user, but what exactly is the binomial distribution, let us start there. So, we spoke about this class of distributions and if you did not, then let us just do that right now, so let us. So, there are these class of distributions where called the Bernoulli distributions and the idea behind that distribution is that, it is very similar to the first example we saw in the previous lecture, where you have an event and it has some probability. So, the 30 percent chance is going to rain, that is what the exact example we use. And, so therefore, the probability that it is not going to rain is 70 percent. What is key about it is that, there are only two possible outcomes and these two possible outcomes, discrete outcomes sum to 100 percent and that class of distribution is called Bernoulli distribution. So, our standard example of tossing a coin and saying, what is the probability of heads and what is the probability of tails, would be an example for Bernoulli distribution. Now, if just, so that there is no confusion if it happened that the probability of heads and tails are both equal, you can also think of it as a discrete uniform distribution and, but out here we are just saying we saying something different. We saying that Bernoulli distribution are distributions, where there are only two possible outcomes. The probability of the two outcomes need not be the same as required by the uniform. But, the key out here is that there are only two possible outcomes, two possible discrete outcomes that sum to a 100 percent. So, great, so we spoken about the Bernoulli distribution, what is it happen to do with the binomial distribution, is not that, what this slide is about. Well, if you take the problem and you quantify the probability of heads and tails, then you are talking about Bernoulli. But, instead if I rephrase that problem and said, what is the probability of getting 5 heads out of 10 tosses. Then, I am describing the problem associated with a binomial distribution. So, an example of a binomial distribution would be, if I said I am going to toss the coin 10 times, I am going to take part in a BernoulliÕs process, n number of times and I asked myself the question, what is the probability of getting k successes and success could be defined as getting a heads or it raining or whatever it is. It is one of those two outcomes essentially; you call one of those two possible outcomes of a Bernoulli process as a success. And then, you say, what is the probability of getting k successes out of n possible trials? So, if I say I am going to toss a coin 10 times, can you tell me the probability associated with 0 heads, 1 head, 2 heads, 3 heads, 4 heads all the way up to 10 possible heads. Now, note if I toss the coin 10 times, I can either get 0 heads or any of those numbers in between till 10 heads, I cannot get minus 1 heads I cannot get 11 heads. So, the probability associated, if I toss the coin 10 times, the some probability that I am going to get 0 heads, the sum probability that I am going to get 5 heads. But, each of these numbers each of these probabilities when added together should again be equal to 100 percent or 1. So, quantifying the probability of getting k successes out of n trials of a Bernoulli process is the binomial distribution and you can think a various real world example. So, for instance the probability of, you know let say there are 10 mergers the companies considering. So, in mergers and acquisitions since it is a small enough number, what is the probability of getting 3 out of 10 of them or you might say, probability of having 5 defective products in a batch of 20 products, what is the probability of the 3 defective products, answering questions like that. And one just I noticed, you might have noticed that I have take an example of small enough a numbers. So, technically the binomial could be answering a question like, what is the probability that out of you know 1 million possible toys. Let us say that I distribute, I am a toy maker and I send out 1 million, what is the probability that I get 500, 5000 toys as broken. And technically that would still be a binomial distribution, but what we will learn probably in the next classes, why thatÉ The computation of that is a little messy, you get very large numbers and you have some other distributions that can approximate something like this to solve problems like that. But, the core concept is this. You have a Bernoulli process, where something isÉ You are looking at something that is binary a or b, 1 or 0 and you turn around and you say, what is the probability of getting k successes out of n trails of this process. And, so logically the formula and note that I have used the word PMF of here, that is an important distinction that is worth mentioning. PMF is the same thing as PDF, PDF Probability Density Function, we spoke about that we spoke about probability density functions and cumulative density functions. PMF is just the standard way of calling a PDF if you are dealing with a discrete distribution. And since this is a discrete distribution, because you cannot get out of 10 tosses of a coin you cannot get 3 and half heads. So, you can only get either 0 heads, 1 head, 2 head or 3 head and so. Since this is a discrete distribution, it is technically called Probability Mass Function or PMF. So, the PMF, this distribution should be fairly intuitive, it is nothing but, n choose k, so this symbol out here just means you might have seen it like this. So, that is n choose k and that is has to do with a combinations, something you might have studied in the permutations and combinations. The idea is, how many ways are there of choosing n from k trails. So, here I think the words n and k are swapped as suppose to, what I was mentioning earlier here. So, here for instance I was interested in finding out, what is the probability of getting 5 heads, then it would really be 10 would be the n c 5. So, how many ways are there of getting 5 of choosing 5 out of 10, so I toss the coin 10 times, there are many ways in which, I could get 5 heads out of 10 tosses. It is either that the first 5 could all be heads and the next 5 could all be tails or you can have 1 head 1 tail 1 head 1 tail. So, that number of different ways in which I can get 5 heads out of 10 tosses is, what is being quantified by this number, which relates to this part of the formula. Once I figure out the numbers of possible ways, this part, which is p power k tells me, what is the probability of getting the 5 heads in the first place. So, it might be 0.5 power 5, if the probability of head was different from a tails, let us say we were dealing with the problem where 60 percent chance of heads following, because it is an un even coin, then it could be 0., it will be 0.6 power 5 and this part, which is the remaining part talks about the probability of getting the tails, the remaining 5 as tails. So, essentially the 3 parts of this formula are the different ways of getting those heads, probability of getting so many of those heads, probability of getting the remaining number as tails. So, that is essentially, what the PDF captures and the formula for the cumulative density function again is the same logic that we were discussing with the uniform, which is that for a given k, which might mean for let us say I am interested in knowing, what is the probability of 6 heads out of 10 tosses. You are essentially looking at nothing but, everything to the left of the curve and to the left of the curve here means, what is the probability of getting 0 tails I mean. So, let us see what is the probability of getting 6 heads is the question out of 10 tosses and that would be nothing but, the PDF of getting 0 heads out of 10 tosses, 1 head out of 10 tosses, 2 heads out of 10 tosses, all the way up to 6 heads out of 10 tosses, that summation of those probabilities, because that is essentially what would be to the left of the curve. And here I am doing nothing but, thinking of a curve, where 1 out of 10 tosses. We should start with 0 out of 10 tosses and going all the way to, you know 2 dot, dot, dot, dot 6 and then, it will go 7 and it will go all the way till 10 out of 10 tosses. So, if this was some kind of PDF, I am essentially looking for this area and the curve and you know, the top part of the line is not uniform I am just representing, which side I am interested in, that is all I am doing there and also it would be discrete. So, you have discrete rectangles popping out of the 1, 2 and 3 and you just summing them up and the formula also with the summation does not really simplify too much, so you are dealing with the CDF as it is. As I mention it is more useful for small values of n, when you get really large values of n, you took it other approximations. The mean is nothing but, the number of times you are looking at times of probability. So, if the probability of getting the heads is 60 percent and you are going to toss the coin 10 times, the average number of heads you are going to receive is nothing but, 10 times 60 percent, which would be 6. So, on average I should get 6 heads out of 10 tosses, because this is 60 percent chance of getting heads. And, so the intuition of that should be fairly obvious and again with variance, the intuition itself might not be very obvious. It is really about looking at how much the deviation, how much of a deviation there is from the mean. So, how much does one out of 10 deviate from the mean and we would look at the probability that you would first of all see of 1 out of 10. And again I think there the formulas for variance might probably help you to get a better idea of it, great. So, let us move to our next distribution this is also a discreet distribution and this is very similar to the binominal in many ways both of them are discrete, but essentially while this is discrete, which also counts the number of possible occurrences x. So, it says, what is this quantifies the probability of having getting x occurrences. So, probability of getting one of those occurrences two of those occurrences, but it is over a certain period of time or space see in the binominal we were saying, what is the probability of 5 heads out of 10 tosses. So, the 5 heads was discrete but, so was the 10 tosses here you are asking the question, what the probability of x is where x is discrete meanings, what is the probability of there being 5 people. But, instead we wonÕt say out of 10 people we will instead say, what is the probability of there being 5 people coming to this bus stop over the next 5 minutes the key difference is 5 minutes is continuous as oppose to 10 tosses, which is discrete the ramifications of that is; however, low the probability is of people arriving in a given time there is still technically a probability, that you have a really large number of people infinite number of people coming over the next 5 minutes, where as I can say with certainty in the case of the binominal that out of 10 tosses I cannot get 11 heads. And this has nothing to do with the probability of getting a heads in the first place even if I had a probability of 0.1 of getting a heads in a toss, because of such an unfair coin I still know or 0.9 whatever you know whatever the probability is I still know that its technically not possible to get 11 heads out of 10 tosses. Here, you are looking at a discrete occurrence such as probability that of n number of defaults in a given month or you know number of people, who are going to arrive at the bus stop in next 5 minutes the number of defaults the number of people, who arrive in the bus stop are discrete you cannot have less than 0 people I mean this in this particular case you cannot have less than 0 and you cannot have two and half people who arrive at the bus stop, so it is a discrete distribution. But, the distribution is defined over at time or a space, which is continuous. So, it is not how many heads can I get out of 10 tosses its more, how many of a certain occurrence can happen over a certain period of time and that is the core concept there the PMF again note it is not the PDF, because it is a discrete distribution its characterized as lambda power k and lambda here is a parameter. So, it is essentially a number that that you have and it represents like the average rate. So, 3 people are arriving per minute that would be more like a three out there and k is the variable of interest where you say k is equal to 1 and you get a probability you say k is equal to 0 you get a probability. And technically this k can go all the way up to infinity like I was discussing right and the sum of all those possibilities discrete possibilities is equal to 1. So, if you wanted to know the probability that 3 people will arrive at the bus stand all you will do is you will say, what is the average arrival rate. So, may be the average arrival rate, which is what is essentially represented by lambda. So, the average arrival rate could be something like well two people arrived per minute, so that is two and I want to ask the question what is the probability 0 people will arrive over the next minute. So, I would say on a average two people arrive at the bus stop k is that 0. So, I will put this I will substitute k with 0 and this will this formula will spit out a answer, which is the exact probability of 0 people. And you will replace k with 1, 2, 3, 4 and so on, all the way up to infinity and the sum of all those probabilities are is essentially, what is essentially the distribution is essentially, what quantifying. Because, of how it is defined the mean is lambda we defined lambda as essentially that rate parameter and its interesting property of the that variance also is lambda and lambda has to be greater than 0. But, again just to recap this stateÕs space for a Poisson distribution that is the possible value that k can take are always greater than 0 in greater and less than and goes all the way up to infinity. But, it is a discrete distribution, because you can never have two and a half people arriving at a bus stop. So, the next distribution we are going to look at is the geometric distribution the geometric distribution is also an discrete distribution, but it is a very interesting counterpart to the binominal distribution. So, take the fact that we said that is a Bernoulli process, which means there is some probability of event happening and some probability that the event will not happen and they add up to one. So, probability of a heads, let us say is 60 percent. Therefore, by definition probability of the tails is 40 percent that was Bernoulli and then, we said the bi nominal is nothing but, the probability of getting k successes out of n trails. So, what is the probability of getting 2 heads out of 10 tosses 3 heads out of 10 tosses that space is defined by the binominal the geometric defines the probability of the number of times you need to toss the coin before getting your first heads or you can think of it as getting the next heads. So, number of attempts before an event is what you are looking at and before you can think of it as in inter arrival counterpart to your binominal distribution. So, if you take the coin toss case right essentially it is more like, how many times do I need to toss the coin before I get my first heads or a next heads. So, you start at some point and then, you say so; obviously, anything that I have tossed before it is not influence my future tosses, because they are independent. And now, I am going to start tossing the coin and tell me the probability that I will have to toss the coin 0 times before I get my first head or 1 you can count you can say, what is the number of tails I am going to see before I get my next heads. So, that could be 0,1,2,3 and technically it can be infinity meaning they could be this really bizarre world, where even though there is a finite non zero probability of getting a heads whatever that number is it could be 0.9 or it could be 0.1 its technically possible that I wind up tossing the coin and infinite number of times I keep getting tails and, so am still waiting for my heads. So, this is again a distribution, which starts with 0 and goes all the way to infinity depending on how itÕs defined exactly the geometry is sometimes defined as the number of tails you see before the next heads. And there the distribution starts at 0, because you can see 0 tails another version of this distribution can could start with, how many tosses do you need to make to see the first heads in that case the very you need to at least toss the coin 1 time to see the heads. And, so both these distributions you might find in text books the PMF and CDF should be fairly intuitive all we are doing with the probability mass function you are saying, what is the probability of getting k minus 1. So, here you might say k minus 1 is nothing but, the number of tosses before the actual success that you keep getting the tails. So, if p is the probability of getting the heads you are saying, what is the probability of getting k minus 1 tails. So, if you here we are defining the distribution as on the first toss I get the heads then what is the probability that, so let us say I want to know the probability of getting it taking 3 tosses before I get a heads, then you are saying; that means, for the first two tosses 3 minus 1 is 2 the first 2 tosses I should have gotten a tails. So, what is the probability of getting a tail like that and that is 1 minus p. So, 1 minus p to the power of k minus 1 says before that success happens I need to say I need to say k minus 1 failures and what is the probability of that and then finally, the probability of that one success. Again essentially the CDF you are doing a summation, but the summation kind of neatly simplifies to the formula that we have shown here and the mean being 1 by p should be fairly intuitive meaning if the probability of getting a heads is let us say 10 percent it should be intuitive that it on average should take 10 tosses before I get my first heads, so 1 by 0.1 would be 10 and so on. And again even if it is not entirely intuitive to you may be working through some problems and formulas, where we will be discussing those can help. Finally, the variance is also 1 minus p by p square and even that is something that you might have to work through a couple of times. So, we come to the last distribution that we are going to be discussing in this class and that distribution is the exponential distribution in many ways the exponential distribution essentially you know how the geometric was looking at the inter arrival time of a binominal distribution the same way the exponential looks at the inter arrival times of the Poisson distribution. So, what does that mean in terms of our examples you know how we spoke about the Poisson distribution as being discrete distribution, where you said, what is the possibility of three people arriving at the bus stop in the next 5 minutes what is the probability of two people arriving at the bus stop in the next 5 minutes, so on. So, you have fixed the time 5 minutes or ten minutes whatever is of your interest and you looked at the probabilities of various discrete possible occurrences. So, you said what is the probability zero people arrived one person arrives two people arrive three people arrives. Here with the exponential you are describing the inter arrival that is how long should I wait before the next person arrives. So, it is the exact same thing the exponential is the same thing to the Poisson the way the geometric is to the binomial. The binomial is quantifying the number of the probabilities of getting 3 out of 10 out of 10 tosses the probability of getting 3 heads, where as the geometric saying how long should I wait before the next head heads arrives. The Poisson is quantifying over some time scale or space scale its quantifying the probabilities of different occurrences like, what is the probability of 1 occurring 2 occurring 3 occurring this is how long should I wait before the next thing occurs. So, you can think of the exponential as moving in time and waiting for that next occurrence and how long should I wait, what you can think of it a space I keep walking and you know I encounter occurrences over some length scale the poisons gives me the probability of seeing n number of such occurrences. But, if I start walking on that scale how long should I walk before I get the next occurrence and long can be in length or it can be in time. So, think of it is time or space, but essentially the continues version of the you can think of it as the continuous version of the geometric distribution. The more again the PDFs and CDFs it is there for your reference, but the important thing about this distribution is that people call it memory less meaning that the probability of something occurring over a time if the same if you condition that it is not happened yet. So, think of this way I am not saying for instance, so let us think exponential is often used to say to describe may be the failures of a light bulb over time. So, how long should I wait before this light bulb fails, but as Poisson would describe saying out of 100 light bulbs over a tenure horizon how many would fail, but let us leave that. So, out here an exponential am dealing with one light bulb I am saying how long should I wait before it fails. So, if this distribution were uniform the probability that it would fail between year 1 and 2 would be the same as probability that it would fail between year 5 and 6 that would be uniform what we mean by seeing exponential is memory less is that. The probability that the bulb would fail between the year 1 and 2 is the same as the probability of the bulb would fail between year 5 and 6 if I tell you at the start of year 5 that the bulb is already not failed. So, if you are standing at time 0 the probability the bulb will fail between year 1 and 2 is very different from the probability the bulb will fail between the year 5 and 6. But, the probability that the bulb will fail between year 1 and 2 is the same as the probability of the bulb will fail between 5 and 6 if I go to year 5 and tell you that the bulb has not failed yet same way if I go to year one and tell you that the bulb has not failed yet. So, condition on the bulb not failing the probability is over a future time horizon or length horizon would be the same. And just for your convenience we have kind of put together the four distributions that are partly related and saying that these are discrete over the time arrivals versus how the distribution is counted. And I have used the color coding to describe which are continuous distribution and which are discrete distribution and you can see that clearly its only the exponential that is continuous the other three are discrete distributions great. Now, before I sign off from this class I just wanted to give you some formulas on how to calculate mean and standard deviation and so on. But, before we do that let us start with the more basic thing, which is given a PDF, how are you going to get the cumulative density function and the idea here is we have always said it is the area to the left side of the curve. So, the left side of the curve is from minus infinity, so this is the PDF and the left side extreme is referred to as minus infinity. Now, if you know that distribution starts from another location you just essentially want to get to the starting point all this minus infinity means it is the starting point of the distribution if a distribution can technically go to minus infinity it can, but like the uniform for instance it starts at a. So, that would be a you would replace that and x is you leave x as it is, because your cumulative density function is a function of x and this integral is what would solve it. Obviously, if you are dealing with a discrete distribution you would instead have this summation not integral, but everything else about the formula would stay the same way if you are going from CDF to PDF you will essentially differentiate the CDF and there is not much more to that. So, now, coming to mean and variance the idea here is that the mean x bar is the kind of mean that we have discussed, so far. So, you have some data points you take them up you average them divide by, but if you wonÕt start thinking of a mean in terms of a distribution. So, you given instead a distribution not actual data or you use the data and created a distribution out of it you created an f of x out if it. Then, how do you calculate a theoretical mean of the distribution and that you do by essentially this formula for the continuous distribution and this formula for the discrete distribution and these you can call them expected values these typically gets called mu it is that Greek alphabet mu, that the core idea here is that you take each possible outcome in the discrete case multiplied by the probability that there outcome can take on and we all know that these probabilities multiply all the way to the sum of these probabilities is going to be equal to one. But, you are multiplying pi and xi for each I and; that is the discrete case the equivalent of that is nothing, but f of x is the equivalent of pi and x is x in both cases. So, this is the equivalent of the continue this is the continuous version of the same problem. The standard deviation is the same here you have the n minus 1, because you are dealing with x bar, but if you have a theoretical mean that is given to you can use the n and in the discrete case this formula, which is the same formula that you have used to with the exception of one by n takes place. Now, this actually simplifies and for your convenience on the continuous version I have given you the simplified version this to me this version, which is used for the discrete is more intuitive but this formula simplifies to a formula, which looks more like summation of x square and so on. Separately just like on a case of the mean discrete uses a summation and continuous uses an integration we have the same distinction even here, but for your benefit on this one I have shown you the simplified formula. So, even here originally you have started off with integral minus infinity to infinity x minus mu the whole square dot f of x and so on. But, that is not what we are doing here and here actually this would not be dx this would just be probability, so it will be p of I out here. But, out here it would be d x and it would be f of x because it represents that. So, I hope these formulas give you some idea and we will definitely be looking to see you apply some of these formulas on distributions to get answers. But, thatÕs it for this class next class we will talk about the distribution we have not talked about, so far which is the normal distribution. Thank you. English - NPTEL Official
9) Probability Distributions(contd)
Hello and welcome to the third and last lecture on the series on Random Variables and Probability Distributions. In the first lecture we spoke about, we introduced the concept of random variables spoke about, how probability distribution can be discrete or continuous and we also introduced the idea of PDFs and CDFs, Probability Density Functions and Cumulative Density Functions. In the second lecture, we targeted about five or six distributions, commonly used distributions and we introduced them. As well as talking a little bit about, how one can get the CDF if you are given the PDF, what is the relationship between the PDF and CDF and vise versa and how do you get to the PDF given a CDF, symbolically. And we also spoke about, how you can mathematically using given a distribution compute it is mean, compute it is variance and so on. In this lecture, we are going to focus more on a single distribution called the normal distribution, many of you might have already heard about it. But, we are also going to look at some applications associated with this distribution and one really important application has to do with inferential statistics, which is something that will be quite central to the next 4 or 5 lectures. So, it is in that idea that, we are introducing the normal distributions. So, the normal distribution itself you might have come across it, if you not you might have heard of this thing called the bell shaped curve. So, the distribution itself looks like the shape of a bell. So, just like the uniform looks like a flat line and you know different distribution have different shapes, this looks like a symmetric bell, bell shaped curve and the probability density function of this distribution is characterized by this formula. This formula that is shown here and one thing that is noteworthy is that, this distribution has two parameters mu and sigma. So, the distribution itself is defined by the mean and variance, so the mean and variance of this distribution go into the formula and they defined it. So, there is no point saying tell me, what is the probability of value x for a normal distribution, because that question does not make sense. In order to say for a distribution with this mean with this variance, what is the probability of value equal to a greater than x? So, that question means more or you know, what is the probability of finding a value between x and x plus delta, for a normal distribution with a mean mu and a sigma equal to sigma and standard deviation equal to sigma. But, once you given the mean and sigma, it is quite simply this formula that you would use and you can compute the probabilities. So, what is the mean of a particular normal distribution defined by mu and sigma? Well, that is very straight forward, it is the mu, because the distribution is defined by mu and the variance is nothing but, your sigma square. So, you can, it is quite straight forward there is well. The CDF; however, is not something that simplifies very elegantly. So, to define the CDF you would still use your traditional procedure of using the integral and by the way the normal distribution goes from minus infinity to plus infinity, so it make sense to actually use the minus infinity here. So, you would actually use the minus infinity to x, f of x, which is the PDF, which is nothing but, this formula, so dot d x. But, while in many distributions this whole thing simplifies and you are able to do the integration and there is an actual value, with the normal distribution it does not simplify very elegantly without using more complex algebraic terminology. So, the CDF is often just stored in tables, sometimes especially for the normal with mean 0, standard deviation 1 or it is just something that you integrate each time to get. Now, this is a very interesting distribution, because there are lot of things that are normally distributed. So, things like peoples height, weight well height; obviously, with each gender, grades in a class, marks that people score in exams. The core idea with the normal distribution is that, unlike the uniform distribution, which says everything is equally likely. It is the normal distribution says that things in the extremes are less likely, things in the center are more likely within certain limits, which is what gives it its characteristics bell shaped curve. I mean, if this is any attempt to the bell shaped curve, we basically saying that things that are on the extremes, like here and here are less likely and things in the center like here are more likely that is why they have a greater height, with all of these things the y axis is the probability. So, if you take a look at something like heights or let us say weights and you fix a gender, let us say male and you take something like people, who are registered for introduction to data analytic course, then you will find that there might be very few people, who weigh less than I do not know 40 kg, so or 50 kgs, men especially. And you will find very few of them probably weighing more than 100 kgs or so and then, you know and so that kind of tapers off, an either extreme you find less, in the center you find more. But, there are many other distributions are also like this, but this that this is that is key feature of being in a bell shaped curve. The other thing is you know many things after you remove outliers start to look normal and we will talk about an example of that. In this slide, I am just not going to talk about the other things that we will talk about in this lecture, so I am not kind of rushing through it. We will especially take up from here and till here and go through them in detail with slides. But, you are also encounter that there is this things called the binomial approximation, which isÉ We briefly spoke about this when we introduce the binomial distribution that certain problems, which just by definition look like they fall so cleanly as a binomial distribution, for computational reasons could be quite easily approximated to a normal distribution. Although, the binomial is a discrete distribution and the normal is a continuous distribution. We will also talk about something called the central limit theorem, which makes the normal distribution very useful for many applications and also a very interesting concept per se and finally, we will look at the idea of sampling distributions. The core idea being that, if you take a random sample of size x of associated with any variables, so I randomly select five people and measure their heights. Is there a distribution associated with the parameters that I get like the mean and standard deviation? But, we will talk about this in greater detail. So, the first thing is things after removal of outliers. So, here is an example of some real data, where we looked at the total annual household income and you know, so the graph that you see to the left hand side is you know, it is essentially all these households with income up to and we just stop the x axis at a certain point and so, we said let us look it income up to a certain value and the y axis is the number of households. So, I have created essentially a histogram, but that is a proxy for finding the probability distribution itself. So, you can think of the probability distribution as something that looks like this, in this particular case. People cannot have incomes less than 0, so on and so forth. Now, look at the same graph, where I said I am not going to look up to 4 lakh rupees income, but I am just going to concatenate the x axis in 90000 rupees. So, the whole idea was to say that some of these values could have been outliers and we took a certain value beyond, which we go. And already you can see that this graph is starting to look a lot more bell shape. Probably not perfect, but the core idea is this, which is that sometimes once even though the distribution originally might not look normal with sufficient amount of outlier removal, the distribution could truly be know. The second concept that we want to speak with respect to this is the binomial approximation. So, let us just very quickly review, what the binomial distribution is about. We spoke about, how this term in the PDF of the binomial distribution was really, n choose k. So, n combinations, k combinations out of n was the core idea and that is fine. So, if you have problem of the type saying, what is the probability of finding, you know 3 heads out of 10 tosses. This works fine, you can substitute the values get the PDF. Now, somebody came and asked you saying, what is the probability of getting 2100 heads out of 5000 tosses. Then, you essentially need to, if you want to use this formula you need to plug in 5000, you know c 2100 or whatever the number is and you know; that is a very large number; that is a very hard computation and you could 5000 and 2100 just an example that could be 5 million and you know 200000 and it is very hard to do those calculations. So, one thing that you can do, when n becomes really large is you can essentially use this formula that you have for mean and variance of the binomial distribution and construct a normal distribution with this mean and this variance and used to answer distribution related question. So, you for instance if there is a 50 percent chance for instance of a coin falling head and tails, you can say well the mean of 5000 tosses is 2500, because you have 5000 tosses times 50 percent probability. So, that is 2500 and that is your mean and your variance also you would similarly calculate by plugging in n equals 5000 and p equals 0.5. And once you do that, you can essentially construct a normal distribution with these parameters and you can answer questions like, what is the probability of there being more than 2100 heads or what is the probability that the number of heads would be between 2000 and 2500 out of 5000 tosses. You; obviously, cannot answer a question like, what is the exact probability of getting 2112 heads, because you essentially converted this to continuous distribution. And the idea of answering a question like, what is the exact probability of 2121 tosses out of 5000 or I mean 2121 heads out of 5000 tosses becomes relatively meaningless, because as n keeps becoming large the probability of any one thing exactly occurring becomes really small becoming close to 0. So, you are interested more in intervals, which is in spirit this, what you can do with continuous distributions and you can use a normal approximation of the binomial distribution to achieve that as long as n is fairly large. Next, we will move to something called the central limit theorem and the core idea here is that the aggregation of a sufficiently large number of independent random variables results in a random variable, which will be approximately normal. So, what is that mean? It just means that look, if you have some process and it is some distribution from that process, so let us say flipping a coin or throwing a dice is the process. Now, central limit theorem says as long as I am aggregating many such processes. So, if I said instead of asking you the simple question of the distribution associated with what I would get, if the roll the dice once. I instead say I want to know the distribution associated with rolling the dice twice and I am going to add them up. So, the first time I will roll the dice and then, I get some number I write it down, I will roll the dice another time and I will get another number and I am going to add those two numbers. Now, the distribution associated with that sum is also probability distribution, because you know it is still a random process; there is still some chance that I can get each value. I clearly cannot get any value less than 2, because first time I can roll 1, second time I can roll 1. So, I cannot get 1, I can only get 2 as the minimum value and the maximum value is 12, I can roll 6 and 6 and that is 12. So, the idea that is being put forth here with central limit theorem is that aggregating it and the word aggregating can be thought of is, you know taking the sum or you can think of it is taking the average, both a forms have, both are essentially the same thing. The difference between sum and average is, average is just divided by the number of times. But, this form of aggregation of a sufficiently large number of random variables results in a random variable, which will be approximately normal. So, let us see how that works. So, on the left hand side of graph out here, I talk about the distribution associated with the single row and this view seen and we have discussed this is uniformly distributed. Why? Because, the heights are all in the same, which is discrete distribution and you see, it is uniformed distribution and it is 1 by 6; that is what I have shown here today. On the right hand side, I show you the distribution of the sum of two rows. So, you can think of it is, rolling it once writing it down rolling it second time. So, you can think of your hands having you know two dice and you roll both of them and you sum up, what you see and what shows up. And already you can see that the distribution is started moving from uniform to something else. This happens to be triangular, but that is just the first step towards starting to look more and more bell shape. What is happening? Now, although the probabilities of rolling 1 through 6 were uniform, the summations; however, are not equal. So, the probability of getting a 2 is lower than the probability of getting a 3 and that should be fairly intuitive. For you to get a 2, you need to roll a 1 the first time and roll a 1 the second time. But, there are many ways in which you can get 3, mainly 2. You can roll a 1 the first time and then, the roll a 2 or you can roll a 2 and then, roll a 1 and that kind of keeps increasing till you hit the point at 7, where 7 you can get in so many ways, you can roll a 6 the first time and then, roll a 1 the second or if you not, if you thinking of rolling both of the same time you can get a 6 and 1 or 1 and 6, 3 and 4, 4 and a 3 or 2 and a 5. So, there are more ways of achieving the same thing of achieving a 7, there are fewer ways of achieving a 2 or 3 and so, you already have something that is looking more like a normal. Now, you go further as I discussed; obviously, the average of two dice is the same as the sum of two dice. So, these two graphs are identical, this one and the next one on the next slide. These two are identical, except that are changed to average, so this axis is different. It goes through 1 through 6, the other one went from 2 to 12, but these are essentially identical graphs and this is also a triangular distribution. But, look it, what is already happening. Now, if I say the average of three dice, so I am going to roll three dice at the same time or I am going to roll one after the other after the other. There are all independent either way. What you going to see is that, now this is started looking a little bit more you know triangular, let us starting to get that little bit of inflection and so on. And, so as you increase this number more and more, as the idea I said you get something that looks fairly normal and that is about the central limit theorem is about, that you aggregate a sufficiently large number of distributions when you start getting a normal distribution. Now, this is the really important point for what we are going to say in next associated with sampling distributions. So, we going to start a fresh and sampling distributions, but I just want you to keep in mind, what we have discussed now in central limit theorem. So, jumping give us, now to sampling distributions the idea here is very simple. So, lets the you have some original distribution and lets for now, say this distribution is normally distributed. And let us say this normal distribution has some mean, which I have shown with this blue vertical line and lets call that mu, so this point is mu. And let us say it has some standard deviation I am just referring to the dispersion through the arrow that is not the exact link of the standard deviation. But, it is have some standard deviation, which can be represented is the variance is represented as sigma square and by the way this mu comma sigma square is fairly norm nomenclature that just means it is a normal distribution with that looks like a with mu and sigma square all though that looks like an m, so may be a little bit more like mu norm. So, you have this distribution, now let us say N let us give it a name, so let us say this is the distribution of heights this is the distribution of what we lets keeps weights. So, this is the distribution of weights staying consistence with the previous example of the men or the male members, who registered for introduction to data analytics. So, may be this distribution starts somewhere at I do not know 50 kgs and goes all the ways to say 100 kgs this is this is the distribution technically it can go all the way to infinity. Because, by definition and normal distribution can go to infinity and on this side it can go to minus infinity. So, this is this is the normal distribution. Now, let us say that I took a sample from this distribution. So, these data points represent the different samples and in this particular case I have taken just six samples, but well that can be more and the heights mean nothing the sample just mean, where they fall on the distribution. You can; obviously, use that and build a histogram and the idea is that if you build a if you take a sample large enough that histogram will fit very neatly to this curve, which is the normal distribution if that sample is very large. If the sample is not you might get a different histogram, but what we most interested in is taking this sample and computing some key statistics from this sample. For instance if you took this sample and computed the arithmetic mean of the samples you will take each data point right and let us say you call it x 1 and the next data point got called x 2 and so on. Then, what you looking at is x 1 plus x 2 plus dot, dot, dot divided by N that is your arithmetic mean and, so you compute an arithmetic mean. But ,since you got some finite sample got like 6 points and may be you have in an others instance 10 points the question is will your arithmetic mean always be equal to mu remember mu was, what defined this distributions this distribution is by definition mu comma normal mu comma sigma square. But, if you take a sample if you take some axis and compute x bar we differentiate between mu and x bar meaning mu is the theoretical mean, where as x bar is the sample mean it is. If you take a sample of size n and compute an x bar will this x bar be equal to mu and both intuitively another wise the answer is no theoretically if your sample size is equal to infinity; that means, you take infinite number of samples, then perhaps your sample means will be equal to, then your sample mean again an theory will be equal to mu. But, that is not a practical situation, who takes infinite samples like that that by definition does not does not make is not very useful. So, if you take a finite sample and in this case 6 and another case can be 20 next less say 20 and you compute a sample mean it is not going to be equal to mu. But, the idea is that it might the idea is that it is also a random variable, what do you mean by that you mean that say I mean one time you go about you take a sample you take a sample of 10. Let us say and you take the mean of that sample you will get a particular value that will not be equal to mu it could be equal to mu. But, you know it could be little less than mu little greater than mu now, you go do that exact same thing again you will get some other new value. So, what; that means, is you have a random variable on your hands and the random variable is about the distribution of the sample means for a given size end. So, that is what that is a core idea associated with sampling, which is that from the original distribution you take a sample and you compute a means and you get a certain value and, but that value itself belongs to a distribution that distribution changes based on the sample size. So, suppose we were like I said if you took infinite if your sample size was really large if it was infinite, then perhaps you will not even have a distribution you just have a line out here which is that you almost always get mu because your sample size is, so large. But, if you , but think of the other extreme suppose your sample size is equal to one that is each time you took one point from the distribution and you computed the mean of that point, what is it means to compute the mean of a single points it is that number itself. So, let us say we were looking at 50 kgs to a 100 kgs you took a random sample of one. So, 75 you know 65 kgs this time that was the random number I picked the average of 65 is 65. So, if you had a sample size of one what could the distribution of sample means look like the answer is it would look exactly like this distribution, because you taking a sample size of one its essentially like and you computing the average of that, which is nothing but, that number itself. So, it is essentially like just re plotting that graph, now if your sample size was greater than 1, but less an infinity, what happens is if your sample size as the sample size gets larger and larger you are dealing with the distribution step. Because, each time you take a sample of, let us say 5 or 10 or 20 you are going to get some sample mean from that and that sample mean is not going to always be equal to the exact overall population mean. And, but it is going to be some number nearby and the idea is that as in this particular case we had a normal distribution and the idea is that as long as we taking the average of a some number. Let us say 10 or 20 or 30 or 40 or 50 samples you are going to get a mean. But, that mean is not certain it is not certain, what that mean is going to be you know it is you know it need not be mu you know that for a fact. So, what you are essentially getting is another distribution you getting a random number from another distribution and this the distribution of the sample means now it is. So, happens that when your original distribution is normally distributed the distribution of sample means is also normally distributed, but they might be some questions you have in this regard. So, for instance what is the shape of this distribution the quick answer to the question is when the original distribution is normal like we said this distribution of sample means is also normal. But, we also went through this central limit theorem where we said as long as you are aggregating is sufficiently large number of distributions the resulting distribution starts to look normal. So, even if your original distribution is not normal as long as your aggregating a sufficiently large number this distribution of sample means becomes normal. So, that is the shape, now what is the mean of this distribution, what is the mean of the distribution of sample means the quick answer is because your just taking the average of some numbers if you what to do this is sufficiently large number of times you should not get a mean that is biased. So, the mean of this distribution will also be equal to mu, but it is clear that the standard deviations are not the same right the standard deviation would be the same if your sample size was one in which, case you are not really sampling you are just taking a single data point. But, depending on the size of the sample the standard deviation is going to be typically lower a it will always be lower as long as the sample size is greater than 1 and the relationship is nothing but, sigma square. So, if you are using sigma square it would be sigma square divided by n when you use small n refer to the sample size, but you can also think of it as taking the square root of this you can also think of it is sigma divided by square root of n. So, this would be the standard deviation and this would be the variance. So, this is var this is the variance and this is yours standard deviation. So, that is the that is relationship that is very useful to remember, now you might have a question saying. So, we did all of this work to say let you have an original distribution you randomly sample from that distribution and you compute a mean an arithmetic mean then that arithmetic mean that you compute belongs has a distribution of its own and we spoke about the mean and shape and standard deviation. Similarly, if you take a sample from the original distribution and you compute a standard deviation of that sample. Then, would you be is that sample standard deviation also coming from a distribution and the quick answer to that question is yes and in the if you are using a normal distribution to start with that is distribution of the sample standard deviations tends to be chi square distributed and that is also something that we will encounter. But, are focus for now has when on the distribution of sample means and the important things to take away are if you start with an original normal distribution, then by theory you will have a normal distribution for your sample means for whatever sample size. But, given that we also learnt about the central limit theorem even if you start with an original distribution that is not normal as long as you aggregate sufficiently large number of as long as your sample size is large enough and the distribution of sample means is likely to be normally distributed. We spoke about how the mean of the distribution of sample means should be no different from the mean of the original distribution, because you are not adding or subtracting any number you just taking average numbers you are just taking numbers and taking the average of that. So, if you do that many times the distribution that you get from that should also be centered around the overall grand mean of the original distribution we spoke about how the standard deviation. However, keeps reducing, so as long as you are aggregating more numbers your standard deviation will reduce in this rate it which, is reduces is as a function of this square root of n the sample size. So, sigma divided by square root of N is the rated, which the sample size your standard deviation of the distribution of sample means is with respect to the original distribution and actually that phenomena you should be able to see even in the examples that we took of the central limit theorem just two kind of show that you again see in this particular example and I will erase the red mark in this particular example I was focusing more and showing a central limit theorem about how the shape changes. But, if you take this graph, which is this uniform distribution out here and there is some standard deviation out here right the sum spread around the mean correct this sum spread. Now, take a look at the average of two dice the mean is the same centered around the 3.5, but this spread has decreased right before this spread was like this. So, there was there was the higher probability of seeing values in the in the earlier graph up here you had data points that were with the higher probability further away from the center at 3.5. Now, you do not see the probability of finding points far away from the center has reduced the these are low probabilities, but the probability finding things close to the center is increased. So, therefore, the standard deviation of this distributions is lower than the standard deviation of the uniform distribution and that effect is going to just increase the probability of extremes keeps becoming lower there by the standard deviation becomes lower given that you are for all of these you are starting with one and ending with 6. So, the that example shows both the central limit theorem meaning the change in the shape, but you can also capture this idea which is the distribution associated with sample means and in the previous cases the sample size was two in the first example and the sample size was three right because we were averaging two dice or three dice. So, the distribution that results from that is having a lower standard deviation. So, that should give you an idea of the whole idea behind sampling distributions and this is the good concept to revise or understand deeply. Because, a lot of inferential statistics is based half of this and with that we conclude our lecture on random variables and probability distributions. We will continue a next class and focus more on inferential statistics. English - NPTEL Official

10) Inferential Statistics - Motivation
Hello and welcome to our lecture on Inferential Statistics. In this particular lecture, we are going to be talking more, giving you more about Motivation, for why we need inferential statistics and we will be talking a little bit about, what kinds of problems you can solve using these techniques and you know, where you would applied and when you would applied. In the subsequent lectures, we will talk a little bit about how you would applied and also, why you do some of the math that you do, why you do some of the computations. So, understanding it a little bit more on the nuts and bolts level is something that will follow. But, in this lecture we are going to focus on why you need inferential statistics in the first place and we are going to do this through the lens or something called hypothesis testing. So, hypothesis testing is very widely used an excepted tool for a lot of data analysis and if you understand inferential statistics through hypothesis testing, many other concepts in inferential statistics just fall in place. So, things like confidence intervals and so on, which you might have heard become fairly easy to understand and process. So, having said that let me jump into the subject. The idea behind inferential statistics is to make some inference about the population from the sample. Just to jog your memories, I think we have spoken about population and sample a couple of times. But, this is different from, what you would have done with descriptive statistics. With descriptive statistics you do not care about population or sample, in the end of the day you have some data set. And for simplicity sake, assume that was a sample that you got from population and this sample you will very content in descriptive statistics with describing that data set with describing that sample in case you have a sample. But, here the kinds of problems that you are more interested with inferential statistics, a ones where you have a population and you are getting as just a sample from that. But, from this sample I do not want to just say something about the sample, I do not want to talk about the mean of the sample, I do not want to talk about the variance of the sample and I do not want to talk about the centrality dispersion. My goal is to say something about the population. I only have data, which is the sample. I do not have the data associated with the population, but with this sample can I say something about the population. So, that is the core idea and just a kind of jogs some of your memories, the idea behind population and samples is fairly simple, we looked at it to the couple of examples. But, you can think of the population in one of two ways. The first and the more obvious way is that, there exists this really large data set associated with the phenomena. So, let us say the phenomena was the height of all boys, who are in 10th standard in public school. So, that is, so that could be a very large data set and let us say that was for India. So, it is very large data set, there are lot of children, who are in the 10th standard, who are in public schools across in India. And you can think of that as a population and you want to say something about that population and you might not have the data. So, you go to take a sample, so you select 5 schools or 10 schools or you select 200 students through a census process randomly and take that as a sample. So, you take a sample of, you know some subset of students from the population, that is one way of thinking of population sample and another way could be that the population itself is moreover theoretical abstract concept. So, you could not have an actual data set, but it could be something like I have created this new machine and this new machine is going to start making certain products and let us say, you are very interested in a product dimensions. So, let us say the diameter of a product is machine makes. So, you put a raw material in to this machine, the machine splits out some finished product and this finished product should have a diameter, I mean has some diameter. Here you do not, because it is a new machine you might not actually have a population; that is a really large data set that exists somewhere. The population here is the concept that, if this machine going to create infinite such products without any change in time space, the dimensions of these products would be the population and ultimately you might say, hey let us just for the first time run this machine and make 10 products and these 10 products that I physically have and I measured and so on, is the sample. So, here is the case, where I still want to use the sample to say something about the machine in general. Not just the 10 products the machine is turned out, but the concept of the population here is not an actual finite large data set in my hands, it is more about concept. Now, having revisited population and sample here let us again see the statement, which is that inferential statistics you make, you want to say something about the population from the sample. So, as I said the major aim of this lecture is to motivate you to see why inferential statistics is important. So, I felt that the best way to inference to that might be to give you some examples. So, that is what we are going to do and I start with some simple examples. I have broken them down in to one sample and two sample examples and pretty soon, that will become clear what that distinction is. So, let us start with the first example which is a one sample example on that upper left of your screen. So, the idea here and I am just, so you know we are in this part and the idea here is that, let us say we were interested in noting the average phosphate levels in our blood and I do not have a medical background or anything, so do not look at the medical aspect of this examples. But, let us say that your doctor or doctors in general or you know public health advocates, say that the average phosphate levels in bloods should be less than 4.8 milligrams per I do not know deciliter. So, again irrespective the units, so the whole idea is that this number, which you can get if you go measure your blood should be on average less than 4.8. The key here is an understanding that they should be less than 4.8 on average. So; that means, the doctors or the public health advocates understand that sometimes it could be greater than 4.8 and that perhaps in this particular case is not a cause for along. Again do not focus on the medical aspect, I do not know if it is not, but this is the situation I am creating. So, but the important thing the doctors have told you, it is on average it should be less than 4.8. So, let us say you say and you know this; obviously, variation. So, it really depends on what you ate that day, it depends on what time of the day you take the measurement, it depends on what instrument you used to take the measurement, it depends on how much water you had. So, let us say there are lot of factors that you do not seek to control and that is the whole idea behind this. But, you want to take a set of measurements and you want to take a set of measurements and answer the question us to whether the average phosphate levels in your blood. In general, not just on the sample that you have taken, you do not I mean the sample could be anything, but you care about, in general is my average phosphate level and blood less than 4.8. So, why you… So, the question might arise you know, why you distinguishing between what the sample says and what reality is and that is going to become clear in second. So, let us first go about trying to answer this question. So, the first thing is, if you took a set of measurements and let us say, you got consistently very low values. So, you let us say you got 2.4, 2.5, 2.1, 2.7, 2.3, 2.9 fill of four more numbers in the two points. Then, I guess you really do not need a statistician, you can look at the sample that you got and you can say, look I am fairly certain that even if I went on taking more and more samples or that if I woke up another day through all these data or took another sample or if I took infinite many samples, in either case it looks like my average is going to be less than 4.8, that is fine, you know intuitively that seems obvious. Similarly, the flip sides, suppose you wanted to take this data and you consistently got 5.5, 6.1, 7.7, 6.9, so on, where every single data point is significantly greater than that 4.8 mark and approximately in that same region, meaning it is not widely moving around. So, that is also an intuition that you might have, that if one second it is 6.5 and the next second it is 2.1, you know it is widely moving around. But, here I am giving you examples, where 6.1, 6.7, 7.1, 7.3, 7.4. So, it consistently significantly greater, again you do not need a statistician. Somewhere your intuition, you just say look, I mean based of the sample I am willing to bet that my average phosphate levels are less than 4.8 mg per dl. But, then it gets a little tricky. What happens if you had, you know numbers, you know some of them below some of them above. So, some of them are less than 4.8 some of them are greater than 4.8 and in some sense, what you do then and the instinct to the intuition there sometimes just to say well, let me take an average of the sample. If that average is greater than 4.8, then perhaps I should conclude that my average is greater than 4.8 and this is small problem with that kind of an approach. I mean, assume that you got the readings like as such as 5.1, 4.8, 4.9, 4.7, 4.7, 5.3 and you got some set of variance and you took the average and that average was 4.85. So, you saying, you know what the sample showed that it is greater and you conclude, for instance that the average phosphate level in my blood in general is greater than 4.8, based off of the sample that I just saw. The problem with that could be that may be if you just took two more data points. Let us say you took two more data points, you increased your sample by two more and you got a 4.6 and a 4.7 and all of a sudden, because of these new data points, your average you know slights just below 4.8. Something about doing this process, we just take the average of the sample and make a conclusion, does not seem correct for this reason. And you know, another way of looking at it is also that, if we looked at this notion of sampling distributions. So, we know that, let us take a look at the same graph that we looked at the end of the last class. So, you had this thing, which we described as the original distribution. And let us say for now, let us just say for now, the truly your… This distribution by the way represents the amount of phosphate in mg per dl that you will see in your blood if you do a test at any given point of time. So, you get numbers from this distributions, so sometimes you get a 5.1; that is what, that is the dot there, sometimes you get a 4.4 that is the other dot there. But, in the end of the day it looks like on average, given how I have drawn it. On average it is only 4.7 mg per dl, which is less than the 4.8 mark that we were interested. Now, you go and take a sample, this is the same example as a last time, so you took the sample and you some data points. So, you took 6 data points and this is what you got. Now, if I want to just take the sample average and I am just eye balling it here, if I just took the average of these numbers I would say that average falls somewhere here, would it to see. So, this might be the average of these numbers or maybe it will fall right on this data actually. So, let us say this is somewhere out here is the average and the true is this sample average is greater than the 4.7 and if you want to just go by the sample average, we might have conclude it that the amount of mg per dl is greater than 4.8 or whatever. But, here is a good part, you now had a class in statistics that told you that the average that you get from the sample is not always going to fall on this 4.7. As your number of samples tends to infinity? Yes, it will converge to this point. But, if you got a finite number of samples and that is not, so it is not going to always be exactly on 4.7. So, what is it going to be? What it is going to be is another distribution. So, if you had N samples and this distribution will change with more samples, if there are many, many, many, many samples, then literately if as the number N tends to infinity, this distribution will pretty much like flat on this line. But, if not, you still getting the sample mean, the mean that you calculate from the samples literally a random variable that you getting from this distribution. So, it is literally like you just pick random points from this distribution. So, you get a point any where here, you probably will not get a point here, because it does not… The probability of getting this point from this distribution is very low, it is almost 0. So, you will be getting points from this distribution and as a result, just because this number is greater, that 4.7 should not make you conclude that this mean, which is what you are trying to conclude. You are trying to say something about this line; you are trying to say something about this line, which is the mean of the population and, because you get a sample mean which is nothing but, the number from a distribution should not make you conclude that therefore, it is greater than 4.8. So, that is why you need to do something more, you need to do something more complex than just blindly taking the sample average. So, again we are going to talk later about how you do it, but and what and when you do, but right now I am just trying to motivate for you why you need something else. So, take another example and in this example, we take a problem of proportions. So, let us say the health department or some dentist related body says; only 5 percent of the toothpastes of any given brand can be out of specification. So, out of specification might mean that, you have some ratings on the amount of fluorides tooth paste can have. So, let us say you are allowed any 1000 parts per million of fluoride and you, there are other chemical limits. But, the health department understands, that not every toothpaste can match exactly the ideal requirements. So, let us say the set out limits on the chemicals and they say, look if you are a toothpaste manufacturer, only I am going to only allow 5 percent of your toothpastes to, you know be out of specification, 95 percent of your toothpastes that I see in the market need to fall within my guidelines. Same problem comes up again. You can take a sample of 10, 20 toothpastes and it could very well be that truly this toothpaste brand is involved in a chemical process or manufacturing process, that creates on average only 4 percent. Only 4 percent of the toothpaste that this company makes are actually out of specification. But, it is perfectly possible that you went and took 10 toothpastes from the market and your luck, 7 of those 10 are out of the specification; that is perfectly possible. It might not be the most probabilistic thing, but it is perfectly possible. It is possible that, this toothpaste company is involved in a manufacturing process and chemical process that creates toothpastes and on average, 4 percent of all the toothpastes they make. So, when I say toothpaste, think of it as a toothpaste tube; on average 4 percent of all the tubes they make. Have a chemical composition that is not acceptable, which is fine, because the health department says you cannot go more than 5 percent and this toothpaste company has rising it is hand and saying hey you know, which only 4 percent. But, I now go and randomly sample 5 toothpastes, 10 toothpastes and I find that out of the 5 toothpastes that I randomly sampled, 3 of them are defective. All of a sudden, I am saying 3 out of 5 that 60 percent, you say only 5 percent is allowed and I find 60 percent. And so, is that does not mean the company is not creating toothpastes less than 5 percent rate, which are in conformance less than 5 percent rate, probably no. Again you need a little bit more new on thinking and you need little more statistics to actually answer this question, based off of the sample you cannot just take the sample average. Another example, the third example that we have on the one sample cases, imagine that you are in an insurance company and you find, that there is this particular mechanic shop that is new garage, which does repairs and because most people are required to have insurance. Once the garage kind of writes out an invoice and people who file the insurance claim attach this mechanics invoice and tell the company to reimburse them for this rectification that is claim to the car. And let us say, this is a new garage and you know the insurance company is suspecting that these guys are cheating, that their set up as a place to not do any real work, but just write really high invoices. So, that the insurance company, so they are involved in some kind of a fraud. So, one thing that the insurance company can do is saying, I am going to look at that next 10, 20 or whatever repairs. So, let us say up to this point this garage is not made a single you know claim, but it is just being set up, but the inside word is that they are trying to cheat this system. So, once a garage gets set up, this insurance company is ready. The first 10 claims or 20 claims at this garage files or 30 claims, they take those claims and they see how that compares to the national average in terms of average claims. Again the problem is just, because this sample is greater than the national average, can we conclude yes these guys are cheaters and just, because his sample average is less than the national average, can we conclude these guys are not cheaters and the answer to both those questions is no. In some cases, like when I was talking to you about the case, it might be brutally obvious, where every single data point is so high or so low, that you are like I do not need a statistician to tell me that the answer to this question. But, when it is not that case you need little more you need inferential statistics to answer that question. So, let us we look at the single sample cases by that we essentially mean that there was one data set and you are essentially comparing that data set to some bench mark number that you had in your head. Lets now, move to two sample situations the example here is let us say that I am running up foundry and some guy comes in consultant comes in says you know if you change the temperature a little bit of the molten metal that you pouring in whatever. Let us say change the temperature down by degree over two and I assure you that average number of defects that you see in your costs in metal costs will decrease. So, like a mechanical engineering application you like may be the consultant knows what is talking, but how do I test it, how do I test it and the answer to that question is it is you can do the following, which is before you do that before you go change things you can measure the average number of defects in your costs and you do that for 10, 20 data points. And then, you do with the consultant, which is change the temperature and then, you collect the another 10, 20 data points. Now, let us go back to the question if the average of the sample the first sample, so now, we have sample a sample b sample a corresponds to before the temperature was change sample b corresponds after the temperature was change. Now; obviously, I mean in all likely hood there is going to be some average to sample a and there will be some average to sample b. In all likelihood these two are not going to be the same one is going to be higher or lower than the other just like in the single sample case if they. So, dramatically different these samples and when I say dramatically different I mean dramatically different with respect to some amount of variability there is in the two samples as well. Then, you do not need a statistician to tell you it is like, so obvious that changing the temperature dramatically reduce the number of defects. But, in many cases you do not know and in those cases it is not obvious to say the average of sample A was different from the average of sample B, I mean go back to I am going to erase this, but erase the red ink, but go back to this example. Let us say that, lets say the this is the original distribution of the number of defects, so this is the original distribution and let us say this is 3 defects and this is like 9 defects, now that is too high let us say this is 5 defects and this is 1 defect. So, this original distribution is what I care about that goes from here to hear these numbers of defects you will see in a costs. Now; obviously, if I take a sample of 6 or, so I get a random point I let us say this is the random point I get and the erase the other one done erased. So, I basically I took the original distribution I took the sample of 6 costs and when I take the sample of 6 costs like we discuss we the average is not going to always we exactly 3 is going to be some number that falls in to this distribution the distribution or sample means and I drawn a random number that I got out here from that distribution. So, good now let us say this consultant who is telling you to go increase or decrease the temperature was completely wrong. Let us say he had no clue, what he will say he was just lying, but fortunately, unfortunately whatever is said does not make a different I mean he lied in that you know he said is going to improve the process it did not improve the process luckily it dint make the process was. Now, you go take and because a processes not change the original distribution is not change after that temperature is change the number of defects you going to be receiver also exactly in conformance of this distribution it is in conformance to this outside distribution the original distribution. So, that has not changed essentially this mean has not changed this mean has not changed. Now, you go take a sample of sets you get another sample average and this sample average again is going to belong to this distribution and let us say this sample average was this value. So, this is the new sample average, so this is new it is that is new now you cannot say hey this new sample average is higher than the old sample average. Therefore, the population mean is different it is not I mean I just gave you an example, where the population mean truly did not changed, but how you could have seen two different sample averages in concluded that one is greater than the other. So, this is why again you need more than looking at the sample average of a and sample average b and saying hey one is higher than the other. So, we should believe that what the consultancy said was correct or the other way around you know if the if you conclude that what the consultancy said is definitely made things was that is also marked correct perfectly possible that there was truly a change, but because of luck you know you saw things other way. Now, another example you can think of and this the next example is one where I wanted to emphasis variation the rather than just the mean, let us say you have two different manufacturing processes and you want compare their variance of the finished product in each batch. So, you have manufacturing process A manufacturing process B and they make batches of you know finished material finished product and within each batch this some amount of variance of each products that some amount of variance right variances the inherent variation between one part to the next. And, let us say I care about that the variance let us say I do not in a particular batch I do not want one product to look very different from the next product I do not care about the mean. But, I want them to all be consistent again you would use the same concept you would take the first batch, which is made from machine A calculate the variance of it calculate the variance of that sample take the second batch made from machine B calculate the variance of that sample. And again the overall concept exists you cannot just say sample of sample variance of machine A is lower than sample of machine B. Therefore, I conclude that machine A better than machine B it is perfectly possible than machine A actually worse in machine B. But, because you ultimately only have a sample that machine A got lucky ultimately it goes back to this idea that you see in this distribution. But, sometimes you get number on this side of the distribution sometimes you get a number on this side of the distribution. The more data point should take the overall variance of this distribution reduces, which is why if you had infinite number of points right none of these problem is exists, but you do not. And, if you dealing with that then there I think to do this to use inferential statistics to look closely at the data. Another example that is often coated is things like are tenth standard girls taller than tenth standard boys in India for instance we all know that in terms of average heights men have larger average heights than women, but we also heard that girls start growing taller earlier. So, I do not know is 10th standard breakeven point at least some statistics text books sink to think, so. So, you could have a simple question like the population here is 10th standard girls in India tenth standard boys in India you are ultimately taking the sample and based of the sample, what can you say about the population. And you know sometimes the story is obvious from the sample itself sometimes you need inferential statistics to come in and tell you whether you can say something concrete or perhaps you cannot say anything concrete and that is also something inferential statistics will tell you. But, ultimately it is not as simple as just saying the average of sample A is greater than the average of sample B, therefore I am going to conclude one way or the other. So, let us go to what it is that we will be trying to do with inferential statistics I am just going to go through the overeating principle and keep this in mind will revisit this slide a couple of times. But, I think the ultimate test in some sense of you understanding, what it is in, what it is and how it is and why it is will really become clear once we do the actual math with each test. And ultimately these will come in the form of test I mean some of you might have heard of these test like t test, z test, chi square test, f test, ANOVA and so on and we will go through each of these tests. But, here is the overarching principle with respect to hypothesis testing is to have this have a null hypothesis and an alternate hypothesis. So, for instance in the fluoride case the null hypothesis could very well be let me erase this, the null hypothesis could very well be that you have less than 4.8 mg per dl. So, the null hypothesis is that the actual phosphate levels in the average phosphate levels in blood for person x is less than or equal to 4.8. And, so the alternate hypothesis would be that its greater than 4.8 the important thing is the null hypothesis and the alternate hypothesis in some sense together should be mutually exclusive, which means that if it is greater than 4.8, then it cannot be less than or equal to 4.8 and vise versa and you know collectively exhaustive there should essentially cover the entire space that you are interested in talking about. So, I mean meaning that the average phosphate level is either less than or equal to 4.8 or greater than 4.8 it cannot be neither. So, its collectively exhaustive, so then you what we will be doing and you not been talk this yet, but you will be doing some basic calculations or arithmetic on the data to create a single number call the test statistic. So, you do not know what that is yet, but what you will do is you do the reason I am explaining this to you is to give you an idea that you are going to be working with the sample. So, it is not like a magic in that you are not going to say something about the population without dealing with the sample. So, you will be doing some math you know and it some of that might involved taking things like the sample mean sample standard deviation, but you will be doing some math on that and when you finish with that math you will getting something called the test statistic some of you might have heard of the these test statistics it may be called the z statistics or the t statistic and so on. The crucks of null hypothesis the crucks of hypothesis test is that if we assumed the null hypothesis is to be true. And make some assumptions about distributions of various variables and those we won’t go into that much, but if we assume the null hypothesis is to be true. Then technically the test statistics should be no different than drawing a random number from a specific probability distribution. So, in some sense what we saying is if the null hypothesis is to if that if the true it is a true mean is equal to 4.8, 4.8 this time, because the null hypothesis is true the null hypothesis was is it less than or equal to 4.8, so here the null hypothesis is true. So, let us take the extreme case the true mean is 4.8 and let us say there are some assumptions like that this distribution is normal may be that these vary the samples that you are taking are independent of each other some set of assumptions that you have to take. If all that of its true, then we want to do certain things such that you will get another new distribution you will be doing some math with these data points. See these data points that you got from the sample you will be doing some math with those data points you will do that math’s such that the test statistic would be no different than a single random number that you draw from a very specific probability distribution. If that is the case, then you test the probability that the test statistic you calculated belongs to this theoretical distribution you basically say hey it looks to me like if the null hypothesis is true, then whatever I have calculated of here with the sample should be like drawing a random number from this distribution. So, let me calculate the actual test statistic and see how likely it is that this number came from that probability distribution and that is what we call is a P value. Now, once you have done this process you might say look if the null hypothesis is true, then in the test statistic that I should have calculated should have come from this distribution. But, look at the number that I got in my hand it is, so unlikely that I could have gotten this test statistic from this distribution. Therefore, the null hypothesis perhaps was not true and I am going to reject the null hypothesis or in some cases the test statistics that you get looks like it does belong to this distribution the specific distribution. And therefore, you can only you cannot really say anything it is like you just have no grounds for rejecting the null hypothesis you can just say I feel to reject the null hypothesis the important thing for you might want to look at the this procedure a couple of times, but the important thing that you might want to digest from this is that the P value itself is associated with the probability of seeing this data if the null hypothesis were true and not really the of probability of saying of this hypothesis being true given the data. So, it is probability of data given hypothesis not probability of hypothesis given data. So, I hope that kind of clarifies inferential statistics for you and in the next class we will look at some specific tests and go over how you actually do these tests and even go deeper and talk about why we do some of the mathematical operations that we do. Thank you. English - NPTEL Official

11) Inferential Statistics - Single sample tests
In our previous lecture, we spoke about the need for… We tried to motivate the need for inferential statistics through the context of hypothesis testing. So, we spoke about why we needed it, where it would apply and so on. We concluded that lecture by coming up with a template, coming up with a rubric – essentially of what it is that one needs to do with hypothesis testing. So, we started off with instructions like you need a null hypothesis, an alternate hypothesis; and, the whole thing was fairly general. So, in today’s lecture, what we are going to do is… Today’s lecture continues our focus on hypothesis tests. And, we are going to talk about something called single sample tests. In the last class, you would have seen that we gave two sets of examples; we gave examples of single sample tests and two sample tests. And, today, we are to going focus on single sample tests. And, what we are going to do is we are going to illustrate that template that you saw with by illustrating one test. So, the test is going to be the single sample z-test and we are going to show you the mechanics behind it and kind of give you the reason of why we do some of the math that we do. And then, we will talk about some of the other tests that are there as well. So, the single sample z-test is a test that is used when you want to make some inferences about the population mean. Note that again there is the clarity here is that, you are not saying using the sample, but you are not interested in just reporting the sample mean, which is what you might have done with descriptive statistics; but, you are interested in ultimately making a statement about the population means. And, this is a test, where you need to know the variance of – you can think of it as variance or standard deviation; but, you need to know this variance of the population; and, that is a requirement. So, it is not the same thing as computing the variance from the sample. So, that is called the sample variance. And, there is a formula for that. There is actual data. But, this is more useful when there is pin historic data and you actually know the population variance. But, ultimately, you are doing a test to see if the population… You are doing a test about the population. So, another case where this test finds an application is when the sample size is fairly large. So, this is the one exception to this rule that you need to know the variance. In some cases, when the sample size is fairly large and larger gets defined by approximately 30. So, that is the magic number that people use here. And, when the sample size is 30 or larger, you consider, you reason that you have a large enough sample; and, in that one instance, you do not need to know the variance; you can actually calculate the sample variance from the data itself and use that for this test. So, just to quickly summarize with the single sample z-test, you have a single sample and then you are testing… You are making some inferences about the population mean based up of the sample. And typically, in a single sample z-test, you need to know the variance with the small exception that you can also use the single sample z-test if you have a large enough sample size and you do not know the variance. So, the example we are going to use to motivate this test is one that you have already seen in the previous class. So, we spoke about this problem on average phosphate levels in blood; and, we said that we created… I want to be very clear; we created this kind of a medical scenario; I am not… I am not saying this is medically accurate; we are interested in the statistics of it. So, we imagine this doctor or this public health system, which says that your average phosphate – the average phosphate level in your blood should be less than 4 point 8 milligrams per deciliter. And, the idea here is that, doctors or whoever understands that not every time. So, you take a blood reading; you take a blood and you take a reading of the phosphate level. Not every time is it going to be 4 point 8 or less; sometimes it might be more, sometimes it might be less. But, the whole idea is you are trying to see if the average is less. And, when you say I am interested in the average, you are not interested in the average of some sample that you have taken; and, here a sample would be if you took 5 blood tests; perhaps different machines give different results; perhaps different times of the day give different results; perhaps what you ate in the morning affects it; but, the whole point is you just have that sample at hand; but, you are interested in saying something about the population. So, let us just step back and go through each of the bullet points one more time in the context of this example. So, we said what are we testing? I am interested in the population mean. So, you might ask yourself in this particular example what is the population? And, you can think of the population in a couple of different ways; you can think of it as a concept of what your true average and true distribution is. So, let us say that, yes, you can take a blood test. And, when you take a blood test, it is like your taking a sample. But, there is the concept of what – of what the phosphate is in your blood at all times. So, doing a test just gives you one peak into the reality; but, there exists this concept of reality, which is that, there is some distribution. And, we are assuming that distribution does not change over time; and, that distribution has a certain mean and we are very interested in this mean. And, this is the distribution of the phosphate levels in your blood. This is that reality, which we do not know and you can think of it as this oracle somewhere that knows what your true phosphate levels in blood is at all times and at some distribution. And, at any point of time, you go take a blood test, it is like you are getting a random, you are getting a number from this random variable, which is in the form of distribution. And, you are very interested in the mean of the distribution. So, that is the population; population is your true phosphate levels in your blood at any point of time. And so, it is a concept. And, if you kind of like to think of population as actual data points; another way you can think of this is to say the population is what is this data set – this is very large data set that you would see if you were to continuously take blood tests – infinite number of times with infinite number of machines over multiple days or whatever. So, you can think of constructing this population in your head as a very large data set. But, the truth is ultimately, you never have the data set or you do not… otherwise, we would not be doing inferential statistics. But, what you do have is a sample. And, the sample can be of some size; we have not discussed that. It can be 5, 10, 20, 30, 40 data points. And, the idea here is that you know the variance. So, I have said that it is a known standard deviation of 0.4 milligrams per deciliter. So, just mind you that, this is the standard deviation, not of the data set, of the sample set you got; but, it is the standard deviation of the population. So, it is like this. This population distribution – we are trying to answer some questions about its mean. But, someone somehow you know what the standard deviation of this distribution is someone whispered it to you or you know it from fundamental principles or you might reason that historically it has been equal to this value and should not have changed. But, you know the standard deviation in this particular example. And, we have already talked about how – if you have a large enough sample size, you do not actually need to know the standard deviation, you can compute it. So, here is the data. So, I have told you that, we know the standard deviation; but, you also… This out here is the data and I have explicitly not given you the exact data points. So, just to give you an idea, each of these data points comes from going and doing a blood test. So, you did a blood test and you got 4.1; you did it again, you got 3.9 and so on. And, there is this list. And, this is what we call as a sample. So, what do you do with this data? How do you conduct the test? Let us go back to this rubric that we created that is just it is like this template that we discussed in the end of last class and to conduct any hypothesis test. So, the first bullet point have a null and alternate hypothesis. And, that is what we are going to do. The null hypothesis here is to say that, mu naught, which means – which refers to the population mean. And, you can use the… I have used mu naught here, you can use mu as well; that also you might see text books do that. But, the idea is that, the null hypothesis says that, the true mean of the population is less than 4 point 8 I do not know the answer to this question; that is what I am hypothesizing. We are going to do some mechanics; we are going to go through some process. At the end of it, we are going to see if the null hypothesis is true or not colloquially speaking. So, a null hypothesis like we said, the null and alternate together need to be mutually exclusive, collectively exhaustive. Mu naught says that, null is less than 4 point 8; that is the null hypothesis. So, the alternate hypothesis should be that mu naught is greater than 4 point 8. The next step we said is do some basic calculations – arithmetic on the data to create a single number called the test statistic. And, the math that we are going to be doing here is fairly straightforward. We are going to take x bar, which means a sample mean. The sample mean here would just mean that, you take these data points out here and you take their average. So, you take all the data points that you have collected and you take their average. So, we do that. And, we then calculate x bar minus mu naught. And, here mu naught is 4.8; it is the number that you are hypothesizing. And, you divide that by sigma divided by square root of n. So, here sigma refers to that standard deviation that you already know. So, that is the 0 point 4 that we spoke about out here. So, we are already given that, the standard deviation is 0 point 4. So, given this, we compute x bar minus mu naught. So, x bar is the average of the sample; mu naught is the 4 point 8 – the number that you are hypothesizing; sigma is the standard deviation that you are given; and, n is the number of data points in that sample. So, if your sample size is 10, 15, you would substitute 10 of 15. So, that is how you calculate something called the z-statistic. So, why you are calculating this value? We said in the next bullet point that, if we assume the null hypothesis to be true; and, make some assumptions about. Distributions – I would not say that every time; but, if we assume the null hypothesis to be true, then technically, the test statistics should be no different than pulling a random number from a specific probability distribution. So, if that… The whole idea is if the test statistic – if the null hypothesis is true, then this test statistics z-stat should be equivalent calculating z-stat. So, you can plug in numbers for x bar, mu, mu naught, sigma and n. And, you will calculate a z-stat. Once you calculate the z-stat, if the null hypothesis is true, then the z-stat that you are getting should be equivalent, should be the same thing as pulling a random number out of a specific probability distribution. What is this specific probability distribution? In the case of the single sample z-test, this distribution is called the z-distribution. And, the z-distribution is nothing but a normal distribution with mean of 0. So, I have used this nomenclature; we have discussed how this is standard nomenclature; but, the n here means it is a normal distribution with a mean of 0, which is the first number that you see. So, you have a normal distribution with a mean of 0 and a standard deviation of 1. And, 1 square is a convenient way to represent it because you know very clearly that, you are talking about… 1 refers to the standard deviation and 1 square is also equal to 1. So, the standard deviation or the variance is equal to 1. And so, this is what is known as z-distribution. So, we are saying that, if the null hypothesis is true, the z-statistic that you compute with this data should be the same or should be equivalent to pulling a random number from a z-distribution. This is a very useful thing to say. And, what we are going to do is we are going to come back to what we do next, because of the statement. But, before we do that, I want to make sure that, you understand why. If the null hypothesis is true, that the z-statistic – computing a z-statistic from the data is equivalent to pulling a random number from a distribution that is normally distributed with mean 0 and standard deviation – 1. So, let us go to the next slide to kind of do that. So, at the start, you had this distribution. So, this distribution is the population distribution. So, this is the population. The population – if the null hypothesis is true, would have a mean equaling mu; correct? I am using mu and mu naught little interchangeably out here. But, I do not want you to get confused by that; but, the idea is whatever your hypothesis is, if your null hypothesis is true, then the mean of this distribution is equal to 4.8. Technically, it is less than or equal to 4.8; but, we are going to take the extreme case. So, we are going to come to one end of it and say it is equal to 4.8. So, this is the extreme situation, where the mean is actually equal to 4.8. And, we are already given the standard deviation. So, you have already been told that, sigma is 0.4 and you are given that value. So, first, let… This is just the distribution of the population. So, you have built the distribution of the population. Now, when you go to take a sample from this population of some size n, what do you get? What you get is as we have discussed, if I take sample of 5 data points, 6 data points; and then, compute a mean from that sample, we know that, the arithmetic mean that you compute from a sample need not always be equal to the mu exactly. Sometimes it is mu, sometimes it is little higher than mu; sometimes it is a little lower than mu. But, we have already discussed as to how. That is also a distribution and that distribution called the sampling distribution is also normally distributed with the same mean mu, but with a standard deviation of sigma by square root of n; where, n is the number of samples you took to compute that mean. So, it you took 5 data… So, if you technically just take one sample and compute the mean from it, you will get the same distribution again – get the same original distribution. So, if you substitute n is equal to 1, nothing changes. And, that should be intuitive. If your sample size is just one data point; when you are computing the average of one data point, it is literally like you are recreating the distribution. If that n goes to infinity, then your variance goes to 0. So, as your sample size keeps on increasing, you are literally going to be sitting on top of this line and you really would not have this distribution. But, for all finite sizes of samples, what you have is a distribution for samples, which is normally distributed with mean mu and standard deviation sigma by square root of n. Now, what we are going to do? Now, mind you, this is the distribution of sample means. So, in some sense, this is the distribution of x bar. Now, what we are going to do is we are going to subtract mu from this distribution. From this distribution, we are going to subtract mu. What effect does that have? From a distribution – from a normal distribution, essentially, if you just subtract a number, it is literally like just shifting the distribution and centering it. So… because you are just subtracting a number, you are not affecting the standard deviation. So, this gets unaffected. But, when you just subtract a number from the distribution, you can think of subtracting from a distribution as you take each data point and then subtract the same number from it. Or, you can think of it as computing that x bar and then subtracting it. But, in either case, it has the effect of just shifting the distribution; it has the effect of centering the distribution at another location. And, that is what happens. When you subtract mu, when you take mu out of the x bar; so, it is… This is what I have done; I have taken mu out of x bar. It has an effect of moving this distribution to another location, which is now centered around the mean equal to 0. Now, what happens if you then take this distribution and divide it by the standard deviation? So, what happens if we divide it by this number – sigma by square root of n? The effect that has is in re-scaling the standard deviation such that you now have a distribution, which is normally distributed. When you divide, something that is already centered around 0. So, the distribution is already centered around 0; which means that it has some positive values, some negative values. And now, you go divide by a number. The effect of the division is that, because it is already centered around 0; it is not actually going to change the central location of the distribution; it is instead only going to widen it or narrow it depending on what you divide it by. So, I have kind of shown the distribution getting narrower; but, that need not be the case; the distribution could have just got wider. It just depends on whether sigma by square root of n was greater than or less than 1. So, if it is greater than 1, then you would by dividing by sigma by square root of n, you will be making the distribution more narrow. But, if sigma by square root of n was smaller than 1, then it would have the effect of widening the distribution. But, ultimately, when you go divide the normal distribution, which is already centered around 0, all you are doing is you are either stretching the distribution or kind of crunching it. You can think it as scaling it. And, now, you have a standard deviation of 1. So, that is how you get the whole idea of x. So, on the first step, we took x bar and then we subtracted the mu from it. That is where we got this value. And, now, after dividing by this number sigma by square root of n, you went and divided this by this number to get your normal distribution with mean 0 and standard deviation 1; which is what we said was the z-distribution; got it; perfect. So, now, what we are going to do is go back to this rubric. So, great; so, we have reasoned that you have to calculate the z-stat. And, we have already said that, the z-stat if the null hypothesis is true, should be like pulling a random number from a normal distribution. What we are going to do now is we are going to test the probability that, this statistics that you got; we are going to say – if your null hypothesis was true, I should have gotten a random number from here. But, let us actually look at the actual number that I got. Does my z-stat actually look like it could be something that I could have pulled out of a normal 0 comma 1 square, because if it does not, then something that I assumed was wrong. I said that, this z-stat should look like something that I pulled out of a normal 0 comma 1 square if the null hypothesis is true. So, let me go to take an actual look at the z-stat number. And, if it looks like it could not have come from this distribution; if it looks like it was unlikely to have come from this distribution, then I can reason that, maybe the null hypothesis was not true and I can reject the null hypothesis. So, that is what we are going to do the next step. The next step is to take the z-stat and plug it in to this normal 0 comma 1 square and see how extreme is this actual number given that we know the normal 0 coma 1 square; and so, we know the potential values it can take. For instance, I have already told you it is a normal distribution – mean 0 comma 1 standard deviation. So, from this distribution, if I were to pull a random number, how likely is it that I would see a number like 55. That is too high. The standard deviation is 1, the mean is 0; it is almost impossible that you would pull a number like 55 or minus 20; so, for that matter. So, if it turns out that the z-stat is too extreme a value to be coming from this normal distribution, then we can maybe make a statement about the null hypothesis. So, that is what we are going to do as a next step. Let me just clean this up. So, I have just restated the official definition. What we are going to be calculating is something called a p value. And, this is the probability of seeing a test statistic as extreme as the calculated value if the null hypothesis is true. With the core idea being that, if it looks too extreme that, if the p value of the probability of seeing this test statistic is really low; then, perhaps the null hypothesis was not true to start. So, for instance out here, if the z-statistic you computed was 1.2; so, I just… I am just giving you a number to go with. Then, the core idea is that, you would calculate a p value based on the standard null hypothesis, which is that, mu naught is less than 4.8. And, you will say… Let me take this distribution, which is the z-distribution or the normal 0 comma 1 square. So, this is the normal 0 comma 1 square. And, I am going to go place 1.2 here. And, I am going to compute a probability; and, this – the probability is a probability to the right side of 1.2. It is the area under the curve out here. And the idea is because you are then quantifying the probability of seeing something as extreme as 1.2 or greater is equal to the area under this curve. And, that might happen to be any value. So, I mean I think in this case, it happens to be something like 13 percent or whatever. But, if this number was really low; if this number was 0.001, you might say look – this number is so low that I am going to reject the null hypothesis. And, if you cannot find something that extreme, the standard thing that you do is you fail to reject the null hypothesis; you technically never accept the null hypothesis. So, that is the core idea. And, you can also think of this in a couple of other ways. For instance, if your null hypothesis were that mu is greater than 4 point 8; then, you would be looking at the area to the left of your curve. But, typically, in a situation like that, you actually would not do the statistical test. So, it is not common to see p values greater than 0 point 5 because at that point, you start by saying look I have computed a z-statistics that is already positive, that is, 1 point 2. And so, I know even before I go put this line out here that, I am going to get a probability greater than 0 point 5. So, for instance, your z-stat was computed to be exactly equal to 0. You know that, if your null hypothesis was mu is greater than 4 point 8; that, it will be 0 point 5. So, any z-stat even greater than that is bound to be greater than 0 point 5 when you do not need statistics for that. But, another interesting situation, which a lot of people work with, is called the two-tailed case. These two are called 1 tailed – 1 tail. So, you also have the 2 tailed case, where your null hypothesis is really that, mu is equal to 4 point 8. And, you are interested in rejecting this null hypothesis whether that mu is too large; meaning it is large enough that you can say that it cannot be equal to 4.8; or, if it is small enough. So, you are happy to reject if you see evidence that shows that mu cannot be 4.8 on either account; maybe because it is the data suggests that it is too large, maybe because the data suggests that it is too small. And, that is called the 2-tailed case. Now, there is lots of different software, where many of the steps that we have done we are actually taking care of and it does not take much. Even a simple excel sheet if you just go down the data and say do a z-test; it will do it. But, somewhere understanding the mechanics of this and getting it to the stage of the z-statistic, at least brings about some sense of control and transparency in your understanding. But, once you get off the stage, computing this area – whether it is on the left-hand side, right-hand side or either side can be done fairly; it is not something that can easily be done by hand. So, what text books do is – if you have taken, most statistics text book will have these pages towards the end. And, they are given in the form of tables. So, you can take z-statistic number that you computed and go plug that in to this table. And, it will tell you the probabilities. And, usually there will be a diagram on top to hint whether they are giving you the probabilities to the left hand side or the right hand side or both sides. And, you know – as long as you know what they are giving you, it is fairly easy to figure out whatever it is that you need. If you want the right-hand side; but, they are giving you only the left-hand side; then, you can just subtract the number they are giving you from 1, because you know the total area under the curve is equal to 1. But, what I am going do is I am just going to give you some simple Excel functions that do this for you. For instance, in Excel, you can just… The convention is to give you the area to the left-hand side. So, instance, what I do here is I subtract that from 1. And, the norm s dist is what refers to the z-distribution. And, the true refers to the fact that I am not interested in just height, I need the area under the curve. So, that is what that… So, for the right-hand side tail, you can use this; for the left-hand side tail, you can use this. A simple multiplication by 2 with the left-hand side case gives you the 2-tail situation. So, with this, we have discussed greater detail the single sample z-test. So, now, let us look at a couple of other single sample test. What I provide you with here is the list of them and the formulas. And, I will give you some idea of the context in which they are used. But, we would not derive it or go through it in the same detail as the z-test. So, we have already discussed the z-test. Let us now look at the next test, which is the t-test. So, we have finished with the z-test. So, now, we are going to look at the t-test. So, with the t-test, it is a very useful test and it also tries to test this… It essentially tries to do the same job the z-test is doing; which is to make some statement about the population mean; so, same problem statement in some sense. The one big difference is you are not given the variance. And, in most situations in life, in statistics, you would not know the variance; I mean just think about how fairly unrealistic it is that you already know the variance of the population in a situation, where you are trying to make a statement about the mean. I mean the only reason you are doing this test is because you do not know what the population mean is. So, you are trying to… You are making a hypothesis, you are taking a sample, and then you are testing that sample, you are working with that sample to make a statement about the population mean. So, there is… I mean think of it as there is some uncertainty about the population mean in the first place and that is why you are doing this test. To assume that in such a situation, you already know the population variance is not very – need not a very realistic. So, this test works the same way. So, if you look at it, it has got the same x bar; it has got the same mu; it has got the same square root of n. But, this s is different from this sigma. And, the difference is here sigma was given in this z-test. So, in the z-test, sigma is given. But, in this test, the s is computed; it is computed from the data. So, you actually go back to the sample data and you calculate the variance or standard deviation from the data using the same formula for dispersion that we would have discussed when we spoke about standard deviation and descriptive statistics using the n minus 1 idea. And, if you do not remember, you can go back and see that lecture. But, the idea is that, you compute the standard deviation and you plug that value in to get the t-distribution. Now, a couple of things that are worth noting is that, we spoke about how if you know the variance, you can use the z-distribution; if you do not know the variance, you can use t-distribution. But, there is this exception. We said if your sample size is large enough; then, you can technically use the z-distribution and just compute the variance and consider the variance to be the truth; consider the variance to be the sigma and go ahead. I personally find that a little confusing; I think that is fine; if… That is what is there in tax; that is what people do and that is the reasonable approximation. But, the point is you cannot go wrong with using the t test when you do not have the variance. So, even if you have a large enough sample size, the idea is that the t-distribution becomes… It approximates z-distribution quite well when your sample size is greater than 30. There for all practical purposes, the t-distribution looks exactly like the z-distribution. But, keep the things really simple; you can just follow this simple rule that, you do not know. If you know the variance, just use the z; if you do not know the variance, just use the t. And, that should keep you clear. The other thing to mention out here is that, this DOF or degrees of freedom – we have mentioned that, out here we briefly spoke about that concept when we were talking again about the standard deviation. Without going into too much detail into degrees of freedom again, the simple thing to keep in mind is that, the t-distribution is not one distribution. I mean it is one distribution, but in the sense that, the t-distribution – you can think of the degrees of freedom as a parameter that goes with the t-distribution. So, just like the normal distribution, if you say the normal distribution, you need to mention the mean and the variance for you to have a… to actually draw the exact distribution or to do some computation on it. It is no point coming to someone and saying how likely is it to see a 1.2 in a normal distribution? That question does not make sense. Normal distribution with what mean and what variance? And then, I can answer your question. You can think of degrees of freedom in a similar light; which is that, the t-distribution itself is not completely defined until I mention to you what the degrees of freedom are. So, t-distribution with three degrees of… – with degrees of freedom equal to 3 looks different from a t-distribution with degrees of freedom 4. And, the core idea that you need to know is that, the t-distribution has a mean of 0. And, it looks very similar to the normal distribution of mean 0 and standard deviation 1. But, the exception that as the degrees of freedom keep increasing; so, when you go to degrees of freedom of 30 and greater; and, at some point, it is exactly the normal distribution. So, the t-distribution with a large enough degrees of freedom is exactly like the normal distribution with mean 0 and standard deviation 1. But, as the degrees of freedom keep decreasing and come all the way down to, the lowest degrees of freedom you can have is 1. When it comes all the way down to degrees freedom equal to 1, you will find that it still looks a little bit like the normal distribution with mean 0 and standard deviation 1; but, it is a little shorter – shorter in the center and has fatter tails on the sides. And so, that is how it deviates from the normal distribution. But, all that you need to know is that, the degrees of freedom get defined by the concept n minus 1. So, number of data points minus 1 tells you the degrees of freedom. And, once you know the degrees of freedom, you know which t-distribution to look up in the tables. So, you know how to draw the curve and then calculate probabilities from it. Again Excel uses – Excel has some slightly nicer functions for it. So, if you are just interested in looking at the left-hand side of the distribution, you just use T-DIST – T dot DIST. If you are interested in T dot… On the right-hand side, you do T dot DIST dot RT; or, on both sides, you do T dot DIST dot 2T. So, you do not need to actually do the 1 minus and so on that we were talking with the z-distribution. Excel already has some inbuilt functions to just point to which side of the distribution you are interested. So, we go now to the next. We are finished with the t–distribution; we go now next to the next test, which is the chi square test. And, the chi square test has a couple of different types of tests. But, the one that we are interested now is the chi square test for variance. I am using the words variance, standard deviation a little interchangeably; one is just the square of the other. The test is ultimately one for variance. And, if you are testing variance, you are essentially testing standard deviation. So, if it is easy for you to think standard deviation, you can keep that in mind. And, an example for instance of the chi square test is you are really interested in looking at a sample, but you are not interested in making a statement about the population mean. You are instead interested in making a statement about the population variance. So, you are interested in saying is the population… Just like in this z-test and the t-test, you are interested in saying something like – is the population mean equal to 4.8? Or, is the population mean less than 4.8? Similarly, here you would be interested in saying things like – is the population variance equal to 0.5, 0.3 – whatever number you have in mind. The important thing is you have a number in mind and you are trying to see if the sample that you are taking… With the sample that you are taking, can you say something about the population variance being equal to this magical number that you have in your head. And, the mechanics of the test is fairly straightforward. And, it is here the sigma naught is the hypothesized variance. So, this is the number that you want to compare it to. This is the equivalent of the 4.8 that was there for means. The s square is the sample, is the variance that you compute from the data, from the sample. You take that data set of the samples and you compute standard deviation, you compute a variance from that. And, that is s square. And, the way you do that again to remind you is this that, 1 by n minus 1 in the formula for the calculation of standard deviation; you would be using that. And, that is how you calculate the test statistics, which then gets compared to a chi square distribution with n minus 1 degrees of freedom; just like in the first case, it got compared to z-distribution and this got compared to… The t-statistics got compared to t-distribution. This is the same way the chi square distribution gets compared to a chi square distribution; great. So, a couple of things to note is that, if again chi square also uses the concept of degrees of freedom; so, think of the degree of freedom as something that defines the exact distribution you are interested in, because a chi square distribution with 3 degrees of freedom is a different distribution than a chi square. It is a different density function. It looks different. It has different mathematical properties than a chi square distribution with 4 degrees of freedom, 5 degrees of freedom. So, the degrees of freedom help you define the exact distribution and its parameters and its mean variance and so on. But, essentially, that is what you would use. You would need to use the degrees of the freedom and that is also the same as before; it is n minus 1. So, number of data points minus 1. And, chi square… With Excel out here just uses chi square dist; this is the left side and chi square dist dot rt is the right side. I do not see them having something for 2-tailed, but I might be wrong. But, as long as you have these two, you can quite easily just draw that graph in your head and figure out which side; if you are interested in a 2-tail distribution, how you would compute that; great. So, we finally, come to our last single sample test, which is called the proportion z-test. And, the idea here is you are testing something that is a proportion. So, you are testing something like… If you are given… So, you are testing a hypothesis like less than 30 percent of the shoppers, who come to my online store are women. So, you can say again; we can go the 2-tailed way or you can go the 1-tailed way; you can say less than 30 percent; you can say 30 percent of my shoppers in my online store are women or you can say greater than 30 percent of the shoppers are women. The key out here is that, whatever sample you collect to actually test this hypothesis, the hypothesis… So, let us fix on the hypothesis. Let us say the hypothesis is less than or equal to 30 percent of the shoppers, who come to my online store are women. The idea is like all hypothesis testing, you will now… – you have this hypothesis; you will now go and collect a sample. The sample in this particular example could be something like you actually give a survey at the end of the purchase or something and people actually say them – male or female. So, you collect a sample. How does this sample look? The answer is that the sample unlike in the previous examples, where you would have seen an actual number. So, in the previous example, in the phosphate examples, you saw numbers like 4.1, 3.5 – these were actually readings from blood tests. Here you are going to get something that is binary. The person is either going to say that either they are female or not. So, it is a series of 1s and 0s – very similar to the idea behind Bernoulli trials. And, what you are doing is you are now looking at the sample data of 1s and 0s, which… and then answering the question of whether… and then saying something about the hypothesis, which is less than 30 percent of the people, who come to my shop are women. And, this has the same intuition as all the other forms of inferential statistics, which is if for instance, you take 100 samples and you know all 100 of them point to the shoppers being women; then, you are likely to reject the idea that only less than 30 percent of the shoppers have come to my online store are women. But, the idea is if you notice something, this looks in every way shape and form like the Bernoulli distribution. And, the fact that you are counting how many. So, the Bernoulli distribution had to do with probability of heads or tails. So, that would be probability of it being male or female. But, if you are interested in out of size of n people, who arrive; how many of them? Sort of hundred people who arrive are… Is it less than 30 who are women? That ties us to the binomial distribution. And, yet you do not see the binomial distribution being used in the test, you instead see the same idea of z. So, you are again calculating a z-statistic with this test and you are comparing to the z-distribution, which is normal – which is normal 0 comma 1 square. And, the idea here is fairly simple. If you remember, we spoke about the binomial approx… – we are approximating this binomial distribution to a normal distribution. That is a right way to put it. And, that is what you are doing out here. And, it should also be intuitive in that the p hat out here is a calculated proportion. So, you will take a data set. Let us say you took 30 people or 40 people or 50 people who came to your store and you actually found that, exactly 23 of those 50 were females. So, that proportion is what? The proportion that you get from the sample is what you have is p hat. p naught is the hypothesized proportion. So, p naught would be the 30 percent. So, this would be sample. And, this is the equivalent of mu naught. So, this is the population – proportion – the number that you are hypothesizing; and of course, is the sample size. And, if you look at it, this also looks very much like that x bar minus mu concept. And, in some way, this formula out here in the denominator should remind you of the formula that you saw for standard deviation in the binomial distribution class. So, you are doing something very similar to the x bar minus mu by sigma over square root of n; it is in construct identical to that. But, you are using the binomial distribution for calculating things like the variance. But, you are also saying – hey, this I believe should be… you can approximate to the normal distribution. So, I am just going to use the z-distribution to calculate my p values or I am going to use the z-tables. Just in quick conclusion, just going back to the rubric that we created, I will just clean this up. The final idea is that, you use these z-tables; you can use z–tables; by z-tables, I mean like you can use the back of the statistics text book, you can use Excel or Matlab or R. Just important that you know how to do with at least one of these softwares. And, the core idea is that, if you get a low enough p value; all of these help you calculate p value or a probability. And, if you get a low enough p value, you can use that as grounds for rejecting the null hypothesis. And, I am saying I reject the hypothesis that mu naught is less than or equal to 4.8. And also, just keep in mind – on the flip side, you never say I accept the null hypothesis and you can only say that, I fail to reject the null hypothesis. I hope that clarified the use of… Give you one illustrative example of the single sample test and the idea behind the mechanics of it; and, introduced you to the other tests. And, in the next class, what we are going to do is we are going to talk about 2-sample tests and we are going to go also beyond that. We are going to talk about the idea of having multiple samples. Thank you. English - NPTEL Official

12) Two Sample tests
Hello and welcome to our next lecture in inferential statistics. So, today, we will continue our series of lectures and hypothesis tests; and specifically, we will build upon our previous lectures. So, just as a reminder, we motivated inferential statistics and the use of hypothesis tests two lectures ago and, in the previous lecture, we focused more on the single sample tests. Now, if you remember from our previous classes Ð two classes back, when we provided examples, I spoke about the use of the one-sample situation as well as the two-sample situation. So, this table that you see in front of you is something that we discussed previously and we have gone over this. And, what you see on the left-hand side, the one sample situations Ð some examples; and, on the right-hand side, here are some examples of the two sample situations. In the last class, when we did some tests, we focused on the one-sample case; and today, we are going to focus more on two-sample case. So, what is the big difference? So, again, just if you jog your memory, we spoke about various single sample tests. So, let us go back to the single sample tests. In the one-sample testÉ I am using the word single in one sample interchangeably. In the one-sample situation, we were either testing Ð doing a hypothesis test on the population mean or we would do a hypothesis test on the proportion Ð a proportion Ð again associated with the mean proportion Ð associated with mean; and, or sometimes we would do a hypothesis test on the population standard deviation. But, in all these cases, whether it is a proportion that you are testing, whether it is the mean that you are testing or whether it is the standard deviation/variance that you are testing; in all these cases, you always had a single dataset. Your sample came from a single distribution. And so, you had a single sample in your hand. A single sample does not mean a single data points; you had many data points, but the data points represented one particular distribution or one particular context. And, in all these cases, you would invariably compare this dataset to a single number that you had in mind. So, you would test the hypothesis that, the average phosphate levels in blood were less than 4.8. Here the dataset that you had was the dataset associated with phosphate levels in blood for may be a person or a machine or a set of people; whatever the context is, there is still one dataset and that set was used and compared to a specific number. In this particular case, the specific number was 4.8; that number could be something else. In the case of the proportion test with the health department, it was 5 percent that you were testing against. And, in different cases, in the case of the garage, it was comparing it to the national average, which conceivably would have been a particular number. So, that is the third example. But, in all these cases, whether you are testing for mean, whether you are testing for proportion or whether you are testing for standard deviation, just keep in mind that, what makes it a single sample test is that, you do not have multiple sets of data; you have the single set of data. And, you are always comparing that set of data to the set of data you sample. But, using that sample, you are saying something about the population. And, what you are saying about the population is essentially a comparison with a particular number that you have in your head. So, you are trying to see if it is less than 4.8. So, those are the single sample cases. But, in the case of the two samples, you actually will get two sets of data. And, typically the way it happens isÉ For instance, in the first example, you see there in this table, you actually go change the temperature; so, you actually go mess with a particular variable and then you look at the number of defects. So, you have dataset one, which might be a set of 10 or 20 or 30 data points and the number of defects in different casts. And then, you went and changed the temperature and you get another set of 10 or 20 or 30 data points. Sometimes these numbers are not always equal Ð meaning Ð cannot have the same number of data points on either side, more often they are not; it is good that you do. But, the core idea is that, you now have two sets of data that represent two separate distributions. Distribution 1 is for the number of defects that you would expect to see before you change the temperature. And, distribution 2 is the number of defects you would expect to see after you change the distribution, after you change the temperature. And, these two distributions correspond to their respective populations. And, what you are doing is you are taking a sample from both of these populations and near comparing these two samples and you are ultimately trying to make a statement about the mean of population 1 versus the mean of population 2. So, that is essentially the first example that you see here. Now, the second example is again one where nothing was changed over time; but, let us say you just had two parallel different manufacturing processes. And, this is a case, where you want to compare the variance; you do not really care about the mean of the manufacturing processes. So, let us say these two processes are coming up with some finished product. You are more interested in the variability of this finished product. So, you do not care really what the mean is. But, again the idea said you will have manufacturing process A and there will be some distribution associated with it. And, there will be a dataset associated with that sample and you will have manufacturing process B and you will have another dataset associated with that sample. And, even the last example which is tenth standard girls taller than tenth standard boys is also fairly is just a straightforward extension of what we discussed, where you have two different datasets and you are comparing the two datasets. So, keep in mind that you do not have to have a number in your mind. And, I will talk about the odd case where you do have a number in your mind; but, the one big difference is with two sample situations, you are ultimately having two separate datasets. So, instead of thinking of the words as one sample and two sample, that sometimes gets confusing. I just like to think of it as a single set of data versus two sets of data; that kind of emphasizes that there are many data points. The other way of thinking about it, which sometimes helps me is just think of it as for the first time, the two-sample test, you are dealing with two variables; you are always going to have the variable that you are measuring. So, in the case of theÉ Let us take each of these examples. In the first case, the variables Ð I am going to call them 1, 2 and 3. So, let us do that. So, this is 1, this is example 2; and, this is example 3. You do not have to circle it. So, in example 1, the variable that you are measuring is the number of defects. So, that is your output variable. So, in some sense, you can say you have got one variable, which is the number of the variable as a number of defects. But, in addition to that, you have another variable, which is the temperature. And, the temperature takes on only two values. And, that is why it is a two-sample test. The temperature takes on the value that it was before you changed it and the value Ð it is after you changed it. So, for the first time, you are seeing two variables. When you go back to the single sample situations; so, if you take the average phosphate levels example, the phosphate levels in blood was your response; so, essentially your output variable. And, that was the only variable that was involved; there was nothing else that you were changing. For the first time in the two-sample case, you will have two variables, which is the number of defects, which is your output, your response variable, your standard variable that you always see. And then, there is another thing that is changing, which is the temperature. So, the temperature beforeÉ And, the temperature can take on only two states: 1 and 2. And, those are the two datasets before and after; same thing with the manufacturing process to compare variance of the finished products; so, the variance of the finished product, the variance of a certain number. So, it might be the dimensions of the product of which you are interested in the variances of. So, you are interested in the variance of the dimensions of a certain product. Let us say that was the diameter of a finished product. So, the diameter itself is the output variable and you are concerned about the variance of that. So, that is your output variable. But, the manufacturing process is your other variable with two difference states. So, there are two different manufacturing processes. So, if you call it manufacturing process A and then manufacturing process B; then, manufacturing process becomes your second variable. And, this variable can take on only two states: A and B; you can call it 1 and 2 or whatever you want. Again out here in the third example for instance, what is your output variable? Quite straight forwardly height. So, you are measuring height in both cases; and that is your output variable in the third case. And, the second variable of interest; and, you can think of it as the input variable is gender. So, girls verses boys. So, you have datasets for girls and then you have a dataset for boys. Tenth standard is not a variable, because it is consistent across both. So, it is just a detail in some sense. But, what you will be measuring is height 1, height 2 and so on dot dot dot; and, same thing out here also. So, it is like you haveÉ One way to think of the difference is obviously that you are dealing with the single dataset with the single sample case and you are always comparing that to a number. In a two-sample case, you are dealing with two datasets. And, another way to think of it isÉ For the first time, in the two-sample case, you will actually be encountering two variables. One Ð very clear output variable or the response variable that is the variable of interest, that is, what you are comparing. And then, the input variable; essentially, what are the two classes that you have created in your system that you are comparing this response variable across. But, otherwise, you will find that the core steps; this rubric that we created last time in terms of steps for hypothesis test statistics, which is that you still need to have a null and alternate hypothesis. In this case, the null and alternate hypothesis looks a little different. If you remember, in your previous example, you would have a null hypothesis such as a mu 1 is equal to 4.8 or mu 1 is less than or equal to 4.8. Here you will typically have a hypothesis like mu 1 is equal to mu 2; you can have different types of null and alternate hypothesis. If you remember in the single sample class, we spoke about how you can have a single tail tests and two-tail tests, where typically something going back to your single sample class thinks like mu 1 is equal to 4.8 would lead to two-tail test. And, we explained how that works. So, the same thing applies here. When you say mu 1 equals to mu 2, that is a two-tail test; whereas, when you say mu 1 is less than or equal to 4.8 or mu 1 is greater than or equal to 4.8, that would be one single-tail test. Similarly, out here you can have something which says mu 1 is less than or equal to mu 2; that can be null hypothesis. And correspondingly, the alternate hypothesis will also change and you can also have something that says mu 1 is greater than or equal to mu 2. So, all of these are possible. But, accordingly, just remember Ð you will calculate the same test statistics. This formula will not change. But, accordingly, where you are, which part of the distribution you are interested in will change. And, if you still have some doubts about how that works, I strongly suggest that you go back to the lecture on single sample test and see how, which part of the distribution gets covered. And, we will also try to support that with some problems in your science. But, the important thing for you to remember with the two sample case is that, the same core concept of single-tail and two-tail tests hold. There will be a small exception to this and we will quickly cover that. It is not really an exception, but it is more of an addition and we will cover that towards end of this lecture in terms of creating more complex a hypothesis. But, for now, just seals as a simple extension of the single sample tests; but, you have a mu 1 and a mu 2. The second step still hold, which is that you will do some basic calculations or arithmetic on data to create a single number called the test statistics. Last time we solved different formula for the single sample z-test. Here, you have given your formula for the two-sample z-test Ð two-sample z-test. And, the core idea here is for instance thatÉ So, let us just go through this formula to get you some idea. The idea as such you have an x 1 bar and an x 2 bar. So, it is different from your old formula of x bar minus mu divided by sigma by square root of n. Now, this is the formula that you would have seen for the single sample z-test, because you had a single dataset and you have calculated an x bar from that; and, mu was a number you had in mind. Here you have two datasets: 1 and 2. So, for both these datasets, you are going to calculate x 1 bar and x 2 bar. So, just to give you some idea, I mean I have represented as a table. Just for your convenience, just focus on this table. We are going to use this table for some other purpose. So, on this table, with the arrow, you have two sets of data. So, one set of data corresponds A; one set of data corresponds to setting B. You can think of it as the boys and the girls or you can think of it as manufacturing process A versus B or whatever it is. And, this number out here in the bottom, which is x 1 bar and x 2 bar Ð essentially, there needs to be a small dash above it, if it is not clear. So, there is x 1 bar and x 2 bar. And, these correspond to the average. So, x 1 bar corresponds to the average of these numbers; x 2 bar corresponds to the average of these numbers. So, you can think of it as x A bar and x B bar, but you can alsoÉ x 1 bar and x 2 bar are also just fine; it is just convenience. So, this x 1 bar and x 2 barÉ Let me clean this page up. x 1 bar and x 2 bar is essentially what gets plugged into these two formulas. And, I am going to come to d naught in a second; but, similar to the z-test of the single sample case, you will need to know the standard deviation of the populations associated with the two means. So, x 1 bar and x 2 bar are essentially the sample means that you get from this distribution. But, you have this distribution associated with 1 and 2 separately with A and B separately. And so, if you are going to stick with the nomenclature of sample 1 and sample 2, sample 1 has a mean Ð mu 1; sample 2 has a mean mu 2. We are testing the hypothesis that mu 1 is equal to mu 2; but, there is a standard deviation associated with sample 1 and a standard deviation associated with sample 2. That is sigma 1 and sigma 2. We need to know sigma 1 and sigma 2. So, those are numbers that need to be given to us. So, you need to be able to derive it from first principles, not from the data. So, you can plug those in; and, n 1 and n 2 would correspond to the number of the data points that is there. So, if these are 10 data points that are here; then, you would actually see. So, there are 10 data points in A; maybe there are 15 data points in B. So, you would plug in that 10 and 15 correspondingly and that would be n 1 and n 2. So, you plug all of that in. And, I said I will come back d naught. And, this d naught is this small addition to the simple hypothesis that you can form here. So, if have a simple hypothesis like mu 1 equals to mu 2, then your d naught would be equal to 0. This is the idea where you say something like manufacturing process A; that mean of the manufacturing process A is equal to the mean of the manufacturing process B. And then, that is fairly straightforward. But, what if you said something a little bit more complex like the mean of manufacturing process A is 3 units higher than the mean of the manufacturing process B. In that case, your null hypothesis is really that mu 1. Suppose you were to say mean of manufacturing process A is equal to 3 units; mean of manufacturing process B plus 3 units. So, it is three units greater than mean of manufacturing process B. So, that becomes your null hypothesis; then, your null hypothesis becomes something like that, which is mu 2 plus 3 is equal mu 1; and then, d naught will take on a value. And then, d naught would take on the value 3. So, you can create more complex hypothesis. More often than not, d naught tends to be capital 0. You are more interested in questions like is A equal to B. But, sometimes if you are interested in questions like is A 3 units greater than B or is A equal to B plus 3 units just to keep it consistent as a two sample test. But, if you are more interested in saying is A greater than B plus 3 units. In those cases, again it will become a single sample test, but you would still need the user something like 3. So, now, you do this and you calculate test statistics the same way as you did before. The formulas are little different. But, once you calculate it and you have a z number out here, the distribution you are talking about is the same distribution that you were dealing with in the single sample test, which isÉ So, if the null hypothesis is true, which is that mu naught is equal mu 2; mu 1 is equal to mu 2. And, you make some assumptions; and then, the test statistics, which is what you have derived z should be no different than a single random number from a z distribution or a single normal distribution with mean 0 and standard deviation 1 square. So, that part of the logic is exactly the same. And, the way you test the probability using either the z tables from that is, these are some tables that you see in the back of statistics test books or you could use Excel or Matlab or R. And, we discussed some formulas in excel during the previous class. Those formulas are all identical; you are doing the exact same thing. And, the core idea is that, you use the software to calculate the probability, which is called a p value. And, if the p value is low enough, then you reject the null hypothesis; you reject this hypothesis that you created, which is that mu 1 equals mu 2. So, guys, like everything out here in this part is exactly the same as your single sample test. The only thing that change is this formula and your null and alternate hypothesis. Again the core concept of using two-tailed and one-tailed tests also is the same. So, having described this, let us briefly go into the tests that are actually available. So, we have already discussed the z test. Another example that I can think of if you kind of learn by examples isÉ So, for instance, it is often thought that calcium could reduce Ð calcium supplements could reduce blood pressure. So, you could conceive of a simple test, where you give calcium to some people, let us say 20 people; and then, for others, you give placebo. So, people do notÉ These are just sugar coated pills where they think they are taking calcium. And, you can compareÉ So, let us say you gave it a 20 in 20 people; you can compare the blood pressures of these two sets of people. So, you will have one set, which is a calcium set and then you have one set, which is the placebo set; and then, you can test the hypothesis that the mu of calciumÉ So, mu of calcium is equal to the mu of the placebo. Or, in this case, because there is the hypothesis goes out and says that calcium should create lower blood pressure, you can test the hypothesis that mu of calcium is less than or equal to mu of the placebo. So, just another random example for you on that; you can alternatively use the t-test. And, the reason for using the t-test Ð again the goal of the z and the t test are always the same with the same difference as in the case of the single sample tests. The difference being that, in the t-test, you do not know the standard deviation already. In the z-test, you know sigma 1 and sigma 2. This is given to you in the t-test. This is not given to you. And so, you have to calculate s 1 and s 2 in order to figure out. Now, if you remember, in the single sample case, we said there is an exception to this rule. So, we said the basic rule is if you know standard deviation, use z; and, if you do not know standard deviation, use t. And, in most cases in life, you are not going to know the standard deviation. So, the t tends to be popular for that reason; I mean think about it right; it goes back to saying you are actually testing the hypothesis associated with means because you do not know mu 1 and mu 2. If you knew mu 1 and mu 2, you would not be doing any of this ifÉ Now, imagine a world, where you do not know mu 1 and mu 2; which means you do not know the population means of the two distributions. But, someone comes and tells you what the population standard deviations are; kind of rare to find. So, more often than not, the t gets used; but, we also spoke about this exception, where if your dataset is really large; and, large was defined by greater than a dataset size of 30, then you can actually compute the standard deviation and then the t-distribution approximates to the z-distribution. So, you can still use the z-test. If some of you find that confusing, you can keep it simple; you know the standard deviation, use the z; you do not know the standard deviation, use the t; you cannot go wrong with that. Now, there is one small complication. And, I know these formulas can sometimes be a little scary. But, we are going to step through each of them. In the t-test, there are two versions; there are actually multiple versions. And, there is another version called the paired t-test; and, we are going to come to that. But, we are not talking about that now. For now, I am talking about a straightforward t-test just the way you did the z-test for the same purpose of defining the difference in mean. But, in this particular case, in the t-test, you need to figure out whether the standard deviations or the variances are equal or not on principle. If you can say the standard deviation should be equal, you would use this formula, which is the equal variance formula; if you do not and you believe the standard deviation should not be equal, then you would use the unequal variance formula. So, the quick idea is that, the idea of x 1 bar, x 2 bar and d naught is the same as in the case of the z-test. And, you use a single formula for standard deviation because its equal variance; and, that you get from computing s 1 square and s 2 square, which is nothing but the variance that is computed from the actual dataset. So, here is the dataset. And, this s 1 and s 2 are actually the standard deviations that are computed from this data. By the way, just for your reference, the sigma 1 and sigma 2 what you see here are not computed from that data; I have just put them under A and B to tell you where they belong; but, sigma 1 and sigma 2 are given to you as s 1 and s 2 are the standard deviation ofÉ Like for instance, s 1 is the standard deviation of the this dataset; hope that make sense. So, let us also clean up this table grid. So, when you have equal variances, you can compute something calledÉ And, this is called a pooled variance and that is what you will plug into this formula. And, we have also covered the concept of degrees of freedom. And, the degrees of freedom are nothing in this case, but the idea of the total number of data points. So, using the total number of data points to figure that out. And, you will see that you can use a similar formula for the unequal variance case, where you will not have a concept of pooled variance and you will have a separate s 1 and s 2 square and the formula for degrees of freedom also differ. Again if you have any questions on degrees of freedom, feel free to refer to the previous lecture; that should give you a similar insight. You will also notice that again in both those cases; this is extra term d naught; sometimes some texts will not even include this formula; but, the idea is that, if your null hypothesis is quite straightforward, which is x 1 equal to x 2, or is x 1 less than or equal to x 2 or is x 1 greater than or equal to x 2; then, d naught is equal to 0. But, if there is an offset such as is x 1 equal to x 2 plus d naught. Then, you will need d naught. Or, is x 1 greater than x 2 plus d naught. We will now move on to the next form of t-test. And, this is called the paired t-test. It is also trying to test the same core concept, which is mu 1 equal to mu 2. But, it is doing so in a slightly different way. And, the idea behind the test is the following. In a typical test, you have two datasets. And, here I am referring to everything that you can see on your left-hand side. Dataset A and you have dataset B. And, if you are trying to see if mu A equals mu B; and, you are doing that using x 1 bar and x 2 bar and so on. Now, if there is some kind of logical pairing between the rows; so far, we have actually ignored any connection between this data point to this data point, because there need not be any connection. And in fact, the number of data points in B need not be equal to the number of data points in A; which is why we have two completely different terms n 1 and n 2 in all these formulas. But, if n 1 was equal to n 2 and there was a logical connection between each point, then you want to use something called the paired t-test. What do we mean by a logical connection between the two points? We mean the following. Either thatÉ Let us say you are doing this same test of the calcium and placebo. We discussed this test in the context of a z-test. Suppose you did not know the standard deviations, you could have very well used it as a t-test. But, let us say that you were doing a calcium and placebo; but, the way you were doing the test was that, for each person Ð for each individual person, you would give a calcium tablet; look at the change in blood pressure; and then, on a separate day, give the same person the placebo tablet; that means, for each person, there would be one recording in calcium, one recording in placebo; which could mean that, each row could signify the calcium for person x and the placebo for person x. So, there is actually a very clear logical pairing. So, the second row could be something quite simply calcium for person y and placebo for person y. So, in that sense, these two data points whileÉ Yes, A continues to mean calcium, B continues to signify placebo; but, this is specifically forÉ There is a logical connection that both of these are connected by this person y. So, lot of times, applying the same treatment on the same x essentially experimental unit could be that logical paring. And, when you have that logical paring, a great way to get more out of your test is your paired t-test. And, the idea here is that, instead of computing x A bar and x B bar, x or B; used to call it x 1 bar and x 2 bar, we instead take the differences of A and B. And, each difference is computed here. So, this is the difference between 23.1 minus 21.1. And, that is 2.2. Once like that you compute all these differences; and then, you get a d bar, which is nothing but the average of these differences. And, you get an s d, which is nothing but the standard deviation of these differences. And, you go ahead and plug that into this formula. So, you have this d bar s d Ð square root of n is just the number of data points and this d naught is the same concept as the d naught here; where, if you are saying if A is equal to B, then your d naught is 0; if A equal to B plus 3, then d naught is 3. But, outside of that, d naught is the same core concept; but, the idea here is that you are using d bar s d instead of x 1 bar, x 2 bar separately. You can think of other common examples about a logical pairing. For instance, if a common example is Ð if we are interested in knowing Ð if a particular track creates more wear and tear on the left-hand side of the particular tyre; let us say you are on a formula 1 context; it is a sport and you are really captain of a particular track creates more wear on the left-hand of the tyre than the right-hand of the tyre; then, you might do a two-sample test and you might compare left tyres and measure their wear. So, wear just means how much they have eroded. So, let us say there was some logical way of measuring that. Then, you would measure the wear on some sample of 20 left tyres and the wear on 20 right side tyres. Now, if these 20 left-hand side tyres and 20 right-hand tyres came from completely different cars, you would have to stick to your standard t-test. However, for each car, if there was one left tyre and one right tyre as a data point; and then, you had 10 cars or 20 cars. So, you had 20 left tyres and 20 right tyres; then, you can use the paired t-test, because for each particular left tyre, there is one corresponding right tyre. You could use that logical association to use the paired test. We then move on to the equivalent of a proportion test. We saw this in the single sample case; but, again it works the same way as your old proportion test; and, that you have two samples here and you want to see if the number of defects through process A is worst than process B. So, again you are doing this comparison not against a particular number; but, you are doing this comparison between two samples andÉ But, you are comparing the proportion that you see in both the samples. So, that is what you are doing in the proportion z-test. The formula here is quite straightforward. The proportion associated with sample 1; the proportion associated with sample 2. Always remember that, in a proportion test, your data is essentially extreme of 1s and 0s. So, you are not getting the actual numbers; you are getting either yesÕs and noÕs or males and females or whatever; it is that, you are measuring. But, you are getting this p 1 comes from a dataset, which is 1, 0, 1, 1 dot dot dot. And, that p 1 is nothing but the average. And, p 2 is similarly coming from another dataset 1, 1, 0, 0, dot dot dot. And, for the sake of variance, you would use this. This is kind of like the concept of pooled variance. And, that p hat in general comes from this, where x 1 and x 2 are the number of 1s that you see overall. And, n 1 and n 2 are the total number of data points that are there overall. So, that is a fairly straightforward extension of the proportion z-test. And, the final we come to is the F-test. The F-test is used when you want to compare the standard deviation of dataset 1 with the standard deviation of dataset 2. You can conversely think of it as comparing the variance of dataset 1 with the variance of dataset 2 because that is essentially what you are doing. And, the idea is that, you take sample variance from dataset 1, sample variance from dataset 2. And, that should logicallyÉ This is very similar to the chi square test; but, that should logically give you something called the F-distribution. This is the first time we are saying that distribution called F-distribution. But, the one difference between the F-distribution and some of the distributions we have seen so far is that, the F-distribution has two parameters that define it and there are the two degrees of freedom. So, it is actually called numerator degrees of freedom and denominator degrees of freedom. And, numerator degrees of freedom is defined by n 1 minus 1, which is what is a number of data points on dataset 1, which is what goes on the numerator. And, the denominator degrees of freedom is n 2 minus 1, which corresponds to the s 2 square that you calculated, which is the sample variance of the dataset 2. Just to again recap for you the concept degrees of freedom with a z-distribution, that concept does not exist; the z-distribution is nothing but a normal distribution with mean 0 and standard deviation 1 square. With the t-distribution, by definition, t-distributions have mean 0. But, to describe exactly the t-distribution, you are talking about you need to signify the degrees of freedom; and, that gets signified by these formulas for equal and unequal variances. And then, you have the special cases of the paired t-test; where, again the degrees of freedom is n minus 1. Again with the proportion test, because of the binomial approximation to the normal, you are only getting a z-distribution at the end of it. So, ultimately, you will be Ð you use this test statistic; but, once this test statistic is computed, you are still dealing with a normal 0 Ð mean 0 standard deviation 1. And finally, for the F-test, which does a test of compares two variances to see if they are equal or not; you have to define it based on numerator degrees of freedom as well as denominator degrees of freedom. I hope that was clear and that you have a good feel for the use two-sample test. Again there is a lot of software out there, where you can just plug in dataset 1 and dataset 2. So, for instance, you might be able to just completely dump the entire dataset in its native format and tell the software what test to do. But, using these test statistics you have a better understanding of what you are doing when you understand the formulas. And, once you compute the formulas and carry out your tests, then you can use a software to get the exact probabilities. Thank you and look forward to seeing you in the next lecture . English - NPTEL Official


13) Type 1 and Type 2 Errors
Hello and welcome to our lecture on Type 1 and Type 2 Errors. This is something that you might have heard and it is a fairly central and important concept with respect to hypothesis tests. So, let us get into, what the core concept of type 1 and type 2 errors are. So, the idea is the following. Take any of the hypothesis tests that you learnt so far, in this course we provided you with a template for how the hypothesis tests are conducted. And in this template, you would notice the first step is to form a null and an alternative hypothesis and your last step is that you essentially get a p value associated with this particular test and based of the p value, you either reject the null hypothesis or you fail to reject the null hypothesis. So, this is what we have characterized in this table. So, you actually make a null and an alternative hypothesis and you do not know which is true, that is why you are doing this test. If you knewÉ So, your null hypothesis was that mu 1 equals mu 2. So, if you knew that was true you would not be doing this test, so you do not know that is true, that is why you are doing this test. So, you might have a null hypothesis that says mu 1 equals 4.8, which is a single sample case or you might have a null hypothesis, which says that mu 1 equals mu 2. In either case, you do not know which is true. But, let us say there was this, all knowing world, where you knew which was true. So, that is what marked out here in actual, you say actually what is true. So, here I am saying the null hypothesis is true and in this part, I am saying the alternative hypothesis is true. So, here I am saying mu 1 is equal to, let us say mu 2 and here the alternate hypothesis is true, mu 1 is not equal to mu 2. Now, this is the truth. But, based of some data, you did a test. So, you might have done a two sample z test or you might have done a two sample t test and let say, that you did the test you actually computed the z or the t statistic. Based off of the z or t statistic, you calculated a probability or a p value and based of a p value, you took a decision and the core idea has always that p value is too small, then you reject the null hypothesis and the term too small is a subjective term and typically people use some line in the some threshold and that threshold is called alpha. So, if the p value is less than alpha, we reject the null hypothesis. Typically, the value of alpha tends to be something like 0.05 or 5 percent. So, based off of this, let us say you rejected the null hypothesis, that is one decision and here you fail to reject the null hypothesis, for whatever reason. So, the idea is that out here you would have rejected the null hypothesis, because your p value would have been less than your alpha. So, p value is less than alpha, so you rejected the null hypothesis. Here the opposite was true, p was greater than or equal to alpha that is it. You do not have to have that equal to sign or so, just say greater than and I am not sure, you might want to say less than or equal to. So, this is what you did; now here is the problem. If the truth was that the null hypothesis was true, the null hypothesis was true which means that mu 1 was equal to mu 2. But, you went ahead out here in this quadrant, you went ahead and you rejected the null hypothesis. You said mu 1 is not equal to mu 2, but in reality mu 1 was equal to mu 2. You did something wrong and that error is what is called is a type 1 error. It is when the null hypothesis was true, but you went ahead and you said that I am rejecting the null hypothesis and we are going to come back to quantifying that value which is called the type 1 error. It is just noteworthy that is also called the producer risk, in sometimes the false positive or the alpha risk. We will not go in to each of those terms, for instance producer risk is seen more from a manufacturing context, false positive is seen more from a medical context. But, a few kind of think about it, it definitely makes sense. Now, here is the case out here, where the null hypothesis was true and you fail to reject the null hypothesis. So, that is okay, you are happy with that, reality was that the null hypothesis was true and you did not find any evidence to reject the null hypothesis. So, you did the right thing, so you are happy in this quadrant and let us come to this quadrant out here. Here, the null hypothesis in reality, because this is truth, so this is truth. So, here mu 1 was not equal to mu 2 and you correctly rejected the null hypothesis, this is still the null hypothesis. Your null hypothesis is this, this mu 1 not being equal to mu 2 is the alternate hypothesis, but it is so happened, that the alternate hypothesis was reality that the null hypothesis was wrong and when the null hypothesis was wrong, you correctly rejected it. So, you did the right thing even here and that is a very good thing that you did. You actually detected that mu 1 was not equal to mu 2 and you said no, I am rejecting the null hypothesis, so that is a great thing. Now, come to the final quadrant. Here, the truth was that mu 1 was not equal to mu 2, but you failed to reject the null hypothesis. You are not able to say, I am rejecting the idea that mu 1 is equal to mu 2, I am rejecting the null hypothesis. So, you are not able to reject the null hypothesis, whereas you should have, because mu 1 is not equal to mu 2 and that is the truth. So, that is also not a great thing that you did and this error is called the type 2 error and again, it is call the consumer risk in the more manufacturing production settings, it is called the false negative in medical settings in couple of other settings or beta risk. Now, in really simple words, type 1 error is the concept. So, just to be really clear, we are right here. So, the idea is that your type 1 error, even before you see any data, even before you do anything could be as high as your alpha and I say after analysis; it is exactly equal to your p values. So, let me explain that a little bit. So, let see you have not collected any data, we just discussed what alpha is. Alpha is the idea that you are going to do this hypothesis test and you are going to get some p value and you already said, if the p value is too low I am going to reject the null hypothesis. How low is too low that is the line in the stand that you draw and that value is called alpha. So, if your p value is less than alpha, you are going to reject the null hypothesis. What is that mean when your p value is less than alpha? First of all, you are going to get a very low p value, let us say you are going to get a low p value and this p value is less than alpha. Because, you type one error first of all comes about only, when your null hypothesis is true and you go ahead and reject the null hypothesis. So, by definition; that means, your p value must have been less than alpha, only then you would have rejected your null hypothesis. Now, only when you reject your null hypothesis or you even putting yourself up for the possibility of a type 1 error. So, the idea is that I am going to tell you that up front, your type 1 error is associated with this idea that you rejecting the null hypothesis and you reject the null hypothesis when your p value is less than alpha, which means your p value is really low. Now, let us take a step back and think about, what the p value is. The p value is this idea that, if the null hypothesis is true, this is the definition that we looked at much earlier, even before we discussed type 1 and type 2 errors. P values by definition express the idea that if your null hypothesis is true, this is the probability of seeing a test statistics as extreme as this, if your null hypothesis is actually true. So, this probability is really low, then essentially what you are saying is that, you are willing to take the risk and saying, the probability of seeing this statistics is just 1 percent if the null hypothesis is true. If the null hypothesis is true, the probability of seeing something so extreme is just so small, it is just 1 percent, then I am willing to take the risk and reject the null hypothesis. So, write there by definition, the risk that you took was that 1 percent risk, it was that p value. So, whatever your p value is, essentially if you do the entire calculation and you calculate the p value, then your p value is essentially your type 1 error, that is the risk you are taking, that because that is you actually calculated the probability that this data could actually occur with the null hypothesis being true. So, if you are still going ahead and rejecting the null hypothesis fully knowing this probability, then that is the risk you are taking of making a wrong decision. Now of course, before you even see any data and even before you calculate the p value, given that you set yourself with an upper threshold of alpha, means that you could get a type 1 error as high as alpha. So, I hope that makes type 1 errors fairly clear. Now, type 2 errors are more complicated, now and there is a reason for it. It is a function of variety of parameters, it is a function of something called delta, which is not the delta that we would have discussed so far and the core concept behind delta is the following. Type 2 errors, when do they occur? They occur in the situation, where your alternate hypothesis is actually true. So, mu 1 is not equal to mu 2, but you failed to reject the null hypothesis. But, in order for me to quantify how likely that is, you need to tell me, how much is mu 1 not equal to mu 2. So, if I said mu 1 is not equal to mu 2, because mu 1 is equal to mu 2 plus a very, very, very, very, very small numbers. So, let us say 1 micro meter or whatever it is, whatever the metric of mu 1 mu 2 is, but it is a very small numbers. So, mu 1 let say is 4, but mu 2 is 4.000001, then it is true mu 1 is not equal to mu 2. The alternate hypothesis is true, but the probability that I am going to reject the null hypothesis, just became very low. My ability to discern between a difference, this difference between 4 and 4.000001 is much lower than my ability to discern between the difference of 4 and 5, all other things being equal. So, it is really a function of how different are they and that is what gets captured in delta, it is also a function of the sample size we take, which is as the sample size becomes infinite. The uncertainty around mu 1 and mu 2 becomes smaller and smaller and theoretically, even a small difference between 4 and 4.000001 could potentially be found as long this sample size is large enough. It is a function of type 1 error, because your type 1 error tells you, how conservative or liberal you are being in rejecting the null hypothesis. So, at the more conservative you are in type 1 error, meaning that you do not want to make that type 1 error, the more likely you are to make a type 2 error and similarly, the more liberal you are with the type 1 error, meaning you are with some amount of error the better you are going to be with type 2 errors. So, it is a little bit of given take in terms of the type 1 and type 2 error and essentially, a lot of people are very interested in understanding the relationship of beta, which is this type 2 error. So, it is a bad thing versus delta, which is something we would discussed here and for a given sample size and they will show it from many different sample size and that is known as an OC curve or an Operational Characteristic curve. Another word that you might come across, it is known as the power of the test and that is equal to 1 minus beta and it essentially a very positive thing. It is 1 minus a bad thing. So, the higher the power of the test; that means, the more strength you have in being able to detect a difference between mu 1 and mu 2 and I have use the concept of mu 1 and mu 2 coming from the two sample scenarios, but all of these core concepts also applied to a single sample tests. I hope that clarified and that give you an idea of type 1 and type 2 errors. Thank you. English - NPTEL Official

14) Confidence Intervals
Hello and welcome to our next lecture in inferential statistics. Today, we are going to be talking about confidence intervals. So, statistical inference even in terms of classification mainly is discussed in terms of hypothesis testing and estimation. So, these are the two broad categories in statistical inference. And, hypothesis testing is something that we have discussed in good detail in this class; we have talked about single and two-sample tests Ð various tests. Today, we are going be primarily talking about estimation. And, you might notice that, almost any text that talks about statistical inference, talks about these two topics. And, some of them might be introducing estimation or confidence intervals before hypothesis testing. But, that does not really matter. Essentially, these are two sides of the same coin if you meant So, today, we are going to be talking about estimation. Now, the idea behind estimation is that estimation can be in terms of point or interval. What we mean by that is it is a same core concept as what we introduced with inferential statistics during hypothesis testing; which is that, we are interested in some population parameter. What we mean by that is that, there is this concept. So, the examples that we have used in this class are things like amount of phosphate in our blood, the height of tenth standard students in public schools in India. In all these cases, you define some population and you are interested in some parameter. More often than not, we would discuss the parameter being the mean. So, what is the average amount of phosphate in blood? What is the average height? But, it does notÉ That is just one of the parameters. That is the most common one. But, it can be other parameters. So, stepping back, we are interested in some population parameter. But, you do not know this parameter. It is like it is some truth that you do not already know. If you did know that, there would be no need for any of this or any of the statistics. But, what you do have is a sample. So, you have 5 data points, 10 data points, 20 data points, 30 data points. Some sample from this population. And, what you are most interested is you are most interested about this population parameter. So, in hypothesis testing, you would hypothesize that, this population parameter is equal to 4.8 or 2.3 or it is less than 4.2. And then, you would go about and look at this sample and see if that is true or not. With estimation, you are not having any hypothesis; you are not having any hypotheses in mind in that sense. What you are trying to do is you are trying to take the sample. And, with point estimation, you are trying to come up with a single point estimate of the population parameter. And, that might seem fairly straightforward. So, for instance, let us say you are interested in the population parameter, which is the average amount of phosphate in blood. And, you took a data, you took some sample. And, that sample was about 20 data points. A simple point estimate of the population mean could be the sample mean. So, you take the sample of 20 data points; take their average. And, that is your best; that could be one of your best point estimates of the population mean. So, point estimate just means you are making as good a guess on the population parameter based on the data that you have. But, today, we are going to be talkingÉ And more interestingly, this is what getsÉ This is what people are more interested in, which is interval Ð estimation in the form of an interval. So, this goes back to again the core concept with hypothesis testing, which is fine. You do not know the population mean; you have a sample mean; and, you acknowledge that, if you go to take another sample of another 20 points, you might not get the exact same value. And, both these values the first time around and the second time around might not be exactly equal to population mean. If it is not exactly equal to the population mean, then can I come up with some range around my point estimate. So, I have a point estimate, which is actually my sample mean; my sample mean is my best bet; let us say at my population mean. But, I acknowledge that, I might not have exactly hit target. The population mean might be a little higher or a little lower than my sample mean; in which case, I ask the question Ð can I come up with the range around this sample mean? By which means it is essentially like I am giving myself a margin of errors by which I am fairly certain that I have covered the population mean. So, that is the goal that we are going to embark upon. And, in many ways, it is the same map, because we have introduced hypothesis testing; it becomes a little easier, so that we can reason by the same logic in map that we have already discussed. So, let us do that. So, the core idea is that, let us take an example that we have looked at many times. So, which is that we might be hypothesizing the amount of phosphate in blood is equal to exactly 4.8. Now, we discussed that, in confidence intervals, you do not have this number; you do not come up with the hypothesis. You are only interested in coming up with some bounds around your point estimate. So, what you essentially do? One way of thinking about confidence intervals given that we have already introduced hypothesis tests is well, for different values of mu naught. So, let us say you have some dataset and you calculate some x bar. Essentially, 4.8 gets plugged in out here to calculate your z-statistic. If you are doing a z-test, you are given a sigma; if you are doing a t-test, you take an s; but, in neither case, that gets plugged in; n is again the number of data points. So, you get some z-value Ð some z value; that is out here. And, based out of that z-value, you calculate some probability; you calculate a p value. Now, the core idea with hypothesis test was that, if this p value is really small, you reject the null hypothesis. So, the question with confidence intervals Ð one way of thinking about is given that, I do not have some hypothesis, I ask myself the question within what range can my mu be such that I will calculate a z such that I will get a p value, which I will not reject. I am just going to repeat that; given that you do not have a mu naught, you can think of a confidence interval as what is the range of values that mu could potentially be such that given that, for a given dataset, you will get some x bar, some sigma, some root n such that you will calculate a value z such that you will get a p value, which you will not reject. So, if you would not reject, that means you need to have some bounds. Suppose you start off by saying well, I am going to reject any p value less than 0.05. So, that is something you started off with. Now, given that you started off with that; then, is there some rangeÉ For a given data set, is there some range of mus such that you would not be rejecting this hypothesis test. And essentially, to compute that, all you do is just rearrange the terms out here. So, you keep the mu naught onÉ Ð the mu or the mu naught Ð I am using those two terms here interchangeably. But, you keep that on one side and you essentially move the terms to the other side to get this formula for confidence interval. So, typically, if you know the formula for the hypothesis test or the test statistic, you can just essentially rearrange it. But, the core idea is that, this is your point estimate, which is your x bar. So, for your best estimate of mu is your x bar; but, you create a margin of error or you create a range around it as plus or minus the z associated with this alpha. And, alpha here is that 0.05 that you said. Essentially, you said within a certain range. So, within that range and sigma and square root of n are the same. Another way a more formal definition associated with confidence interval is that, if we were to repeatedly take identical samples of the same size and build similar confidence interval bounds for each sample, then you are building a bound such that 95 percent of such confidence interval bounds will cover the true mean. Or, in other words, we are 95 percent confidence slash certain that the true mean mu is within our confidence. So, for single sample tests, the process of creating confidence interval is fairly straightforward. I explain to you how in the z-test, it essentially just becomes a rearrangement of terms and this sigma byÉ This concept of sigma by square root of n essentially goes towards the z and then the x bar goes here such that it is a plus or minus. And, that is how you get the formula for that interval. The same core idea for the t-distribution; the formula is no different, except that, the t-distribution gets also defined by the number of degrees of freedom; so, not just the alpha, but the number of degrees of freedom. And, in all these cases, mind you, because I am putting a plus or minus, this is the equivalent of the two-tailed z-test or the two-tailed t-test. You could also create a one -sided bound if you were interested. And, that would be again the formulae equivalent of having a one sample test. Again it would be the same sigma divided by square root of n. The formula itself would not be different. But, the way this alpha gets used up will be different. Again it goes back to the core concept of how you would shade that region under the probability distribution. With the chi square distribution, that rearrangement is not obvious. To some extent it is. But, the plus or minus is not, because you do not have a plus or minus term; it is essentially like this sigma naught goes out here, the chi square distribution comes down here. But, the way we differentiate between the lower bound and the upper bound is by changing the alpha in the bottom of the chi square distribution. So, it is the same rearrangement; it is a same core concept of taking the hypothesis test and rearranging; that is, in this case, it would be to put the sigma naught out here and bring the chi square down here and you would have the same formula that you see here. But, you get an upper bound and a lower bound by looking at the 1 minus alpha by 2 and alpha by 2. And, by the way, this notion of alpha by 2 depends on how you define it. Now, if you say I want a 95 percent confidence bound; that means, you are left with 5 percent. And, if it is two-tailed test, that 5 percent gets divided into 2.5 percent times 2. That is how you get the alpha by 2. So, it really depends if someone starts by stating alpha and you know it is a two-tailed test; then, technically, the correct way to do this would not be to just have an alpha out here, but it would be to have an alpha by 2, so that you are being technically correct. And, the same thing goes for here as well; you will have an alpha by 2. Again the same core idea with respect to the z-test. If this alpha is more generic term that I have used out here. So, if somebody comes and says I need a two-tailed test; so, there is a plus or minus and the alpha gets divided by 2. But, if it is a one-sided test, that alpha can stay as alpha. So, depends on how it gets firmly defined typically. If it is a two-tailed test, you will represent it as alpha by 2. But, it isÉ Again if you look at it, it is a same rearrangement from your test statistic to create the confidence interval. So, the same idea goes towards two-tailed tests. Just to give you an idea of how it works for a two-tailed test, I have given a single sample for the z-test, for the t-test and the proportional z-test. We have consciously left it out; that might be a part of your assignments that you could work on. But, the idea is the same. We are interested in this term. We are interested in the term x 1 bar minus x 2 bar. And so, we want to create a confidence bound around x 1 bar minus x 2 bar. If you remember this simple way of thinking about x 1 bar minus x 2 bar was to say to test the null hypothesis that x 1 bar is equal to x 2 bar. But, that is logically the same as asking the question Ð what is the confidence bounds around x 1 bar minus x 2 bar? And, seeing if that essentially covers a 0 or not. Of course, this d naught is an extra term. Suppose you were interested in a hypothesis that looked more like this; if this was your null hypothesis, then you could Ð you would have this d naught being something nonzero; otherwise, d naught is just equal to 0. But, this is the idea. So, you are interested in some bounds around the term x 1 bar minus x 2 bar. And, again you would do the same logical rearrangement. This goes here and the bounds go around x 1 bar minus x 2 bar. And, you are essentially creating bounds around this value. So, there is x 1 bar minus x 2 bar and what is your range around that; great. Again similar to the chi square test, the F-test is not so straightforward. So, I am including the formula associated with that out of here. You do the same thing that you did with the chi square test, which is you have the same s 1 square by s 2 square. That is your core upper bound and lower bound. But, you divide by this F. This F kind of comes down and you divide by that. But, the lower bound is 1 minus alpha by 2 and the upper bound is alpha by 2. Of course, also remember that, when this is a two-tailed test, the alpha that you see here becomes alpha divided by 2. If it is a one-tailed test, meaning you just had a plus or a minus; you can keep that as alpha. I hope that clarified this concept of confidence intervals. Thank you. English - NPTEL Official

15) ANOVA and Test of Independence
Hello and welcome to our lecture on ANOVA or also known as analysis of variance; and, that chi square test of independence – the TOI is test of independence – the chi square test of independence. We present this as the last lecture in the series on inferential statistics. But, it is only fair to say that, while some set of techniques are presented is a part of inferential statistics. Most of the techniques in statistics use statistical inference in some way or the other. So, for instance, after this lecture, we will be talking about regression and so on. And, there is a significant component there, which is associated with statistical inference. But, in any case, this lecture, we are going to talk about the analysis of variance and the test of independence. So far, statistical inference was confined to input variables that could take up two possible values; and, that was the two sample cases – the two sample tests, where you would compare two different samples. And, the variable of interest would be for instance, either male or female. So, we came up with an example, where we said the heights of tenth standard boys in public schools versus the heights of tenth standard girls in public schools. So, if you were interested in seeing if the average height of a boy in tenth standard in public schools in India is equal to the average height of a girl in tenth standard in public schools in India; here the output – essentially you can think of there are two variables that are involved and you can think of the output variable as one of height and your input variable you can think of as gender. So, you have two different sets of data and you are comparing them. Now, there are also these cases, where it does not make sense to think of these as input and output variables; that is just confusing. So, and, these are the single sample test cases, where you just have one variable height and you have defined all the other parameters around it. And so, it could be something like I have already defined that I am interested in studying the average height of boys in tenth standard in public schools in India; and, I am interested in seeing that average height is less than a 120 centimeters or something – some such number. There is no concept of it; there is just one variable, which is height and that is it and you are comparing it to some number that you had in your head. So, that is essentially summarizing the two sample case and the single sample case. For the first time with something like the ANOVA, we take on a case, where this variable, which is essentially like this input variable, can take on two or more states. So, essentially, you will see it taken three or more states. And, that is the real differentiator about ANOVA. So, just to kind of give you a little bit more color on this, a standard application of ANOVA would be something like the following, where you are still dealing with an output variable that is typically quantitative and continuous like for instance, like height. But, your input variable need not have just two states, it can have three or more states; and, you are still probably interested in comparing their average. So, extending the example that we saw in the two sample t-test case; if you wanted to ask a question such as – is the mean height of tenth standard boys in… is the average height of tenth standard boys in Tamil Nadu equal to the average height of tenth standard boys in public schools in Maharashtra; is it equal to the average height in Karnataka. So, you are not just interested in comparing two sets of samples; but, you had multiple sets of samples. And typically, what you are comparing, the output essentially, variable is still like a continuous quantitative variable like height. It can be anything else; but, essentially, it is a quantitative variable. And, you have multiple discrete settings of your input variables. So, you are not just interested in comparing boys versus girls or method A versus method B; but, you are now going from… In the t-test, you are comparing method A versus method B; you can now say is method A equal to method B equal to method C equal to method D or are they different, or a some subset of them different from the others. So, that is essentially an application of ANOVA. Now, the chi-square test of independence is one that can be used when you want to compare again multiple proportions. When you are interested in two variables; and, both of these variables are categorical variables. So, it could be the same thing like Tamil Nadu, Karnataka. So, the different states of India could be one of the variables. And, the other variable could be the number of people at different age groups. So, people between – number of people between 0 to 20 years – between 20 to 40 years and 40 to 60 years. So, there is one variable, which is the state, which is a categorical variable. And, here is another variable age, which I have in some sense made categorical. But, essentially, I am just interested in these two variables, which are categorical and I am just interested in looking to see there is any relationship between them. Another way to think of chi-square test of independence is see it as some form of an extension of your proportion z-test. In a proportion z-test, you would typically handle problems of the following nature. You will say if you took a question like our men more likely to purchase a certain product than women. That would be a proportion z-test, because you have two categories: men and women. It would be – has to be precise; it would be a two sample proportion z-test, because you would have two categories; men would be one category; women would be the other category. And, their likelihood of purchasing a certain product would be represented by a binary. So, upstream of 1’s and 0’s. So, 30 percent women purchase this product out of a 100 samples; and, 20 percent men purchase this product out of 25 samples. But, you take that same problem, where you had just two categories, which is purchased or not purchased, men and women, and you extend that to multiple categories on both dimensions. So, that is what we did when we said state could be multiple different states; and, we said age could be multiple different ages. So, if you are able to take these two variables and extend them to multiple categories, you can use a test of independence in sets. So, let us first jump into the ANOVA and try and explain the core concept and the math behind it. The idea behind the ANOVA again would be to test a hypothesis of this nature, which is mu A equals mu B equals mu C equals mu D. A typical t-test for instance would have just looked at something like is mu A equal to mu B. So, I think a very natural question that quite often comes up is what so special about 2 versus 3? Why is it that for 2, you can use the t-test? 3 and more… It said the ANOVA is built for this kind of multiple comparisons; but, it works just as well even if you just want to compare two samples. So, you can essentially replace your t-test with a two sample t-test with an ANOVA and you would be doing something mathematically identical. So, before we go any further, we have identified the hypothesis. Now, let us get some nomenclature ready. So, these are the different samples. So, this is the sample corresponding to A; and, A could be anything; it could be fertilizer A versus fertilizer B versus fertilizer C. A could represent the state Karnataka; B could represent Tamil Nadu; C could represent Maharashtra, whatever. So, it depends on the question. Let us take fairly consistent examples. So, let us take the example of a height of boys in public schools in tenth standard for four different states. So, these are the four different states. And, this is the dataset. So, this is data point 1. So, this is data point 1 for state 1. So, that is represented as y 1,1. Now, data point 2 for state 1 is represented as y 1 comma 2 and it goes on till y 1 comma n. And similarly, it goes in this direction; y 2 comma 1; and ultimately, you have y 4 comma n in the bottom corner. Now, having defined this, I am going to throw you in some sense in the deep end with the formulas of how an ANOVA is actually computed. So, this is the idea. And so, let us just go through this step by step. The idea here is to compute an F-statistic. And, what if we learnt about… If there is one thing we have learnt about hypothesis testing, it is that you come up with a hypothesis, which we did; and a hypothesis was said mu A equals mu B equals mu C. And, the alternate hypothesis by the way out there is that, not all mus are equal, because there are many ways in which mu A – that you can violate the statement mu A equals mu B equals mu C equals mu D. You can violate it by saying mu A equals mu B, but it is not equal to mu C equals mu D or you can put that not equal to anywhere in that equation. So, quite simply, the alternate hypothesis is that, not all the mus are equal. So, we took care of the first step, which is to create a null and alternate hypothesis. The second step is to do some kind of computation to come up with the test statistic. We are going to talk a little bit about how… First, we are going to talk about the overall structure, which is the math behind this table leads to an F-statistic with a minus 1 degrees of freedom and n minus 1. If you remember, we discussed about how the F-statistic has a numerator degrees of freedom and a denominator degrees of freedom. And, you will calculate their F-statistic and the rest of the hypothesis testing is the same. You will then calculate a p value and then choose to reject or not reject. Now, let us take one step back. We say the F-statistic that we have computed here is nothing but something called MSB divided by MSE. We are going to talk a little bit further about these terms. But, for now, you can you can take this; again go one step back to these two values. And, you technically do not need to know the mean square total or any of these terms. So, this entire part you do not need for actual calculation of the test statistics. But, it is there to give you a bigger picture. So, again we say this MSB is nothing but SSB divided by something called the degrees of freedom. And, that is fairly obvious in terms of what math needs to happen there; and, again the same concept with MSE. So, at least we know how things flow from one side to the next. Now, let us get to the core of the formula, which is calculating the sum of squares between and the sum of squares error. The sum of squares between is essentially y bar i dot minus y bar bar dot dot; what do we mean by that? So, let me first… We will talk about the terms here and then I will come to n and a and so on. Essentially, what y i bar dot means is y A bar dot, is nothing but the average of all these terms. y bar bar dot dot is nothing but the average of all the numbers here. So, if I want a row-wise addition, I say y A bar dot. And, similarly, I would say B bar dot for this row. So, this would be y A bar dot; this could be y B bar dot and so on. And, the idea is that, if you are going to change A, B and so on; and, A, B and C and D can be coded as 1, 2, 3, 4. Then, I might as well just say y i bar dot and put that through a loop essentially. So, that is what I am doing here. I am saying y i bar dot to say I am going from i equals 1 to a; where, a is the total number of treatments, which in my particular case is 4. So, there are four different treatments or there are four different sets of samples that I am comparing. So, essentially, I am taking i from 1 to 4 and I am taking each of those means that I calculate. So, I am essentially taking this average, this average, this average, this average separately. I am taking each of those and I am subtracting that from the grand mean – the overall mean. The overall mean is nothing but every single output variable. This entire table in a sense; this entire table average is y bar – y bar bar dot dot. So, essentially, what I am doing out here is I am doing something… If you look at this formula very similar to standard deviation. But, it is essentially the standard deviation of the means. So, there is an overall mean and you have individual treatment means. So, I am essentially looking at out here this term looks… It is not the standard deviation because I have not done the square root and I am not divided by the number – n minus 1. In this case, it would be a minus 1. So, I have not divided that. So, that is why it is still; I am going to divide it. Once I divide it, it is kind of like variance; but, I have not divided it yet. So, out here it is essentially like a sum of squares. Once it gets divided by that a minus 1, then it becomes that MSB. But, in concept, this is… And, we are going to talk about the multiplication by n in a second. But, in concept, this is a lot like the standard – the variance of the means. So, it is like the variance. So, if you were to compute a set of means for each row, a set of like y bars i dot for each row, the standard deviation of these values or rather the variance of these values is essentially what you are computing with MSB. Now, what is the concept behind here? Here what you are doing is you are looking at the standard deviation of each data point. So, i comma j just means that i goes from 1 to a; which means I am going through each row and I am going through each column out here; j goes from 1 to n. So, j is going from 1 to n; i is going from 1 to a. So, it is literally like I am going through each data point and all through this matrix; and, I am looking at the deviation of each data point from its row average. We saw these two are the same terms that we are using here. The concept here is that, we are looking at essentially the standard deviation within each row. I am not looking at the deviation of y 1 1 from the grand mean y bar bar; but, I am looking at the deviation of y 1 1 from this rows – essentially, this rows average. And, I am averaging that across every row. So, it is almost like I am taking a row-wise average of every data point and I am averaging that. So, that is called the sum of squares error. And, again you are dividing by the total number of points that you see; it is the set of N – capital N by the way out here. There is a difference between the small n and the capital N; capital N is essentially nothing but the total number of data points. So, small a is the total number of treatments; in this case, it is equal to 4, n; we have not actually defined out here; but, essentially, it is the total number of columns. So, that is the number of data points that each combination has. And, capital N is the total number of data points. So, essentially, it is nothing but a times n. So, a times n is capital N. So, using that nomenclature, again calculating MSE also should be obvious. So, the mechanics of how you do the ANOVA with the F-test and calculate a p value should at this point be fairly clear to you. But, let us spend a few minutes and try and get a little bit more intuition on this; which is, why are we using an F-test? Which we know from earlier experience is used to see the difference between two variances. We use the two sample F-test for detecting the difference between two variances. Now, why are we using that to calculate the difference between means? And, the core idea is the following that, the mean square between is one way of calculating the total variance. Now, mean square error is another way of calculating total variance. Now, these three, which is mean square between mean square error and mean square total, which is what is described here are all going to be equal in a sense. Again they are going to be statistically equal, not actually equal. If the null hypothesis is true; so, if mu – if we go back to the null hypothesis, which is that mu A equals mu B equals mu C equals mu D; then, MSB, MSC and MSD would be statistically equivalent. However, if it is not true, then you will find that MSB will be greater than MSD will be greater than MSE. So, you have got two different ways of computing variance. And, what you are doing is you are doing an F-test of these two different variance calculations. So, you are doing an F-test on these two different variance calculations and you will not reject the null hypothesis, if the null hypothesis is true, which is mu A equals mu B equals mu C equals mu D. Now, if those means are not equal, then these two computations of variance do not really represent the total variance. And, this computation of variance becomes an over estimate and this computation of variance becomes an under estimate of the overall variance. And therefore, a test of whether these two are different variances will show up to be true. Essentially you will wind up rejecting the null hypothesis that these two variances are equal; and thereby, commenting on the fact that the null hypothesis of means was probably not true. So, let us get a little bit more intuition on why this becomes if mu A… if the null hypothesis is not true. And, here the null hypothesis is that, mu A equals mu B equals mu C dot dot dot. If this null hypothesis is not true, let us take a look at why this computation becomes an overestimate and this becomes an underestimate. So, here is the first case where the null hypothesis is true. So, the null hypothesis is true; that means, mu A equals mu B equals mu C equals mu D; which means the mean of mu A. So, this is the distribution of A; this is the distribution of B; and, this is the… So, this is just a sample case. So, there is A, B, C. And, here are the distribution. And, hey, look at the means of this the same. And, here I have gone further and even made the distributions look identical. So, if you were to take samples from A, samples from B, samples from C and you are just going to put them all in one bucket called total distribution of Y, this is how it would look, because essentially, since these distributions are identical and their means are identical, it looks this is also identical. So, how do you compute mean-squared error? The way you compute mean-squared error is – essentially, look at the variance of each distribution; you basically take each data point and look at the standard deviation or variance of the data point with respect to its mean. So, you are essentially computing the variance of each of these distributions and you are just averaging them. And, because the variance of these distributions are going to be the same, that is going to match the variance of the total distribution of y; and, we just explained why. Now, let us step to mean square between. Mean square between is… So, here is the total distribution of Y and you are interested in capturing the variance of the total distribution of Y. And, the way you are going to do that is you are going to take the mean of distribution A; you are going to take a sample mean of distribution of A. And, as you have known from the earlier part of this course, if you take 5 data points or 6 data points on A and you compute a sample mean, that sample mean need not exactly fall on the population mean. So, you might get another value here for distribution B. Now, as long as the null hypothesis is true, which is the distribution A equals distribution B equals distribution C equals [FL] then you are going to get some sampling distribution if you take the means of each of these distributions. And, if you take the means of each of these distributions; then if all of these are equal, then you are going to get a new distribution called the distribution of the means of A, B, C. And, what is it that we have discussed about the variance of this distribution? We have discussed that as long as you are sampling from essentially the same distribution – you are sampling from the same distribution, because A now looks identical to B looks identical to C. They all have the same means. As long as you are doing that, we know that this standard deviation is equal to the overall standard deviation, that is, the standard deviation of the total distribution or you can think of it as the standard deviation of any of these distributions, because they are all identical – divided by square root of n. Or, you can think of it – you can think of it in variance terms and say sigma square divided by n. So, as long as I can compute this variance and then move this and multiply it by n, I could cancel this out and I can get an estimate of this variance. So, that is what I am trying to do. What I am trying to do out here is I am calculating means from each of these distributions and I am taking the standard deviation of those means; I am taking the variance of those means. I am taking the variance of those means and multiplying it by n with the belief that, if the null hypothesis is true, that should be a great estimate of the variance of the total distribution; which is fine. That is going to work great as long as the null hypothesis is true. So, as long as the null hypothesis is true, this approach to calculating the variance of this distribution – variance of this distribution; and, this approach to calculating the variance of this distribution. So, the MSE approach to calculating variance of the total distribution and MSB approach to calculating the variance of the total distribution – both should work perfectly fine. But, what happens when the null hypothesis is not true? Take this case, where the null hypothesis is not true. Now, if you take the total distribution of Y; because the null hypothesis is not true, you get some data points from distribution A, some data points from distribution B, some data points from distribution C; it is going to fall over a much larger region. Now, if you try to estimate the total variance; let us take a look at how MSE would estimate the total variance. MSE would take the variance or take the deviation of each data point with respect to its mean, not with respect to some grand mean. And, even that mean is essentially a sample mean that you are going to calculate. But, the idea is that, your estimate is going to – of variance out here is going to be of some value; your estimate of variance here is going to be another value. And similarly, out here it is going to be of some value. And, all you are doing with MSE is you are taking the average of these three numbers – of 1, 2 and 3. Now, what is the average of these three arrows? It is an arrow with a magnitude that is much smaller than the real variance. So, when the null hypothesis is not true, the MSE method of calculating variance becomes an underestimate of the total variance. Now, let us look at what happens on the MSB side. Now, the null hypothesis is not true. And so, despite the fact that there is one source of variation, which is that the sample means are not following exactly on top of true means; but, in addition to that, the sample means are further separated from each other, because they are trying to go after a totally different line. They are true means themselves are different. So, the standard deviation or the distribution essentially of the sample means is going to be a much wider distribution, because the means are not equal. And now, when you multiply that by n, you wind up making a huge over estimate of the total distribution of Y, because you are essentially out here you are trying to look at different – you are trying to get sample means of distribution, which have truly different means and you are looking at the standard deviation of the sample means of distributions with truly different means. It is going to be an overestimate of the total variance of the distribution of Y. So, I hope that gives you some intuition on why the F-test for the ANOVA works the way it does. But, the important thing for you to remember is that the F-test; the F is the ratio of two variances. And, the core idea is you are using the F-test, which is used for ascertaining whether two variances are equal; you are using that in an indirect way to make a statement about the means of many distributions. And, you are doing that by saying that, if the null hypothesis is true, method A of calculating variance should be equal to method B of calculating variance. But, if the null hypothesis, which is mu A equals mu B equals mu C is not true; then, these are not accurate methods of calculating variance; great. The big question is what do you do? So, now, we know the mechanics of it; we have some intuition for the ANOVA; but, the idea is... So, you did this; you calculated an F-statistic; you then went and plugged that F-statistic and found out the probability. And, let us say your p value was really small. So, you want to reject the null hypothesis. So, what do you do after rejecting the null hypothesis? So, what is the statement you are making when you reject the null hypothesis? You are saying I am rejecting the null hypothesis that mu A equals mu B equals mu C equals mu D; great. But, can you tell me which of these mus are different from which others? So, one great way to do this is called the Tukey test. It is not the only way to do it; but, it is a fairly common way of doing it. So, that is why I have called it method 1; it is the Tukey test. But, there are other methods as well. The idea here is to decide on an alpha value, which we do it many hypothesis tests; even before we start, we say if that p value is less than a certain value, I am going to reject it. So, let us say you start with an alpha value and then you calculate something called the critical Tukey distance based on this alpha value. So, there is a distribution called the Tukey distribution and that is again something that you can get usually from software or you can get that from the back of text books. But, you will essentially plug in the values of alpha, which you just decided in one step above. And, you know the value of i; and, i out here is the number of treatments. So, it is the equivalent of what we had as a. But, I did not want to confuse you in between alpha and a. So, I have called it i out here; N is the number of replicates. So, small n remains the same – the same concept; and, the large N remains the same. The only thing that I have changed is I have called what we used to call a; I have called that i and the reason I have done that, so that I avoid confusion with alpha out here. So, we do that and we use that mean squared error formula out here and we calculate something called a critical distance. And then, you do a very simple thing; which is you do a complete enumeration of all pairs. So, for instance, this is the mathematical way of showing it; but, essentially, all I am saying is let us calculate y bar A dot. And similarly, calculate y bar B dot, y bar C dot. And, look at the difference between each pair of y bar i dots; where, the two i's are not. So, the different combinations you can come up with are – in our particular example, you will also have A comma B, A comma C, A comma D, B comma C, B comma D, C comma D. So, there are 6 possible combinations and because… And, you are essentially just taking the absolute value. So, b minus a is the same as a minus b. So, you just calculate 6 differences and you see which of these 6… It is 6 in this particular case, because we had 4 treatments; we had 4 different… We had A, B, C and D. So, we had 4 possibilities, 4 sets of data. So, that is 6 treatments, because you get the concept here is combination. So, you will have 4 c 2 combinations. But, how many ever treatments you have, you will similarly have the concept of whether you like to use the word i or whether you want to use a, c – two combinations; you will calculate that many combinations of distances and you will see which of these distances are greater than the Tukey distance. And then, those are the ones that are not equal to each other. So, you will calculate a critical distance. And, to figure out between… See you just rejected the null hypothesis that mu A equals mu B equals mu C equals mu D is wrong. So, you have said that; you said that, that is wrong. But, which of these are different from each other? For that, you calculate a critical distance and see which of these combinations have a mean difference that is greater than the critical distance; and, those are different ones. So, hope that gives you an idea of what the Tukey test does. So, now we move on to the next topic, which is the chi-square test of independence. It is noteworthy at this point to say that, we have already looked at one chi-square test; we have looked at the chi-square test for standard deviation in the case of the single sample test. And, you might also encounter another test called the chi-square goodness of a test; and, that is used when you have a set of data and you want to see how well it fits to a particular distribution. But, out here what we are interested in is essentially using that chi square test of independence; and it is used here, where you have two categorical variables. So, here is the first categorical variable, which is the smoking habit. Is it heavy, regular, occasional or never. And, here is the second categorical variable exercise – frequent, some or none. And, what you are trying to do is you are creating a contingency table of how frequently various values occur. And, you are trying to see if they are independent of each other. Does smoking habit have anything to do with exercise or vice-versa. Again not implying causation, but you are taking two categorical variables and seeing if these two variables are independent of each other. So, the core idea between that is to convert this table to a set of percentages. So, for instance, you would take each of these values and you would see how frequently they occur within each row essentially. And similarly, you would do that for each row. Finally, what you would do is would create theoretical values for this table in accordance to the assumptions of independence. So, for instance, if I believe that these were completely independent, I would take the row-wise sums and the column-wise sums and I would for instance conclude that row 1 – there would be a 48.7 percent chance of seeing a particular value occur in a particular spot. And so, I would essentially say look since… So, this is… I am getting that 48. So, the numbers I am getting are for instance out here is 48.7. And, the number for this category is 41.5. And, for this would be I believe the remaining, which is about 9.7 percent; so, 48, 41. But, the idea is the following. The idea is that, you get percentages based of the sum of each rows. So, you do 7 plus 9 plus 12 plus 87; and, you divide that by the total sum of all values. And, that is how you get the 48.7 percent. Now, given that, there is a 48.7 percent chance that you will find yourself in row 1; I ask you the question that, if you are a heavy smoker and there are a total of 11 heavy smokers, the number of heavy smokers that frequently exercise that I should expect is 11 times 48.7 percent. So, just note the map that we did here; we computed the probability of being in each row. The way we did that is we took the sum of each row. So, the sum of the first row is 7 plus 9 plus 12 plus 87; and, we divided that by the sum of all the numbers here. And, by doing that, we got a number 48.7 percent. So, we told ourselves the probability that, if I did not know anything; that I would find myself in the first row is 48.7 percent. Now, assume that I am a heavy smoker; then, there are a total of 11 heavy smokers; if I would randomly guess whether I was a frequent exerciser or not, then my probability is nothing; the number of heavy smokers that frequently exercise – the expected value of that is nothing but the 11, which I get from adding 7 and 3 and 1 and multiplying that by the 48.7. And, a small correction from before… So, you would… This I know for a fact is 48.7. This probability is also 41.5; you would just need to compute the remaining to get this value. And, that is close to about 10 percent. So, I do not believe it is exactly 9.7 percent; that is why I am correcting this up. But, that is the core idea. The core idea is you calculate percentages based on frequency of occurrence and then you come up with what is an expected value for each cell. Once you come up with an expected value for each cell; and, you can do that row wise or column wise; I explained it to you row wise. So, I took each row, calculated a percentage; and then, I calculated an expected value. But, essentially, what you have computed is a set of expected values out here. Now, you compare each original value. So, you compare that 7. So, I compare this 7 to what I get when I multiply 48.7 by 100 – 48.7 percent times 11. I believe that is approximately like close to 5 point something. So, what I am essentially doing is I am doing that 7, which I see minus that expected value, which is the 5 point dot dot that I see; and, I am squaring that. And, I am doing that for each cell. So, I am doing that for each row and each column and thereby doing that for each cell. And, I am dividing it by their expected value. So, this 5 will also come in the denominator. And, that summation should essentially give me a chi-square distribution with r minus 1 times c minus 1 degrees of freedom. Now, note that the chi-square distribution has only one parameter associated with degrees of freedom. So, if you just expand this, you will get this formula, which is nothing but just linear algebra r times c minus c minus r plus 1. So, you have just calculated a test statistic using this formula. And, that test statistic should be chi-square distributed with this degrees of freedom. And again, you can use that same core concept to calculate a p value associated with it and thereby either reject the hypothesis. And, what is… And, just as a refresh, what is the hypothesis here? The hypothesis – the null hypothesis is that there is no relationship between this variable and this variable; that is, these two variables are independent of each other. And, the alternate hypothesis is the rejection of that. There is some relationship that depending on whether you are heavy regular or occasional smoker, that impacts whether you are going to be a frequent exerciser or not; I mean it is not a causal relationship; but, knowing this, helps me make – say something about this other variable. Or, it can be the other way round – knowing that you are a frequent exerciser, does that help me in anyway predict or in any way make a statement about your smoking habit. Is there some relationship or are these two variables independent of each other? The null hypothesis being that they are independent of each other and a very low p value in this chi-square test statistic will enable you to reject that null hypothesis or you would fail to reject that hypothesis. So, I hope this gives you an idea of the two tests that we have introduced in inferential statistics that have to do with multiple samples; that just do not have to do with two samples; that is the ANOVA or the analysis of variance and the Chi Square test of independence. In our next lecture, we are going to start with regression. Thank you. English - NPTEL Official
