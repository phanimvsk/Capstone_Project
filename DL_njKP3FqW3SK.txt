"hi everyone, let's get started. Good\nafternoon and welcome to MIT 6.S191!",
 'TThis is really incredible to see the\nturnout this year. This is the fourth',
 "year now we're teaching this course and\nevery single year it just seems to be",
 'getting bigger and bigger. 6.S191 is a\none-week intensive boot camp on',
 'everything deep learning. In the past, at\nthis point I usually try to give you a',
 "synopsis about the course and tell you\nall of the amazing things that you're",
 "going to be learning. You'll be gaining\nfundamentals into deep learning and",
 'learning some practical knowledge about\nhow you can implement some of the',
 'algorithms of deep learning in your own\nresearch and on some cool lab related',
 'software projects. But this year I\nfigured we could do something a little',
 'bit different and instead of me telling\nyou how great this class is I figured we',
 "could invite someone else from outside\nthe class to do that instead.  So let's",
 'check this out first. Hi everybody and\nwelcome MIT 6.S191',
 'the official introductory course on deep\nlearning to taught here at MIT. Deep',
 'learning is revolutionising so many\nfields from robotics to medicine and',
 'everything in between.',
 "You'll the learn the fundamentals of this field and how you can build some of these\nincredible algorithms.",
 'In fact, this entire speech and video are not real and\nwere created using deep learning and',
 "artificial intelligence. And in this\nclass you'll learn how. It has been an",
 'honor to speak with you today and I hope you enjoy the course!',
 'Alright. so as you can tell deep learning\nis an incredibly powerful tool. This was',
 'just an example of how we use deep\nlearning to perform voice synthesis and',
 "actually emulate someone else's voice, in\nthis case Barack Obama, and also using",
 'video dialogue replacement to\nactually create that video with the help',
 "of Canny AI. And of course you might as\nyou're watching this video you might",
 "raise some ethical concerns which we're\nalso very concerned about and we'll",
 "actually talk about some of those later\non in the class as well. But let's start",
 'by taking a step back and actually\nintroducing some of these terms that',
 "we've been we've talked about so far now. Let's start with the word intelligence. I",
 'like to define intelligence as the\nability to process information to inform',
 'future decisions. Now the field of\nartificial intelligence is simply the',
 'the field which focuses on building\nalgorithms, in this case artificial',
 'algorithms that can do this as well:',
 'process information to inform future\ndecisions. Now machine learning is just a',
 'subset of artificial intelligence\nspecifically that focuses on actually',
 'teaching an algorithm how to do this\nwithout being explicitly programmed to',
 'do the task at hand.\nNow deep learning is just a subset of',
 'machine learning which takes this idea\neven a step further and says how can we',
 'automatically extract the useful pieces\nof information needed to inform those',
 "future predictions or make a decision\nAnd that's what this class is all about",
 'teaching algorithms how to learn a task\ndirectly from raw data. We want to',
 'provide you with a solid foundation of\nhow you can understand or how to',
 'understand these algorithms under the\nhood but also provide you with the',
 'practical knowledge and practical skills\nto implement state-of-the-art deep',
 'learning algorithms in Tensorflow which\nis a very popular deep learning toolbox.',
 'Now we have an amazing set of lectures\nlined up for you this year including',
 'Today which will cover neural networks\nand deep sequential modeling. Tomorrow',
 "we'll talk about computer vision and\nalso a little bit about generative",
 'modeling which is how we can generate\nnew data and finally I will talk about',
 'deep reinforcement learning and touch on\nsome of the limitations and new',
 'frontiers of where this field might be\ngoing and how research might be heading',
 "in the next couple of years. We'll spend\nthe final two days hearing about some of",
 'the guest lectures from top industry\nresearchers on some really cool and',
 'exciting projects. Every year these\nhappen to be really really exciting',
 'talks so we really encourage you to come\nespecially for those talks. The class',
 "will conclude with some final project\npresentations which we'll talk about in",
 'a little a little bit and also some\nawards and a quick award ceremony to',
 'celebrate all of your hard work. Also I\nshould mention that after each day of',
 'lectures so after today we have two\nlectures and after each day of lectures',
 "we'll have a software lab which tries to\nfocus and build upon all of the things",
 "that you've learned in that day so\nyou'll get the foundation's during the",
 "lectures and you'll get the practical\nknowledge during the software lab so the",
 'two are kind of jointly coupled in that\nsense. For those of you taking this class',
 'for credit you have a couple different\noptions to fulfill your credit',
 "requirement first is a project proposal\nI'm sorry first yeah first you can",
 'propose a project in optionally groups\nof two three or four people and in these',
 "groups you'll work to develop a cool new\ndeep learning idea and we realized that",
 'one week which is the span of this\ncourse is an extremely short amount of',
 'time to really not only think of an idea\nbut move that idea past the planning',
 "stage and try to implement something so\nwe're not going to be judging you on",
 'your results towards this idea but\nrather just the novelty of the idea',
 'itself on Friday\neach of these three teams will give a',
 'three-minute presentation on that idea\nand the awards will be announced for the',
 'top winners judged by a panel of judges',
 'the second option in my opinion is a bit\nmore boring but we like to give this',
 "option for people that don't like to\ngive presentations so in this option if",
 "you don't want to work in a group or you\ndon't want to give a presentation you",
 'can write a one-page paper review of the\ndeep learning of a recent deepening of',
 'paper or any paper of your choice and\nthis will be due on the last day of',
 'class as well also I should mention that\nand for the project presentations we',
 'give out all of these cool prizes\nespecially these three nvidia gpus which',
 'are really crucial for doing any sort of\ndeep learning on your own so we',
 'definitely encourage everyone to enter\nthis competition and have a chance to',
 'win these GPUs and these other cool\nprizes like Google home and SSD cards as',
 'well also for each of the labs the three\nlabs will have corresponding prizes so',
 'it instructions to actually enter those\nrespective competitions will be within',
 'the labs themselves and you can enter to\nenter to win these different prices',
 'depending on the different lab please\npost a Piazza if you have questions',
 "check out the course website for slides\ntoday's slides are already up there is a",
 "bug in the website we fixed that now so\ntoday's slides are up now digital",
 'recordings of each of these lectures\nwill be up a few days after each class',
 'this course has an incredible team of\nTAS that you can reach out to if you',
 'have any questions especially during the\nsoftware labs they can help you answer',
 'any questions that you might have and\nfinally we really want to give a huge',
 'thank to all of our sponsors who without\ntheir help and support this class would',
 'have not been possible ok so now with\nall of that administrative stuff out of',
 "the way let's start with the the fun\nstuff that we're all here for let's",
 'start actually by asking ourselves a\nquestion why do we care about deep',
 'learning well why do you all care about\ndeep learning and all of you came to',
 'this classroom today and why\nspecifically do care about deep learning',
 'now well to answer that question we\nactually have to go back and understand',
 'traditional machine learning at its core\nfirst now traditional machine learning',
 'algorithms typically try to define as\nset of rules or features in the data and',
 'these are usually hand engineered and\nbecause their hand engineered they often',
 "tend to be brittle in practice so let's\ntake a concrete example if you want to",
 'perform facial detection how might you\ngo about doing that well first you might',
 "say to classify a face the first thing\nI'm gonna do is I'm gonna try and",
 'classify or recognize if I see a mouth\nin the image the eyes ears and nose if I',
 "see all of those things then maybe I can\nsay that there's a face in that image",
 'but then the question is okay but how do\nI recognize each of those sub things',
 'like how do I recognize an eye how do I\nrecognize a mouth and then you have to',
 'decompose that into okay to recognize a\nmouth I maybe have to recognize these',
 'pairs of lines oriented lines in a\ncertain direction certain orientation',
 'and then it keeps getting more\ncomplicated and each of these steps you',
 "kind of have to define a set of features\nthat you're looking for in the image now",
 'the key idea of deep learning is that\nyou will need to learn these features',
 "just from raw data so what you're going\nto do is you're going to just take a",
 'bunch of images of faces and then the\ndeep learning algorithm is going to',
 'develop some hierarchical representation\nof first detecting lines and edges in',
 'the image using these lines and edges to\ndetect corners and eyes and mid-level',
 'features like eyes noses mouths ears\nthen composing these together to detect',
 'higher-level features like maybe jaw\nlines side of the face etc which then',
 'can be used to detect the final face\nstructure and actually the fundamental',
 "building blocks of deep learning have\nexisted for decades and they're under",
 'underlying algorithms for training these\nmodels have also existed for many years',
 'so why are we studying this now well for\none data has become much more pervasive',
 "we're living in a the age of big data\nand these these algorithms are hungry",
 'for a huge amounts of data to succeed\nsecondly these algorithms are massively',
 'parallel izybelle which means that they\ncan benefit tremendously from modern GPU',
 'architectures and hardware acceleration\nthat simply did not exist when these',
 'algorithms were developed and finally\ndue to open-source tool boxes like',
 "tensor flow which are which you'll get\nexperience with in this class",
 'building and deploying these models has\nbecome extremely streamlined so much so',
 "that we can condense all this material\ndown into one week so let's start with",
 'the fundamental building block of a\nneural network which is a single neuron',
 "or what's also called a perceptron the\nidea of a perceptron or a single neuron",
 "is very basic and I'll try and keep it\nas simple as possible and then we'll try",
 "and work our way up from there let's\nstart by talking about the forward",
 'propagation of information through a\nneuron we define a set of inputs to that',
 'neuron as x1 through XM and each of\nthese inputs have a corresponding weight',
 'w1\nthrough WN now what we can do is with',
 'each of these inputs and each of these\nways we can multiply them',
 'correspondingly together and take a sum\nof all of them then we take this single',
 "number that's summation and we pass it\nthrough what's called a nonlinear",
 'activation function and that produces\nour final output Y now this is actually',
 "not entirely correct we also have what's\ncalled a bias term in this neuron which",
 'you can see here in green so the bias\nterm the purpose of the bias term is',
 'really to allow you to shift your\nactivation function to the left and to',
 'the right regardless of your inputs\nright so you can notice that the bias',
 "term doesn't is not affected by the X's\nit's just a bias associate to that input",
 'now on the right side you can see this\ndiagram illustrated mathematically as a',
 'single equation and we can actually\nrewrite this as a linear using linear',
 'algebra in terms of vectors and dot\nproducts so instead of having a',
 "summation over all of the X's I'm going\nto collapse my X into a vector capital X",
 'which is now just a list or a vector of\nnumbers a vector of inputs I should say',
 'and you also have a vector of weights\ncapital W to compute the output of a',
 'single perceptron all you have to do is\ntake the dot product of X and W which',
 'represents that element wise\nmultiplication and summation and then',
 'apply that non-linearity which here is\ndenoted as G',
 "so now you might be wondering what is\nthis nonlinear activation function I've",
 "mentioned it a couple times but I\nhaven't really told you precisely what",
 "it is now one common example of this\nactivation function is what's called a",
 'sigmoid function and you can see an\nexample of a sigmoid function here on',
 'the bottom right one thing to note is\nthat this function takes any real number',
 'as input on the x-axis and it transforms\nthat real number into a scalar output',
 "between 0 & 1\nit's a bounded output between 0 & 1 so",
 "one very common use case of the sigmoid\nfunction is to when you're dealing with",
 'probabilities because probabilities have\nto also be bounded between 0 & 1 so',
 'sigmoids are really useful when you want\nto output a single number and represent',
 'that number as a probability\ndistribution in fact there are many',
 'common types of nonlinear activation\nfunctions not just the sigmoid but many',
 'others that you can use in neural\nnetworks and here are some common ones',
 "and throughout this presentation you'll\nfind these tensorflow icons like you can",
 'see on the bottom right or sorry all\nacross the bottom here and these are',
 'just to illustrate how one could use\neach of these topics in a practical',
 "setting you'll see these kind of\nscattered in throughout the slides no",
 'need to really take furious notes at\nthese codeblocks like I said all of the',
 'slides are published online so\nespecially during your labs if you want',
 'to refer back to any of the slides you\ncan you can always do that from the',
 'online lecture notes now why do we care\nabout activation functions the point of',
 'an activation function is to introduce\nnonlinearities into the data and this is',
 'actually really important in real life\nbecause in real life almost all of our',
 "data is nonlinear and here's a concrete\nexample if I told you to separate the",
 'green points from the red points using a\nlinear function could you do that I',
 "don't think so right so you'd get\nsomething like this oh you could do it",
 "you wouldn't do very good job at it and\nno matter how deep or how large your",
 "network is if you're using a linear\nactivation function you're just",
 "composing lines on top of lines and\nyou're going to get another line right",
 "so this is the best you'll be able to do\nwith the linear activation function on",
 'the other hand nonlinearities allow you\nto',
 'approximate arbitrarily complex\nfunctions by kind of introducing these',
 'nonlinearities into your decision\nboundary and this is what makes neural',
 "networks extremely powerful let's\nunderstand this with a simple example",
 "and let's go back to this picture that\nwe had before imagine I give you a train",
 'network with weights W on the top right\nso W here is 3 and minus 2 and the',
 "network only has 2 inputs x1 and x2 if\nwe want to get the output it's simply",
 'the same story as we had before we\nmultiply our inputs by those weights we',
 "take the sum and pass it through a\nnon-linearity but let's take a look at",
 "what's inside of that non-linearity\nbefore we apply it so we get is when we",
 "take this dot product of x1 times 3 X 2\ntimes minus 2 we mul - 1 that's simply a",
 "2d line so we can plot that if we set\nthat equal to 0 for example that's a 2d",
 'line and it looks like this so on the x\naxis is X 1 on the y axis is X 2 and',
 "we're setting that we're just\nillustrating when this line equals 0 so",
 'anywhere on this line is where X 1 and X\n2 correspond to a value of 0 now if I',
 'feed in a new input either a test\nexample a training example or whatever',
 "and that input is with this coordinates\nit's has these coordinates minus 1 and 2",
 'so it has the value of x1 of minus 1\nvalue of x2 of 2 I can see visually',
 'where this lies with respect to that\nline and in fact this this idea can be',
 'generalized a little bit more if we\ncompute that line we get minus 6 right',
 'so inside that before we apply the\nnon-linearity we get minus 6 when we',
 'apply a sigmoid non-linearity because\nsigmoid collapses everything between 0',
 'and 1 anything greater than 0 is going\nto be above 0.5 anything below zero is',
 "going to be less than 0.5 so in is\nbecause minus 6 is less than zero we're",
 'going to have a very low output this\npoint Oh 200 to',
 "we can actually generalize this idea for\nthe entire feature space let's call it",
 'for any point on this plot I can tell\nyou if it lies on the left side of the',
 'line that means that before we apply the\nnon-linearity the Z or the state of that',
 'neuron will be negative less than zero\nafter applying that non-linearity the',
 'sigmoid will give it a probability of\nless than 0.5 and on the right side if',
 "it falls on the right side of the line\nit's the opposite story if it falls",
 'right on the line it means that Z equals\nzero exactly and the probability equals',
 '0.5 now actually before I move on this\nis a great example of actually',
 "visualizing and understanding what's\ngoing on inside of a neural network the",
 "reason why it's hard to do this with\ndeep neural networks is because you",
 "usually don't have only two inputs and\nusually don't have only two weights as",
 'well so as you scale up your problem\nthis is a simple two dimensional problem',
 'but as you scale up the size of your\nnetwork you could be dealing with',
 'hundreds or thousands or millions of\nparameters and million dimensional',
 'spaces and then visualizing these type\nof plots becomes extremely difficult and',
 "it's not practical and pause in practice\nso this is one of the challenges that we",
 "face when we're training with neural\nnetworks and really understanding their",
 "internals but we'll talk about how we\ncan actually tackle some of those",
 'challenges in later lectures as well\nokay so now that we have that idea of a',
 "perceptron a single neuron let's start\nby building up neural networks now how",
 'we can use that perceptron to create\nfull neural networks and seeing how all',
 "of this story comes together let's\nrevisit this previous diagram of the",
 'perceptron if there are only a few\nthings you remember from this class try',
 "to take away this so how a perceptron\nworks just keep remembering this I'm",
 'going to keep drilling it in you take\nyour inputs you apply a dot product with',
 "your weights and you apply a\nnon-linearity it's that simple",
 'oh sorry I missed the step you have dot\nproduct with your weights add a bias and',
 "apply your non-linearity so three steps\nnow let's simplify this type of diagram",
 "a little bit I'm gonna remove the bias\njust for simplicity I'm gonna remove all",
 'of the weight labels so now you can\nassume that every line',
 "the weight associated to it and let's\nsay so I'm going to note Z that Z is the",
 "output of that dot product so that's the\nelement wise multiplication of our",
 "inputs with our weights and that's what\ngets fed into our activation function so",
 'our final output Y is just there our\nactivation function applied on Z if we',
 'want to define a multi output neural\nnetwork we simply can just add another',
 'one of these perceptrons to this picture\nnow we have two outputs one is a normal',
 'perceptron which is y1 and y2 is just\nanother normal perceptron the same ideas',
 'before they all connect to the previous\nlayer with a different set of weights',
 'and because all inputs are densely\nconnected to all of the outputs these',
 "type of layers are often called dense\nlayers and let's take an example of how",
 'one might actually go from this nice\nillustration which is very conceptual',
 'and and nice and simple to how you could\nactually implement one of these dense',
 'layers from scratch by yourselves using\ntensor flow so what we can do is start',
 'off by first defining our two weights so\nwe have our actual weight vector which',
 'is W and we also have our bias vector\nright both of both of these parameters',
 'are governed by the output space so\ndepending on how many neurons you have',
 'in that output layer that will govern\nthe size of each of those weight and',
 'bias vectors what we can do then is\nsimply define that forward propagation',
 "of information so here I'm showing you\nthis to the call function in tensor flow",
 "don't get too caught up on the details\nof the code again you'll get really a",
 'walk through of this code inside of the\nlabs today but I want to just show you',
 "some some high level understanding of\nhow you could actually take what you're",
 'learning and apply the tensor flow\nimplementations to it inside the call',
 "function it's the same idea again you\ncan compute Z which is the state it's",
 'that multiplication of your inputs with\nthe weights you add the bias right so',
 "that's right there\nand once you have Z you just pass it",
 "through your sigmoid and that's your\noutput for that",
 "now tension flow is great because it's\nalready implemented a lot of these",
 "layers for us so we don't have to do\nwhat I just showed you from scratch in",
 'fact to implement a layer like this with\ntwo two outputs or a percept a multi',
 'layer a multi output perceptron layer\nwith two outputs we can simply call this',
 'TF Harris layers dense with units equal\nto two to indicate that we have two',
 'outputs on this layer and there is a\nwhole bunch of other parameters that you',
 'could input here such as the activation\nfunction as well as many other things to',
 "customize how this layer behaves in\npractice so now let's take a look at a",
 "single layered neural network so this is\ntaking it one step beyond what we've",
 'just seen this is where we have now a\nsingle hidden layer that feeds into a',
 "single output layer and I'm calling this\na hidden layer because unlike our inputs",
 'and our outputs these states of the\nhidden layer are not directly enforced',
 "or they're not directly observable we\ncan probe inside the network and see",
 "them but we don't actually enforce what\nthey are these are learned as opposed to",
 'the inputs which are provided by us now\nsince we have a transformation between',
 'the inputs and the hidden layer and the\nhidden layer and the output layer each',
 'of those two transformations will have\ntheir own weight matrices which here I',
 'call W 1 and W 2 so its corresponds to\nthe first layer and the second layer if',
 'we look at a single unit inside of that\nhidden layer',
 "take for example Z 2 I'm showing here\nthat's just a single perceptron like we",
 "talked about before it's taking a\nweighted sum of all of those inputs that",
 'feed into it and it applies the\nnon-linearity and feeds it on to the',
 'next layer same story as before this\npicture actually looks a little bit',
 'messy so what I want to do is actually\nclean things up a little bit for you and',
 "I'm gonna replace all of those lines\nwith just this symbolic representation",
 "and we'll just use this from now on in\nthe future to denote dense layers or",
 'fully connected layers between two\nbetween an input and an output or',
 'between an input and hidden layer',
 'and again if we wanted to implement this\nintensive flow the idea is pretty simple',
 'we can just define two of these dense\nlayers the first one our hidden layer',
 'with n outputs and the second one our\noutput layer with two outputs we can cut',
 'week and like join them together\naggregate them together into this',
 'wrapper which is called a TF sequential\nmodel and sequential models are just',
 'this idea of composing neural networks\nusing a sequence of layers so whenever',
 'you have a sequential message passing\nsystem or sequentially processing',
 'information throughout the network you\ncan use sequential models and just',
 "define your layers as a sequence and\nit's very nice to allow information to",
 'propagate through that model now if we\nwant to create a deep neural network the',
 'idea is basically the same thing except\nyou just keep stacking on more of these',
 'layers and to create more of an more of\na hierarchical model ones where the',
 'final output is computed by going deeper\nand deeper into this representation and',
 'the code looks pretty similar again so\nagain we have this TF sequential model',
 'and inside that model we just have a\nlist of all of the layers that we want',
 "to use and they're just stacked on top\nof each other okay so this is awesome so",
 'hopefully now you have an understanding\nof not only what a single neuron is but',
 'how you can compose neurons together and\nactually build complex hierarchical',
 "models with deep with neural networks\nnow let's take a look at how you can",
 'apply these neural networks into a very\nreal and applied setting to solve some',
 "problem and actually train them to\naccomplish some task here's a problem",
 'that I believe any AI system should be\nable to solve for all of you and',
 "probably one that you care a lot about\nwill I pass this class to do this let's",
 "start with a very simple two input model\none feature or one input we're gonna",
 "define is how many let's see how many\nlectures you attend during this class",
 'and the second one is the number of\nhours that you spend on your final',
 'projects I should say that the minimum\nnumber of hours you can spend your final',
 "project is 50 hours now I'm just joking\nokay so let's take all of the data from",
 'previous years and plot it on this\nfeature space like we looked at before',
 'green points are students that have\npassed the class in the past and red',
 'points are people that have failed we\ncan plot all of this data onto this',
 'two-dimensional grid like this and we\ncan also plot you so here you are you',
 "have attended four lectures and you've\nonly spent five hours on your final exam",
 "you're on you're on your final project\nand the question is are you going to",
 "pass the class given everyone around you\nand how they've done in the past how are",
 "you going to do so let's do it we have\ntwo inputs we have a single layered set",
 'single hidden layer neural network we\nhave three hidden units in that hidden',
 "layer and we'll see that the final\noutput probability when we feed in those",
 'two inputs of four and five is predicted\nto be 0.1 or 10% the probability of you',
 "passing this class is 10% that's not\ngreat news the actual prediction was one",
 'so you did pass the class now does\nanyone have an idea of why the network',
 'was so wrong in this case exactly so we\nnever told this network anything the',
 "weights are wrong we've just initialized\nthe weights in fact it has no idea what",
 'it means to pass a class it has no idea\nof what each of these inputs mean how',
 "many lectures you've attended and the\nhours you've spent on your final project",
 "it's just seeing some random numbers it\nhas no concept of how other people in",
 'the class have done so far so what we\nhave to do to this network first is',
 "train it and we have to teach it how to\nperform this task until we teach it it's",
 "just like a baby that doesn't know\nanything so it just entered the world it",
 'has no concepts or no idea of how to\nsolve this task and we have to teach at',
 'that now how do we do that the idea here\nis that first we have to tell the',
 "network when it's wrong so we have to\nquantify what's called its loss or its",
 'error and to do that we actually just\ntake our prediction or what the network',
 'predicts and we compare it to what the\ntrue answer was',
 "if there's a big discrepancy between the\nprediction and the true answer we can",
 'tell the network hey you made a big\nmistake right so this is a big error',
 "it's a big loss and you should try and\nfix your answer to move closer towards",
 "the true answer which it should be okay\nnow you can imagine if you don't have",
 "just one student but now you have many\nstudents the total loss let's call it",
 'here the empirical risk or the objective\nfunction it has many different names',
 "it's just the the average of all of\nthose individual losses so the",
 'individual loss is a loss that takes as\ninput your prediction and your actual',
 "that's telling you how wrong that single\nexample is and then the final the total",
 'loss is just the average of all of those\nindividual student losses so if we look',
 "at the problem of binary classification\nwhich is the case that we're actually",
 "caring about in this example so we're\nasking a question will I pass the class",
 'yes or no binary classification we can\nuse what is called as the softmax',
 "cross-entropy loss and for those of you\nwho aren't familiar with cross-entropy",
 'this was actually a a formulation\nintroduced by Claude Shannon here at MIT',
 "during his master's thesis as well and\nthis was about 50 years ago it's still",
 'being used very prevalently today and\nthe idea is it just again compares how',
 'different these two distributions are so\nyou have a distribution of how how',
 'likely you think the student is going to\npass and you have the true distribution',
 'of if the student passed or not you can\ncompare the difference between those two',
 'distributions and that tells you the\nloss that the network incurs on that',
 "example now let's assume that instead of\na classification problem we have a",
 "regression problem where instead of\npredicting if you're going to pass or",
 "fail to class you want to predict the\nfinal grade that you're going to get so",
 "now it's not a yes/no answer problem\nanymore",
 "but instead it's a what's the grade I'm\ngoing to get what's the number what so",
 "it's it's a full range of numbers that\nare possible now",
 'and now we might want to use a different\ntype of loss for this different type of',
 "problem and in this case we can do\nwhat's called a mean squared error loss",
 'so we take the actual prediction we take\nthe the sorry excuse me we take the',
 'prediction of the network we take the\nactual true final grade that the student',
 "got we subtract them we take their\nsquared error and we say that that's the",
 "mean squared error that's the loss that\nthe network should should try to",
 'optimize and try to minimize so ok so\nnow that we have all this information',
 'with the loss function and how to\nactually quantify the error of the',
 "neural network let's take this and\nunderstand how to train train our model",
 'to actually find those weights that it\nneeds to to use for its prediction so W',
 'is what we want to find out W is the set\nof weights and we want to find the',
 'optimal set of weights that tries to\nminimize this total loss over our entire',
 'test set so our test set is this example\ndata set that we want to evaluate our',
 'model on so in the class example the\ntest set is you so you want to',
 "understand how likely you are to pass\nthis class you're the test set now what",
 "this means is that we want to find the\nW's that minimize that total loss",
 'function which we call as the objective\nfunction J of W now remember that W is',
 "just a aggregation or a collection of\nall of the individual w's from all of",
 'your weights so here this is just a way\nfor me to express this in a clean',
 "notation but W is a whole set of numbers\nit's not just a single number and you",
 "want to find this all of the W's you\nwant to find the value of each of those",
 "weights such that you can minimize this\nentire loss function it's a very",
 'complicated problem and remember that\nour loss function is just a simple',
 'function in terms of those weights so if\nwe plot in the case again of a',
 'two-dimensional weight problem so one of\nthe weights is on the x-axis one of the',
 'weights is on this axis and on the z\naxis we have the loss so for any',
 'value of w we can see what the loss\nwould be at that point now what do we',
 'want to do we want to find the place on\nthis landscape what are the values of W',
 'that we get the minimum loss okay so\nwhat we can do is we can just pick a',
 'random W pick a random place on this\nthis landscape to start with and from',
 "this random place let's try to\nunderstand how the landscape is changing",
 "what's the slope of the landscape we can\ntake the gradient of the loss with",
 'respect to each of these weights to\nunderstand the direction of maximum',
 "ascent okay that's what the gradient\ntells us now that we know which way is",
 "up we can take a step in the direction\nthat's down so we know which way is up",
 'we reverse the sign so now we start\nheading downhill and we can move towards',
 'that lowest point now we just keep\nrepeating this process over and over',
 "again until we've converged to a local\nminimum now we can summarize this",
 "algorithm which is known as gradient\ndescent because you're taking a gradient",
 "and you're descending down down that\nlandscape by starting to initialize our",
 'rates wait randomly we compute the\ngradient DJ with respect to all of our',
 'weights then we update our weights in\nthe opposite direction of that gradient',
 'and take a small step which we call here\nADA of that gradient and this is',
 "referred to as the learning rate and\nwe'll talk a little bit more about that",
 'later but ADA is just a scalar number\nthat determines how much of a step you',
 'want to take at each iteration how\nstrongly or aggressively do you want to',
 'step towards that gradient in code the\npicture looks very similar so to',
 'implement gradient descent is just a few\nlines of code just like the pseudocode',
 'you can initialize your weights randomly\nin the first line you can compute your',
 'loss with respect to those gradients and\nwith respect to those predictions and',
 'your data given that gradient you just\nupdate your weights in the opposite',
 'direction of that event of that vector\nright',
 "now the magic line here is actually how\ndo you compute that gradient and that's",
 "something I haven't told you and that's\nsomething it's not easy at all so the",
 'question is given a loss and given all\nof our weights in our network how do we',
 'know which way is good which way is a\ngood place to move given all of this',
 "information and I never told you about\nthat but that's a process called back",
 "propagation and let's talk about a very\nsimple example of how we can actually",
 "derive back propagation using elementary\ncalculus so we'll start with a very",
 'simple network with only one hidden\nneuron and one output this is probably',
 "the simplest neural network that you can\ncreate you can't really get smaller than",
 'this computing the gradient of our loss\nwith respect to W to here which is that',
 'second way between the hidden state and\nour output can tell us how much a small',
 "change in W 2 will impact our loss so\nthat's what the gradient tells us right",
 'if we change W 2 in the differential\ndifferent like a very minor manner how',
 'does our loss change does it go up or\ndown how does it change and by how much',
 "really so that's the gradient that we\ncare about the gradient of our loss with",
 'respect to W 2 now to evaluate this we\ncan just apply the chain rule in',
 'calculus so we can split this up into\nthe gradient of our loss with respect to',
 'our output Y multiplied by the gradient\nof our walk or output Y with respect to',
 'W 2 now if we want to repeat this\nprocess for a different way in the',
 "neural network let's say now W 1 not W 2\nnow we replace W 1 on both sides we also",
 "apply the chain rule but now you're\ngoing to notice that the gradient of Y",
 'with respect to W 1 is also not directly\ncomputable we have to apply the chain',
 "rule again to evaluate this so let's\napply the chain rule again we can break",
 'that second term up into with respect to\nnow the the state Z ok and using that we',
 'can kind of back propagate all of these\ngradients from the output all the way',
 'back to the input that allows our error\nsignal to really',
 'propagate from output to input and\nallows these gradients to be computed in',
 "practice now a lot of this is not really\nimportant or excuse me it's not as",
 'crucial that you understand the\nnitty-gritty math here because in a lot',
 "of popular deep learning frameworks we\nhave what's called automatic",
 'differentiation which does all of this\nback propagation for you under the hood',
 'and you never even see it which is\nincredible it made training neural',
 "networks so much easier you don't have\nto implement back propagation anymore",
 "but it's still important to understand\nhow these work at the foundation which",
 "is why we're going through it now ok\nobviously then you repeat this for every",
 'single way in the network here we showed\nit for just W 1 and W 2 which is every',
 'single way in this network but if you\nhave more you can just repeat it again',
 "keep applying the chain rule from output\nto input to compute this ok and that's",
 "the back prop algorithm in theory very\nsimple it's just an application of the",
 "chain rule in essence but now let's\ntouch on some of the insights from",
 'training and how you can use the back\nprop algorithm to train these networks',
 'in practice optimization of neural\nnetworks is incredibly tough in practice',
 "so it's not as simple as the picture I\nshowed you on the colorful one on the",
 "previous slide here's an illustration\nfrom a paper that came out about two or",
 'three years ago now where the authors\ntried to visualize the landscape of a of',
 'a neural network with millions of\nparameters but they collapsed that down',
 'onto just two-dimensional space so that\nwe can visualize it and you can see that',
 "the landscape is incredibly complex\nit's not easy there are many local",
 'minima where the gradient descent\nalgorithm could get stuck into and',
 'applying gradient descent in practice in\nthese type of environments which is very',
 'standard in neural networks can be a\nhuge challenge',
 "now we're called the update equation\nthat we defined previously with gradient",
 "descent this is that same equation we're\ngoing to update our weights in the",
 "direction in the opposite direction of\nour gradient I didn't talk too much",
 'about this parameter ADA I pointed it\nout',
 'this is the learning rate it determines\nhow much of a step we should take in the',
 'direction of that gradient and in\npractice setting this learning rate can',
 'have a huge impact in performance so if\nyou set that learning rate to small that',
 "means that you're not really trusting\nyour gradient on each step so if ADA is",
 "super tiny\nthat means on each time each step you're",
 'only going to move a little bit towards\nin the opposite direction of your',
 'gradient just in little small increments\nand what can happen then is you can get',
 "stuck in these local minima because\nyou're not being as aggressive as you",
 'should be to escape them now if you set\nthe learning rate to large you can',
 'actually overshoot completely and\ndiverge which is even more undesirable',
 'so setting the learning rate can be very\nchallenging in practice you want to pick',
 "a learning rate that's large enough such\nthat you avoid the local minima but",
 "small offs such that you still converge\nin practice now the question that you're",
 'all probably asking is how do we set the\nlearning rate then well one option is',
 'that you can just try a bunch of\nlearning rates and see what works best',
 'another option is to do something a\nlittle bit more clever and see if we can',
 'try to have an adaptive learning rate\nthat changes with respect to our lost',
 'landscape maybe it changes with respect\nto how fast the learning is happening or',
 'a range of other ideas within the\nnetwork optimization scheme itself this',
 'means that the learning rate is no\nlonger fixed but it can now increase or',
 'decrease throughout training so as\ntraining progressive your learning rate',
 'may speed up you may take more\naggressive steps you may take smaller',
 'steps as you get closer to the local\nminima so that you really converge on',
 'that point and there are many options\nhere of how you might want to design',
 'this adaptive algorithm and this has\nbeen a huge or a widely studied field in',
 'optimization theory for machine learning\nand deep learning and there have been',
 'many published papers and\nimplementations within tensor flow on',
 'these different types of adaptive\nlearning rate algorithms so SGD is just',
 "that vanilla gradient descent that I\nshowed you before that's the first one",
 'all of the others are all\nadaptive learning rates which means that',
 'they change their learning rate during\ntraining itself so they can increase or',
 'decrease depending on how the\noptimization is going and during your',
 'labs we really encourage you again to\ntry out some of these different',
 "optimization schemes see what works what\ndoesn't work a lot of it is problem",
 'dependent there are some heuristics that\nyou can you can get but we want you to',
 "really gain those heuristics yourselves\nthrough the course of the labs it's part",
 "of building character okay so let's put\nthis all together from the beginning we",
 'can define our model which is defined as\nthis sequential wrapper inside of this',
 'sequential wrapper we have all of our\nlayers all of these layers are composed',
 'of perceptrons or single neurons which\nwe saw earlier the second line defines',
 'our optimizer which we saw in the\nprevious slide',
 'this can be SGD it can also be any of\nthose adaptive learning rates that we',
 "saw before now what we want to do is\nduring our training loop it's very it's",
 "the same stories again as before\nnothing's changing here we forward pass",
 'all of our inputs through that model we\nget our predictions using those',
 'predictions we can evaluate them and\ncompute our loss our loss tells us how',
 'wrong our network was on that iteration\nit also tells us how we can compute the',
 'gradients and how we can change all of\nthe weights in the network to improve in',
 'the future and then the final line there\ntakes those gradients and actually',
 'allows our optimizer to update the\nweights and the trainable variables such',
 'that on the next iteration they do a\nlittle bit better and over time if you',
 'keep looping this will converge and\nhopefully you should fit your data no',
 'now I want to continue to talk about\nsome tips for training these networks in',
 'practice and focus on a very powerful\nidea of batching your data into mini',
 "batches so to do this let's revisit the\ngradient descent algorithm this gradient",
 'is actually very computationally\nexpensive to compute in practice so',
 'using the backprop algorithm is\na very expensive idea and practice so',
 'what we want to do is actually not\ncompute this over all of the data points',
 'but actually computed over just a single\ndata point in the data set and most',
 "real-life applications it's not actually\nfeasible to compute on your entire data",
 "set at every iteration it's just too\nmuch data so instead we pick a single",
 'point randomly we compute our gradient\nwith respect to that point and then on',
 'the next iteration we pick a different\npoint and we can get a rough estimate of',
 'our gradient at each step right so\ninstead of using all of our data now we',
 'just pick a single point I we compute\nour gradient with respect to that single',
 "point I and what's a middle ground here\nso the downside of using a single point",
 "is that it's going to be very noisy the\ndownside of using all of the points is",
 "that it's too computationally expensive\nif there's some middle ground that we",
 'can have in between so that middle\nground is actually just very simple you',
 'instead of taking one point and instead\ntaking all of the points let take a mini',
 'batch of points so maybe something on\nthe order of 10 20 30 100 maybe',
 'depending on how rough or accurate you\nwant that approximation of your gradient',
 'to be and how much you want to trade off\nspeed and computational efficiency now',
 'the true gradient is just obtained by\naveraging the gradient from each of',
 'those B points so B is the size of your\nbatch in this case now since B is',
 'normally not that large like I said\nmaybe on the order of tens to a hundreds',
 'this is much faster to compute than full\ngradient descent and much more accurate',
 "than stochastic gradient descent because\nit's using more than one point more than",
 'one estimate now this increase in\ngradient accuracy estimation actually',
 'allows us to converge to our target much\nquicker because it means that our',
 'gradients are more accurate in practice\nit also means that we can increase our',
 "learning rate and trust each update more\nso if we're very noisy in our gradient",
 "estimation we probably want to lower our\nlearning rate a little more so we don't",
 "fully step in the wrong direction if\nwe're not totally confident with that",
 'gradient if we have a larger batch of\ngradient of data to',
 'they are gradients with we can trust\nthat learning great a little more',
 'increase it so that it steps it more\naggressively in that direction what this',
 'means also is that we can now massively\nparalyze this computation because we can',
 'split up batches on multiple GPUs or\nmultiple computers even to achieve even',
 'more significant speed ups with this\ntraining process now the last topic I',
 'want to address is that of overfitting\nand this is also known as the problem of',
 "generalization in machine learning and\nit's actually not unique to just deep",
 "learning but it's a fundamental problem\nof all of machine learning now ideally",
 'in machine learning we want a model that\nwill approximate or estimate our data or',
 "accurately describes our data let's say\nlike that said differently we want to",
 'build models that can learn\nrepresentations from our training data',
 "that's still generalize to unseen test\ndata now assume that you want to build a",
 'line that best describes these points\nyou can see on the on the screen under',
 'fitting describes if we if our model\ndoes not describe the state of',
 "complexity of this problem or if we\ncan't really capture the true complexity",
 'of this problem while overfitting on the\nright starts to memorize certain aspects',
 'of our training data and this is also\nnot desirable we want the middle ground',
 'which ideally we end up with a model in\nthe middle that is not too complex to',
 'memorize all of our training data but\nalso one that will continue to',
 'generalize when it sees new data so to\naddress this problem of regularization',
 "in neural network specifically let's\ntalk about a technique of regularization",
 "which is another way that we can deal\nwith this and what this is doing is it's",
 'trying to discourage complex information\nfrom being learned so we want to',
 'eliminate the model from actually\nlearning to memorize the training data',
 "we don't want to learn like very\nspecific pinpoints of the training data",
 "that don't generalize well to test data\nnow as we've seen before this is",
 'actually crucial for our models to be\nable to generalize to our test data so',
 'this is very important the most popular\nregularization technique',
 'deep learning is this very basic idea of\ndrop out now the idea of drop out is',
 "well actually let's start with by\nrevisiting this picture of a neural",
 'network that we had introduced\npreviously and drop out during training',
 'we randomly set some of these\nactivations of the hidden neurons to',
 "zero with some probability so I'd say\nour probability is 0.5",
 "we're randomly going to set the\nactivations to 0.5",
 'with probability of 0.5 to some of our\nhidden neurons to 0 the idea is',
 'extremely powerful because it allows the\nnetwork to lower its capacity it also',
 "makes it such that the network can't\nbuild these memorization channels",
 'through the network where it tries to\njust remember the data because on every',
 'iteration 50% of that data is going to\nbe or 50% of that memorization or memory',
 "is going to be wiped out so it's going\nto be forced to to not only generalize",
 "better but it's going to be forced to\nhave multiple channels through the",
 'network and build a more robust\nrepresentation of its prediction now we',
 'just repeat this on every iteration so\non the first iteration we dropped out',
 'one 50% of the nodes on the next\niteration we can drop out a different',
 'randomly sampled 50% which may include\nsome of the previously sampled nodes as',
 'well and this will allow the network to\ngeneralize better to new test data',
 "the second regularization technique that\nwe'll talk about is the notion of early",
 'stopping so what I want to do here is\njust talk about two lines so during',
 'training which is the x-axis here we\nhave two lines the y-axis is our loss',
 "curve the first line is our training\nloss so that's the green line the green",
 'line tells us how our training data how\nwell our model is fitting to our',
 'training data we expect this to be lower\nthan the second line which is our',
 'testing data\nso usually we expect to be doing better',
 'on our training data than our testing\ndata as we train and as this line moves',
 'forward into the future both of these\nlines should kind of decrease go down',
 "because we're optimizing the network\nwe're improving its performance",
 'eventually though there becomes a point\nwhere the training data starts to',
 'diverge from the testing data now what\nhappens is that the training day',
 'should always continue to fit or the\nmodel should always continue to fit the',
 "training data because it's still seeing\nall of the training data it's not being",
 'penalized from that except for maybe if\nyou drop out or other means but the',
 "testing data it's not seeing so at some\npoint the network is going to start to",
 'do better on its training data than its\ntesting data and what this means is',
 'basically that the network is starting\nto memorize some of the training data',
 "and that's what you don't want so what\nwe can do is well we can perform early",
 'stopping or we can identify this point\nthis inflection point where the test',
 'data starts to increase and diverge from\nthe training data so we can stop the',
 'network early and make sure that our\ntest accuracy is as minimum as possible',
 'and of course if we actually look at on\nthe side of this line if we look at on',
 "the left side that's where a model is\nunder fit so we haven't reached the true",
 "capacity of our model yet so we'd want\nto keep training if we didn't stop yet",
 "if we did stop already and on the right\nside is where we've over fit where we've",
 "passed that early stopping point and we\nneed to like basically we've started to",
 "memorize some of our training did and\nthat's when we've gone too far I'll",
 "conclude this lecture by just\nsummarizing three main points that we've",
 "covered so far first we've learned about\nthe fundamentals of neural networks",
 "which is a single neuron or a perceptron\nwe've learned about stacking and",
 'composing these perceptrons together to\nform complex hierarchical',
 'representations and how we can\nmathematically optimize these networks',
 'using a technique called back\npropagation using their loss and finally',
 'we address the practical side of\ntraining these models such as mini',
 "batching regularization and adaptive\nlearning rates as well with that I'll",
 "finish up I can take a couple questions\nand then we'll move on to office lecture",
 "on deep sequential modeling I'll take\nany like maybe a couple questions if",
 'there are any now thank you']