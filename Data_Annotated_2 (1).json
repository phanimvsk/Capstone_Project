{
  "data": [
    {
      "paragraphs": [
        {
          "qas": [
            {
              "question": "How many stages does Map reduce consist?",
              "id": 535195,
              "answers": [
                {
                  "answer_id": 600195,
                  "document_id": 1139572,
                  "question_id": 535195,
                  "text": "The Map-Reduce as you can imagine consist of 2 stages: the map stage and the reduce\nstage.\nAnd in between there is a group and redistribute phase.",
                  "answer_start": 119304,
                  "answer_end": 119450,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Under what circumstances would classical analysis work?",
              "id": 535951,
              "answers": [
                {
                  "answer_id": 606530,
                  "document_id": 1139572,
                  "question_id": 535951,
                  "text": " the quick answer is, a classical analysis\nessentially works really well in an environment of high error or noise.",
                  "answer_start": 215515,
                  "answer_end": 215629,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": " What is density based clustering?",
              "id": 535974,
              "answers": [
                {
                  "answer_id": 606554,
                  "document_id": 1139572,
                  "question_id": 535974,
                  "text": " density based clustering the idea behind density based clustering is\nthat a cluster is essentially a dense region of objects that are surrounded by regions\nof lower density.\nSo, the idea here is that, because there are not such well defined clusters like in the\ncase of the well separated idea that you are not really looking too often do a complete\nclustering and there is a lot of noise in the data and you know the clusters themselves\nare irregular or intertwined and there are lots of outliers and so on.\nSo, the idea here is that you acknowledge all of that, but you just try to identify\nspots of extreme density, where once you identify that spot that dense region becomes a cluster\nand so that is fairly useful in defining a cluster, where there is a lot of noise and\nso on.",
                  "answer_start": 151653,
                  "answer_end": 152435,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Explain about bias variance dichotomy?",
              "id": 535952,
              "answers": [
                {
                  "answer_id": 606531,
                  "document_id": 1139572,
                  "question_id": 535952,
                  "text": "which is this dichotomy that you see between high noise versus low\nnoise and fitting to any shape you want versus not being able to fit any shape you want has\na lot to do with something you seen before, which is the bias variance dichotomy.\nAnd this bias variance dichotomy is what you seeing in these two extreme approaches.\nThe statistical way can sometimes balance that out and, so the truth is almost any supervised\nlearning algorithm could be fair game, the only context is that the data set is fairly\nsmall.",
                  "answer_start": 216766,
                  "answer_end": 217279,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are two very popular clustering techniques ? ",
              "id": 535975,
              "answers": [
                {
                  "answer_id": 606555,
                  "document_id": 1139572,
                  "question_id": 535975,
                  "text": "two very popular clustering techniques. The first is K mean clustering and the second\nis called the hierarchical clustering, where both these techniques are fairy old, this\nstill enjoy immense popularity in terms of being actually used.",
                  "answer_start": 153310,
                  "answer_end": 153546,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is the process of design of experiments?",
              "id": 535953,
              "answers": [
                {
                  "answer_id": 606532,
                  "document_id": 1139572,
                  "question_id": 535953,
                  "text": "process of design of experiments is the way\nwe have done it with full factorials and the way we explained it makes it essentially a\none shot approach.\nIt basically means you decided even before you see any results the entire set of points\nthat you want you look at the input space.",
                  "answer_start": 218025,
                  "answer_end": 218306,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is the difference between K and hierarchical clustering ?",
              "id": 535976,
              "answers": [
                {
                  "answer_id": 606556,
                  "document_id": 1139572,
                  "question_id": 535976,
                  "text": " K\nmean clustering and this choice of K mean and hierarchical, you should find to be also\nfitting.",
                  "answer_start": 153565,
                  "answer_end": 153663,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "How many stages does the Map-Reduce consist of ?",
              "id": 535198,
              "answers": [
                {
                  "answer_id": 600201,
                  "document_id": 1139572,
                  "question_id": 535198,
                  "text": "In the reduce stage you essentially compute aggregate statistics corresponding to the\nkey.\nWe could add things up; we could summarize them, we could transform them, we could do\nany kind of filtering that you want based on the key, whatever it is we can compute\naggregate values based on the keys.",
                  "answer_start": 120034,
                  "answer_end": 120330,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": " What is the other name for optimal experimental design and explain it?",
              "id": 535954,
              "answers": [
                {
                  "answer_id": 606533,
                  "document_id": 1139572,
                  "question_id": 535954,
                  "text": "Active learning is often seen as a semi supervised learning approach and understandably it is\nalso called is optimal experimental design.\nBecause, active learning is a process where the system sequentially chooses to query the\ndesign space and therefore, be able to build knowledge, on what we see and how it can be\nbetter understood.",
                  "answer_start": 219466,
                  "answer_end": 219800,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "K means?",
              "id": 535977,
              "answers": [
                {
                  "answer_id": 606557,
                  "document_id": 1139572,
                  "question_id": 535977,
                  "text": " K means\nas a prototype based approach, where that is also something that we discuss in terms\nof how clusters are formed. And the prototype based approach is one where, there is like\nthis representative for each cluster and you use these representatives in some fashion\nto express, what a cluster is and to also form the clusters.",
                  "answer_start": 154153,
                  "answer_end": 154483,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "How do you group the key value pairs ?",
              "id": 535199,
              "answers": [
                {
                  "answer_id": 600211,
                  "document_id": 1139572,
                  "question_id": 535199,
                  "text": "all the key value pairs which have concept of the key get grouped and they are send to\none reducer, likewise all the key value pairs which have for as a key get grouped and are\nsend to another reducer and so on so forth.",
                  "answer_start": 121283,
                  "answer_end": 121503,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are some prominence strategies in the whole active learning frame work?",
              "id": 535955,
              "answers": [
                {
                  "answer_id": 606534,
                  "document_id": 1139572,
                  "question_id": 535955,
                  "text": "what are some prominence strategies in the whole active learning frame work is that\nin a sense all of these strategies at we going to talk about now, rely on one thing.\nIt relies on you know evaluating the, how informative different points on the input\nspace can be if you query them and you got an answer, what you mean by queried is you\nchoose a particular point in the input space and say can you please give me an output for\nthat.",
                  "answer_start": 220677,
                  "answer_end": 221111,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Explain K ?\n",
              "id": 535978,
              "answers": [
                {
                  "answer_id": 606558,
                  "document_id": 1139572,
                  "question_id": 535978,
                  "text": "K means\nas a prototype based approach, where that is also something that we discuss in terms\nof how clusters are formed. And the prototype based approach is one where, there is like\nthis representative for each cluster and you use these representatives in some fashion\nto express, what a cluster is and to also form the clusters.\nSo, it is essentially this prototype based approach, where you create K clusters and\nit is also noteworthy that it is an iterative procedure. So, even right at the end of the\nfirst iteration you already have K clusters and they might not be good clusters, because\nit is just the first iteration. And then, like most optimization procedures like steepest\ndescent, any of these other iterative producers you will find that over time you are just\nrefining a solution and at some point, it does not make sense to refine any more. You\nare not, your clusters are not changing essentially prototypes or not changing either location\nor who they are and, so you stop at some point",
                  "answer_start": 154154,
                  "answer_end": 155155,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are the strategies that are involved with active learning?",
              "id": 535956,
              "answers": [
                {
                  "answer_id": 606535,
                  "document_id": 1139572,
                  "question_id": 535956,
                  "text": "the strategies that are involved with active learning can be broadly classified\nin these four and you know this these is an area of you know where there is a active research\nand, so there they might be some strategies that also follow fall that are new that might\nfall out it these have been historically the more popular ones.",
                  "answer_start": 221250,
                  "answer_end": 221577,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "When is K ideal to use?\n",
              "id": 535979,
              "answers": [
                {
                  "answer_id": 606559,
                  "document_id": 1139572,
                  "question_id": 535979,
                  "text": "This procedure is ideal when all variables\nare quantitative, whether what we really mean is that you should be able to take each data\npoint and you might have some other data point or some other location and your location is\ndefined across the multiple attributes associated with the data point. So, a data point, again\nthe conception you have in your mind is these rows and you are basically grouping data points,\nthat is your job. And each data point is described by some attributes, which are these columns in a table and each column means something. So, with the K means procedure, what you doing\nis you want to say that that you want to take this conception and you want to actually,\nyou want actually be able to compute distances between two points across these dimensions, which means that attributes themselves need to be quantitative. This in a lot of ways\nto remind you of this K nearest neighbors",
                  "answer_start": 155157,
                  "answer_end": 156064,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "what does partitioning graphs mean?",
              "id": 535225,
              "answers": [
                {
                  "answer_id": 600385,
                  "document_id": 1139572,
                  "question_id": 535225,
                  "text": "Partitioning, again interestingly comes more from the computer science community, where\nyou are breaking a graphs, so you are partitioning graphs.\nBut, again the core idea is that you have some data and taking the data and based on\ncertain properties of this data, you choose to group it.\nSo, you can think of it as actually grouping data objects based on various attributes associated\nwith this data.",
                  "answer_start": 131552,
                  "answer_end": 131953,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": " How to efficiently aggregate the each event into a graph.?",
              "id": 535201,
              "answers": [
                {
                  "answer_id": 600228,
                  "document_id": 1139572,
                  "question_id": 535201,
                  "text": "Map-Reduce to efficiently aggregate the each event into a graph.",
                  "answer_start": 123054,
                  "answer_end": 123118,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is query by committee?",
              "id": 535957,
              "answers": [
                {
                  "answer_id": 606536,
                  "document_id": 1139572,
                  "question_id": 535957,
                  "text": "also fairly popular one is this idea of query by committee, let me just\nmark, where we are we finished this and the second is query by committee.\nQuery by committee approach essentially involves having committee of different models, which\nare all trained on the current data set and when we say data here we are talking about\nthe data set for which, we have the outputs you have a data set with the outputs in those.",
                  "answer_start": 222829,
                  "answer_end": 223245,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are initialize cluster centers?",
              "id": 535980,
              "answers": [
                {
                  "answer_id": 606560,
                  "document_id": 1139572,
                  "question_id": 535980,
                  "text": "is, essentially think of this\nand it is easy to think of it in a graphical sense or you can think of it in a mathematical sense, but the idea is that each data point can be expressed in terms of an x axis, y\naxis, z axis, w axis, so on and so forth, depending on the number of attributes there are that represent each data points. Now, while each data point is represented based\non these different attributes, you can create a cluster center also on these attributes.\nAnd often it really make sense to put cluster centers, where you think the clusters are going to be form, but you see how even if you do not do that perfectly sometimes the\nclustering the K means clustering will adjust for it. But, the most important thing to take\naway from this is that, the K means cluster algorithm therefore, could be sensitive to\nwhere you place the cluster centers. If you do not place them in sometimes the right places,\nyou could get to a solution, which is not necessarily a good solution.\nSo, the idea is that you initialize some cluster centers, so specifically you would initialize\nK cluster center, so you will, because it is K mean clustering. So, K is usually a number\nbetween 2 and may be 10 or 20 depending on the… The higher limit is a little more vague,\nmost problems you looking at between 2 to 6 or 7 clusters at the most. But, there are\ncontexts, where your data set is really large or what you intend to do with the clustering\nis such that you do not mind having more clusters. ",
                  "answer_start": 156854,
                  "answer_end": 158340,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is expected model change?",
              "id": 535958,
              "answers": [
                {
                  "answer_id": 606537,
                  "document_id": 1139572,
                  "question_id": 535958,
                  "text": " expected model change the idea behind expected model change is to\nsee if we knew the label of a particular data point.\nThen, which label of the input space if we knew would contribute to the greatest change\nin the current model we have based on the current data we have of the inputs and outputs.",
                  "answer_start": 224494,
                  "answer_end": 224791,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is centroid and where is it ?",
              "id": 535981,
              "answers": [
                {
                  "answer_id": 606561,
                  "document_id": 1139572,
                  "question_id": 535981,
                  "text": "Centroid\nessentially is in many senses like an average. It is, again it depends on exactly what measure\nof distance you choose, but if you are choosing Euclidean distances, the centroid is essentially like the average. And we say it is the average of these data points, but it is called the\ncentroid, because it is the average across all those dimensions. The number of dimensions are the number of attributes, so across those dimensions, what is the center.\nSo, once all these points have been assigned you kind of compute a new cluster center based\non the centroid. And it is essentially like you forget the old cluster center and now, this new cluster center is your cluster center and you do this for each assignment. ",
                  "answer_start": 159834,
                  "answer_end": 160556,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is unsupervised learning technique and supervised learning technique ?",
              "id": 535227,
              "answers": [
                {
                  "answer_id": 600387,
                  "document_id": 1139572,
                  "question_id": 535227,
                  "text": "the clustering is primarily seen as an\nunsupervised learning technique, whereas classification is supervised learning technique.",
                  "answer_start": 132781,
                  "answer_end": 132909,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What kind of idea is excepted change model?",
              "id": 535959,
              "answers": [
                {
                  "answer_id": 606538,
                  "document_id": 1139572,
                  "question_id": 535959,
                  "text": "And the idea is to kind of query that that point very specifically the last two sets,\nwhich is the expected error reduction and various reduction use the following approaches\nthe approach is to basically say with error reduction the idea is there some deviation\nof the predication verses actual.",
                  "answer_start": 225094,
                  "answer_end": 225389,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are dissimilarity matrices ?",
              "id": 535982,
              "answers": [
                {
                  "answer_id": 606562,
                  "document_id": 1139572,
                  "question_id": 535982,
                  "text": "dissimilarity matrices? What I mean is you have all this data points, let us start calling them 1, 2, 3, 4. Now, with K means the distances between and I am\ngoing give you the same data points in the columns, now with K means I can compute the\ndistances between the data points 1 and data point 2 through some form of Euclidean distances\nand mark a value of x. But, what if, so the whole process of using Euclidean distances\nrequire that I go into each attributes look at how different it is and then compute that square distances takes the square roots. But, what if I did not have that process,\nwhat if I had a process, where I gave you just the differences between each data points.\nSo, I have something like a dissimilarities matrix, so when I have a dissimilarities matrix,\nthat I am giving you and you can get the dissimilarities matrices through completely differences ways\nit can be user survey or it can be something extremely subjective, where you say I feel\nlike 1 and 2 are different by this much and I can tell you how different 1 and 2 are and\nI can tell you how different 1 and 3 are you know and I can tell you how different one\nand four are and so on. But, I cannot really break it down into five different attributes and give it to you that way. So, people do this lot of times in social",
                  "answer_start": 167045,
                  "answer_end": 168350,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is variance reduction approach?",
              "id": 535960,
              "answers": [
                {
                  "answer_id": 606539,
                  "document_id": 1139572,
                  "question_id": 535960,
                  "text": "the variance reduction approach is a deviation from that it is a deviation in that\nit is you do not look at the error between the fitted model and the data points, But\nyou just look to see reduce the overall variance in the output space.\nNow, that is done probably because it is much easier to do this, but what the core idea\nhere is that you have some variance associated with the outputs.",
                  "answer_start": 227028,
                  "answer_end": 227418,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "hierarchical clustering ?",
              "id": 535983,
              "answers": [
                {
                  "answer_id": 606563,
                  "document_id": 1139572,
                  "question_id": 535983,
                  "text": "hierarchical clustering with hierarchical\nclustering you do not really windup fixing fix number of clusters. And essentially the approach itself starts with either one very large cluster and breaks it down step by step,\nso the first one, so one way of doing it is called the divisive way, which is you have\none large cluster, which has all the data points in it is and then, after that I break it in to 2 clusters and then, after that I break it in to 3 clusters.\nSo, I look at the two clusters that were broken up as choose, how to break them down further.\nSo, and that is called the divisive approach you also have the other approach, which is\nagglomerative and definitely the more popular approach, which is bottom up, where each data\npoint becomes a cluster, so you have all these. So, you have the number of actual cluster\nyou have is equal to the number of the data points, because of each data point is a cluster.\nAnd then, after that you choose the two closest clusters and merge them together and we will\ntalk about how closest is defined. So, you choose the two closest clusters and merge them together. So, the end of the first step you are essentially having we had n data points\nyou having n minus 1 data points, so the end of one step and after that the next step you\nhave the n minus 2. Because, now you have n minus 1 cluster where all of them expect\none have one data point and one clusters has two data points. But, you are just treating them as clusters and you are saying, now order this n - 1,\nwhich are the two closest clusters that I can merge and that is seen as bottom of approach.\nNow, because of how we do this essentially you are going to have nested clusters. Think\nof this divisive approach if you created two clusters. Now, when you go to create three\nclusters in this divisive approach you are going to take either cluster 1 or cluster\n2 and break it further and the others is going to be same, so one break up could be that the cluster 1 stay the same and cluster 2 gets broken up into two pieces.",
                  "answer_start": 170635,
                  "answer_end": 172665,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are hash functions?",
              "id": 535205,
              "answers": [
                {
                  "answer_id": 600249,
                  "document_id": 1139572,
                  "question_id": 535205,
                  "text": " hash functions, they are functions that take a key as an input and\nthen the hash it into one of n buckets.",
                  "answer_start": 124641,
                  "answer_end": 124748,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is full factor design?",
              "id": 535961,
              "answers": [
                {
                  "answer_id": 606540,
                  "document_id": 1139572,
                  "question_id": 535961,
                  "text": "the full factorial design, where it is essentially\na complete enumeration of a discrete input space, where even if you have continuous input\nvariables you break them into 1, 2 or may be sometimes 3 discrete points.",
                  "answer_start": 208779,
                  "answer_end": 208993,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "orthogonal arrays ?",
              "id": 535984,
              "answers": [
                {
                  "answer_id": 606564,
                  "document_id": 1139572,
                  "question_id": 535984,
                  "text": "orthogonal arrays and a specific type of an orthogonal arrays\ncalled the full factorial which is what I show here. So, take a system where variable A can take on two states.\nSo, let us go back to this casting problem and let us say you are interested in variable A which is let us say the temperature of the molten metal that is poured in and so the\ntemperature could be something like 250 Fahrenheit and you might be interested in studying the\neffects at 350 Fahrenheit. Now, I am not an expert in this I do not know those are reasonable temperatures for molten\nmetal I am guessing it is a little too low actually, but who knows. Now, typically what you do is when you have just these two settings, you kind of lay code\nthem and you called them 250 as minus 1 and 350 as plus 1. Now, you do the same thing with the second variable input variable of interest, now variable\nB could be something like pressure, where you have lope or let us say the time that\nyou weight before you remove the cast. So, that could be 1 hour or 2 hours again I do not know those are reasonable numbers,\nthey to illustrate a point is to 1 hour again you call it minus 1 and 2 hour you call it\nplus 1. So, what you go about doing in a full factorial is you try every combination of every variable\nwith every other variable.",
                  "answer_start": 203786,
                  "answer_end": 205086,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is a categorical variable ?",
              "id": 535230,
              "answers": [
                {
                  "answer_id": 600390,
                  "document_id": 1139572,
                  "question_id": 535230,
                  "text": "in classification you have some input data and you use the historic\ninput data and the specific output and in the case of classification, the output actually\nhas classes, it is a categorical variable.",
                  "answer_start": 132942,
                  "answer_end": 133142,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is Map reduce ?",
              "id": 535184,
              "answers": [
                {
                  "answer_id": 600056,
                  "document_id": 1139572,
                  "question_id": 535184,
                  "text": " one of the most popular paradigm that is used for handling big data computation\nis Map-Reduce.\nSo, this was a programming model that was pioneered by Google which were reusing initially\nproprietary implementation.",
                  "answer_start": 117971,
                  "answer_end": 118185,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is the precursor to further data analysis ?",
              "id": 535231,
              "answers": [
                {
                  "answer_id": 600391,
                  "document_id": 1139572,
                  "question_id": 535231,
                  "text": "the clustering is used is that, it serves as a precursor\nto further data analysis.",
                  "answer_start": 135028,
                  "answer_end": 135110,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "How to evaluate the distance between the point you are interested, in KNN?",
              "id": 535963,
              "answers": [
                {
                  "answer_id": 606542,
                  "document_id": 1139572,
                  "question_id": 535963,
                  "text": " to evaluate the distance between the point you are interested in and every\nother data point and then pick the K closest neighbours.\nNow, that can become computationally very, very, very hard and you need to keep either\nthe memory and you need to do the computation from scratch, a simpler approach could be\nthat you just take the K you do the clustering on the data.\nAnd if you look to see, which cluster center is closest and once you identify cluster center\nthat is closest you now, take your point under question and only evaluate it is distance\nto all the points in that cluster.",
                  "answer_start": 140572,
                  "answer_end": 141156,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Applications or few examples of cluster analysis?",
              "id": 535232,
              "answers": [
                {
                  "answer_id": 600392,
                  "document_id": 1139572,
                  "question_id": 535232,
                  "text": " evaluate the distance between the point you are interested in and every\nother data point and then pick the K closest neighbours.",
                  "answer_start": 140575,
                  "answer_end": 140704,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is vector quantization?",
              "id": 535964,
              "answers": [
                {
                  "answer_id": 606543,
                  "document_id": 1139572,
                  "question_id": 535964,
                  "text": " to evaluate the distance between the point you are interested in and every\nother data point and then pick the K closest neighbours.\nNow, that can become computationally very, very, very hard and you need to keep either\nthe memory and you need to do the computation from scratch, a simpler approach could be\nthat you just take the K you do the clustering on the data.\nAnd if you look to see, which cluster center is closest and once you identify cluster center\nthat is closest you now, take your point under question and only evaluate it is distance\nto all the points in that cluster.\nAnd the assumption here is that the points the points under that cluster are the once\nthat are going to be closest to this point, because this cluster center was the closest\nfor instance.\nIt is also used in certain other forms of you know compression of data specifically\nsomething called vector quantization, which is used a lot in image or sound or video data,\nwhere you typically find that in a particular image, if you break it up into pixels there\nis the whole host of data points that will look very, very similar to each other.",
                  "answer_start": 140572,
                  "answer_end": 141691,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "When did Map-Reduce became very popular ?",
              "id": 535186,
              "answers": [
                {
                  "answer_id": 600092,
                  "document_id": 1139572,
                  "question_id": 535186,
                  "text": "a Hadoop an open source platform, that implements Map-Reduce became very popular.",
                  "answer_start": 118199,
                  "answer_end": 118280,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is Partitional clustering ?",
              "id": 535965,
              "answers": [
                {
                  "answer_id": 606544,
                  "document_id": 1139572,
                  "question_id": 535965,
                  "text": "Let us say you had some cluster A cluster B it is not like you are going to take some\npoints from cluster A and some points from cluster B and call it cluster C. It is in\nthat sense nested it is in the sense that you make one split and very, very similar\nto decision trees you keep making further splits and this is contrasted to an approach\nof partitional clustering.\nPartitional clustering is not of this kind of nested approach it is just that you explicitly\ndecide on the number of clusters in some sense and you go and partition the data.\nits it is It is simply a division of the set into non overlapping set, so it is essentially\nclusters.\nAnd, so in that sense there is no tree diagram or anything of that nature with this.\nSo, for instance a partitional cluster that if I decide to do the partitional cluster\nand create four clusters and I contrast that to a partitional clustering approach, where\nI had three clusters.",
                  "answer_start": 143983,
                  "answer_end": 144910,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Types of clustering?",
              "id": 535967,
              "answers": [
                {
                  "answer_id": 606547,
                  "document_id": 1139572,
                  "question_id": 535967,
                  "text": "exclusive verses overlapping\nversus fuzzy.",
                  "answer_start": 145682,
                  "answer_end": 145724,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What do you meant by complete enumeration?",
              "id": 535945,
              "answers": [
                {
                  "answer_id": 606524,
                  "document_id": 1139572,
                  "question_id": 535945,
                  "text": "complete enumeration, which means that every variable is set to\nevery possible value it can be set to with relation to every other variable being set\nto all their possibility.",
                  "answer_start": 209019,
                  "answer_end": 209194,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is Exclusive clustering?",
              "id": 535968,
              "answers": [
                {
                  "answer_id": 606548,
                  "document_id": 1139572,
                  "question_id": 535968,
                  "text": " exclusive clustering each object is assign to a single cluster\nand that object therefore, cannot be assigned to another cluster.",
                  "answer_start": 145746,
                  "answer_end": 145875,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Explain full factorial design?",
              "id": 535946,
              "answers": [
                {
                  "answer_id": 606525,
                  "document_id": 1139572,
                  "question_id": 535946,
                  "text": "full factorial design associated\nwith three input variables A, B and C. And the idea here is that what we have is a full\nfactorial design, because A can take on three values minus 1, 0, plus 1 and B can take on\ntwo values and C can take on two values just minus 1 plus 1 and that gives you 12 combinations\ntotally and we have taken, the output variable is Y.\nNow, I call them Y 1, Y 2, Y 3, Y 4, only because they are Y 1, Y 2, Y 3 and Y 4 are\nreplicates, so it is a same core variable.\nBut, essentially you conduct the experiment at, the settings for instance minus 1, minus\n1, minus 1.\nYou conduct that experiment 4 times; again it can be a parallel effort or a sequential\neffort either way.",
                  "answer_start": 209631,
                  "answer_end": 210324,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is overlapping clustering?",
              "id": 535969,
              "answers": [
                {
                  "answer_id": 606549,
                  "document_id": 1139572,
                  "question_id": 535969,
                  "text": " in overlapping, in case in clustering algorithm that allow for over\nlapping, you can.\nBasically it is not exclusive a data point can choose to belong to more than one cluster\nat a given point and decision on which, of these are really depends on the underlying\nsystem.",
                  "answer_start": 145980,
                  "answer_end": 146249,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is classical analysis?",
              "id": 535948,
              "answers": [
                {
                  "answer_id": 606527,
                  "document_id": 1139572,
                  "question_id": 535948,
                  "text": "The idea behind classical analysis is to take each variable individually, each input variable\nindividually and ask the question, at what setting am I getting the best results.",
                  "answer_start": 211741,
                  "answer_end": 211916,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is a complete clustering ?",
              "id": 535971,
              "answers": [
                {
                  "answer_id": 606551,
                  "document_id": 1139572,
                  "question_id": 535971,
                  "text": "A complete clustering basically just assigns every objects to a cluster, where as a partial\nclustering does not it starts with the data points chooses to you know cluster as many\npoints to different clusters and data points that do not really help in terms of belonging,\nso clearly to given cluster just not cluster they are just left out.\nAnd these might and the and the motivation there is that you are more interested not\nin kind of pigeon holing each data point into a cluster, but you are more interested in\nthe cluster formation itself.\nAnd there you do not want data points are not, so clusterable, that do not really belongs,\nso clearly into 1 of 2 clusters to kind of ruin the cluster center or ruin that nice\ndivision that you created.",
                  "answer_start": 147408,
                  "answer_end": 148153,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is Hadoop is a distribution frame work ? and its applications?",
              "id": 535194,
              "answers": [
                {
                  "answer_id": 600192,
                  "document_id": 1139572,
                  "question_id": 535194,
                  "text": " Hadoop is a distribution frame work, that fundamentally uses Map-Reduce as that computing\nparadigm, but on top of it takes care of data distribution, communication, and fault tolerance;\nand makes it very convenient to use this kind of a distributed; set up and solving everyday\nproblems.\nAnd hence Hadoop is being wildly deployed commercially, and variants of that are continuing\nto be very popular even today.",
                  "answer_start": 118845,
                  "answer_end": 119256,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is the two major difficulties with design of\nexperiments?",
              "id": 535950,
              "answers": [
                {
                  "answer_id": 606529,
                  "document_id": 1139572,
                  "question_id": 535950,
                  "text": "The two major difficulties are the following.\nOne is that, you could have some interactive effects between the input variables, which\nmeans the effect that n input variable will have on the output variable really depends\non how some other input variable is set.\nSo, that is called an interactive effect.",
                  "answer_start": 214788,
                  "answer_end": 215091,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Êcliques ?",
              "id": 535973,
              "answers": [
                {
                  "answer_id": 606553,
                  "document_id": 1139572,
                  "question_id": 535973,
                  "text": "Êcliques, which essentially just means that at the set\nof nodes in the graph of a completely connected to each other.\nAnd, so a lot of that that kind of clustering ideas that go in to graph based clustering\nare once that kind of look out for cliques and say these guys are all connected to each\nother, so they must be a cluster.",
                  "answer_start": 151307,
                  "answer_end": 151635,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are multi core massively data parallel computations ?",
              "id": 535210,
              "answers": [
                {
                  "answer_id": 600283,
                  "document_id": 1139572,
                  "question_id": 535210,
                  "text": "Map-Reduce variant with local memory and then there are\nGP-GPUs, which are multi core massively data parallel computations.",
                  "answer_start": 128583,
                  "answer_end": 128706,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What are the different types of clustering?",
              "id": 535943,
              "answers": [
                {
                  "answer_id": 606522,
                  "document_id": 1139572,
                  "question_id": 535943,
                  "text": "full factorial design",
                  "answer_start": 208783,
                  "answer_end": 208804,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is Clustering Analysis ?",
              "id": 535214,
              "answers": [
                {
                  "answer_id": 600297,
                  "document_id": 1139572,
                  "question_id": 535214,
                  "text": "Clustering is the idea of dividing data into groups and we do that, because sometimes there\nis inherent meaning to doing such an activity.\nAnd in other cases, it serves as the first step, it is fairly useful to do this.",
                  "answer_start": 130785,
                  "answer_end": 131004,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What do you meant by conducting the experiment 4 times in full factorial design?",
              "id": 535947,
              "answers": [
                {
                  "answer_id": 606526,
                  "document_id": 1139572,
                  "question_id": 535947,
                  "text": "What we mean by that is, when we say we conduct the experiment 4 times, you might have one\nexperimental unit and you separately conduct the experiment 4 times on it or you could\nhave 4 experimental units and you might, you choose to parallely try the same setting of\nminus 1, minus 1, minus 1 on those four different experimental units.",
                  "answer_start": 210325,
                  "answer_end": 210661,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "What is fuzzy clustering ?",
              "id": 535970,
              "answers": [
                {
                  "answer_id": 606550,
                  "document_id": 1139572,
                  "question_id": 535970,
                  "text": " clustering fuzzy clustering kind\nof takes this over lapping even further, where each data point is not really assigned to\na cluster, but it basically gets a number between 0 to 1 that that talks about the weight\nassociated with that data point belonging to the different clusters.\nSo, for a data point each data point gets total weight of 1 and it takes that data that\nweight of one and says, I am going to assign 0.3 in belonging to cluster A, I am going\nto assign 0.7 in belong to cluster B and I am going to assign myself 0 belonging to cluster\nC. So, the constrained is that the sum of its weights in terms of belonging to the different\nclusters adds up to 1, but it can use its weight of one in any way chooses to belong\nto the different clusters.",
                  "answer_start": 146473,
                  "answer_end": 147226,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "Where would you want to use classical analysis and where would you want to use this?",
              "id": 535949,
              "answers": [
                {
                  "answer_id": 606528,
                  "document_id": 1139572,
                  "question_id": 535949,
                  "text": "where would you want to use classical analysis and where would you want to use take\nthe best.\nAnd to answer that question we need to answer that question in terms of, if you want to\ntake a one factor at a time approach, what are two reasons that experiments that particular\napproach fails or for that matter,",
                  "answer_start": 214416,
                  "answer_end": 214724,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            },
            {
              "question": "center based clusters ?",
              "id": 535972,
              "answers": [
                {
                  "answer_id": 606552,
                  "document_id": 1139572,
                  "question_id": 535972,
                  "text": "the cluster\nand when I describe it I can try to think of it as there is a cluster and there is there\nis the main representative of the cluster, the prototype the one that kind of signifies\nthat what the cluster is in the center of that cluster.\nSo, in the prototype based approaches, because its representative is in some sense so central.\nIt is also sometimes called center based clusters, but the idea is as following, where each data\npoint is looked at more from the prospective of how close this data point is to the cluster\ncenter to the prototype.\nAnd, how far this data point is from the prototypes of the other clusters or the other representatives\nand the classification is done in this fashion.",
                  "answer_start": 149650,
                  "answer_end": 150354,
                  "answer_category": null
                }
              ],
              "is_impossible": false
            }
          ],
          "context": "\nSUPPORT VECTOR (CONTINUE)\nHello and welcome back to our discussion on Support Vector Machines.\nSo, we were looking at the optimization problem corresponding to the optimal separating hyper\nplane. So, to solve this problem, so with one of the techniques for solving these kinds\nof constrained optimization problems is to set up a Lagrangian, which essentially looks\nat the original objective function, which is half beta square and the second component\ncorresponding to the constraints that we have. So, if you look at this quantity in the square\nbrackets here, so you can see that this is the term on the left hand side of the inequality\nand that is the term on the right hand side of the inequality and we really want to make\nsure that, this difference is not negative. If this difference is negative, then that\nwould mean that y i times x i transpose beta plus beta naught is actually less than 1.\nSo, we do not want this to be negative, so what we essentially say is, this we added\nhere as with a minus sign. So, this essentially means that, when I minimize\nthis whole expression, so this term will become as large as possible, as largely positive\nas possible. So, that essentially means that I will go and try and make this as larger\nthan 1 as possible. So, this term here alpha i let us me control how much weight I want\nto give to satisfying the constraints versus how much I really want to minimize the objective\nfunction. So, we really need to satisfy the constraints as much as possible and since,\nthere are solutions that will satisfy the constraint and give you a good optima.\nSo, we should essentially be trying to derive this thing to as larger value as possible.\nSo, this is called the primal of the problem and your goal is to minimize the primal. So,\nI am going to do something fairly technical right now. So, if you do not understand all\nof it in the first goal that is fine, you might have to do a little bit more reading\non this side, but this is essentially give you an idea of how people go about solving\nthese kinds of problems. So, we are going to try and create, what is\ncalled the dual of this, the primal objective function. So, the dual is a way to create\nsomething that create an optimization problem, that is simpler to solve in some sense than\nthe primal and the dual at all points provides you some kind of a lower bound on the kind\nof solutions that you can achieve with the primal and that the optima of the dual you\nideally like the optima of the primal also to be achieved.\nSo, we are going to create a problem called the dual, we are going to solve the dual and\nwhen we reach the optima of the dual, you would like the optima of the primal to be\nalso achieved. The same solution that gives you the optima in the dual problem should\ngive you the optima and the primal problem and there are technical conditions under which\nthis is satisfied and we are not going to go in to the technical conditions and this\ngoing to be give you a flavor of kind of results that will be looking at.\nSo, let us start by\nsetting the derivative of L p to 0, derivative with respect to beta and beta naught. So,\ntaking the derivative with respect to beta and setting it to 0 and solving for beta gives\nme… So, you can figure there out by little bit of algebra here and likewise setting that\nderivative with respect to beta naught to 0 and solving it gives me. So, you can substitute\nthese back into the primal problem and do a lot of algebra, do a lot of algebra really\nand then I can simplify this and I will get what is known as the dual, we write the dual\nhere. So, this is just really obtained by substituting your beta into the expressions\nhere and then, using the fact that alpha i y i will be 0 at the optimum.\nSo, that is the thing, but then it is subject to be constrained. So, note that I said, so\nyour dual is always going to give you a lower bound on the solution of the primal problem.\nSo, really if you are minimizing the solution in your primal, it should be maximizing the\nsolution in the dual, so that the two of them can coincide at some point. So, essentially\nyou would be maximizing this subject to the constraint that, all your alpha i’s are\ngreater than or equal to 0.\nSo, if you think about it, this is the much easier constraint to wrap our heads around,\nbecause it just says that you will only be doing it in the positive co ordinates and\nwhile this had a more complex set of constraints. So, you kind of reduce the constraint to do\nsomething easier and therefore, the dual problem is sometimes easier to solve. So, for the\ndual and their primal to be at optima at the same time, so you really want them to satisfy\na set of conditions, which are essentially to with the derivative of the primal problem.\nSo, we required that this should hold, we required that this should hold, we write them\nas 1, 2, 3. In addition, you required that this condition should also be required, that\nthis condition should also be satisfied, these are called the KKT or the Karush Kuhn Tucker\nconditions. And so far, the optimization problem to have the same solution, we require that\nthe KKT conditions should be satisfied. So, once you have an optimal solution for\nthe dual and the primal problem, because these KKT conditions have to be satisfied, you can\nmake certain observations, especially we are working from condition 3 here. So, if alpha\ni is greater than 0, so what does it mean. So, this has\nto be equal to 0, then the term in the square bracket has to be equal to 0; that means,\ny i into x i transpose beta plus beta naught should be 1. So, what does that mean?\nIt means that, it is exactly on the edge of the margin when it is equal to 1, because\nit is greater than equal to 1 is what we needed to satisfy, so when it is equal to 1; that\nmeans, it is exactly on the margin. Likewise if so, if the quantity in the square bracket\nis greater than 1, then alpha i has to be 0, but that essentially\nmeans is, if your data point is something; that is far away from the hyper plane let\nus just more than the margin away from the hyper plane, then the corresponding alpha\nis will become 0. So, what does this mean for us, so if you\nthink about it. So, the solution that we get, which is essentially beta that is the solution\nthat we want to get is formed by taking the product of alpha i, y i and x i. So, if saying\nthe alpha i is going to be 0, it essentially means that the corresponding x i has no role\nto play in determining, what my beta should be if I say that it implies if x i is 0 that\nimplies that x i has\nno role in computing beta. So, which are the data points, which will\nactually effect the solution beta here exactly those points for, which y i times x i transpose\nbeta plus beta naught is 1; that means, these are exactly the points, which lie on the margin\n. So, only these points will influence, how the solution beta looks like and all the other\ndata points that we have, which are farther away from the separately high per plane, then\nthese points do not matter in the solution. So, these points are called\nsupport vectors. So, you don’t really have to solve this\noptimization problem yourself there are enough tools that actually can do it for you the\nwhole goal of this lecture is to get you to appreciate, what is said that you are doing\nwhen you are using a support vector machine for solving a problem. So, at the end of the\nday all we are going to do is fire up tool that is going to tell you, what is the separating\nhyper plane given a bunch of data, But, it is good to have an appreciation of how the\nclassifier is actually build. So, once you figure out the beta, then I can\nsubstitute that I can substitute that into the KKT the third condition here and solve\nfor beta naught. So, typically what you do is that you use every x i that is a support\nvector and you substitute that here and then, try to solve for beta naught and typically\nend of taking the average value of that. So, the couple of things, which I want to point\nout about support vector machines. So, one thing is we should be very clear that\nthe training data none of the training data will fall within the margin, but that it is\nnot to say that the test data might fall might not fall within the margin the test data might\nfall within the margin it might actually fall on the other side of the hyper plane. So,\nfor all we know that the test data that could be errors on the test data it is just on the\ntraining data it tries to fix something there is as far away as possible from the data points.\nSo, the idea here is that, so if I give as much gap between the classes as possible,\nthen the classifier would be more noise on either side. So, this is the assuming that\nthe noise could be in this class or in this class if you know for sure that one class\nis noisier than the other or if one class is more valuable than the other. So, you might\nwant to actually modify your objective, so that the line does not go write in the middle,\nbut it is goes to one side or the other. So, having said that under the assumptions\nof the support vector machines if assumptions hold good, then is a very, very robust classifier.\nSo, the reason is it pays attention only to the points that are closest to the class boundary.\nSo, you know I can have as many data points here I say want I can have as many data points\nhere I say want of the corresponding class it will be does not affect my classification,\nbecause truly the once that are close the boundary are the once that need attention.\nSo, that essentially makes support vector machines more robust and on the other hand\nif you are going to have some kind of stochastic process that is generating the data right.\nSo, if there are the few data points there are by chance or noise data points that actually\nclose to the hyper plane that will affect the support vector machines tremendously.\nAnd therefore, it will try to reduce the margin by a large extent while classifier that looks\nat the entire data and tries to find the distribution for the entire data might be a little bit\nmore robust to this kinds of noise. So, this is the, this is how you solve the basic optimization\nproblem for support vector machines.\n \n \n \nSupport Vector Machines for Non Linearly Separable Data\nNow, we look at the case where the data is not so-well behaved as you wanted to be. Specifically\nthe data is not separable, right, is not linearly separable. So, you look at the non-separable\ncase. So, I am going to introduce some additional data points whatever have looking at so far.\nSo, this is a non-separable case, right, linearly non-separable case because some of my data\npoints are really mixed up here, right. Now, still we will like to have large margin, but\nnot only have I allowing data that is not separable, but I am also allowing data points\nto fall within the margin. So, its essentially two sides of the same coin, right. So, if\nI am allowing data to fall within the margin, so in some sense I am having the flexibility\nto make some kind of errors as well.\nSo, I essentially look at how far away am I from the margin, right in the long direction.\nSo, I am going to denote these distances by which I am away from the margin by the symbol\nzeta, right, but we still have same constraints here, but now going back to our optimization\nproblem, but I am really looking at here is, I am going to modify my constraints such that,\nso y i times x i times beta plus beta naught, right is really greater than or equal to 1\nminus zeta i, but zeta i is some kind of slack variable that allows me to satisfy this constraint\nwith some error in it. So, essentially if you look at this first data point that we\ndrew here, if you look at the first data point I drew here, right. So, it has the slack of\nzeta 1, right and the second data point as a slack of zeta 2, right and this one of the\nreally fairly large slack, but I still it is possible under the circumstances. This\nallows me to have a larger margin, right. So, if you think about it if I did not, even\nif this data point was not there, if I did not allow these kinds of data point appear\nin the margin, right. If I dint allow these things to appear in the margin, my classifier\nwould actually have been here, right. My classifier would have been here trying to separate this\nx n from this o, right and margin would have been very small. In all likelihood I am fitting\na noise data point here which is not an optimal thing to do, right.\nSo, now I am allowing some amount of data points to fall within the margin, right. I\nam able to expand the size of the margin that I can have and I can also incorporate linearly\nthe small number of errors that I have to make because the data is not linearly separable,\nright, but then I donÕt want to this become arbitrarily large, right. I just cant say\nthat hey it doesnÕt matter you can have slack variable for every data point that we have\nand the slack variable can be as large as you wanted. I need to have a control on this\nas well and therefore I try to minimize that, right. So, are these conditions sufficient\nfor us to define a new problem now, but still one more condition that we need. So, we are\nreally measuring zetas in one direction and not in the other direction, right. So, we\nhave to be careful. So, I also need to add another set of constraints, let us say that\nthe zeta have to be positive. So, that is the complete constraint optimization problem\nfor us in the case where the data is not linearly separable.\nSo, you have to minimize beta square, norm beta square plus the sum of the zetas that\nwe are using subject to the condition that y i x i times for beta plus beta not is greater\nthan 1 minus zeta i and zeta is greater than or equal to 0. So, what happens to our primal\nobjective function now? So, what is that we need a component that corresponds to the actual\nobjective function, and you need a component that corresponds to the constraints that we\nare using, right. So, the first part of it remains as it is, right, but then we have\nto add the newer components that we are bringing in, right. So, this is the second part of\nthe objective function and this is the first constraint, right and this is the second constraint\nand since these has to be applied to each and every data point, so you have the summation\nover all the data point you have, likewise here.\nSo, how do we go about deriving the dual in this case? So, just like we did earlier, so\nstart differentiating the primal with respective of various parameters. So, you end up with\nthe same condition that we had earlier, and here you end up with the same condition when\nyou differentiate with respect to the zetas, you end up with so, one condition for each\ni, right. So, alpha i equal to c minus mu i. So, I can put everything back into the\nprimal, do some algebra and derive my dual which turns out look exactly like this, except\nthat my alpha has to lie between 0 and c that you can actually see from that condition that\nwe have there, right. alpha i y i have to be equal to 0. That we already have as a condition.\nSo, the remainder of the KKT condition that we have, I am just going to go through this\nvery quickly because I donÕt want to do the complete derivation here.\nSo, the remainder of the KKT conditions will be, this is what we had last time except for\nthe zeta i part, right, but you also have. It is essentially or initial constraint written\nin a slightly different form, right so the third constraint here which is the original\nconstraint with 1 minus zeta taking to the other side. So, what is that we notice from\nhere? So, let us go back and let us do the same argument. If alpha i is greater than\n0, then y i x i transpose beta, beta naught is actually less than 1 and only, then this\nwill be zero because right. So, that would mean that for the particular choice of zeta\ni. So, this goes to 0. That would mean that this is on the wrong side of the margin, right\nor on the margin, right. If the zeta i is 0, then it will be on the margin and if the\nzeta i is not 0, it will be within the margin. So, all the data points that are on one side\nof the margin or all, right because again like last time, so beta depends only on those\nvector for which alpha i is greater than 0, right.\nSo, if the data happens to be on the right side of the margin, right then your alpha\ni have to be 0 as we saw earlier. So, those data points have no role in computing. So,\nthis is essentially kind of takes us over the entire optimal separating hyper plane\npart, where it was linearly separable. So, we had a very easy solution, but when the\ndata is not linearly separable, so we have to allow for the possibility of data points\nto lie on the wrong side of the margin. So, when we do that, we can essentially take advantage\nof the fact and try to push our margin away by allowing data point which are correctly\nclassified, but are within the margin is small fraction of such data points are permitted\nand therefore, we need to expand the margin a little bit. Therefore, we can come up with\nthe solution. So, you donÕt have to, at this point here\ndonÕt have to go in details of solving this optimization problem, but there are many powerful\noptimization techniques it have been developed that allow you to solve these kinds of problems.\nIn fact, SVM have let to the revival of popular class of optimization algorithms called interior\npoint methods because these are very efficient in solving these kinds of optimization problems.\nThey are pretty robust classifiers and are very widely used for wide variety of applications,\nbut some of you will be probably thinking right now, but whenever I talk about a support\nvector machines, people always tell me something about kernels, right. I thought to support\nthat machine all about kernels and I have not talked about kernels at all at any point\nhere, yes. So, the kernel idea is very crucial in support\nvector machines, and I will be looking at that in more detail in the next module. What\nyou should remember is the basic optimization problem that you are trying to solve with\nsupport vector machine is the one of optimal separating hyper plane. So, in fact the kernel\nidea is called the kernel trick because it allows you to solve really wide variety of\nproblems which is not easily amenable to linear classification by using a very powerful idea,\nbut then the under lying optimization problem that you are solving is still this, the same\noptimization problem that on the boards so far in the last two modules.\n \n\nSupport Vector Machines and Kernel Transformations\nSo far, we have been looking at the problem of the optimal separating hyper-plane in the\nprevious two modules, but then the idea of Support Vector Machines is to be able to use\nit in data which is not nearly linearly separable or only working in that space of linear hyper-planes,\nright.\nOne way of looking at extending this problem to complex settings is to think of taking\nyour original data, right and transforming it into something else, and then trying to\napply the same idea to the transformed function. This idea should not be new to you because\nyou have always seen this in linear regression where you can look at transforming the input\nvariables into some other kind of basis function, and then trying to do linear regression on\nthat. So, the idea is similar to that, but we are going to make use of a very powerful\ntechnique here, right. So, look at how predictor is going to work. So, we are going to have\nf of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is\nlesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will\npredict it as being of class plus 1, right. As on this is, so this is separating hyper-plane\nthat we have, fine. So, if you think about it, we said we have solution betas, right\nor going to be of this form. Therefore, I can rewrite this, right. So, if you look at\nit, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner\nproduct of xi and similarly, if you look at how you are solving the optimization problem,\nthis is what we wrote down last time. So, the duel is essentially going to have again\nx i transpose x k, right. So, you can see that the xÕs appears in our problem always\nas inner products, right and if somehow you are able to compute this inner products efficiently,\nthen you should be able to solve the problem more efficiently. In particular, given that\nyou are going to be looking at this kind of transformation, right. So, I can now write\nthis as , where this denotes the inner products. This is what we can say inner product, right.\nSo, likewise the dual also can be written as\nSo, if you stop and think about it, it essentially tells you that I really donÕt need to know\nh of x, right. If you have an efficient way of computing the inner product of the transformed\nfunction, right, then you really donÕt need to know what the actual transformation itself\nis, right. So, there is a class of function which I am going to call as kernel functions\nhere. It is called as kernel function which allows you to compute this inner product efficiently.\nSo, essentially it is going to say that right. So, the kernel corresponding to function h,\nright when you give it as input x and x square, which is going to compute the inner product\nof h of x and h of x square, right.\nSo, one of the things that we require for a function to be a kernel function, right\nis there it should be symmetric. So, k should be symmetric, positive-definite. So, if we\ndo not really understand that, so k should be symmetric in the sense that if I give it\nthe set of x and x prime x prime, so the kernel functions for x and x prime should be the\nsame as x prime, x. The positive-definite essentially means that if I take any vector\nx transpose K x, that should always be positive. So, there are technical reasons for why this\ncondition should be satisfied. For one rough way to think about it to say that this essentially\nyou would want this to be whole thing to be positive, so that your optimization problem\nwill work as you wanted to. So, that is rough intuition behind why you\nneed this condition. Some of the popular choices for the kernel functions are the polynomial\nkernel. This is essentially 1 plus. I got the parameter d is something that we choose.\nThe other one is Gaussian or the radial basis function. The other one is sometimes called\nthe neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,\nright? So, as I was mentioning earlier, you have a data there is given to in the original\ndimensions, right. The data might be badly mixed up in that original dimension, it might\nnot be easily separable at all in the original dimension, but then when you look at the transformed\ndimension, then the data becomes linearly separable, right.\nLet us take the example of the polynomial kernel and see what happens. So, I am going\nto look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I\nam assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these\nare like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.\nSo, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality\nof the underlined space, but in this case I am assuming this has. So, if we take the\nsquare of this, so I essentially end up with the expression that has six components to\nit, right. So, if you think about it, this is somewhat\nlike taking the inner product in a very higher dimensional space than the original space.\nOriginally x and x prime were residing in a two-dimensional space, but if you look at\nwhat is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,\nh2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;\nh4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine\nthat I transformed my original two-dimensional representation into a six-dimensional representation\nis essentially taking all the second order terms along with original terms.\nNow, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with\nthis expression, right. Inner product of h of x and h of x prime, right. So, the entire\nsix-dimensional vector if I take the inner product, we are basically going to end up\nwith this expression. So, what we have done here, we took the inner product in two-dimensional\nspace, right and performed the squaring operation. So, this is going to give me number which\nis equal to the number I will get by first transforming the data point from two-dimensions\nis six-dimension and then, taking the inner product in the six-dimensional space. So,\nessentially this allow to work in much higher dimensional space than originally intended,\nbut by only looking at inner product computation in the original space, right.\nSo, why this is a useful property to have in the case of support vector machine, is\nthat all over operation here operate only within inner product whether we are finally\ntrying to predict the output f of x, or when you are trying to solve the dual problem ld.\nSo, all we really need to do is know what the inner product is, right. Now, I am able\nto take the inner product in a higher dimensional space, but then do the computational only\nin the lower dimensional space. This allows us to have a much greater advantage than simply\noperating with the original dimension.\nSo, to see how this polynomial transmission really helps us, let us look at a very simple\nexample, right. I am going to assume that the single dimension, right and then I have\ndata points, let us assume this is 0. I have data point that look like this, right. So,\nthis is the dimension x1. So, obviously there is no single line that I can draw to separate\nthese into two classes neatly, right. So, I can assume my slack variables and try to\ndraw the line here that says ok I am not making too many errors bla bla, so on and so forth,\nbut still that is not the satisfactory solution, but let us looks at what happens if I try\nto plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square\nas my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that\nwill be x1 that is corresponding to this here and then looking at the square of that, but\nthen looking at these data points, so this is going to go here, this will probably go\nhere, right. Now, it is very clear that I can draw straight\nline, right. I can draw if can find the linear decision boundary that separates these two\nclasses once I have done the appropriate transformation. So, this essentially allows us to solve a\nlarger class of problems. You see the kinds of basis transformation, then we could do\njust by operating in the original space and trying to solve the linear optimal hyper-plane\nproblem. So, this is essentially what all your choices of kernel functions or all about,\nright. So, if you look at any SVM tool, they will tell you to pick one kernel function\nwhich is either the polynomial kernel. So, in which case you have to pick in the appropriate\nd or you have to pick radial basis function or Gaussian kernel, in which case you have\nto pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or\nwe can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa\n2 which are the parameters that define how quickly this sigmoid function rises. We will\nsee more about that when you look at neural networks.\nSo apart from this, you still have one further parameter that we will have to worry about\nwhich is this constant c, right. So, this tells you how much slack that you are willing\nto tolerate, right. So, if you think about it, if c is very large, if c is infinite,\nthen we have to be in the completely separable case because even a very small value of zeta\nwill cost this objective function to become very large, right. Even for a small value\nof zeta, right so you will find that this thing is actually very bad, right. c has to\nbe if c is very large, then zeta has to be very small, right and if c is small, then\nzeta can be large. So, what this is going to tell you is that in the higher dimensional\nspace, where you are assuming that the higher the dimensionality that you are projecting\ninto, more likely is the data will be separated, right because if c is large, right because\nyou are going to try in fit more complex surface, right. c will look, the surface will look\nvery vigil, right and if c is small, then you will really get much smoother surface,\nbut the possibility of you making errors is also higher. So, thatÕs trade of that you\nhave to figure out empirically by looking at how you are doing the, how you are performing\nin the actual data. This brings us to the end of the module on\nSupport Vector Machines and these are very powerful classifiers and quite often they\nare the first classifiers of choice for people when they are trying to solve new problem\nwhich they really donÕt know much. So, one thing I should point out that people have\ncome up with different kinds of kernel functions. So, I have given you three choices of kernels\nhere. These are essentially the most popularly used kernel choices, especially if you are\noperating with text data, you would like to use linear kernel d is 1 and in most other\nforms, you will be looking at RBF kernels, but then for special forms of data like graphs\nand strings and so on and so forth, people have defined their own kernels and as long\nas they are satisfying your properties of the kernel function, you can define your own\nkernels and then have them help you solve the problem, right. That is topics for another\nday or perhaps for another course. So, we will stop here. Just let me reframe that SVM\nare one of the most popular and powerful classifier that are currently being used widely.\n\n \n \n \n \nEnsemble Methods and Random Forests\n \n \nSo far, we have been looking at the problem of the optimal separating hyper-plane in the\nprevious two modules, but then the idea of Support Vector Machines is to be able to use\nit in data which is not nearly linearly separable or only working in that space of linear hyper-planes,\nright.\nOne way of looking at extending this problem to complex settings is to think of taking\nyour original data, right and transforming it into something else, and then trying to\napply the same idea to the transformed function. This idea should not be new to you because\nyou have always seen this in linear regression where you can look at transforming the input\nvariables into some other kind of basis function, and then trying to do linear regression on\nthat. So, the idea is similar to that, but we are going to make use of a very powerful\ntechnique here, right. So, look at how predictor is going to work. So, we are going to have\nf of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is\nlesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will\npredict it as being of class plus 1, right. As on this is, so this is separating hyper-plane\nthat we have, fine. So, if you think about it, we said we have solution betas, right\nor going to be of this form. Therefore, I can rewrite this, right. So, if you look at\nit, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner\nproduct of xi and similarly, if you look at how you are solving the optimization problem,\nthis is what we wrote down last time. So, the duel is essentially going to have again\nx i transpose x k, right. So, you can see that the xÕs appears in our problem always\nas inner products, right and if somehow you are able to compute this inner products efficiently,\nthen you should be able to solve the problem more efficiently. In particular, given that\nyou are going to be looking at this kind of transformation, right. So, I can now write\nthis as , where this denotes the inner products. This is what we can say inner product, right.\nSo, likewise the dual also can be written as\nSo, if you stop and think about it, it essentially tells you that I really donÕt need to know\nh of x, right. If you have an efficient way of computing the inner product of the transformed\nfunction, right, then you really donÕt need to know what the actual transformation itself\nis, right. So, there is a class of function which I am going to call as kernel functions\nhere. It is called as kernel function which allows you to compute this inner product efficiently.\nSo, essentially it is going to say that right. So, the kernel corresponding to function h,\nright when you give it as input x and x square, which is going to compute the inner product\nof h of x and h of x square, right.\nSo, one of the things that we require for a function to be a kernel function, right\nis there it should be symmetric. So, k should be symmetric, positive-definite. So, if we\ndo not really understand that, so k should be symmetric in the sense that if I give it\nthe set of x and x prime x prime, so the kernel functions for x and x prime should be the\nsame as x prime, x. The positive-definite essentially means that if I take any vector\nx transpose K x, that should always be positive. So, there are technical reasons for why this\ncondition should be satisfied. For one rough way to think about it to say that this essentially\nyou would want this to be whole thing to be positive, so that your optimization problem\nwill work as you wanted to. So, that is rough intuition behind why you\nneed this condition. Some of the popular choices for the kernel functions are the polynomial\nkernel. This is essentially 1 plus. I got the parameter d is something that we choose.\nThe other one is Gaussian or the radial basis function. The other one is sometimes called\nthe neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,\nright? So, as I was mentioning earlier, you have a data there is given to in the original\ndimensions, right. The data might be badly mixed up in that original dimension, it might\nnot be easily separable at all in the original dimension, but then when you look at the transformed\ndimension, then the data becomes linearly separable, right.\nLet us take the example of the polynomial kernel and see what happens. So, I am going\nto look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I\nam assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these\nare like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.\nSo, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality\nof the underlined space, but in this case I am assuming this has. So, if we take the\nsquare of this, so I essentially end up with the expression that has six components to\nit, right. So, if you think about it, this is somewhat\nlike taking the inner product in a very higher dimensional space than the original space.\nOriginally x and x prime were residing in a two-dimensional space, but if you look at\nwhat is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,\nh2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;\nh4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine\nthat I transformed my original two-dimensional representation into a six-dimensional representation\nis essentially taking all the second order terms along with original terms.\nNow, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with\nthis expression, right. Inner product of h of x and h of x prime, right. So, the entire\nsix-dimensional vector if I take the inner product, we are basically going to end up\nwith this expression. So, what we have done here, we took the inner product in two-dimensional\nspace, right and performed the squaring operation. So, this is going to give me number which\nis equal to the number I will get by first transforming the data point from two-dimensions\nis six-dimension and then, taking the inner product in the six-dimensional space. So,\nessentially this allow to work in much higher dimensional space than originally intended,\nbut by only looking at inner product computation in the original space, right.\nSo, why this is a useful property to have in the case of support vector machine, is\nthat all over operation here operate only within inner product whether we are finally\ntrying to predict the output f of x, or when you are trying to solve the dual problem ld.\nSo, all we really need to do is know what the inner product is, right. Now, I am able\nto take the inner product in a higher dimensional space, but then do the computational only\nin the lower dimensional space. This allows us to have a much greater advantage than simply\noperating with the original dimension.\nSo, to see how this polynomial transmission really helps us, let us look at a very simple\nexample, right. I am going to assume that the single dimension, right and then I have\ndata points, let us assume this is 0. I have data point that look like this, right. So,\nthis is the dimension x1. So, obviously there is no single line that I can draw to separate\nthese into two classes neatly, right. So, I can assume my slack variables and try to\ndraw the line here that says ok I am not making too many errors bla bla, so on and so forth,\nbut still that is not the satisfactory solution, but let us looks at what happens if I try\nto plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square\nas my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that\nwill be x1 that is corresponding to this here and then looking at the square of that, but\nthen looking at these data points, so this is going to go here, this will probably go\nhere, right. Now, it is very clear that I can draw straight\nline, right. I can draw if can find the linear decision boundary that separates these two\nclasses once I have done the appropriate transformation. So, this essentially allows us to solve a\nlarger class of problems. You see the kinds of basis transformation, then we could do\njust by operating in the original space and trying to solve the linear optimal hyper-plane\nproblem. So, this is essentially what all your choices of kernel functions or all about,\nright. So, if you look at any SVM tool, they will tell you to pick one kernel function\nwhich is either the polynomial kernel. So, in which case you have to pick in the appropriate\nd or you have to pick radial basis function or Gaussian kernel, in which case you have\nto pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or\nwe can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa\n2 which are the parameters that define how quickly this sigmoid function rises. We will\nsee more about that when you look at neural networks.\nSo apart from this, you still have one further parameter that we will have to worry about\nwhich is this constant c, right. So, this tells you how much slack that you are willing\nto tolerate, right. So, if you think about it, if c is very large, if c is infinite,\nthen we have to be in the completely separable case because even a very small value of zeta\nwill cost this objective function to become very large, right. Even for a small value\nof zeta, right so you will find that this thing is actually very bad, right. c has to\nbe if c is very large, then zeta has to be very small, right and if c is small, then\nzeta can be large. So, what this is going to tell you is that in the higher dimensional\nspace, where you are assuming that the higher the dimensionality that you are projecting\ninto, more likely is the data will be separated, right because if c is large, right because\nyou are going to try in fit more complex surface, right. c will look, the surface will look\nvery vigil, right and if c is small, then you will really get much smoother surface,\nbut the possibility of you making errors is also higher. So, thatÕs trade of that you\nhave to figure out empirically by looking at how you are doing the, how you are performing\nin the actual data. This brings us to the end of the module on\nSupport Vector Machines and these are very powerful classifiers and quite often they\nare the first classifiers of choice for people when they are trying to solve new problem\nwhich they really donÕt know much. So, one thing I should point out that people have\ncome up with different kinds of kernel functions. So, I have given you three choices of kernels\nhere. These are essentially the most popularly used kernel choices, especially if you are\noperating with text data, you would like to use linear kernel d is 1 and in most other\nforms, you will be looking at RBF kernels, but then for special forms of data like graphs\nand strings and so on and so forth, people have defined their own kernels and as long\nas they are satisfying your properties of the kernel function, you can define your own\nkernels and then have them help you solve the problem, right. That is topics for another\nday or perhaps for another course. So, we will stop here. Just let me reframe that SVM\nare one of the most popular and powerful classifier that are currently being used widely.\nThank you.\n \n\nArtificial Neural Networks\nHello and welcome to this module on Artificial Neural Networks.\nSo, artificial neural networks, are computing models inspired by biology. So, we have neural\nnetwork architectures that have been proposed for a variety of different analytics tasks\nlike regression, classification, clustering, feature extraction, etc. So, these architectures\nessentially are networks of simple computing entities and so this is like a very simple\nthreshold entities that are connected together in the specific network architecture that\ngive rise to complex computing functionality. Now, oscillate there has been significant\nresurgence in interest in artificial neural networks, especially under the domain of T\nnetworks about which we will see in one of the later modules. So, for this module and\nthe discussion about artificial neural networks is concerned in this course, we will look\nat only the classification task and many of the ideas we talk about here for classification\nare generalizable to regression, like for while for the other kinds of analytics task\nwe need different architectures and, but we are not going to cover that in this course.\nSo, the inspiration comes from biological neuron. So, let us not worry about the complete\ncomplex structure of a neuron, what we really have to focus here is on the input and the\noutput. So, the neuron receive inputs from the dendrites or from the dendrite branches\nfrom other neurons and when the input signals is above a certain threshold, it is going\nto produce an output, that is going to be transmitted via the synopses to neurons that\nare further down the line.\nSo, these connections to the dendrites and synopses are going to be result in a very\ncomplex network and even though, the computing done by each element is very, very simple\nsummation and thresholding. The sum total of this taken across the entire network can\ngive rise to daily complex computations, which we will see.\nSo, the completing unit is something that is very simple. So, it is going to take a\nset of inputs x 1 to x n and it is going to compute some functional arm, it is function\nis very simply incredible and then it will produce an output. So, we will look at what\nthis function is going to be in detail in the next few slides.\nSo, the initial model for this for a biological neuron was proposed by McCulloch-Pitts in\n1943 it is called the McCulloch-Pitts unit, it is only binary signals, so 0s and 1s. So,\neither an input is active, then these cases are represented by 1 or if it is not active,\nin this case it is represented by 0 and the nodes also produced only binary results. So,\nthe outputs could be either 0s or 1. So, the edges between these different nodes were directed,\nunweighted, they could be of two types that could be excitatory or inhibitory and again\nI can mentioned earlier, the transfer binary signals.\nSo, what is the computation that happens here? So, I let assume that the McCulloch-Pitts\nunit gets inputs x 1 to x n through n excitatory edges. So, these are positive edges and inputs\ny 1 to y m through inhibitory edges. So; that means, these are edges that could produce\nthe depression in the function or could actually stop the functioning of the neuron. So, the\nassumption that was made is, if m is greater than or equal to 1 that is at least one inhibitory\nedge and if any one of the inhibitory edges is 1, so if there is a one inhibitory input\nthen the unit as a whole does not produce any output, regardless of what the inputs\nx 1 to x n are. If none of the inhibitory inputs are 1 or\nif there are no inhibitory inputs at all, the unit computes the summation of x 1 to\nx n, let us call it x and if x is greater than the threshold that is specified for each\nunit, if it is greater than the threshold theta then the result of the computation is\n1 as the result is 0. So, it is very simple, so essentially you can think of it as adding\nup all the inputs that come to the neuron and if the summation is greater than our threshold\ntheta, your output 1; otherwise, your output 0. So, the inhibitory edges in some sense\nhere acting act as a gating signal. So, if it is 1, the output is always 0, if the inhibitory\nis only 0 then the output is the result of the computation.\nSo, it is essentially the McCulloch-Pitts unit, it is implementing just threshold function.\nIf the input is below theta you are going to see a output of 0, if the input is above\ntheta you are going to see a output of 1, that this is essentially a step function.\nSo, what kind of computations can you do with this? So, you can actually do almost all your\nfamiliar Boolean operations with the McCulloch-Pitts neuron. So, you can think of doing an AND\noperation, you have two inputs x 1 and x 2 and the threshold is set a 2. So, if only\nboth x 1 and x 2 are one, so it will be greater than or equal to the threshold and therefore,\nthe output will be 1 and for implement in a OR you can set the threshold that one. So,\nif either x 1 or x 2 is 1 to the output will be 1 after complimenting a NOT unit it can\nimplement the NOT unit by having x 1 act as an inhibitory input. So, this circle here\nindicates an inhibitory input. So, if x 1 is 1; that means, they neuron and\ninhibitory output will be 0 on the other hand x 1 is 0 then the output will be whatever\naccording to the result of the computation. But, we can see here that the threshold for\nthis neuron set as 0 and that is for the output will be always 1 as long as there is no inhibitory\ninput. So, if x 1 is 1 then the output will be 0, x 1 is 0 output will be 1, that how\nwe have implemented NOT function. Now, once we unable to implement this kinds of AND,\nOR and NOT then you know that you can connect neurons together and then implement any Boolean\nfunction that we want and is this what really we are interested in.\nSo, we are not really interested in that because we want to be able to do more complex classification\nproblems, then we would like learn simple things like linear surfaces or more complex\nsurfaces that is separate two classes. So, that is has been the goal of classification\nwe have looked at so far. So, in 1957 rosenblatt proposed a very simple extension to the McCulloch-Pitts\nmodel which we called the perceptron, the more crucial thing what the perceptron is\nthat a it introduced weights at the inputs, crucial differences from the perceptron from\nthe McCulloch-Pitts module is that the perceptron introduced weights at the input.\nAnd then it the output could be either a one or a minus one depending on whether the weighted\nsum of the inputs is greater than threshold that one that is the computing unit with a\nthreshold theta So just repeating it. So, the output of the neuron is 1 if the weighted\nsum of the inputs is greater than or equal to theta is equal to minus 1 other wise.\nSo, what is the goal here in perceptron learning, when perceptron learning we are essentially\ntrying to learn a hyper plane, trying to learn a separating surface as we have done in the\npast in the other classification problems, we are trying to learn the separating surface\nthat can separate one class from the other. So, what would the classes be in our case,\nclasses in our case would be plus 1 and minus 1. So, this essentially means if w i x i is\ngreater than equal to theta, it essentially defines the equation of a hyper plane as we\nhave seen in the previous modules. So, if this you can take the theta to the\nother side. So, we like w i x i minus theta is greater than or equal to 0. So, we have\nseen that was greater than 0 to some one side of the hyper plane if it is lesser than 0\nit is on other side of the hyper plane and we are going to say that data points to one\nside of the hyper plane belong to class 1 data points other side of the hyper plane\nbelongs to class minus 1. So, now, the question is given a set of training data that gives\nyou the x x the vector x and the decided output y.\nHow would we find these weights w i's such that the perceptron is actually implementing\nthat hyper plane, implementing the right separating hyper plane. So, the weighted all this is\nfollows, you start of the randomly initializing the weights to some value and then we look\nat the prediction that is made by the way. So, the prediction that is made by the current\nsetting of the weights, let us call it o and the target is the two class of the data point\nx. So, with this, it will be plus 1 or minus 1 and likewise o is also plus 1 or minus 1.\nSo, your goal is to make sure that here perceptron output matches the target value.\nSo, the perceptron training algorithm has a very simple rule. So, at every presentation\nof an input point, we change the weights by an amount that is proportional to difference\nbetween the target value and the actual output produce times that the input on the particular\nit. So, w i changes by an amount that is proportional to t minus o times x i. So, eta here is a\nsmall constant may be 0.1 or 0.01 as called the learning rate.\nSo, one thing to note here if for a particular input x I will produce the correct output.\nSo, the class is minus 1 and I produce minus 1, the class is plus 1 and I produce plus\n1. This expression evaluates to 0. You can see that this expression evaluates to 0 and\ntherefore no changes in the weights will happen. So, essentially what happens here is you change\nthe weights only whenever you make a mistake and that to you change the weight proportional\nto the input variable. So, if x i is say a small value say 0.1 or 0.2 then will be changes\nin the weight will be small and as for as the poster when x i is the large value let\nus say 1 or 0.95 and things like that then the change it be next will be large.\nSo, this essentially because the larger the input variable the more important it is going\nto be in the production of the output at least the way we are set up this perceptron. So,\nthat is essentially the simple training rule. So, whenever you make a mistake, you take\nthe vector for which we have made a mistake add some small fraction of that vector to\nthe weights.\nSo, this looks like a very simple rule, but then back in 50's this perceptronÕs created\na lot of human cried the people saw that the perceptronÕs by the able to learn from scratch\ntrying to solve something which are considered hard learning problems and then they used\nthe perceptronÕs they were able to solve that, so much so you can see here the hype\nwas that they are going to build the computer that expects to be able to walk, talk, see,\nwrite, weight reduce itself and be conscious of it is existence, such the significant amount\nof hype and it is always hard to live up to any height that this proportionate and to\nthe actual effect that was achieve that point.\nSo, let us take a look let us just back and take look at what can of perceptron learning\nis a news paper article really true or what are the limits to the perceptronÕs learning\nability. So, here is a very simple perceptron here, so it has a two input variable x 1 and\nx 2 and that is the threshold of 1 and the weight w 1 is 0.9 and w 2's 2. So, if you\nlook at it essentially it implements this straight line here, so everything above the\nstraight line this light color regions belong to one class and the dark color regions belong\nto another class. So, we know that these are data which are\nlinearly separable; you saw this in the case with SVM's. So, these are data that are separated\nby a linear hyper plane or the linear separating surface. So, all data points for which the\nw transpose x evaluate, so greater than x is 1 will get a class of plus 1 all those\nthat evaluate to lesser than 1 and get a class of minus 1.\nSo, again let us go back and look at the simple logic function that we saw earlier. So, it\ncan implement that OR. So, essentially OR requires you to have a hyper plane and this\npassing here. So, everything to this side this become plus 1 everything to this side\nbecomes minus 1 and likewise you can implement and so you can draw a simple hyper plane.\nSo, everything to this side become plus 1 and everything this side becomes minus 1 or\n0, I mean depending on how you wanted to predict the output.\nAnd let us look at another one, look at simple problem just like OR and AND the XOR problem.\nSo, Minsky and Papert in 1969 in a famous monograph called the perceptrons showed that\nwell a simple problem like XOR. So, where the truth table is given here is the inputs\nof the same output is 0, if the inputs are differently output of 1, the simple problem\nlike XOR is not linearly separable, you cannot draw a hyper plane that separates these two\nclasses. So, forget about walking, forget about talking\nand doing all those wonderful things that was claimed to news paper article perceptronÕs\ncannot even solve this as simple problem as XOR is essentially says that two things are\nsame, the output is 0, two things are different the output are 1 that we cannot recognize\nthe similarity between this simple inputs like 0's and 1's what kind it do to complex\ncomputations. So, once Minsky and Papert showed this, it is a kind of you dampened the research\ninto neural networks for a long time until there was revival much later.\nSo, perceptronÕs can learn only linear decision boundaries that is the take away message here.\nSo, that is make that is whole idea of neural networks completely useless, because they\ncan learn only linear decision boundaries in case of SVM's we saw that we could get\nit to do all linear boundaries by going into Kernel expansion this has something similar\nthat we can do here.\nLet us look at how we can change the representations and try to do something more clever. So, if\nyou look at the original problem the XOR problem, so I have my inputs x 1 and I have my input\nx 2 and now we can see that in this space the problem is not separable. But, let us\nlook to do a simple transformation on my data points, so instead of looking at x 1 I will\ndefine my first variable as NOT x 1 and x 2 and similarly I will define my second variable\nas x 1 and NOT x 2. So, if you think about it, so we can now plugging\ndifferent values of x 1 and x 2 here and see what the outputs will be and then you can\nsee that when x 1 is 0 and x 2 is 0, the output is going to be 0, when x 1 is 1 and x 2 is\n0. So, the output here will be x 1 is 1 and x 2 is 0, the output here will again be 0\nand x 1 is 1 and x 2 0 the out here will be 1. And we know that 0 1 the output has to\nbe 1, so that we get it here and likewise for the symmetric case this will be the output\nand so you can see that this is again going to be 1 and when x 2 x 1 x 2 both are 1 again\nthe output will be 0 0 and therefore, this is the resulting point.\nNow; obviously, this representation the data points are linearly separable. So, now, the\ntask becomes one of finding the right representation, such that the data becomes linearly separable\nfor the next level, next stage of computation. So, people realized this very quickly, so\neven though a single perceptron cannot solve complex problems like XOR which are not linearly\nseparable, he could actually stack layers of neural neurons and then have the first\nlayer compute something that is simple. So, you can always compute NOT of x 1 we saw\nthat earlier and also can be computed by a single neuron. So, you can this get have layers\nof neuron that exactly compute your features and of NOT x 1 comma x 2 and then have another\nneuron, which takes the output of this neurons combine same together and produces the output\nthat you want. So, people very quickly realize that stacking this kinds of neurons into layers\nallows you to do more complex computation. In fact, it is easy to show that stacking\nthese neurons into layers actually builds a universal function representation that learns\nto a represent any Boolean function, you see a combination of neurons. So, what is a problem,\nnow we know how to solve this more complex problems, why did the research in neural networks\npick up again.\nSo, the question here is when I start connecting all of these neurons into layers. How do I\nfind the weights? So, perceptron learning algorithm might no longer work in this case\nactually does not work in this case and people were struggling to come up with the mechanism\nfor training all these weights. So, you can see that the way of started putting these\nthings into layer. So, that is one input layer and one output layer, so there is one input\nlayer, there is one output layer and in between this you could have many layers of neurons,\nthey are typically called hidden layers because you do not observe their outputs directly.\nSo, now, we have this many, many hidden layers of weights and it is little hard to find out\nwhat this weight should be and so in the mid 80's around 83 an algorithm was proposed called\nback propagation which allow you to learn the weights of this and we solve this hidden\nlayers.\nSo, for the rest of the presentation, we will be looking at the standard three layer network.\nSo, there is an input layer x 1 to x d and the output layer which will denote by f of\nx and one hidden layer of neurons. So, these take the inputs from the input layer do the\nweighted sum do your thresh holding function and then produce an output and then the neuron\nand output layer will take all this outputs of the hidden layers take their weighted sum\nand take the threshold or not and that produce the output, instead of using hard threshold\nwe use a kind of a soft threshold like in order to do this competitions this is needed,\nso that you can derive more efficient training algorithms later.\nSo, the output of a hidden units and it given by g of the bias term, this is the theta that\nwe had earlier. So, instead of theta so it is going to call it b 1 and plus w 1 times\nx and the output of this will be note by h of x and the output of the final layer of\nneurons is given by some function o of b 2 plus w 2 times h of x. And so now, the goal\nhere is to figure out what this w 1 and w 2 are going to be. So, this is called the\nthree layer network, even though there are only two sets of weights that we have to learn.\nSo, the layers here talk about the neurons here, so we use for each input variable we\nare same that there is separate neuron that is activating the hidden units. So, this is\ncalled the standard three layer network structure.\nSo, what are the different activation functions you can use? So, we already looked at one\nwhich is the threshold function, we can also have just a linear activation function that\nbasically takes the summation of weighted summation of all the inputs and outputs as\nit is. We can also look at the sigmoidal function, sigmoid logistic function which takes the\nsummation input and then squashes the input. So, that it remains between 0 and 1 and then\nthere is a steep raised somewhere around the threshold. So, that it transitions rapidly\nfrom 0 to 1. When if you are interested in having signed\noutputs then you can think of using a hyperbolic tangent, where the outputs are going to taxation\nbetween minus 1 and plus 1 and again around the threshold. So, there are parameters at\ncontrol where the threshold would be and how steep the price would be. So, another transition\nfunction some time gives is this squashing function, which is 0 before the threshold\nand one at a certain distance higher than the threshold and in between you have a...\nSo, linear approximation adds to the step function, this called the squashing function.\nSo, typically in most of the neural network architectures that we look at will be looking\nat either the hyperbolic tangent or this, the logistic sigmoid or the linear activation,\nbecause these are different shape and this allows as to derive efficient training algorithms\nfor the same. So, if you are doing a classification problem then the output layer could be the\nhyperbolic or a logistic sigmoid and if your solving a regression problem, the output neuron\ncould be a in linear neuron. So, that you can do appropriate regression fit.\nThe hidden layer almost always has to be a non-linear function and where the little bit\nof what you can show that if the hidden units have a linear activation, like you might as\nfor not have them at all. And what is the function that is implemented is something\nwhich can be as well implemented by a single layer of neurons. And the next module we look\nat how you the exactly find out these weights given the assumption that they are working\nwith the sigmoidal logistic function. So, the function for the sigmoidal logistic\nthing is given by f of net is 1 by 1 minus e to the power of minus net. So, that is the\nfunction and look at, given that this is the activation function how we are going to derive\nthe weights of the two layer standard three layer neural network. So, that is in the next\nclass.\nArtificial Neural Networks(cont\\'d)\nHello and welcome to the module on back propagation or how you are going to train artificial neural\nnetworks and determine the weights.\nSo, to keep things simple, let us start off with a single neuron and that is going to\nhave a logistic sigmoid as the output function. So, you are going to write here f hat of x\nis equal to b plus w transpose x and then, you pass that through your sigmoid function\no. So, o in this case would be the logistic sigmoid, where we will say o of v is 1 by\n1 plus e power minus v. So, we saw this in the last module, so the error measure that\nwe will be using is the squared error. So, the excepted error of the parameters w\ngoing to be half times, the excepted error w is going to be summation i equal 1 to n,\nwhere n is the number of training data points that you have of the squared error for each\ntraining data point. So, the way we are going to use this for changing the weights is essentially\nto compute the gradient of the error. So, essentially that will be the error times the\nderivative of the output function times minus x i.\nIt is essentially taking the derivative of this with respect to the w function. Once,\nyou have computed the gradient of the error with respect to the weights, then we essentially\njust change the weights in the direction opposite to the eta times the gradient of this weights,\nwhere eta is the essentially the step says parameter. So, this was fine when you have\na single neuron. So, what about the case when you have layered networks like this?\nSo, this is our standard three layer neural network. So, the first layer is essentially\njust the inputs. So, the second layer of neurons takes in the inputs and computes the output,\nthese are called the hidden neuron that is we discussed in the last module and then the\noutput layers finally, take the output from the hidden units, again do the appropriate\ntransformation and give you the final outputs. So, here we will denote the hidden layer outputs\nby h and h is given us g of the weighted summation of the inputs and let me introduce the temporary\nvariable here called t, which essentially is the summation of the outputs of the hidden\nunits and f is finally, the transformation o of the outputs of the hidden units.\nSo, one thing to note here is that, so I have used different functions g and o for the hidden\nlayer and the output layer and typically for two class classification problems both g and\no are logistic sigmoid as we saw earlier. So, g as well as o would be of the same form\n1 by 1 plus e power v, but then if you are having a regression problem you can essentially\nuse the same setup that we have here, except that o would be linear for the regression\nproblems, so in which case o would be essentially just passing on the inputs that it is getting.\nSo, suppose I have this multiple layers, how do I go about finding the weights of this\nstandard three layer output? So, in some sense finding training rule for w 2 is not very\nhard. So, w 2 if you think about it, it is just like a single neuron network. So, I can\njust take all the weights that come to f 1 and then, essentially use the same rule that\nI used earlier here for a single neuron. I can use the same rule for training f 1, except\nthat instead of x i I will be using the output h, so that is clear. So, for finding w 2 I\nreally can just stick with what I did earlier.\nSo, I am going to write that here again just for clarity sake. So, I have rewritten this\nin an appropriate fraction for working with this multi layer networks. So, I am going\nto look at the gradient of the error with respect to a single weight that runs from\nthe m'th neuron here to the k'th output neuron, this weight runs from the m'th hidden neuron\nto the k'th output neuron. So, this is essentially the k'th output minus the prediction given\nby the neural network. So, that is essentially our error times the derivative of the output\nfunction of the k'th neuron with respect to the input that it receives times the input\nthat is coming on the m'th line, which will be essentially h m i.\nSo, if you think about it that is exactly the update that we had here except that I\nhave change the notation to apply to the two net, the second layer weights in the three\nlayer network. So, now, the interesting partÉ So, this is fine, so this we can just get\nfrom the single neuron update. So, what we do about the first layer weights? So, let\nus see how you will do this, this essentially uses the very simple idea of chain rule from\ndifferentiation, I am going to take a very specific weight here. So, I would like to\nfind the derivative of the error with respect to the first layer weight that runs from the\nl'th input neuron to the m'th hidden neuron. So, it runs from some l'th neuron to the mth\nneuron, so I am just looking at this one weight here, we are trying to find out what is the\ngradient of the error at the output with respect to that one weight. So, I can rewrite this\nas follows, so the derivative of the error with respect to the first layer weight is\nessentially the derivative of the error with respect to the output of the m'th neuron times\nthe derivative of the m'th neuron with respect to the weight.\nSo, this makes sense, because the weight to the m'th neuron affects the output only via\nthe output of the m'th neuron, so it affects the overall output of the network only via\nthe output of the m'th neuron. So, I can essentially apply the chain rule here. So, let us take\nthis bit by bit. So, if you look at this, so you can see that this is immediately obvious,\nbecause this is the function we are talking about here. So, if you take the derivative\nof this with respect to any specific w here, so we will essentially getÉ\nSo, if we take the derivative of h m i with respect to w m l, then you essentially going\nto get the derivative of g times the derivative of this expression with respect w m l which\nwill just be x l i. So, this is the second term in the derivative. So, the first term\nin this derivative is the one that requires a little bit more work, so let us see how\nwe will do that. So, I am going to try and evaluate that expression here, so if you think\nabout it, so the different ways in which the output of the m'th neuron can influence the\noverall error is essentially through each one of the output neuron that m'th hidden\nneuron connects to. So, it can influence the error through this\noutput or it can influence error through this output. So, we are essentially summing over\nall possible outputs k of the gradient of the error with respect to f k and the gradient\nof f k with respect to the output of the hidden neuron. So, you can easily evaluate the derivative,\nso the derivative of the error with respect to f k. So, if think about it, so this is\nthe expression we have. So, the derivative of the error with respect\nto f k, so going from this expression derivative of the error with respect f k going to be...\nSo, negative of y i minus f hat k x i, because we are taking derivative with respect to f\nk here. So, we do not have to worry about the further terms that constitute f k. The\nsecond term is concerned that is essentially looking at this. So, derivative of f k with\nrespect to h m is essentially the derivative of o with respect to h times the derivative\nof the argument of o with respect to h which gives us just the weight w k m complex expression\nfor this. So, before I put these things together to\nmake things a little simple let me introduce the small notational thing. So, that it makes\nlie for little easier for us, so I am going to say delta and I am going to say delta k\nfor the input i this essentially the error term times the derivative of the last layers\noutput function and also define this term row m for the input i as the essentially the\nderivative of g with respect to w times the summation of w k m into delta k of i.\nSo, putting these together now we can write our expressions more compactly essentially\nwe can say that dou. So, the error of the derivative of the error with respect to the\noutput layer weights, with respect the w to weights is essentially delta k i times h m\ni, so likewise the derivative of the error with respect to the first layer weights, the\nweights from the input layer to the hidden layer is given by row m i times x l i.\nSo, if you think about it, so this part corresponds to this and that comes from here and x l i\nis what is left out here. So, this expression look pretty compact and now we essentially\nhave... So, you essentially update the parameters by w k m is just change by accept the gradient\nhere and w m l is again change by the gradient that we are computed here. So, one thing I\nwant to point out here that if you are function, the function g or your function o happens\nto be a sigmoid, then g dash is\nequal to essentially g of a into 1 minus g of e.\nAnd suppose you are using a tanh function, because you want here outputs to run from\nminus 1 to plus 1, suppose then g dash it is 1 minus g v squared essentially\nthis gives you the... So, you can see that the sigmoid functions have a very convenient\nform for the derivatives. So, one thing that we have to be careful about here is the fact\nthat these are gradient following methods and therefore, and there is a good chance\nthat will get stuck in local optima and there are many techniques for getting out of these\nlocal optima, but we are not discuss too many of them there one of the simplest one is of\ncourse, to try this with multiple starting stage, starting points for your weights and\ntaking the other set of weights that gives you the best possible result at the end of\nsome kind of experimentation. So, that brings us to the end of this module\non training your neural network using back propagation. So, but this form of training\nhas it is own draw backs will see what is that in the next module.\nEnglish - NPTEL Official\nDeep Learning\nSo, in the previous module we saw how we can use back propagation in order to find the\nweights of a neural network; upper three layered neural network. So, one of the earning success\nstories of back propagation was learning to recognize hand written digits in an address;\nright. So, this is work done by Yann LeCun back in 89, and essentially they trained a\nslightly more complex network architecture call the convolutional neural network; in\norder to recognize hand written digits. You can see the variation in the digits, that\ntheir network manage to handle; the fairly well.\nSo, another early success story of neural networks was the Backgammon Player built by\nGammon Tesauro from IBM, T. J. Watson labs in 1992. So, he built 2 versions of this backgammon\nplayer one called the Neurogammon which came in 89, which was essentially neural network\ntrend using back propagation in supervise learning manner; in order to play a game of\nbackgammon. So, it is like Ludo of people know about it. So, you throw a dice, you throw\na dice and depending on that die roll; you move your coins around. So, that is a white\nand black side. So, the idea is to move all your coins of the board by repeatedly rolling\nthe dice. So, the TD-Gammon player essentially used the 3 layer, standard 3 layer neural\narchitecture and was trying using a specific form of what is called reinforcement learning.\nWe look at reinforcement learning in the later module, but he used reinforcement learning\nin order to generate the error signals and, but then use back propagation to train the\nweights of the hidden unit. And you manage to build Backgammon player which was able\nto beat human Backgammon champions in game play.\nSo, this was some of the early success and lot of interests was being spent on looking\nat neural networks of solving variety of different problems. But then again people discovered\na 2nd drawback with neural networks. So, the one thing was that It was incredibly hard\nto train a neural network, because you had so many parameters you had to treat them appropriately.\nSo, that they desired to deserved for obtained.\nFurther there was this problem called the vanishing gradient problem. So, people quickly\nfigure out that if we have many layers in the neural network; right. It was easier for\nthe network to represent more complex functions; even though 2 layer network or a 3 layer network\nand depending on the how we contact. The standard 3 layer network was the universal approximator,\nhaving more layers allowed more complex representation to be build. But see, number of layers in\nthe neural network increases, the gradient that we compute by doing back propagation\nare becomes vanishingly small; right. And since there are not enough feedback for the\nearly layers in the network. The rates in the lower layer remains random wherever universalize\nthem ; right. And so, you are not able to learn any deep networks; right. So, the networks,\nneural networks it will learn necessarily had to be shallow; because only a few 3 layers\ncould be, 3 or 4 layers could be trained meaningfully using back propagation. So, people had come\nup with different tricks for training deeper networks.\nSo, and this discovery of these tricks for training deeper networks, is essentially what\nis revive the interest in the field. And now we can build networks at have 8 layers, 10\nlayers and so on so forth. And this produce some fantastic performances in problems that\nwe are talk to be very hard to solve for AI and machine learning. And so, there has been\nrevived interested looking at neural networks. So, I will talk about one specific mechanism\nfor training multiple layers in the network, and then you can I mean if you are interested\nin this follow it up with other material.\nSo, this is proposed in mid 2000 by Hinton 2006 and independently by Ashwa, Bengio and\nothers. In 2007 it is more like a greedy unsupervised layer wise pre training, so what we mean by\nthis? So, you start off with very simple network architecture called an auto encoder. So, you\nhave an input layer and you want to produce the same input at the output. So, the input\nlayer and the output layer need to be identical, but in between the connections will go through\na smaller hidden layer of neurons. So, the idea here is that the smaller layer is going\nto learn some kind of a encoding or some kind of a reduce representation of the input; that\nis sufficient for you to produce the output that you are looking for.\nSo, you could train this using back prop or other slightly more advanced gradient techniques.\nSo, if you think about it, there is no real supervision that is required here; because\nas soon as have the input; right. The output is just the same input. So, I am going to\nset it through a smaller hidden layer. So, that I am learning a reduce representation\nof the input. So, once I have this 1st level of reduce representation;\nright, I am going to iteratively deepen the network. So, what I do? So, once I have the\n1st hidden layer of representation, the first hidden layer of representation I am going\nto add another auto encoder network on top of it. So, this network takes the hidden layer\nrepresentation for the input and tries to produce the same hidden layer representation\nat the output, but in the middle layer is going to have fewer neurons. So, in the effect\nI am taking this larger input and reducing it to a smaller hidden representation here.\nAnd I can repeat this; right. So, once I have trained this, I remove the outer layer; and\nthen take on another layer of auto encoders. So now, you can see that my training has gone\nseveral layers deep. So, instead of just looking at one layer deep auto encoder which have\nI was able to train using back prop efficiently. So, I am essentially using the same construction\nagain and again. So, at any point the training happens only on a one layer auto encoder,\nbut then because of this iterative deepening. Now I have actually taken this input representation\nand progressively reduced it to a much smaller representation at the hidden layer. So, this\nkind of an observation that you can use this layer wise pre training of the data, led to\nresurgence in lot of deep architectures. So, why do we call this pre training? So, far\nI am not talked about any kind of classification task that you have performing. we are just\ntrying to find features in the input space. So, once I have found these features in the\ninput space then I can take this, and I can tack on top of it.\nOn top of this features that I have learned I can tack on other neural network. And now\nI have my full fledged deep auto encoder and then this hidden layer representation can\nnow be used as an input any learning task. So, so this part is call the fine tuning part\nand the layer wise training part is called the pre training part; where I find the hidden\nrepresentation. And after that I can add it to any complex neural network, that can do\nmy classification task or my regression task whatever it is that I am looking at. So, this\nkind of layer wise pre training allowed people to, drive more complex compressed representations.\nThat very useful in a variety of problem solving and I use.\nSo, this again revived a lot of hype in neural networks or in deep networks as they are called.\nSo you can see very human similarities to the news items that I showed you from the\n1959 thing. So, a stimulated brain and you can teach context to computers, machines at\nlearn without humans, etcetera, etcetera. So, skate what you read in news paper with\na pinch of solve, but again there is significant renewed interest in deep networks and neural\nnetworks as there was in the 50s.\nBut the overall feeling in the community is that, deep learning is come to stay; because\nthere are lot of nice properties about this generation of neural networks as compare to\nthe earlier generations. So, the things of more stable and things of reproducible; even\nthough significant computing power is needed. So, most of the successful applications of\ndeep learning that you see, would have had significant amounts of computational power.\nBut then the computational power is also cheap; and you are able to solve really complex problems\nusing neural networks.\nAnd so, deep learning collects have yield at state of the art performance in the variety\nof domains, like image classification, object detection, language modeling, machine translation,\nspeech recognition, image description. These are problem at traditionally considered very\nhard for machine learning algorithms and deep learning seems to have significant in pattern\nthese areas.\nSo, some of the images I used here are taken from the neural networks book by Raul Rojas.\nAnd so, I like to show you few demos of deep networks and action law. So, here is one algorithm\nfrom company called clarify. So, given an image with different kinds of textual labels\nit says that, here given this image is say that is coffee, at there is croissant, there\nis a beverage in it, and this is probably breakfast and it is had in the morning and\noverall hey this looks like food. So, it is able to derive all of this tags just by looking\nat this image. These are all similar images it does manage to retrieve images, similar\nto these. You can see that all of these are images of breakfast or continental breakfast\nand all of these have a cup of coffee in there. And so, even though the variety of this is\nable to cover is truly expounding. So, like wise look at these picture here,\nis able to figure out the here suspension bridge here and there is a river here; even\nthough the river is not explicitly visible. And that is it night because it is lighted\nup and it looks like bridge in the middle of the city. And likewise it has found similar\nimages, which are all suspense bridges in cities on top of rivers.\nAnd. So, likewise it is able to work on variety of outdoor seems, as well as indoor seems\nand it’s able to perform really well. And this is state of the art in terms of image\nunderstanding and labeling at correctly.\nAnd likewise you can look at how well it works in machine translation. So, the both the image\ntagging and the machine translation tasks use some much more complex neural network\narchitecture that we have seen so far. But then so, here it’s a simple example from\nhere. So, I typed in the sentence what a wonderful idea in English and then it gives me the equivalent\nin French. And not only does it do that, it tells me which words correspond to which word\nin re translated language. Which words in the original English language, correspond\nto which word of the translated language. So, I do not know French. So, I am not sure\nthat is a good translation, so let us not do this. So, let us stop with the previous…\nSo, it is then this thing actually you can learn translate, can do translations between\nmultiple language is not just in English and French. It first trained on the data from\nUnited Nations and European parliament. So, it can do translation between the all the\nEuropean languages. So, that brings us to the end of this module on deep learning.\nThank you.\nAssociative Rule Mining\nHello and welcome to this module on Association Rule Mining or Frequent Pattern Mining, which\nis essentially the basic problem underline association rule mining.\nSo, we had a very brief look at association rule mining with very beginning of the machine\nlearning section.\nSo, the idea behind association rule mining is to first mine frequent patterns that occur\nin the data and based on the frequent patterns that you have mined, derive association rules\nwhich is to form at if A happens, then B is likely to happen.\nSo, basically is like a conditional dependence relation that you are mining if A happens,\nthat makes B more likely to happen.\nYou could find such patterns in sequences looking at time series data, like financial\ndata or looking at fault analysis, where one thing causes fault to occur; or you can look\nat in the transactional data column context which is where it was originally proposed\nand that is what we will look at in more detail in the rest of the module.\nAnd more interestingly you could also look at mining frequent patterns and associations\nin graphs, which is appropriately used in social network analysis.\nSo, let us look at a mining transaction for the rest of the module.\nSo, transaction is a collection of items that were bought together.\nThat is the simple definition that we will use for the purposes of association rule mining.\nAnd please note that the set or subset of items, is usually a denoted item set in the\nassociation rule mining community.\nSo, the goal here is to find first find frequent item sets, then you would say that an item\nset A implies item set B; for example, if you could say that somebody buys milk, then\nthey are likely to buy bread, if both A and the event A union B or frequent item sets.\nThat would mean that both A, sorry which is milk in this case and A union B which is bread\nand milk both should be frequent.\nIn which case, I can say that, if you buy milk then, you buy bread as well.\nLet us take a look at exists simple example here.\nSo, here is a set of transactions, so I have 5 transactions and each color here denotes\na different kind of item.\nSo, the first 3 transactions are 4 item sets, the 4th one is the 3 item set, and the 5th\none is a 2 item sets.\nAnd let us assume that we have a frequency threshold of 3.\nSo, we essentially have the following as frequent 1 item sets.\nSo, we have blue, which occurs in all the 5 transactions and then purple which occurs\nin 4 transactions.\nAnd pink which occurs, big item which occurs in 3 transactions.\nSo, none of the other items occur in 3 or more transactions.\nSo, the frequent one item sets are just these.\nSo, the next thing you have to look at is the frequent 2 item sets, in the frequent\n2 item sets you can see or essentially purple and blue which occur in 4 transactions and\npink and blue which occur in 3 transactions and so, none of the other combinations are\nfrequent.\nSo, the only are the things which we really have to look at this purple and pink; and\npurple and pink occur only in 2 of the transactions together therefore, they are not frequent.\nSo, the goal here is to first find such frequent item sets.\nAnd from these frequent item sets, how do we determine which are interesting association\nrules.\nSo, the 2 measures of interestingness for association rules or essentially support.\nSo, the support of a rule is the percentage of item sets that contain A union B; right.\nSo, in this case and then the confidence of a rule is the other measure that we are interested\nin.\nSo, we will go back and look at the data set once we have understood what support and confidences.\nSo, the confidence of a rule is a percentage of item sets containing A, that also contained\nA union B. So, essentially this tells you how confident you are in making the association.\nSo, typically we look for rules with both high support and confidence.\nIf you think about it, if both there in the case of support and confidence we really need\nto find the frequency of the item sets; right.\nSo, once we determine what are the frequent item sets and what their frequencies are,\nthen we can easily determine what are the relevant association rules.\nSo, more effort needs to be focused on counting rather than the association rule itself.\nSo, that is why I said there is frequent pattern mining part of this more interesting than\nthe association rule mining part.\nSo, let us go back and look at the pattern set we have mined so far.\nSo, if you remember the one item sets are in really interesting except to establish\nthe frequency part of it.\nSo, let us look at a rule which says that purple implies blue.\nIf you have bought purple then you likely to buy blue.\nSo, if purple occurs in your transaction then blue is likely to occur.\nSo, if you think about it, this is the valid rule to have because both purple as well as\npurple and blue were frequent.\nIn the earlier slide we saw that both purple and purple and blue are frequency, so it satisfies\nthe A, A union B rule and, what about the support of this rule.\nThis support is essentially all the transactions the number of transactions in which both A\nand B occurs.\nSo, that where A union B occurs divided by the total number of transactions.\nSo, A union B occurs in 4 fifth of this data set; and therefore, the support of this rule is 4 \n5.\nWhat about the confidence of the rule?\nThe confidence of the rule is, whenever A occurs; how many times A union B is occur.\nAnd in this case it turns out that whenever A occurs, A union B occurs; therefore, its\nconfidence is 1.\nSo, likewise for the pink implies blue rule, you can see that the support is 3 by 5; because\nthat is how many times a pink union blue occurs.\nAnd the confidence is 1; because whenever pink occurs, pink union blue also occurs.\nSo, what about this rule?\nI mean is this the valid rule to look at?\nBoth the blue is frequent and blue and purple is also frequent.\nSo, that is a valid rule to look at, but is that the better rule then the one that we\nsaw earlier.\nThat is not necessarily in the case because the support here is 4 by 5 and the confidence\nis 4 by 5 as well.\nSo, the earlier rule which is purple implies blue had a higher confidence than this rule.\nBut both of these are possible rules that you could consider, because both the rules\nhave a high support and a high confidence.\nSo, this is essentially the idea behind the association rule mining.\nSo, it has been applied to variety of applications, Market Basket analysis is one.\nSo, as I had mentioned earlier.\nSo, market basket refers to the fact that when you go to super market, you are going\nto buy a set of items together and put them together in your basket.\nSo, essentially looking at what goes into the basket at the market.\nSo, these baskets are considered as transactions, and you look at frequent pattern mining in\nthese transactions.\nYou could look at co-occurrence of words and that can be used to derive certain kinds of\ntopic relationships.\nAnd people are looked at plagiarism detection in terms of frequent pattern mining.\nNow people have applied this to a biological data looking at bio markers and genes of proteins\nversus diseases.\nSo, try to find out associations between co-occurrence of a certain protein, abnormalities, and diseases.\nYou also looked at in the contrast of time series, where co-occurrence of events can\nbe used to model trigger events and this identifies trigger events.\nSo, that brings us to the end of the first module on frequent pattern mining and the\nsubsequent module will look at techniques for efficiently mining frequent patterns.\nAssociation Rule Mining (contd)\nHello and welcome to the 2nd module of Association Rule Mining. In this module we look at a very\npopular algorithm that is used for association rule mining. As I said earlier the main problem\nin mining association rules is counting part. So, typically how you would do, it is that\nyou generate candidate frequent item sets then, count how many times they occur. And\nthen you prune the candidates based on the count. So, if their count is above certain\nfrequency then, you are going to call them as frequent item sets and if they are below\na certain frequency you are going to reject that.\nBut then, the problem with doing such an approach is that you have a combinatorial number of\ncandidates. So, think about it. So, we have like 10 different items from which your transaction\ncan be drawn; and that is say you are considering 2 item sets. So, that essentially gives you\n10 choose 2 candidates item sets; now this is a small number. Think about real applications\nwhere these candidate sets can be in millions. So, somebody like Amazon or a Flipkart is\ngreat are like millions of candidates. And how would you even generate candidate item\nsets some more than is very small size of candidates. So, really need a clever way of\ngenerating fewer candidates. So, the very first approach for looking at generating fewer\ncandidates came about late 90s which is the apriori property.\nSo, the Apriori property is very clever observations and just says all nonempty subsets of a frequent\nitem set must also be frequent. And even if one of the subset is not frequent, then I\ndo not have to consider the larger item set. You do not even have to count the larger item\nsets. So, the first table that propose this Apriori\nproperty and came up with the fast algorithms for mining association rules was purposed\nin 1994 and it is being a very similar algorithm. First they introduced the problem of finding\nfrequent item sets and data base of transaction. It is said the tone for the entire field.\nIn fact, this use of the term mining association rules, almost was the reason for the whole\nsub field of data mining. And it since then there have been numerous improvements it have\nbeen propose on top of Apriori algorithm, that allowed to main really large data sets\nscale up to will be Billions and so forth. But still Apriori algorithm swap we have to\nstart; alright explanation into association rules. So, in this module I will talk about\nthe Apriori algorithm and give you more like introduction by an example and so, I will\nleave it you to look up other algorithms if you are interested in association rules.\nSo, here we will start up with a very simple example. So, look at very small transnational\ndata base. So, it has got only 4 transaction learning, so each of these have different\nid. And there are total of 5 different items which could be figuring in these transactions.\nSo, we are going to call the A through E. So, let us look at the set of 1 item sets.\nSo, we have A through E and this complete frequency of these item sets. Let us suppose\nthat I have a minimum threshold for the support of 2. So, I have 4 transactions and item set\ncan be called frequently appears in a least 2 of these 4 transactions. So, we can immediately\nsee that item set D is not frequent. So, not only is the 1 item set D is not frequent,\nany larger item set that contains D as element in it can also not to be frequent.\nSo, when I am looking at the candidates, I have to use for generating 2 item sets I can\ncompletely ignore D. And now, all the candidate 2 item sets or those that do not have E as,\nI mean do not have D as part of it. So, essentially the 2 item sets that will have to consider\nare: AB, AC, AE, AC and B E and C E. So, these are essentially generated by looking at all\npossible combinations of the frequent 1 item sets.\nNow, that we have these candidates 2 item sets. So, we are now on the second scan through\nthe data then, we count them frequency of the 2 item sets. Now, we can see that A, B\nand A, E on actually not frequent among these and therefore, we can thrown those away. So,\nwe have a list of frequent 2 item sets, which compares AC, BC, BE and CE. Is it clear so\nfar? So, we had we started out of with all the 1 item sets, these are the candidates\n1 item sets. Counter that frequency we left out the 1 item sets, that was not frequent.\nAnd then from the frequent 1 item sets, we generated a set of candidate 2 item sets which\ncould be frequent. Then, we counted the frequency of these 2 item sets and from there we have\nproven the way the 2 that were not frequent. So, these are the frequent 2 item sets. From\nthese we can generate candidate 3 item sets. So, if you think about it. So, the candidate\n3 item sets could be ABC, ABE, and BCE. So, can ABC be a candidate. It cannot be a\ncandidate because the sub set AB is no longer, is not frequent. That sense the sub set AB\nnot frequent, ABC cannot be a candidate item set; and likewise can we look at BCE. Can\nBCE be a candidate frequent item set? Yes, because, BC is frequent, BE frequent, CE is\nfrequent. And what about ABE? Again that cannot be a candidate item set because AA, AE is\nnot a frequent item set. Essentially we have only 1 candidate 3 item set. And when we count\nthe frequency of the 3 item set and then we find that exactly frequent. And now we have\nthe complete set of all frequent item sets. So, one 3 items sets which is BCE, four 2\nitem sets and then four 1 item sets which are all frequent. You do not have look for\nlarger item sets, they both potentially possible because we do have 5 elements, 5 week events.\nBut, we do not have to look for any further frequent item set because that is only one\nfrequent 3 item set. So, the Apriori property allows us to proven\nthe number of candidate item sets that we will have to generate. So, but still there\nis a problem in that. So, every time we generate candidate sets of larger size, we essentially\ndo another scan over the data. So, the more recent algorithm tries to minimize the number\nof scans, you have to perform over the entire data sets; because that is very expensive\noperation.\nThis is a big challenge but, there are one other Caveat which I want to point out. So,\nscaling up to large data set is a big challenge, but there is a one other caveat which I want\nto point out. So, high confidence is not always a good idea. So, we will have to be really\ncareful about what we mean by high confidence. So, I can say that somebody buys games, implies\nthey buy videos with confidence of 66 percent; the support of 37 percent. So, this is something\nthat was actually observed in a real data set from video rental company, which also\ngoing selling video games with their shops. Because this, so from the real data they are\nfound that somebody buys games, hence they will also buy videos with the confidence of\n66 percent and support of 37 percent. So, essentially it means 37 percent of the people\nthat came to their showroom, shop bought games and videos and 66 percent of the people to\nbought games also bought videos. But then, if you just look at what fraction\nof their customers bought videos there is 75 percent of the customers actually bought\nvideos. And so, even though this rule has a high confidence, you can see that if you\nbuy a game, actually implies negatively on buying videos. This is the video store after\nall and so if we typically come into buy videos but, the occasional person who comes into\nbuy a game, is not that interested in buying videos. So, it actually imply a negative correlation\nand if you had just blindly been using support and confidence to determine rules, then you\nactually trip this out as a important rule in terms of having a high confidence.\nSo, one measure which people gives instead of confidence and support alone, is known\nas Lift. Lift is essentially the ratio of the confidence of the rule to that of a default\nrule. So, if you have a lift of 1 that means, that is really does not imply anything. So,\nwhether A happened or not, B is always going to happen with same frequency. So, if I A\nimplies B is the lift 1 and it is not a significant role, but if A implies B is the lift much\nlarger than one that would be in that. So, the things tend to truly indicator of B. But,\nagain if you think about in this case of games and videos, where lift will be lesser than\n1 and which case is indicates a strong negative corporation.\nSo, lift is a useful measure to have, and that lift is not only measure that people\nhave to proposed the variety of different measures which people have proposed analyze\nassociation rules and association rules mining is a very very active area of research. And\nso this is essentially just an introductory module and if you are interested in association\nrule mining you spend more time. And looking at the various modifications and additions\nthat people have come up with of viewers.\nSo, just to summarize we the major challenges in association rule mining is how do you extend\nto these millions of transactions. So, there could be billions and billions of potential\nitem sets. So, how do you do the pruning efficiently? How do you minimize the number of passes?\nHow do you reduce the number of memory that is required when you are doing the counting?\nSo, there are many many rules and many issues that we have to consider in trying these large\ndata sets. So, we have talked about transnational data\nnow, so some sets easy to count and it was discrete event that are happening; but, what\nabout the continuous data like time series data and images? How would you go about even\nidentifying what are appropriate items more which will be doing this counting. And data\nwith rich structure like graphs. So, frequent pattern mining in graphs is a very important\ntopic which is you used in diagnose area like graph discovery and social influence prediction\nand so on. So, how would you can you average a structure or can you make computation or\nefficient and it handle in see structure data. So, that is something which is a very active\narea of research and experimentation. And one important twist to this whole frequent\npattern mining issue is that, we are not sometimes not interested to in frequency within significance.\nSo, if A occurs let us say in 0.3 percent of all transactions; but when B occurs it\noccurs in 1.2 percent of the transactions. So, it means B implies A and that is a significance\neffect, because it improves the frequency of A from 0.3 to 1.2. And A might be that\nin the A stack that we are looking for and finding B. And actually gives us greater evidence\nfor the presence of A. So, such patterns are significant, but if you just go by the frequency\na loan, these are very infrequent occurrences of a data. So, how do we actually look at\nsuch frequent, none frequent but significant occurances. So, that is a big challenge in\nthe association rule mining community, again like it is a very activity process and people\nkeep developing, that brings us to the end of these.\nBig Data, A small introduction\nHello and welcome to this module on big data analytics.\nSo, what I would do in the next two modules, is trying to introduce you to the problems\nof big data analytics.\nWe will start of looking at what is big data.\nOf course, I cannot hope to cover all the aspects of big data analytics in a couple\nof modules in this course.\nThe whole idea is to give you a very brief or as I say a small introduction to big data\nanalytics.\nSo, when people started talking about data, organizing data, and managing data they initially\nstarted with data base management systems.\nYou might have heard of rational data bases and other forms of data management systems.\nSo, the goal here was ease of access, and to be able to answer simple aggregation select\nqueries; essentially trying to do some form of analytics of the data.\nBut while looking at very specific, very static, amount of the data.\nSo, the volumes of data were not as large as we talk about now a days.\nAnd then from data base management systems, when the data became much larger that you\ncould not make sense out of the raw data itself; and people started talking about data mining\nor data analytic systems.\nWhere the goal was to detect?\nPatterns in the data and people used lot of ideas from machine learning and applied statistics\nin order to do this kind of data mining, try to understand the data better.\nSo, what was happen now a days?\nSo, this whole system is evolved to a point where you cannot just hope to run your machine\nlearning algorithms directly on the raw data.\nBut you have to worry about the data management aspects of it; as well as the analytics aspects\nof it.\nSo, that is essentially the crux of what people call big data in now a days.\nThe fact that the data is so large, that you cannot look at analytics divide of data management.\nSo, give you a feel of what this big data, it is court from rich man oppose the executive\nchairman at Google.\nSo, from the dawn of civilization until 2003, human kind generated five exabytes of data.\nNow we produce five exabytes of data every 2 days, and the pace is accelerating.\nSo, if you can just think about it, what we have produce for 1000s of years since till\n2003, we are able to produce that kind of data every 2 days.\nThat is a volume of data that humans are producing and obviously, we cannot make sense out of\nthis data until some kind of organization and analytics goes handle with us.\nSo, what are the main challenges that come about, when we are talking about data of this\nscale.\nSo, this is the some out of other the coming at shape that view of big data analytic, but\none that is popular.\nSo, I thought I should present it as part of this model.\nSo, the challenges are essentially in capitulated under the 4V’s of big data: that is volume,\nvelocity, variety, veracity.\nAnd I would like to add a 5th V to it; is it called value, which is essentially goal\nof all of data analytics and subsets.\nSo, I will briefly introduce you to each of these ways and another may be the end of the\nfirst module.\nSo, why you said that we are able to generate such huge volumes of data.\nSo, one of the reasons is that all aspects of modern life has been digitized, all aspects\nof modern life has been digitized.\nAnd nowhere is it more a parent than in your social activity.\nSo, we have Facebook; lot of peoples friend significant portion of their breaking hours\non Facebook and possibly significant portion of the sleeping hours reviewing about Facebook.\nAnd then we have such a volumes of data that is probably Youtube, and WhatsApp and twitter\nand flickr and all the web sharing totals at.\nSo, if you think about it at almost every aspect of your life is now being recorded\nsomewhere or the other.\nAnd the next thing is just not the social activities that you perform social media,\nany kind of interactions you are having.\nThis with very high probability is being recorded; for example, phone conversations.\nWe call costumer support calls they are being recorded any feedback and surveys; that will\nhave full fill out of the form anywhere as being that preserve for posterity, any kind\nof online conversation, chats or sub transcript and other things they are also recorded.\nAnd so, it seems like everything that are working at a, it is actually being recorded\nat some point of other.\nAnother aspect is the increasing availability of cheap storage and cheaper bandwidth that\nallows us to share multimedia content at rates and volumes that is never imagine possible\neven 5 years back.\nSo, here are few examples that I took from the internet.\nSo, Instagram is growing at more than one billion photographs.\nInstagram has more than one billion photographs.\nYoutube has 100 hours of new videos uploaded to the site every minute.\nOver one billion unique users vsit Youtube each month.\nAt Facebook adds 30 billion pieces of content every day and likewise you could take any\nof these online portals and you can come up these mind boggling numbers, that tell you\nhow much data is being shared on these portals.\nSo, apart from these the aspects of everyday life and regenerating this data, there is\nbeing significant advance in technology in other area; for example, in biological data.\nSo, 3 gigabases of a human genome can now be sequenced in a few days.\nThis is like 3 times 10 power 9 base pairs which is significant volume of data, but then\nthis is for one human genome.\nAnd you can essentially get your genome sequence for few thousand dollars in the US and so\nit should be made accessible at much lower price range.\nAnd so the data has being generated at every enormous unbelievable mind boggling pace.\nProtein data banks have 10s of thousands of structures amounting to several terabytes\nof data and again people are continuing add to this experimentation.\nSo, to put this in prospective; Donald Knuth one of the fathers of computing algorithms\nand so on so forth.\nHas this to say about computational biology.\nI think the most exiting computer research, now is partly in robotics and partly in applications\nto bio chemistry.\nBiology is so digital, incredibly complicated, but incredibly useful.\nBiology easily has 500 years of exiting problems to work on, it is at that level.\nAnd not only is it in the data analytics appear which is most relevant to us, but the variety\nof other area has to well including data storage and also in computing hardware and access\ntechnologies; biology is challenging computer science at levels never seen before.\nSo, this is about volume but, what about that rate at which this data is being accumulated.\nSo, data is generated at tremendous volumes, but tremendous rates.\nInstagram in may 2012, 58 photographs were being uploaded and a new user was being gained\neach second.\nIn Facebook adds half a petabyte of data every 24 hours, and then let’s not even talk about\nsensor networks; which are generating data most of the continuous rate; for example,\nat IIT Madras we get GPS readings from about 300 buses every 30 seconds.\nAnd if you think about cities city wise transportation, this is really a small scale.\nAnd if you are able to instrument all the buses that run in Chennai city we can get\ndata at a much higher velocity.\nJust hold on one minute, why where we worried about all these big data.\nSo, we are not really, I mean not every one of us is going to work for Facebook or Instagram\nor Google.\nSo, tremendous amount of data is already available in the public domain.\nEither through in Facebook APIs or twitter APIs or through Government data that will\nbeing made available on open forums; and we can obtain significant insights from analyzing\nthis public data.\nAnd you could think about sentiments, you could think about trends and the variety of\nthings.\nIt is the kind of insights you can derive is limited by your imagination and access\nto the data; and there is the significant amount of public data available, that we do\nnot really have to work for Google or Facebook to think about big data.\nThe second thing is that sensors are becoming ubiquitous.\nSo, a lot of everyday I think that sees are being instrumented, and then raw sensors,\nand buildings, there are sensors on bridges, and other kinds of infrastructure out there.\nAnd ubiquitous of internet of things, this essentially the whole idea is to connect different\nkinds of equipment through sensors on low power wireless networks.\nSo, once you have these kinds of data being generated by sensors, then you do not have\nany depth of volume or velocity of data.\nAnd so you really need to develop technique like this; that allow us to work with data.\nCan I say all ready mentioned and large volumes of public data is already available.\nGovernment is pushing for more access to public information.\nSo, the all of these factors really makes sense for us to look at big data and we do\nnot have to work for such large company; is even though the examples I gave you through\nmake you appreciate the volumes that we have to look at, where drawn from this kinds of\nonline social media companies.\nSo, the next V, I am talk about a little bit this variety.\nSo, traditionally we talk about relational data bases, we talk about structure data with\nvery well defined fields; may be of a few types in all could have strange, you could\nhave numbers, you could have categorical variables things like that.\nBut now with big data, data coming from the different kinds of sources, you have many\nvariations of this.\nSo, the first and very widely available source of data is unstructured text.\nSo, we can get things from the web we can get scientific articles, news paper clippings,\nvariety of sources review unstructured text.\nWe keep these are not really marked into sink, this is the heading, this is the most important\nkeyword here and these are all the attributes of this keyword.\nI mean so you do not get this kind of organize data, essentially you have to just read free\nfrom text and from that from some kind of a representation which you can then use for\nyour data analytics form.\nSecond source of information now a days is multi media.\nAs I am mentioning you get pictures, you get videos, and you get variety of different sources\nof videos and pictures, and as well as audio song clippings, and so on so forth on the\ninternet now.\nAnd so, you have to come up with technology for analyzing all of this, especially at a\nscale.\nAnd mention about sensor data and again this is going to look like unstructured data.\nSo, it is going to be company wise time varying signals that we would be measuring from these\nsensors.\nAnd how do you even record these, how do you make sense out of this data?\nThat is the other main challenge and then you have scientific data.\nAnd scientific data comes in variety of different forms.\nWe are taking about medical data, it could come in terms of reading from instruments,\nit could be black and white images, it could be thermal images or here whatever seeing\nhere is snap shot from a micro array data used in computational biology a lot.\nAnd so, making sense of this kind of data its requires not only new computing techniques,\nbut also significant understanding of the domain in which you are operating, and therefore\nwe have to work about close being collaboration with the this scientist; who are handling\nthis kind of data.\nAnd data analytics cannot be performed in isolation here.\nSo, lastly I would like to talk about link data; and this is essentially data that comes\nwith some kind of structure, but not the kind of structure that we are normally used to.\nthis is data and that comes with the network structure.\nSo, we talking, you can talk about the entity; let us look at the small close up of this.\nSo, each of this nodes here this very complex graph and each of this node here is essentially\nan entity; and the links tell you how are these entities connected.\nSo, such large volumes of link data which give you relationship between entities, already\navailable in the public domain.\nAnd apart from that many sources of data that you generate now a days, have this kind of\nlinks structure associated to it.\nSo, now the question is how would you mine such large volumes of big data?\nSo, variety that is tremendous lot of variety and each of these come with its own challenge\nand quite often significant requirement for domain knowledge.\nSo, on the next V, we look at this veracity.\nOne of biggest problems in social media data is figuring out if the data that is given\nto you is true or not.\nSo, the falsification data I am pretty sure that many of you have actually given false\ninformation to create email ids and other things online, and this being cases of people\nmaintaining multiple profile on Facebook; and so, that selectively share information\netcetera.\nSo, it is hard to disambiguate when you are giving false and when you are giving true\ninformation.\nSo, a significant amount of resources are spent by many of these companies in order\nto verify the information which is provided to them.\nOr if you take sensor of data, data could be noisy, that could be missing values.\nSo, how do you account for all of this?\nAnd it is how do you trust the data that you are getting from the sensor is it; or we sure\nthat the sensor is not malfunctioning.\nSo, how do you account for that?\nMaybe there are few sensors; you can do that manually, but suppose you are talking about\nmillion of sensors diploid over high rise building.\nThen the question of manual verification is moved, just cannot do that.\nAnd talk about how biological data is one of the biggest sources of future problems\nfor us, but even though there are huge volumes of data available more often than not.\nThese are estimated interactions, estimated data; and again you are not sure about the\nveracity of the data.\nnot sure with that lets if you say protein interact with another protein, you are not\n100 percent sure interaction happen.\nYou can say with 80 percent confidence, I am sure this interaction happens.\nSo, in such cases how would you handle this kind of unsure data?\nSo, this is just example.\nSo, if we take any source of data coming from any kind of domains will always find that,\nthere are issues of noise and trust worthiness in the data.\nSo, the verification the people still work with this kind of data already.\nWe can, some of you if you already worked or working in some kind of a data analytics\ncompany you know that; you have to work with this kind of data already.\nBut then verification becomes hard due to the scale that we are talking about.\nSo, current technology people typically end up doing some kind of internal consistency\nchecks and some amount of manual verification but, we need something more scalable to work\nout this a big data scale we are talking about.\nAnd at the last V, I wanted to mention very briefly this whole idea of value.\nSo, I have all this big data; how do I monetize the big data?\nSo, we have to think about problems where substance are returns or higher than the investment.\nAnd now, the investment is no long at trivial because we really have to put in the lots\nof resources in order to just manage this data at the scale; and then run analytics\non top of it.\nSo, all the 4 V’s play a role here, that is so basically have to figure out what is\nthe right balance interns of the effort, that you put in and the kind of monetization that\nwe can do.\nSo some example which people actually try out or on recommendation is trying to build\nbetter recommend assistance for different domains.\nLooking at sentiment analysis trying to get a sense of what people feel about new products;\nmay be movies or what do people feel about a particular candidates in election.\nSo, we are looking at that and then other examples include business process optimization,\nsurveillance, healthcare, smart cities; there are many many domains in which people have\ntrying to find out monetization for big data.\nSo, in the next module we will look at some of the challenges in running data analytics\nat the scale that we have talked about today.\nThat brings us to end of this module.\n \nBig Data - A small introduction (contd)\nHello and welcome to the 2nd module on Big Data.\nThat is we saw earlier that the variety and the volume of data; requires has to have a\nnew paradigm for handling all the computing.\nAnd one of the most popular paradigm that is used for handling big data computation\nis Map-Reduce.\nSo, this was a programming model that was pioneered by Google which were reusing initially\nproprietary implementation.\nBut later on a Hadoop an open source platform, that implements Map-Reduce became very popular.\nAnd so, in the next few slides I will give you a very brief on introduction to Map-Reduce.\nSo, Map-Reduce as we will see, is a very intuitive approach to parallelize computation typically\non commodity hardware.\nBut the big problem when you are using, especially using commodity hardware is that machines\ntend to fail a lot.\nSo, fault tolerance becomes a big problem.\nSo, for example Google runs typically a million machines in one of the data centers and assuming\non average 3 years lifetime for 1 machine.\nYou would expect about 1000 machines to fail per day.\nSo, Hadoop is a distribution frame work, that fundamentally uses Map-Reduce as that computing\nparadigm, but on top of it takes care of data distribution, communication, and fault tolerance;\nand makes it very convenient to use this kind of a distributed; set up and solving everyday\nproblems.\nAnd hence Hadoop is being wildly deployed commercially, and variants of that are continuing\nto be very popular even today.\nLet us look at the Map-Reduce computing model.\nThe Map-Reduce as you can imagine consist of 2 stages: the map stage and the reduce\nstage.\nAnd in between there is a group and redistribute phase.\nSo, the map stage, so the each mapper which runs on a individual machine takes a block\nof the data; that is that you have to originally process, extracts some information from the\ndata and output these as key value pairs.\nSo, the key as you know, as identifier the value is any value that you would like to\nsend along with the key.\nSo, we will look at examples later.\nAnd then before sending to the reduce stage, we sort and shuffle all of these key value\npairs and you group them by the keys.\nSo, that all the pairs which have the same key values, will go to the same reduce stage.\nIn the reduce stage you essentially compute aggregate statistics corresponding to the\nkey.\nWe could add things up; we could summarize them, we could transform them, we could do\nany kind of filtering that you want based on the key, whatever it is we can compute\naggregate values based on the keys.\nLook at this an example of a very simple word count, it is like the hello world of the Map-Reduce\nprogramming.\nSo, let us say that you have a document.\nSo, what you would do is you would divide the document into different blocks as shown\nhere.\nSo, the 1st block I have colored it red, the 2nd block green and the 3rd block blue.\nAnd we send it to the different mappers.\nSo, what the mapper does is?\nIt reads each word in the document, that outputs that as a key and the value as the count of\nthe word in the document.\nSo, every time it encounter a word, it just going to output it as that word with the count\nof 1.\nSo, the red mapper is going to output; concept, one, of, one; and then the green mapper is\ngoing to output variety of things on this case is going to say for one, and concept\none.\nAnd the blue marker again outputs for one, various one, weighted one so and so forth.\nSo, in the group stage what happens, you group things by the keys.\nSo, all the key value pairs which have concept of the key get grouped and they are send to\none reducer, likewise all the key value pairs which have for as a key get grouped and are\nsend to another reducer and so on so forth.\nAnd finally, the reduce stage aggregates all these counts regardless of which mapper they\ncome from and then outputs the total count.\nSo, in this case you would get (concept 2) (of 1 for 2 various 1 and weighted 1.\nThis is very simple way of doing word count and your document could be very very large\nas long as we have sufficient number of machines we can compute this very efficiently.\nThat is not only in working with documents and the other things.\nEven simple items, even simple computations is you would have thought they were straight\nforward can become complicated when you are dealing with it scale.\nSo, here is an example I would like to compute degree of a node in a graph.\nOn very large graphs computing even degree of a node becomes little harder.\nWhy is that, because the data itself is not available to you at one time.\nSuppose I am trying to build the graph as of who called who.\nSo, I am going to get things like A called B at time t and spoke for m minutes.\nSo, that is an event that arrive to me after the call has finished.\nSo, at no point of time do I have all the calls of A, stored in some place and we just\ngoing to find the degrees.\nIt is not like a single graph that already being constructed firm.\nOr if you could think of trying to create some kind of an interaction graph on Facebook;\nso user x posted on the wall of users y.\nNow these events even if they are available to you post facto that are so numerous that\naggregating them and to finding the degree of the graph is could take a while.\nSo, you could actually use Map-Reduce to efficiently aggregate the each event into a graph.\nHere is a simple program that would do now.\nSo, the mapper essentially takes each edge event, each event, each interaction could\nbe the posting of a message on Facebook; could be the making of a call, takes each of those\nevents and then it creates 2 key value pairs for every event.\nThat has of A call B it will say, A 1 and B 1.\nand on the reducer essentially now takes in every event or every key value pair that have\nthe same key; that essentially means is going to gather all the node A’s.\nAdd up the events corresponding to the node A it will output the degree of node A. Fairly\nsimple, fairly straight forward and so you do not really have to create a adjacency matrix\nor adjacency list representation of your graph.\nWe will be able to answer to queries like beginning.\nWe can stick with the edge list representation and still answer these kinds of queries.\nSo, let us look at some other questions that not necessarily need Map-Reduce; but become\nharder when you are looking at large data.\nSo, you looked at k nearest neighbors in one of the earlier modules.\nSo, how would you find k nearest neighbors, if you have huge volumes of data?\nSo, linear search is impossible; because any index structure should be small enough to\nfit into memory for you to do linear search.\nThat is not going to happen if data is very large.\nSo, there is a new approach for finding neighbors in data call locality sensitive hashing introduced\nby Andrei Z.\nBroder and others.\nSo basic idea here is to, find hash functions.\nSo, you remember hash functions, they are functions that take a key as an input and\nthen the hash it into one of n buckets.\nSo, here what we do is?\nWe want to find hash functions, such that 2 elements x and y, if their distance is less\nthan a certain threshold the hash to the same buckets or same bin.\nTwo elements, x and y; if the distance is greater than a certain threshold, then x and\ny hash to different bins; and we would like this to hold with very high probability.\nWe would like this to hold for sure, but then it is hard to get something that will work\nalways.\nSo, you would like to hold for this to hold with very high probability.\nSo, now, if I want to find if y is nearest neighbor of x, then all I need to do is look\nthrough the bin into which x hashes.\nAnd since this is only with very high probability, so you might actually miss your nearest neighbors.\nBut you will certainly get some neighbors that is close enough because since the data\nset large, even close neighbors are usually sufficient.\nSo, depending on the kind of distance function that, you want to use on your data.\nIf you remember we talked about different distance function in the various neighbor\ndepending on the distance function you want to use on your data; you are going to have\nto define different hash functions.\nAnd so for example, if you want to look at distance measure between sets; this has groups\nof words and so on so forth.\nYou probably like to usage the Jaccard distance, which is 1 minus the size of intersection\nby the size of union.\nOr if you could say Euclidean distance between points or between vectors you could want to\nsee cosine distance; and for all of these people have worked out what are appropriate\nlocality sensitive hash functions.\nSo, we are not going to get in the details of the locality sensitive hashing, just wanted\nto give you a feel for how hard it can be, when you are looking at large volumes of data.\nEven what you thought are simple operation become harder, when you are looking at large\nvolumes of data.\nHere is another example; we talked about frequent pattern mining and association rule mining.\nImagine counting frequent items in amazon’s transaction data; we have millions of transactions\na month.\nSo, just blindly running apriori is just not going to work.\nSo, we need a different approach to the problem.\nWe can still use apriori property, but we have to think of how you would handle the\nmemory more efficiently.\nSo, here is a very simple approach, to do distribute counting; you divide baskets randomly\namong compute nodes.\nSo, here we are talking about market basket data.\nSo, essentially what I mean here is the transactions are randomly divided among all the nodes you\nhave.\nYou run apriori in each computing nodes separately.\nAnd whatever turns up as frequent item sets in each of those nodes are now candidate frequent\nitem sets.\nSo, what we have to now do is, go back and count the actual frequency of these candidates\nitem set; because the original candidates were determined on a small subset of the data.\nYou will have to go back and count the frequency on the entire data to determine if these candidates\nset are frequent.\nSo, remember that when you are doing this in the distributed fashion the threshold that\nyou are using for defining frequent item set should be lower.\nSo, if you are splitting your data at 10 ways then the threshold that you had for frequency\nshould be lowered by 10 times.\nSo, once you have this candidate frequent item sets you do not have to count it in a\nsingle machine, you could still do this in a distributed fashion.\nSo, again each node will count the frequency of just this one candidate item set and then\nat the reducer we could basically combine the frequency of frequency reported by each\nnode, and then report frequency of the item set on the entire data.\nSo, there are other computing models; I just spoke about Map-Reduce and this top.\nAnd the other thing has Spark which is Map-Reduce variant with local memory and then there are\nGP-GPUs, which are multi core massively data parallel computations.\nAnd then there are other models with shared memory multi core repetition.\nSo, a depending on what is the use case that you have, we will have to use different computing\nmodels.\nIt is not that Map-Reduces one solution for all method.\nSo, depending on the solution, depending on the problem that you have, you will have to\npick the computing model that the shows you.\nOne thing which I would like to point out is that data visualization is still a challenge\nfor big data.\nSo, visualization is a challenge for normal data analytics, but it is a challenge for\nbig data.\nSo, people are working on adaptable interfaces or interactive visualization, but more often\nthan not people are still trying to fit old visualization ideas to big data and that is\nnot working.\nSo, it is very active area of the research and several new generation methods are needed\nhere; I just wanted to draw your attention to the fact that it is something that you\ncould work on.\nSo, in summary; data analytics has matured to a certain level and so now, people are\nlooking at big data challenges.\nSo, it is some sense evolving to a new discipline of data science, where just the analytics\ntechniques are not themselves sufficient, but we need more understanding from the modeling\nfront.\nIt is very exciting time to be in this space, availability of vast amounts of data and the\ninternet of things like picking up as going to be a lot of work, more data available.\nThe new computing models and that could very well be a high impact on society if you are\nable to come up with solutions, handle make sense of this big data.\nThat brings us to the end of this module.\nClustering Analysis\nHello and welcome to our lecture, our first lecture on Clustering Analysis.\nThis is the first of two lectures that we will have on this subject, this one is an\nintroductory lecture.\nIt introduces the basic concepts and algorithms behind clustering analysis and in the next\nlecture; we will actually go into two specific algorithms and see how those two algorithms\nwork.\nSo, deriving to the subject, what exactly is clustering?\nClustering is the idea of dividing data into groups and we do that, because sometimes there\nis inherent meaning to doing such an activity.\nAnd in other cases, it serves as the first step, it is fairly useful to do this.\nNow, you might notice that you might have come across clustering, with some other names\nas well, which such segmentation or partitioning.\nSo, that is kind of why we have written out all three for you in this first bullet point.\nBut, the core idea is the same, it is fairly interesting, it also partly what terminology\nwind up using also has to do with the context or the background.\nSo, typically something like segmentation is used more in a marketing sense.\nSo, when you grouping people or customers, sometimes people use the word segmentation.\nPartitioning, again interestingly comes more from the computer science community, where\nyou are breaking a graphs, so you are partitioning graphs.\nBut, again the core idea is that you have some data and taking the data and based on\ncertain properties of this data, you choose to group it.\nSo, you can think of it as actually grouping data objects based on various attributes associated\nwith this data.\nThe idea behind such an activity is that, the data itself is represented through various\nattributes or features and some data points are similar to others based on these attributes\nor features.\nAnd you want to group those that are similar and put them into one cluster or group and\ndifferentiate that from other groups or clusters, which are similarly formed based on some measure\nof similarity.\nSo, the core idea is to put things that are similar together and therefore, as a result\nthe different groups are as dissimilar from each other as possible.\nSo, data points within a group are similar and data points between groups as a result\nare dissimilar to each other.\nNow, the core idea the clustering often relates to and sometimes it is often confused with,\nis classification.\nI think an important thing to recognize here is the clustering is primarily seen as an\nunsupervised learning technique, whereas classification is supervised learning technique.\nThe idea is that you have some, in classification you have some input data and you use the historic\ninput data and the specific output and in the case of classification, the output actually\nhas classes, it is a categorical variable.\nAnd, so you used some kind of supervised learning technique to look at the relationship between\nthese input variables and the task there is to make a prediction and assign it a class.\nIn the case of clustering, you are again dividing the input data space in some sense into groups,\nbut the groups are not based on labels that have been explicitly given to you.\nSo, in some sense there is no output variable with clustering and what you have is only\nthe input data space and it is not even clear that, there is another there is a classification\nscheme like in the sense of the, in the classification there was this output variable and this output\nvariable had 1, 2, 3 or more categorical states.\nSo, there were the states that are labels that was explicitly given to you and you are\nnow trying to create that relationship between these labels and the input variables.\nIn the case of clustering, not only that you do not have the labels or the output variable,\nthere might be no meaning to having such labels.\nSo, in some sense with clustering you are really breaking the data, input data set based\non the inherent relationship between data points and how similar they are to each other,\nyou do not have external label that is given to you about the data points.\nSo, if two data points are very similar across these attributes, then you group them together\nif they are not you group them apart.\nSo, in the end you might create a few clusters and you can call them cluster A, cluster B,\ncluster C, but that is not the same thing as classification, which is more of the supervised\nlearning process.\nSo, why really do this and the best way to see that is this brought the two major reasons,\none is clustering itself might just be useful to understand the universe of the data that\nyou are dealing with and we are going to look at some examples in that light.\nThe other reason and sometimes the clustering is used is that, it serves as a precursor\nto further data analysis.\nSo, it has some utility from a machine learning sense itself to do a clustering.\nSo, now, let us look at the first case, which is that in some sense you get a better understanding\nof a data when you do clustering.\nA fairly common example of this is in marketing or sales, your business is for instance collect\nyou know lots of information about a customer.\nSo, the way you should be thinking about it is, each data object is essentially a customer\nand the customer has for instance various attributes.\nThey could be things like gender, age and you know it could extend all the way to the\nkinds of products of the person tensed to buy and so on and so forth.\nAnd you have a full data set of a lot of customers or potential customers and you might be interested\nin grouping that.\nSo, there is no explicit output variable, but certain types of customers are very similar\nto certain other types of customers.\nThey could be very dissimilar to the third type.\nSo, creating groups out there could help for instance like a market research initiative\ninto looking into a particular group and coming up with ideas, so how to specifically target\nour market to them.\nAnother example you know, another very common example is in terms of just communicating\ninformation; take a simple example such as Googling or you know searching for a movie\non the internet.\nNow, there are lots of things related to a movie.\nFor instance, there could be a reviews of a movie, they could be trailers and videos\nof a movie, they could be a ratings of a movie and they could be information on which theatres,\nthe movie is running in or where you can purchase a movie.\nNow, a search itself, the first search itself across the web might give you a huge large\nbucket of things that could belong to any of these categories.\nBut, if you have an ability to for instance look to see which pieces of information are\nsimilar to each other, you might be interested in representing one of each type.\nSo, you might have a cluster; that is created, which talks about essentially reviews of a\nmovie and another cluster that gives of potential links that talk about, where this movie is\nplaying.\nAnd, so in some sense you have you could create this clusters and present the viewer or present\nthe person, who is searching with representative of each cluster or typical value in each cluster.\nSo, that a person, who is searching for a review of the movie does not get 20 links\non the first page that give him trailer, give him or her trailers, which should be quite\nfrustrating.\nSo, in terms of information retrieval, communication of the information it might be fairly useful.\nClustering is used a lot in biology mainly in taxonomy, where if you just said the wild\nuniverse of mammals or insects or something like that, you kind of want to group animals\nor mammals that are similar to each other and give them some taxonomy that is different\nfrom animals that are different from each other.\nAnd here for instance each animal would be the data object and the attributes would be,\nyou know various animal related attribute such as, how they feed, what their family\nor genes or species and so on and so forth.\nIt is also used in climate, many times understanding ocean temperature ranging from ocean temperature\nto hurricanes to various other things, it can be better done you know if you cluster\nthe data on weather patterns.\nMedicine of course, again a certain types of disease similar to other types of disease\nor certain medicines similar to other medicines, it can simplify the word in some sense and\nhelp people understand, how for instance certain medicine interact with certain disease and\nso on and so forth.\nNow, these give you an understanding and these kind of talk about, where clustering is useful\nor has been used or just to get a better sense of the data.\nBut, there is this completely different and other utility sometimes to clustering and\nthat is mainly in the form of serving as a precursor to further data analysis.\nThe idea here is that, clustering for instance can be used as a great way to summarize data.\nEspecially, when the algorithm that a person needs to use.\nLet us say a person is interested in performing some form of regression or another more complex\nsupervised, unsupervised learning tasks.\nSometimes with more and more data the task just becomes computationally harder and harder\nand it might be sometimes beneficial to perform a clustering analysis on the data which could\nbe in some cases it is computationally easier to do the clustering.\nAnd then, just have a representative from each cluster as a data point, so you could\nhave a representative or you could have essentially a cluster center the clusters representative.\nSo, was the data point for all the data points in that cluster?\nAnd, so you are now, doing that same machine learning task be it a regression or a factor\nanalysis.\nSo, whatever it is you are really doing it on the prototypes on the representatives rather\nthan the full data set.\nIt is also used an extension of that is also how it could be really useful like say in\na nearest neighbor task.\nWe in an earlier in earlier lectures we spoken about this algorithm called KNN: K nearest\nneighbors when you think about the problem that is involved with that for any given point.\nIf you need to find it is nearest neighbours you need to actually try and look at the distance\nbetween the point under question and every other point in the data set.\nSo, you need to evaluate the distance between the point you are interested in and every\nother data point and then pick the K closest neighbours.\nNow, that can become computationally very, very, very hard and you need to keep either\nthe memory and you need to do the computation from scratch, a simpler approach could be\nthat you just take the K you do the clustering on the data.\nAnd if you look to see, which cluster center is closest and once you identify cluster center\nthat is closest you now, take your point under question and only evaluate it is distance\nto all the points in that cluster.\nAnd the assumption here is that the points the points under that cluster are the once\nthat are going to be closest to this point, because this cluster center was the closest\nfor instance.\nIt is also used in certain other forms of you know compression of data specifically\nsomething called vector quantization, which is used a lot in image or sound or video data,\nwhere you typically find that in a particular image, if you break it up into pixels there\nis the whole host of data points that will look very, very similar to each other.\nLet us say you take a photo of a person almost 20 to 30 percent might be the background behind\nthe person and that might have the same color.\nSo, it would be more efficient to just acknowledge that there is a very good chance that the\npixel on all four sides are going to be the same color as this pixel.\nAnd, so you kind summarize the data you reduce the data to few a number of pixels in a memory\nvalues and; obviously, would this the some loss of resolution either some loss of information\nessentially, but that might be acceptable then the data size itself get substantially\nreduced.\nSo, we spoke about, what clustering is and why use it, now let us just briefly talk a\nlittle bit about the different types of clustering.\nThe major classifications tend to be one hierarchical versus partitional and the major idea here\nis that hierarchical clustering is essentially a nested form of cluster.\nSo, think of it this way you can start with this one mega cluster, which is essentially\na single cluster of all the data points and that is on one end of the scale and then,\nyou go to the other end of the scale where you have as many clusters as the number of\ndata points and each data point it is a cluster is its own cluster.\nNow, some where hierarchical clustering works on creating this tree between a single cluster\nof all data points to each data point being its own cluster now either of these extremes\nare useless.\nBecause, the single cluster of all data point is not really clustering.\nYou just created you put all the data points in one group and its called it group one that\nis not very useful.\nAnd neither is it useful to call each data point its own group.\nSo, somewhere in between these two extremes is the real value at, but hierarchical clustering\nworks on some form of nested or kind of tree form of clustering when you start with this\none you can you can start it either end, but you might start with one large cluster and\nthen, break that into two.\nAnd now, you have these two clusters, now you go into either of these two clusters and\nbreak that into created division there.\nSo, now, you will have three clusters, but the three clusters is strictly a division\nof the two clusters.\nSo, it is not like you are going to take some points from cluster when you have two clusters.\nLet us say you had some cluster A cluster B it is not like you are going to take some\npoints from cluster A and some points from cluster B and call it cluster C. It is in\nthat sense nested it is in the sense that you make one split and very, very similar\nto decision trees you keep making further splits and this is contrasted to an approach\nof partitional clustering.\nPartitional clustering is not of this kind of nested approach it is just that you explicitly\ndecide on the number of clusters in some sense and you go and partition the data.\nits it is It is simply a division of the set into non overlapping set, so it is essentially\nclusters.\nAnd, so in that sense there is no tree diagram or anything of that nature with this.\nSo, for instance a partitional cluster that if I decide to do the partitional cluster\nand create four clusters and I contrast that to a partitional clustering approach, where\nI had three clusters.\nThe four, need not be a further division of the three they just completely you know the\none that said decided to break in into four has a very little or no relationship theoretically\nto the division the separate exercise of the separate effort into creating a partitional\ncluster with three clusters, whereas the same cannot be said for hierarchical.\nBecause, hierarchical went in sequence, it could have started top down or bottom up meaning\nyou could have started with all in hierarchical all the individual clusters, where it is each\nday you know each cluster is a data point and then started grouping them or the other\nway around.\nBut, essentially with hierarchical one is nested into the other and so on.\nThe other major type of clustering that people talk about is exclusive verses overlapping\nversus fuzzy.\nThe idea here is with exclusive clustering each object is assign to a single cluster\nand that object therefore, cannot be assigned to another cluster.\nAnd, so there is no there is no notion that one can simultaneously belong to multiple\nclusters, where as in overlapping, in case in clustering algorithm that allow for over\nlapping, you can.\nBasically it is not exclusive a data point can choose to belong to more than one cluster\nat a given point and decision on which, of these are really depends on the underlying\nsystem.\nAnd some cases it might just make sense to have some data points belonging to multiple\nclusters and in some in some cases that that notion of division is just not sensical.\nAnd finally, you have you have the notion of fuzzy clustering fuzzy clustering kind\nof takes this over lapping even further, where each data point is not really assigned to\na cluster, but it basically gets a number between 0 to 1 that that talks about the weight\nassociated with that data point belonging to the different clusters.\nSo, for a data point each data point gets total weight of 1 and it takes that data that\nweight of one and says, I am going to assign 0.3 in belonging to cluster A, I am going\nto assign 0.7 in belong to cluster B and I am going to assign myself 0 belonging to cluster\nC. So, the constrained is that the sum of its weights in terms of belonging to the different\nclusters adds up to 1, but it can use its weight of one in any way chooses to belong\nto the different clusters.\nSo, that is to give you some idea between of exclusive versus overlapping versus fuzzy\nthe last that is worth mentioning is you can also have a complete process partial clustering.\nA complete clustering basically just assigns every objects to a cluster, where as a partial\nclustering does not it starts with the data points chooses to you know cluster as many\npoints to different clusters and data points that do not really help in terms of belonging,\nso clearly to given cluster just not cluster they are just left out.\nAnd these might and the and the motivation there is that you are more interested not\nin kind of pigeon holing each data point into a cluster, but you are more interested in\nthe cluster formation itself.\nAnd there you do not want data points are not, so clusterable, that do not really belongs,\nso clearly into 1 of 2 clusters to kind of ruin the cluster center or ruin that nice\ndivision that you created.\nSo, these are set different types of clustering, now the algorithm themselves and these are\nmore descriptive of the type of algorithm that goes into it.\nNow, another area focused could be the kind of clusters that you are forming and while\nthis has everything to do with the algorithm also here we are just looking at the end product.\nSo, the algorithm that we are using what kind of end product does it can it give you.\nSo, there is the first one is the well separated idea the idea the well separated is that you\nwant to create clusters, where get the objects where each object is similar to every other\nobject in that cluster.\nSo, the way you defining well separated clusters are is more from the core idea that you are\nvery interested in looking at the relationship between each data point with respect to each\nother data point and you are measuring or your quantifying the clustering itself by\nlooking at how similar that data point is to that other data points that are in that\ncluster.\nAnd therefore, how dissimilar this data point is to the data points that are in the other\nclusters and this kind of thinking you know is very useful especially when the data itself\ncan be very nicely separated.\nWhen the distances between the clusters are fairly significant, then this conception becomes\nvery useful.\nThe prototype based approach really talks more about how each data point is close to\nits cluster representative.\nSo, that the commonly used terminology is to the proto type that defines the cluster\nand when I describe it I can try to think of it as there is a cluster and there is there\nis the main representative of the cluster, the prototype the one that kind of signifies\nthat what the cluster is in the center of that cluster.\nSo, in the prototype based approaches, because its representative is in some sense so central.\nIt is also sometimes called center based clusters, but the idea is as following, where each data\npoint is looked at more from the prospective of how close this data point is to the cluster\ncenter to the prototype.\nAnd, how far this data point is from the prototypes of the other clusters or the other representatives\nand the classification is done in this fashion.\nWe then, have the graph based clustering and this is really useful if the data is represented\nas a graph and you have nodes, which represent each data point or object and you have this\nlinks or edges, which represent some notion of connection between the data point.\nAnd this notion of connection could be the one that talks about how similar a data point\nis to the other data point you could have some kind of a threshold value or especially\nwhen sometimes you are variable itself is not quantitative variables.\nSo, you could have some notion that two data points are connected and the idea here is\nto do some graph based clustering, where you really looking for a high density of connection\nthis between the data points that belongs to a cluster and a very low level of connectivity\nof the data points between two clusters.\nAnd for that reason you know this is whole language that comes from the graph theory\ncommunity, where you define things calledÊcliques, which essentially just means that at the set\nof nodes in the graph of a completely connected to each other.\nAnd, so a lot of that that kind of clustering ideas that go in to graph based clustering\nare once that kind of look out for cliques and say these guys are all connected to each\nother, so they must be a cluster.\nFinally, you have density based clustering the idea behind density based clustering is\nthat a cluster is essentially a dense region of objects that are surrounded by regions\nof lower density.\nSo, the idea here is that, because there are not such well defined clusters like in the\ncase of the well separated idea that you are not really looking too often do a complete\nclustering and there is a lot of noise in the data and you know the clusters themselves\nare irregular or intertwined and there are lots of outliers and so on.\nSo, the idea here is that you acknowledge all of that, but you just try to identify\nspots of extreme density, where once you identify that spot that dense region becomes a cluster\nand so that is fairly useful in defining a cluster, where there is a lot of noise and\nso on.\nWith that we will conclude our lecture on lecture that introduces clustering and the\ndifferent types of clusters and where it is useful and so on.\nIn the next lecture we will looking to basic algorithms one is the K means algorithm and\nthat really belongs to that partitional camp and we will look at another algorithm called\nthe hierarchical clustering algorithm which belongs to the hierarchical camp.\nThank you.\nClustering Analysis (contd)\nHello and welcome to our second lecture on Clustering Analysis. In the first lecture,\nwe introduce the idea of clustering, differentiated it from say classification and other supervised\nlearning techniques. And we explain, what clustering is, how it is useful, where it\ncan used and also gave you brief overview of the different types of clustering and the different types of clusters. In today’s lecture we are going to introduce\ntwo very popular clustering techniques. The first is K mean clustering and the second\nis called the hierarchical clustering, where both these techniques are fairy old, this\nstill enjoy immense popularity in terms of being actually used. The first one, the K\nmean clustering and this choice of K mean and hierarchical, you should find to be also\nfitting. The overall, the first dichotomy that we used when we talked about clustering\napproaches, which is partitioning based approaches and hierarchical approaches. So, the K mean\nclustering is essentially the partitioning based approach. And, what we will do is, we will dive into the algorithm. So, we have already mentioned\nthat is a partitioning based approach. So, it is one, where you partition the entire data set into K clusters essentially and you, it is also very useful to think of the K means\nas a prototype based approach, where that is also something that we discuss in terms\nof how clusters are formed. And the prototype based approach is one where, there is like\nthis representative for each cluster and you use these representatives in some fashion\nto express, what a cluster is and to also form the clusters.\nSo, it is essentially this prototype based approach, where you create K clusters and\nit is also noteworthy that it is an iterative procedure. So, even right at the end of the\nfirst iteration you already have K clusters and they might not be good clusters, because\nit is just the first iteration. And then, like most optimization procedures like steepest\ndescent, any of these other iterative producers you will find that over time you are just\nrefining a solution and at some point, it does not make sense to refine any more. You\nare not, your clusters are not changing essentially prototypes or not changing either location\nor who they are and, so you stop at some point. This procedure is ideal when all variables\nare quantitative, whether what we really mean is that you should be able to take each data\npoint and you might have some other data point or some other location and your location is\ndefined across the multiple attributes associated with the data point. So, a data point, again\nthe conception you have in your mind is these rows and you are basically grouping data points,\nthat is your job. And each data point is described by some attributes, which are these columns in a table and each column means something. So, with the K means procedure, what you doing\nis you want to say that that you want to take this conception and you want to actually,\nyou want actually be able to compute distances between two points across these dimensions, which means that attributes themselves need to be quantitative. This in a lot of ways\nto remind you of this K nearest neighbors, examples that we took up especially in the\ncase of K nearest neighbors regression, where each of these dimensions for those each data\npoint is a continuous quantitative variable. And once you do that you just able to be compute\ndistances, typically whenever it comes to computing distances we always use Euclidean distance, you could also use other measures of distance and you would actually, it would\nstill be a K means procedures, but with different approach, so common once are Manhattan distances,\nEuclidean distances and so on. So, how does it really work? So, let us just go through\nthe algorithm in some senses and then, we will see a graphical representation of it, the idea is to initialize some clusters centers K.\nSo, what you mean by initialize cluster centers? What we mean is, essentially think of this\nand it is easy to think of it in a graphical sense or you can think of it in a mathematical sense, but the idea is that each data point can be expressed in terms of an x axis, y\naxis, z axis, w axis, so on and so forth, depending on the number of attributes there are that represent each data points. Now, while each data point is represented based\non these different attributes, you can create a cluster center also on these attributes.\nAnd often it really make sense to put cluster centers, where you think the clusters are going to be form, but you see how even if you do not do that perfectly sometimes the\nclustering the K means clustering will adjust for it. But, the most important thing to take\naway from this is that, the K means cluster algorithm therefore, could be sensitive to\nwhere you place the cluster centers. If you do not place them in sometimes the right places,\nyou could get to a solution, which is not necessarily a good solution.\nSo, the idea is that you initialize some cluster centers, so specifically you would initialize\nK cluster center, so you will, because it is K mean clustering. So, K is usually a number\nbetween 2 and may be 10 or 20 depending on the… The higher limit is a little more vague,\nmost problems you looking at between 2 to 6 or 7 clusters at the most. But, there are\ncontexts, where your data set is really large or what you intend to do with the clustering\nis such that you do not mind having more clusters. So, the algorithms initially you drop in K\ncluster centers and then, what you do is you take each data point and if you figure out, which of this cluster centers is closest to the data point and then, you assign the data\npoint to the closest cluster center. And, so essentially it is like if you kind of give\nnames. So, just to give you some inside, this is where we are, where the second bullet point.\nSo, what we are doing is we basically saying if you wanted to give those K-cluster centers name as cluster 1, cluster 2, cluster 3, then what we would do is, we would go to each data\npoint and say, which cluster center is closest to you and then, I am going to assign you to that cluster center. Now, once all of these points have been assigned, what we are going\nto do is we are going to recompute the cluster center to be essentially the centroid of this\nassignment of data points derived from step 2. So, what we are going to do is essentially in step 2, what you did is you assigned a\nwhole bunch of data points to different clusters. So, what we will do is, we then, for instance\ntake all the data points that were assigned to cluster 1 to the centroid 1 essentially,\nto the cluster center 1. And we gave them all the labels that you are assigned, the\ndata point x you are assigned to the cluster sector 1, data point y you are assigned to cluster center 1. So, all of them that were assigned to cluster\ncenter 1 we take those data points and compute the centroid of those data points. Centroid\nessentially is in many senses like an average. It is, again it depends on exactly what measure\nof distance you choose, but if you are choosing Euclidean distances, the centroid is essentially like the average. And we say it is the average of these data points, but it is called the\ncentroid, because it is the average across all those dimensions. The number of dimensions are the number of attributes, so across those dimensions, what is the center.\nSo, once all these points have been assigned you kind of compute a new cluster center based\non the centroid. And it is essentially like you forget the old cluster center and now, this new cluster center is your cluster center and you do this for each assignment. If 10\ndata points were assigned to the cluster center 1, then you do this step for cluster center 1 data points. And then, you go to the next 15 data points set for perhaps assigned to\nthe cluster center 2. So, for these 15 data points find out the new centroid and call that the new cluster center. So, you got new K set of cluster centers.\nWhat do you do next? You iterate, for these new K cluster centers you go to each data\npoint and it is almost like you forgot, which cluster center that data point belong to,\nbecause remember that data point was assigned to say cluster center 1 or 2 based on the\nold centroid, based on the old cluster center. Now, because your cluster centers are moved\nbased on the centroid, you now assigned them all over again and once you done the assignment all over again, you find the centroid. Once you find the centroid, you do the assignment\nthat is how it is an interactive process. Essentially you repeating the steps 2 and 3 until the centroid is not moving any further or in some cases, you might say the centroid\nis moving by an amount smaller enough that is within your tolerance.\nSo, this was fairly abstract in terms of bullet points, so let us actually take a graphical\nlook at this if you taken an ultra simplistic example, where there are only two attributes.\nSo, let say attributes is x 1, x 2 always remember that with clustering, which is unsupervised\nthere is no y there is no output variables, what we doing is trying to grouped the data and here is the data. The data is this blue squares and you are trying to grouped this\nsquares. Now, fairly naive look at this can kind of\nmake it obvious that perhaps this is one cluster and this is one cluster. So, the blue dots\nare actually the data points, now that is to the naked eye and the things to remember\nare that you know if for instance there were more dimensions that were there were more\nattributes beyond x 1 and x 2 it would be harder to show you this visually. So, that\npoint the math kind of take over in 3, three dimensions I can show you x 1, x 2, x 3, but\nafter that whatever I do if you have more attributes. So, the squares are essentially the data points and let say the K means clustering algorithms\nstarted with two centroids here obviously; that means, K is equal to 2. So, you initializes\ntwo centroids and you know this like I said the algorithms itself to some extend could\nbe sensitive to how you initializes the centroid. So, there is no right way and the wrong way. Ideal would be if you could actually plot the centroid in the middle of the clusters,\nbut often we do not know that yet that is why we are doing the clusters. So, what is the first step in the clustering algorithms the first step is to take each\ndata points and see, which cluster center is closest. So, let us say we take this data\npoint 1 clearly this the yellow cluster center is closest and at least hope for the set of\ndata points is yellow is closest. Obviously, for another set it looks like the green is closest, so each data point basically gets an assignment either gets assigned the yellow\ncolor or a green color and that is essentially, what I have done. This diagrams are more conceptual\nnot the scales. So, I kind of high balled it and said it looks like this 4 data points are close to the yellow that is this 3 this 4 are close to the green,\nso that is how they have been assigned. What is the next step? The next step is given this assignment the new cluster center for the yellow data points is the centroid of the\nyellow. So, what is the centroid of the yellow the, so you can think of it as the essentially\nthe average of the yellow and average; obviously, needs to be on this axis as well as it needs\nto be on this axis. So, where you would centrally places yellow, so that is you know minimizing the squared deviation, which is Euclidean distances to\neach data point and that is the definition of centroid. And, so we move the yellow yellow\ncircle to be the new centroid and it is probably going to come somewhere here and the green\ncircle, now the green circle has a little bit of problem and it just cannot move to the center of this space out here. Because, it is; obviously, has an assignment.\nSo, this is going to bias where the green moves, so perhaps it will move somewhere here and that is what I do in the next step actually move the yellow and the green. But, once I\nmove it is like the assignments are completely lost you forgotten the assignment, because\nremember that the assignments were made with the old centroids. Now, with the new centroids you just have the new cluster centers and no assignments\nyou redo the process when you redo the process you see now, which blue dots are closest to\nthe yellow and which ones are closest to the green and we repeat the process and as you can see we have already gotten pretty good results out here with the yellow and green\nclassification, which matched our intuition about what the two clusters of this graphs was. Clearly in the next step for instance is green\nwill move into this cluster center the assignments themselves won’t change too much perhaps\nthe yellow will move little bit out here. But, essentially after maybe one or two more steps the centroids will stop moving your assignment will probably the same. So, even\nif you stop the 1 or 2 stages earlier you would have gotten the clusters that you were interested in. So, I hope this gives some idea, what the\nK means algorithms is, now another very popular prototype based approach, which can work and\ngo beyond K means in some senses, which can go beyond a quantitative attributes. Because,\nremember for you for this whole algorithm to work you had to able to be compute distances\nand it is makes senses that you attributes x 1 and x 2 for quantitative and continue straight x 1 and x 2 there is a little there is a medium there is more and this is continuous\nquantitative variable, now many times you do not have that.\nSo, very useful alternative is K medoids in that case, now K medoids allows you to go\nbeyond the quantitative variables. So, when you have categorical or rather more importantly\nnominal variables, where the variables is are things like male, female and the attributes\nis like male female or things like that you can use K medoids. But, more importantly K\nmedoids allows you to go beyond attributes altogether, where x 1 and x 2, where attributes\nto a point, where all you need is some dissimilarities matrices.\nWhat do you mean by this dissimilarity matrices? What I mean is you have all this data points, let us start calling them 1, 2, 3, 4. Now, with K means the distances between and I am\ngoing give you the same data points in the columns, now with K means I can compute the\ndistances between the data points 1 and data point 2 through some form of Euclidean distances\nand mark a value of x. But, what if, so the whole process of using Euclidean distances\nrequire that I go into each attributes look at how different it is and then compute that square distances takes the square roots. But, what if I did not have that process,\nwhat if I had a process, where I gave you just the differences between each data points.\nSo, I have something like a dissimilarities matrix, so when I have a dissimilarities matrix,\nthat I am giving you and you can get the dissimilarities matrices through completely differences ways\nit can be user survey or it can be something extremely subjective, where you say I feel\nlike 1 and 2 are different by this much and I can tell you how different 1 and 2 are and\nI can tell you how different 1 and 3 are you know and I can tell you how different one\nand four are and so on. But, I cannot really break it down into five different attributes and give it to you that way. So, people do this lot of times in social\nscience research whether some kinds of a question I have that is given to people to tell, so\nplease tell me how different this two are. And people are able to say that there are not able to break it down into a set of quantitative variables and say how different they are in\neach of those dimensions. So, K medoids is very useful when you do not have these attributes,\nbut you just have dissimilarities matrices of how each data points is different from every other data point. So, you will essentially just need some kinds\nof leading diagonal of data. So, the data either on this side or data is on that side, because what is different from two the same amount that two is different from one there\nis no different between those two. So, in cases like that K medoids primarily winds\nup not having this arbitrarily cluster center, because you really cannot any longer compute\nthings like centroids there is no centroids there are no dimensions on which. So, what winds up happening is you nominate A data points to be the prototype and you\nuse the same core concept of K means in that you first nominate a data point and then,\nyou do an assignment, where each data point chooses between the nominated data points. And once you have one assignment you find the new nominated medoids; that should become\nthe representative data point. It is kind of like the idea of being the cluster center.\nNow, while this works an important point is that the computational intensity associated\nwhich such an approach. Now, when you had 10 or 20 data points which\nhad an assignment, so let us take all these data points. The computing the average through\nsum of square just was essentially like using sum of square minimization was essentially\ncomputing an average of this points across each of this dimensions. Now, you do not have\nsomething like that with K medoids once you have an assignment to choose which, for this\nassignment, what should be the prototype requires you to see the dissimilarities between each\ndata point to each other. So, the number of computations that you need\nto assess really grows. So, K medoids is often seen as a very computationally intensive approach.\nSo, this should give you some idea about K means and K medoids. The next approach that we are going to talk about is hierarchical clustering with hierarchical\nclustering you do not really windup fixing fix number of clusters. And essentially the approach itself starts with either one very large cluster and breaks it down step by step,\nso the first one, so one way of doing it is called the divisive way, which is you have\none large cluster, which has all the data points in it is and then, after that I break it in to 2 clusters and then, after that I break it in to 3 clusters.\nSo, I look at the two clusters that were broken up as choose, how to break them down further.\nSo, and that is called the divisive approach you also have the other approach, which is\nagglomerative and definitely the more popular approach, which is bottom up, where each data\npoint becomes a cluster, so you have all these. So, you have the number of actual cluster\nyou have is equal to the number of the data points, because of each data point is a cluster.\nAnd then, after that you choose the two closest clusters and merge them together and we will\ntalk about how closest is defined. So, you choose the two closest clusters and merge them together. So, the end of the first step you are essentially having we had n data points\nyou having n minus 1 data points, so the end of one step and after that the next step you\nhave the n minus 2. Because, now you have n minus 1 cluster where all of them expect\none have one data point and one clusters has two data points. But, you are just treating them as clusters and you are saying, now order this n - 1,\nwhich are the two closest clusters that I can merge and that is seen as bottom of approach.\nNow, because of how we do this essentially you are going to have nested clusters. Think\nof this divisive approach if you created two clusters. Now, when you go to create three\nclusters in this divisive approach you are going to take either cluster 1 or cluster\n2 and break it further and the others is going to be same, so one break up could be that the cluster 1 stay the same and cluster 2 gets broken up into two pieces.\nSo, in many ways the clusters that you are creating are nested within one another you\ncan think of them is parent and daughter. And this same of approach goes for the agglomerative, where you started with many individuals clusters and you choosing to group them. It is never\nlike you are breaking one grouping you rethinking an action that you did in the previous step\nyou never in some senses going back in time and recorrecting a decision to either group\nclusters or break clusters, so in some sense you can think of this also partly greedy approach.\nNow, we spoke about, how we are going take in with especially agglomerative, which is\nwhat we focus on the rest of lecture, because it is the more popular one and for very specific\nreason that we will talk about you need to take in the first step. For instance you have n data points the n clusters and you need to take two closest clusters and merge them,\nhow are you defining closest clusters. And the way you would kind of define them is through\nsome measure of dissimilarity between the clusters. An example is for instance that there are many definitions of the dissimilarity one\nthe three of them that are listed here really talk about it more in terms of come more from\ngraph theory. And the first one is called single linkage and the idea behind single\nlinkage as the means of talking about dissimilarity is that it is essentially when you take two\nclusters you take the minimum distances between two data points that can be in the two clusters.\nSo, do not think of it as much in terms in the first step with in each cluster there\nis one data points, so it is not a very interesting case. Now, think of a case, where you got\nthis cluster and you got three data points; that is what I have shown here and let me give you concrete example, so here is one cluster and there are three data points within\nthis cluster. Here is an another cluster, four data points five data points. Single\nlinkage basically takes each combination and sees, which combination is minimum, so in\nthis case probably this would be minimum. And, so you know it is for that reasons it\nis actually called min as a definition of dissimilarity. Complete linkage in contrast\ntakes the maximum this is which, of this combination I need you take one from cluster A and you\ntake from cluster B, which combination of points can I take get the maximum distances and I am probably guessing that this connection that is just I drew would have the maximum\ndistances this group average, which basically takes every combination of every point to\nevery other point and takes the overall average. Now, in addition to that you also have some approaches that try the more prototype based ways and there you would for each cluster\ntry to create a centroid and from that centroid you look at the distance from one centroid to the other and so on. And there are some other approaches there is a there is a wards\nmethod, so on, where again trying to take some kinds of a more prototype based approach\nto define the dissimilarity. But, essentially hierarchical clustering it is all of these\nstill come under broad umbrella of hierarchical clustering. So, there are two important things in connection with the hierarchical clustering as you can\nsee, because it is doing this nested clustering, where you start from with agglomerative you\nstart with each data points being a cluster and you keep merging them till you have this one mega cluster of all the data points. And in the other way around with this divisive\nyou start this one mega cluster with all the data points and keep breaking it till each data points is it is own cluster. So, therefore, you have not really committed\nto creating clusters of a specific size K and you can therefore, take you can kind of\nlook at the results over all and make it make a decision in terms of what here size should\nbe. One important property that we see with all agglomerative when you know in some divisive\nmethods is that they possess this property called monotonicity. And the idea here is\nthat dissimilarity between merged clusters is see there is some way of measuring the\ndissimilarity of a cluster. So, that that dissimilarity between merged clusters is monotonically increasing with the level of merger and that should be fairly\nintuitive. When you think of the dissimilarity of a cluster which has just one data points\nthere is no dissimilarity it is only when you have two data points you can say this\ntwo data points at different by this much. So, with agglomerative clustering in some\nsenses the more number of data points that you have in to the clusters the greater of the amount of dissimilarity there and that monotonicity is strictly maintain with agglomerative\nclustering. So, a very useful way of representing it therefore,\nespecially when you have this monotonicity you can graph quickly represent the agglomerative clustering through something called the Dendrogram and what is shown on this slide is Dendrogram\nand it is essentially this binary tree, which is plotted. So, that the height of each node\nis proportional to the value of the dissimilarity between it is daughters. So, what is node\nhere? Essentially you can think of each partition is being a node and the height of this node. So, let us take something very simple height of this nod as to do with this height as to\ndo with the degree of dissimilarity between b and c between see you are forcing b and\nc to be in a cluster now that is what you doing by this part of the graph that is what\nit is doing. This height as to do with dissimilarity of b and c, which is why for instance perhaps\nd and e was merge first. So, with agglomerative clustering think that you are going bottom up and you are sequentially making decisions. And, so if you choose to put d and e together\nfirst; that means, d and e would have been less dissimilar to each other than b and c and that is why d and e was done first and then, b and c was separately done perhaps\nlater. Now, the really good thing about this dendrogram is that, now you have a full picture\nyou can, now choose to say I am interested in this situation where there are three clusters and you can essentially draw this horizontal line. And you have the three clusters cluster\n1 is just data point a cluster 2 is the data point b and c cluster 3 is data point d e\nf and g you basically see what goes in each of this limbs and that is that is essentially\nyour cluster. So, at any level when you draw a horizontal line you can the number of vertical line cuts across is the number of clusters that you\nhave in your thing. So; obviously, if you had a line out here you drew it here this\nis 1 cluster and here it is the 2 clusters does, so, on. So, this should hopefully give\nsome idea about the two very popular algorithms K means clustering and hierarchical clustering.\nIn hierarchical more specifically K agglomerative. Thank you.\nIntroduction to Experimentation and Active Learning\nHello and welcome to our first lecture on Introduction to Experimentation and Active\nLearning. This is the first lecture of a two part series on these topics and in this lecture we intend\nto motivate the use of these techniques as well as the concept of reinforcement learning.\nSo, the reinforcement learning is a lecture, a separate lecture that you will have from\nProfessor Ravindran. And in this lecture we will motivate the need for these topics experimentation, active learning,\nreinforcement learning, why are we talking about them in a data analytics course and we will also briefly introduce the topic of experimentation or design of experiments and\nin the next lecture, we will continue with experimentation and end with active learning.\nSo, let us get into the subject, where we take a broader look into data science and\nanalytics. The core idea is that data science and analytics need data and if you were to go with the big\nbuzzwords now, we might even need big data, you know where we can really gain useful insights\nand this is quite fairly motivated with the easy availability of storage, the easy availability\nto process data and the internet of things generating a lot of data.\nIt is quite easy to get lots of data and analysis the data and come up with useful insights.\nBut, in not every situation do you start with a data base full of data and in not every\nsituation, is it easy to create this data. It might either be costly or it might not be, the data that you might have is not the\nrelevant data that you need, and in many cases, you just have not started the exercise.\nSo, for all those cases the big question is, basically do you have no scope for data analytics\nand, the answer really is that, there is this whole other set of tools and techniques, the\nquantitative tools and techniques where which focus really are not just the analysis part\nof data, but have something to say about what data gives creative, which then goes and gets\nanalyzed. And that is the focus of these lectures on experimentation active learning and reinforcement\nlearning, which is that data science is not confined or data analytics is not confined to lots of data that are already available, but it is moreover an iterative process, where\nsometimes the question of creating the data is also intrinsically looked into this grand\nproblem statement. Now, sometimes we will not even give a second thought to this dichotomy of creating data\nand analyzing data, because some problems just inherently come with this creation.\nSo, if you take a look at many of the, you know inferential statistics techniques that\nwe saw earlier in this course. Let us take an example where we used a two sample t test, you essentially had to sample\nn number from, you know class a and class b.\nSo, and you know compare the means, this whole process or sampling was in some sense creating\nthe data. So, let me give you a concrete example, one of our favorite examples might have then that\nour 10th standard girls. Is a average height of 10th standard girls higher than the average height of 10th standard\nboys and there we said, we need to go to take a sample of 10th standard girls and sample\nof 10th standard boys and we came up with a conclusion that and we did a hypothesis\ntest there. So, when you do a sample, you are essentially creating the data and in other situations\nas well, where you would doing some kind of an engineering experiment perhaps and somewhere\nwe did not really use a word experiment excessively, but the idea is that sometimes we never thought\nabout it, but those are fairly the simpler cases. There are a lot more cases, where you need to explicitly think rule, the creation of\nthe data before any kind of analysis can take place and that is what we will focus on in\nthis lecture. Now, when we speak about creating data, a very important factor becomes whether this\nis an online context or an offline context for creating the data.\nWhat do you mean by that? Online and offline do notÉ Online does not mean that you are in the internet, it means\nsomething different here. What it means is, essentially when you are in a online setting; that means, you are experimenting\nor you are creating data on a system that is currently creating a product or producing,\nyou know involved in performing a service or a task, which is going to the end user\nand for the purpose of getting this data, you are not just starting a passive observation\nexercise. So, you are actually either querying the system or you either playing with the system in order\nto get the data that you need. Now, that can be a problem sometimes, because you are actively interfering with the system\nto create the data that you need and this system is right now either producing a product\nthat is going to the end user or this system is a live system, which could influence the\nexperience an end user is having and I am using the word end user in a fairly broad way.\nIf you experimented on a traffic signal, the end user or the motorists need to go through the traffic signal.\nIf you clear on with a manufacturing process which is producing a product, the end user\nis the ultimate user of the product that goes and reaches the, you know the customer.\nIf it is a process, then again the process itself has some outcomes. It either gives the end user some information or the process itself performs a task for\nthe end user and all of these things could be getting compromised, if you are playing with the live system.\nAnd so, a major area that we would be looking at is the one of the reinforcement learning,\nwhere you are looking at an online system. So, you care about learning, where you care about this learning in a very supervised learning\nframe work, where you have some outputs and you want to understand how these inputs effect\nthe output, that is one side of the story. But, this is second side of the story which is, you do not want toÉ You are interested\nin doing as well as you can, even while you are experimenting. Because, some consumer or end user is experiencing the effects of your experiment and you yourself\ncould be, the experimenter could also be the consumer. So, we are going a little abstract out here.\nSo, but the advantage of going this abstract is that you can really envision any scenario and any domain and this kind of a frame work should apply there.\nSo, we will see that primarily, the online setting or I liked to kind of call it the\nlive setting, live experimenting gets covered in reinforcement learning and those lectures\nwill support that. Now, in going forward now, in this lecture and the next one we are primarily going to\ntalk about an offline setting. What we mean by an offline setting? Just the opposite of the online, which means, that the unit that we are experimenting on\nis not producing products that are going to go to the end user.\nSo, you either created and you can do this in many ways. So, if you want to conduct an experiment and you want to gather data, you can create a\nmodel of your system and go experiment on that model, you can artificially create a\nlab setting. Let us say I want to experiment on what fertilizers work on my fields.\nI do not have to go pour those fertilizers on my fields, I can actually create a green house and choose some specific plans and try these fertilizers out and I can essentially\ncreate this lab setting, which is suppose to mimic the real world. But, it is not always creating models I mean it could be creating models, it could be creating\nartificial environments, it could be creating computer simulations and then you go experiment\non that. But, it could also be the real process except, now if turned off the real process.\nSo, let me give you an example. Let us say you wanted to do an experiment on a machine and this is a machine that used\nto manufacturing and you wanted to know, what how to set various boiler plate stuff on this\nmachine. So, you wanted to know what the speed should be, what the turning radius should be, different,\ndifferent parameters associated with this machine. Now, if the machine is currently manufacturing the product which is going to the end user\nthat is the problem. But, what if you stopped the entire manufacturing process and you said, we are going to now\nexclusively commit some resources to experimentation. So, we learn about this system and so you clear on with the machine, clear on with various\nsettings on the machine, make it you know create products and then you measure the products\nand see how well you did or how badly you did and you learn about the systems, you created the data, you analyze the data, you learnt about the system, but these products that\nare sacrificed in some sense, they are not going to the end user. In other words, you do not care how well you do when you are experimenting and that is\nthe core of offline experimentation. A bad experiment is not one that gives you poor results, but a bad experiment is one\nwhere you cannot learn about the system, your focus is about creating data to learn about the system and that is not mean there is no cost to experimenting.\nThere is a cost, but you can think of it as a fixed budget that has been sacrificed or\nyou can think of it as there being no cost at all. However, you want, but it is not that while you are experimenting you are trying to perform\nas well as possible that would be the online setting. So, now, that we have understood this difference.\nIt is also important to understand the difference between observational data and offline experimental\ndata and I use the word in DOE. So, for the first time you started using the word DOE and here we mean design of experiments,\nit is a more formal way of talking about experimentation of talking about statistical experimentation.\nAnd the idea here with this difference between observational data and offline experimental\ndata is the fact that with observational data you are not interfering with this system.\nSo, let us go back to the original problem. We said, hey how do you do data analytics when you do not have data.\nOne approach is to say, so I need to start collecting the data and so perhaps I will\nturn on a few sensors or put some sensors in certain places and I have start collecting the data.\nNow, that in itself is not the subject we are talking about here, that is just turning\non this switch of collecting data and once you have the data, you analyze the data and that is fair game that is data analytics in some sense.\nBut, that data analytics has coming from observational data.\nWhat we focusing on right now is, is there some way for me as the agent that wants to\ncollect the data to actively engage for the system and choose, what data need gets collected\nand that is what we will be doing both in design of experiments and active learning. You are in an offline setting, meaning that you are right now committing all your resources\nto collecting the data and you can collect whatever data you want. But, the point is that you are going to be controlling what data gets collected and as\na result, the only way to do that is not to passively observe the data that get generated,\nbut do actively in the case of design of experiments you will actively go and change some of the\nsettings in a system or very specifically go query certain points and that is how the\ndata gets generated. A typical problem statement and experimentation would actually say, go set the machine to\nsetting a and setting b and setting, setting c and then let us see what the output is.\nAnd in active learning is seen a little bit more as that entire data, the input space\nis available and you get a query a particular point in the input space and then get the\nanswer. Now, these are just two difference ways of describing it, because they come from slightly difference context of application, but the core problem statement is the same which is\nthat you as the user and experimenter or learner has the choice of generating an output at\na pre decided point in the input space and that is has some critical difference over\nobservational data and lot of advantages just to give you some intuition, the biggest advantage\nis one concerning multi collinearity, if you just observe the data it is possible that\ntwo input variable are so highly correlated to the point where there is almost a perfect\ncorrelation. In which case you would never know if the output was increasing or decreasing as a result\nof variable input variable a or input variable b, because input variable a and b are so highly\ncorrelated. So, in the case of design of experiments or active learning you would come to that realization\nat some point without passively observing data and say o we need to try out an experiment\nor we need to query a point, where input variable a is high and input variable b is low and\nvise versa to try and understand which of these two input variables is having an impact on the output.\nSo, let us start with focusing our first lessons on experimentation and design of experiment.\nThe core idea design of experiments is that as long as you can conceptualize the operation\nof a systems has some combination of inputs which when use together results in outputs,\nyou have the scope for this kind of black box creation and a black box I mean essentially\nand understanding of the way the inputs and the outputs relate to each other. So, the inputs themselves can be anything you know they can be really broad, the only\nimportant thing is you want to be able to quantify them in some way. The black box that you are experimenting or can be anything, it can be a process, it can\nbe a system, it can be an organization which is performing certain organizational functions,\nit can be an actual product. And typically what are the outputs you are interested in?\nThe result of the black box could be the some products being created and you can measure these products and therefore, measure how good or bad they are.\nThe output could be some services or tasks that are created by this black box and again you should be able to evaluate the output.\nAnd the third is a little bit more abstract which is the information is getting created, again as long as you quantify these outputs and quantify the inputs you have a system\nwhere there is scope for experimentation. So, formal experimentation what is it and how is it different from just observing data\nand analyzing. Formal experimentation essentially involves systematic, it is a very important word systematic\nand purposeful changes that you make two input variables in an attempt to gain knowledge\nabout this system and or find the ideal settings that result in the best output.\nSo, that sentence is little long wind it, but let us kind of break it down, so the idea is that in experimentation you proactively go and systematically or purposefully change\nthe input variables it would mean that you actually go and say, oh I need to understand\nabout little bit about the systems. So, I am going to try setting input variable a to a particular value input variable b to\nanother value and there I am going to go look at what output I get and the purpose of this.\nNow, in some rare cases it could just be that you are not physically changing the input\nvariable, but you are physically choosing the input variable that you want to observe.\nBecause, you do not have the information about how the output is going to look at every point\nin the input space. So, you are not making a modification to the system there exists this large enough repository\nof information, which is very expensive to query. Because, if it is not all you have is a huge data set which requires a supervised learning\ntask. But, for some reason if this is very expensive to query you could also think of experimentation\nand light of choosing very carefully the input variables that you want to gain knowledge about.\nBut, more often than not the typical context is that you have this system where you go actually change the input variable set them to different values.\nSo, think of this perhaps this foundry process, where you are trying to create castes and\nyour input variable could easily be the temperature of the molten metal that you pouring in, the\npressure that is being applied the kind of material that the cast is made off and your\noutput could be the number of defects that you see in the cast.\nNow, in an experimental process you will go and systematically purposefully try different\ntemperatures, different pressures, different materials to make the cast, different practices\nin the cast, how long be you weight before you open out the cast, what kind of a room\ndo you place it, you would go actually physically change these settings to different values\nand observe what happens to the output, which in this case is the number of defects you see.\nSo, the emphasis here is in the systematic and purposeful changes to the input variables. Now, why do you do it, you could do it for two reasons, you could do it to gain knowledge\nand you or you could do it to find those settings that you want to set the inputs to get the\nbest output. Now, the gaining knowledge could just be an independent process that could be the final\ngoal, sometimes the approach is to gain knowledge and once you gain the knowledge.\nAnd what knowledge we are talking about here? We are talking about the knowledge associated with how the input variables affect the output\nvariables. Now, you could then once you gain this knowledge use that information to figure out what are\nthe ideal settings for the inputs. But, you also have a algorithms which say you know what I do not care about the knowledge\nmy goal is to right now find out what those settings should be. So, that I get the best output and that is takes on a different form.\nSo, that is the core idea of experimentation. So, this point a very natural question arises see you got some two or three or four input\nvariables and you got an output variable. What is the problem?\nWhy do not we just clear on with each input variable, you know one at a time and see what\nis the best setting for each input variable and do this sequentially, turns out it is\nnot that simple. This is simple problem with that and we I am going to illustrate that with this diagram.\nLet us say that this plot that you has two input variables, input variable one x called\nx 1 and input variable two called x 2 and the output is nothing but, a hill that is\nprojecting out of this green and this hill is shown with a contour plot.\nA contour plot is basically is one which connects which is often seen in maps, where you basically\ndraw a line, where the height about sea level usually is the same.\nSo, here imagine that there is flat surface which is the base the rectangle and then there\nis hill projecting out of this flat surface, out of this screen and coming towards your\nface as if you are looking at this screen and these eclipse is that you see on this\nscreen these kind of circular looking things are nothing but, the contour plots.\nSo; that means, everything that is on this line is 70 meters or feet or inches whatever\nyou choose to think of it. So, this is an example of response surface are basically try to characterize in this\npicture, how the variables x 1 and x 2 have an influence on the output, why which is the\nhill coming out of the screen. So, it is an abstract concept, now let us take a look at what happens, when you just\nplay with one variable at a time and we are going to call that adaptive one factor at a time experimentation.\nThe idea behind this algorithm is that what I am going to do is at any given point of\ntime I am going to play with anyone variable. So, I am going to start with let us say x 1 and I am going try different values of x\n1 and I am going make a conclusion at some point by saying at what value of x 1 did I\nsee the best y and I am going go with that I am go on a fix x 1 now to that value and\nplay with x 2. Now, sequentially do that with all the input variables until I come to a conclusion.\nNow, take this example where I stop playing around with x 1 and I arbitrarily fix some\nvalue for x 2 and it turns out that I fix the value right here. So, I put a star there, so I started with x 2 set to this value and I just started playing\nwith x 1. So, what is that mean when you playing with x 1; that means, you keep changing x 1 and\nhere we are actually just changing x 1 to different values and as you go to through different values you see different heights.\nFor instance, when you set x 1 to this value you seeing a height of 70.\nWhy? Because, this is the contour line of 70 when x 1 is equal to this value you see a height\nof 80 and so you keep going through this process and until you find the highest point and the\nhighest point is here, because of this point you are touching 80 for instance at this point\nat x 1 you are not touching 80 your some across the 75. So, you conclude that this is the best value of x 1 and then you set x 1 to this value.\nSo, x 1 gets set to this value and then you go about changing x 2.\nSo, and basically when you changing x 2 you are staying on this grey line out here and\nas you keep going through x 2, you find that the point where there is a star is the peak\nand you conclude that is a highest point. So, you choose to set x 1 and x 2 such that you are at star.\nClearly, what is the problem with this, your problem is that you miss the peak, there it\ntwo problems on this, one is that you miss the peak the peak was really out here, if you will take look at this contour map in your understand the contour plot, the plus\nsign is where the peak is and you erroneously concluded that the star is where the peak\nis. And the reason for it is fairly simple, the way this hill is drawn it is clear that x\n1 and x 2 have some interaction effects, there is x 1 is really good out here, when x 2 is\nset to this value. Now, if you said x 2 to another value, let us say x 2 here the highest point of x 1 is\nprobably somewhere here. So, this value of x 1 winds up being highest.\nSo, it really what we mean by an interaction is where you conclude x 1 to get the best\noutput depends on where you set x 2 to. So, this is interactive effect that you can sometimes gets fooled by, there is an another\nthing that we have not really discussed here, which is that few do an experiment at a given\nx 1 and x 2 setting are you always going to get the exact same value and the answer is\nprobably not. Experimentation is typically carried out in sarcastic setting meaning that even if you\nunderstand that your output y is some function of your input variables, in this case there\nare two input variables. So, there is some mathematical function that is associated with input variables x 1 and\nx 2 and the output variable y, but on top of that if you said x 1 and x 2 to specific\nvalues are you going to get the same y. The answer is in a stochastic system you do not, because there is another factor which\nis just you can call it noise, you can call it irreducible error, you can call that luck\nwhatever it is, it is just concept of just there being uncertainty above and beyond you\nare and this kind of uncertainty is what we deal with in supervised learning is what we deal with in much of what we have discussed and in this course.\nSo, that aspect also can throw you off in an approach like this.\nNow, this has been illustrated to you in a continuous frame work, where you are able to change x 1 continuously and see different, but in more practical settings you do not\nhave infinite experiments. So, you might just set x 2 to a particular value and sample x 1 at someone two or three\npredetermined values, more often than not in design of experiments you will see that\nwe are only interested in linear effects most of the cases you are interested in linear effects.\nSo, you would really look at trying out each variable at two different points.\nBecause, where two points you can draw straight line and the two points are typically get\ncoded. So, what is another approach that you can do, that you can employ to overcome this problem?\nThe other approach you can do to overcome this problem and one of them is defined is called is a broadly called as orthogonal arrays and a specific type of an orthogonal arrays\ncalled the full factorial which is what I show here. So, take a system where variable A can take on two states.\nSo, let us go back to this casting problem and let us say you are interested in variable A which is let us say the temperature of the molten metal that is poured in and so the\ntemperature could be something like 250 Fahrenheit and you might be interested in studying the\neffects at 350 Fahrenheit. Now, I am not an expert in this I do not know those are reasonable temperatures for molten\nmetal I am guessing it is a little too low actually, but who knows. Now, typically what you do is when you have just these two settings, you kind of lay code\nthem and you called them 250 as minus 1 and 350 as plus 1. Now, you do the same thing with the second variable input variable of interest, now variable\nB could be something like pressure, where you have lope or let us say the time that\nyou weight before you remove the cast. So, that could be 1 hour or 2 hours again I do not know those are reasonable numbers,\nthey to illustrate a point is to 1 hour again you call it minus 1 and 2 hour you call it\nplus 1. So, what you go about doing in a full factorial is you try every combination of every variable\nwith every other variable. So, you try the minus 1 minus 1 setting, you try the minus 1 plus 1 setting.\nSo, and plus 1 and minus 1 setting and you get the picture, you essentially try every\npossible combination and that is called a complete enumeration of the designed space. Because, you have discrete data points and you might choose to do some experiments at\neach of these combinations of points.\nIn this example of shown you two replicates these are called replicates and these are both the same output variable y, but you choose to take two readings, it is actually unfair\nto call it two readings, because it is not like you do the experiment just once and just use the same measuring device and just take two readings, you actually redo the experiment\nand the reason you do that is because of the concept we just discussed which is you acknowledge\nthat your y is some function of in this case variables A and B.\nBut, on top of that the sum error, this error often you sought of is being Gaussian which\nsum with mean 0 and standard deviation equal to sum value. But, even without going to there.\nThere is just some noise and you can see that even though you set A to minus 1 and B to minus 1 first time around you got 57, second time around you got 56 and you see different\nlevels of uncertainty. Now, the same concept extended to three variables is what shown here, on the right hand side\nand you have so a, b, c three variables and I am also showing you a case where you are\nnot just interested in two levels, but you might be interested in three levels. So, variable A has 3 levels, variable B has 2 levels and variable C has 2 levels.\nSo, complete enumeration of them would be nothing different than three times, two times, two which is equal to 12 and we call the 12 treatment combinations.\nSo, you can have 12 treatment, 12 rows out here and again here I am choosing to take\ntwo replicates just to get a better idea of the noise that is there in this system as\nwell. But, the hope is that to taking on such an approach would enable us to perform a comb\nof analysis on the data which helps us understand which helps us not get strict by this Gaussian\nthis noise that is there on the system which can make you conclude the wrong things, if you an art where of it and at the same time also understand that they can be some interactive\neffects between A and B or A and C or B and C and that is about we will be focusing on.\nSo, in this part we are focused little bit more and how the experiments are itself designed and this is just one way of designing experiments and this is called the full factorial design\nand there are other ways of doing the same thing. In the next lecture we will be briefly talk about, how do you analyze the data that you\ngets from such an experiment and we close to be talking about active learning. Thank you.\n \nIntroduction to Experimentation and Active Learning(contd)\nHello and welcome to our second lecture in the series, where we talk about Experimentation\nand Active learning.\nIn the first lecture we briefly spoke about, we motivated the idea of we using active learning\nexperimentation or re enforcement learning in a broader data analytic setting, where\nwe said these are some approaches that are fairly relevant when you do not have data,\nwhen data does not exist or the data exist and it is not enough or you have only partial\ndata.\nIn the first lecture, we also went into design of experiments and spoke about, how perhaps\nan approach, where you sequentially change one variable at a time need not be the best\nway to conduct an experiment.\nAnd we also spoke about something called orthogonal arrays and specifically, there we spoke about\na form of experimentation called the full factorial design, where it is essentially\na complete enumeration of a discrete input space, where even if you have continuous input\nvariables you break them into 1, 2 or may be sometimes 3 discrete points.\nAnd you essentially do a complete enumeration, which means that every variable is set to\nevery possible value it can be set to with relation to every other variable being set\nto all their possibility.\nSo, all the possible points in the input space are essentially looked at and that was essentially\nlooking at the design meaning, what points in the input space to you choose to experiment.\nIn today’s lecture we will briefly take the just full factorial, which is a very basic\ndesign and talk about some approaches that are used in analyzing such an approach, such\na design.\nSo, jumping into the subject in this slide, what we have is a full factorial design associated\nwith three input variables A, B and C. And the idea here is that what we have is a full\nfactorial design, because A can take on three values minus 1, 0, plus 1 and B can take on\ntwo values and C can take on two values just minus 1 plus 1 and that gives you 12 combinations\ntotally and we have taken, the output variable is Y.\nNow, I call them Y 1, Y 2, Y 3, Y 4, only because they are Y 1, Y 2, Y 3 and Y 4 are\nreplicates, so it is a same core variable.\nBut, essentially you conduct the experiment at, the settings for instance minus 1, minus\n1, minus 1.\nYou conduct that experiment 4 times; again it can be a parallel effort or a sequential\neffort either way.\nWhat we mean by that is, when we say we conduct the experiment 4 times, you might have one\nexperimental unit and you separately conduct the experiment 4 times on it or you could\nhave 4 experimental units and you might, you choose to parallely try the same setting of\nminus 1, minus 1, minus 1 on those four different experimental units.\nBut, essentially this is almost a setting, where you conduct 48 separate experiments\nand in design of experiments, language that just called 48 trails or 48 runs and these\nare essentially your results, these are your outputs.\nAnd the convenience of this is sometimes you can look at this raw data or you can look\nat the average Y and this is nothing but, the average for instance for the average 80.75\ncomes from taking the average of these four numbers.\nSo, each row is average and it is represented and that is the, those are the results that\nwe haven, this is essentially what we look to analyze.\nSo, what is one way that you can take these results and arrive at a conclusion of what\nvalues A, B and C should be set to, because that is kind of the goal.\nThe goal is to figure out, what is said A, B and C to, I mean accurate one of the goals\ncan be on to what values to set A, B and C; such that you get best Y and here we are going\nto treat best as the highest value.\nSo, how should I set value A, B and C, so I get the best Y?\nOne approach is called the classical analysis.\nThe idea behind classical analysis is to take each variable individually, each input variable\nindividually and ask the question, at what setting am I getting the best results.\nSo, A is set to minus 1, A is set to 0 and A is set to plus 1.\nSo, if I am ask the question, what is my Y on average when A is minus 1, what is my Y\non average when A is equal to 0, what is my Y on average when A is equal to plus 1 and\nI look at these three numbers and I will choose the value of A, where my average Y is the\nbest.\nI will similarly do that for to B and C and I will come up with a recommendation on that\nbasis.\nSo, what is that look like for this data?\nIt is a fairly straight forward calculation, when A is set to minus 1 you essentially get\n81.81.\nSo, it just means you can think of it in many ways, you can think of it as the average of\nthese data points.\nRight here, what I circled and the average of these data points or you can think it as\nthe average of these data points, because ultimately each row comes from an average\nfrom that respective row and the sizes are equal.\nBut, you can think of it either way, either way this is the average Y when A is set to\nminus 1 and that is 81.8.\nAnd similarly you get an average for 0, you get an average for plus 1 and you would basically\nsay, I like A at minus 1 it is giving me the best result.\nSimilarly you do it for B at minus 1 and B at plus 1.\nNow, when you do it for B at minus 1 and B at plus 1, how do you take the average?\nIt is the same principle.\nSo, you would for instance at B at minus 1 you would be interested in taking the average\nof these points, B is minus 1 at these points, so it would ultimately be the average of these\npoints.\nSo, essentially that average would be B at minus 1 and that is 77.83.\nSo, you do the same process and it is clear that A at minus 1 is the best, B at minus\n1 is good, because B at minus 1 is greater than B at plus 1 and C at plus 1 is good,\nbecause that is better than C at minus 1, so that is essentially classical analysis.\nAnd it is a fairly heuristic approach and it is like a first cart approach.\nNow that is one way of going about the analysis.\nAnother approach is to take the best.\nThe take the best essentially says, I am going to look at that treatment combination, which\ngave me the average highest average Y.\nSo, would that be at here?\nSo, it looks like 85.75 is the highest average Y and that setting is I believe A 1, 1, 1\nand, so we would essentially go with that recommendation.\nSo, take the best would have selected A is equal to 1, B is equal to 1and C is equal\nto plus 1.\nSo, here are two fairly contrasting approaches and the question you need to ask yourself\nis, where would you want to use classical analysis and where would you want to use take\nthe best.\nAnd to answer that question we need to answer that question in terms of, if you want to\ntake a one factor at a time approach, what are two reasons that experiments that particular\napproach fails or for that matter, what is the two major difficulties with design of\nexperiments.\nThe two major difficulties are the following.\nOne is that, you could have some interactive effects between the input variables, which\nmeans the effect that n input variable will have on the output variable really depends\non how some other input variable is set.\nSo, that is called an interactive effect.\nThe other reason is that sometimes you get fooled, because yes, Y is some function of\nA, B and C from top of that, there is also some noise.\nSo, that noise would have given you results, which you are erroneously interpreting and\nyou are essentially over fitting and you are getting fooled.\nNow, the question is, under what circumstances would classical analysis work and under what\ncircumstance would take the best work and the quick answer is, a classical analysis\nessentially works really well in an environment of high error or noise.\nSo, when this, when the noise, which is so we said Y is equal to f of a, b and c, but\nit is also got this noise component and when this noise component, although this there\ncould be no bias to this noise.\nSo, this noise could be something like it is normally distributed with mean 0 and standard\ndeviation, some standard deviation.\nThen, if sigma e is very high and, so it is a very noisy environment, then classical analysis\nwould work quite well because it is averaging a lot of data points.\nNow, where it move to work well is when there is lots of interactions, so you need low interaction\neffects for classical analysis to work, because it just taking the average at A and average\nat B. In contrast, take the best would work only\nwhen there is very low error or noise.\nIt would not work well when sigma e square is high, but because it just takes the best\ncombination, it is almost like it does not care about the interaction.\nSo, high interactions work very well in its favor and, so you would you take the best\nthere.\nNow, the reason we talked about these two heuristics is to really motivate the statistical\nway of doing things, which is this dichotomy that you see between high noise versus low\nnoise and fitting to any shape you want versus not being able to fit any shape you want has\na lot to do with something you seen before, which is the bias variance dichotomy.\nAnd this bias variance dichotomy is what you seeing in these two extreme approaches.\nThe statistical way can sometimes balance that out and, so the truth is almost any supervised\nlearning algorithm could be fair game, the only context is that the data set is fairly\nsmall.\nBut, any data set, any supervised learning algorithm that is not require a very large\ndata set that requires a very small data set, but they still give you meaningful results\ncould be a fair game.\nAnd in that regard step wise regression is usually popular in analyzing designed experiments.\nYou start with the basic model that Y is equal to linear function of all the inputs a, b\nand c and then, you also try to incorporate two way interactions in the form of saying\na times b, a times c, b times c and you could even have a three way interaction a times\nb times c.\nAnd, because your variables are coded to minus 1 and plus 1, just multiplying a and b as\nan input can have a meaningful interpretation.\nNow, one thing that can be said about this process of design of experiments is the way\nwe have done it with full factorials and the way we explained it makes it essentially a\none shot approach.\nIt basically means you decided even before you see any results the entire set of points\nthat you want you look at the input space.\nSo, if you look at this you already decided, so each treatment combination is a point in\nthe input space.\nWe already decided the all the points and how many times you want to look query each\nof these points even before you look at even one result.\nSo, it is ideal in some sense for a parallel deployment, but if you could do the sequentially\nis there a better way of doing it, which is can you react to the data your seeing to say\nin want to query this point more, because I am less certain about this point or variable\nverses saying o I am very confident about something else.\nSo, I do not want to waste my resources in conducting experiments in a particular place\nand that is primarily the motivation of sequential experimentation this idea that you can you\ncan conduct experiment sequentially and that gives you an opportunity to react to the data.\nNow, in many ways while sequential experimentation has been an effort from the statistics community\nactive learning essentially is this same core idea and it is been motivated more often the\nmachine learning community and the context in which, the problems are applied to do sometimes\ndiffer as the result.\nActive learning is often seen as a semi supervised learning approach and understandably it is\nalso called is optimal experimental design.\nBecause, active learning is a process where the system sequentially chooses to query the\ndesign space and therefore, be able to build knowledge, on what we see and how it can be\nbetter understood.\nThe key different, especially in a in the typical context of application except while\ndesign of experiments often focuses a lot on the sole idea of starting with 0 data often\nactive learning will start with this notion that there is some amount of data and it is\nnot enough and you might want to sequentially query the system to improve upon your understanding\nof the system itself.\nAnd, so in many ways it is kind of seen as semi supervised learning, because there is\nan abundance of unsupervised learning data just means there is an abundance of states\nof the input space and you sequentially get you choose points in the input state for,\nwhich you really want to get answers.\nAnd therefore, get outputs for and by doing that the whole idea is that you can get away\nwith much less data on the output space and you can still come up with predictions that\nare meaningful.\nSo, what are some prominence strategies in the whole active learning frame work is that\nin a sense all of these strategies at we going to talk about now, rely on one thing.\nIt relies on you know evaluating the, how informative different points on the input\nspace can be if you query them and you got an answer, what you mean by queried is you\nchoose a particular point in the input space and say can you please give me an output for\nthat.\nAnd that, now goes into your data set, where you can apply some kind of supervised learning\napproach to make sense of, what you have.\nAnd the strategies that are involved with active learning can be broadly classified\nin these four and you know this these is an area of you know where there is a active research\nand, so there they might be some strategies that also follow fall that are new that might\nfall out it these have been historically the more popular ones.\nSo, let us take look at the first one which, is uncertainty sampling, now this is perhaps\nsome more simplest and also therefore, very fairly common frame work.\nAnd in this frame work essentially the active learner chooses to query instances, which\nit is least certain about, how to classify or how to predict.\nSo, if it is a classification problem it says I am going to ask questions about I am going\nto choose points on which, I want answers on which, I want you to give me an output\nand I am going to choose points, where I am least certain currently given the information\nI already have I am least certain about if it is a classification problem 0 and 1.\nI am least certain about whether to classify it as 0 or 1 in those instances I really want\nyou to I want to conduct my experiment in that point, where I am least certain.\nNow, if you can also think of this as regression problem, where at a certain point in the input\nspace you might have a prediction, but that prediction is not cast and stone some kind\nconfidence bound.\nYou have some kind of bounds around that predication is some amount of uncertainty associated with\nparticular predicted value and you might choose to pick the point where your uncertainty is\nthe highest.\nAnother approach also fairly popular one is this idea of query by committee, let me just\nmark, where we are we finished this and the second is query by committee.\nQuery by committee approach essentially involves having committee of different models, which\nare all trained on the current data set and when we say data here we are talking about\nthe data set for which, we have the outputs you have a data set with the outputs in those.\nSo, you have a some kind of supervised learner, which is capable of interpreting the data,\nbut you might have a committee of models, which are all trained on the current data\nset, but they might represent competing hypothesis.\nNow, each of these members is now, allowed to vote and you basically choose to take you\nchoose to get a data point from the input space, where the committee members have the\nmost disagreement.\nSo, think of it you know one way to one example that I always kind about liked about this\nis it really maps on to lot of ensemble methods something that we saw earlier in the course.\nSo, if you had a set of methods set of supervised learning approaches all trying to predict\nthe same thing.\nSo, think of it as a random forest, where we have multiple trees trying to predict same\nthing.\nSo, for a given input vector each tree does not going to come up with the exact same class\npredication or it might not come up with exact same predication even if it is the output\nis continues variable.\nSo, this method simply says let us go with let us go get an answer for a data point,\nwhere the trees disagree most about what to classify it or what where are the highest\nvariance in the predicated value of the models.\nThe next approach is expected model change the idea behind expected model change is to\nsee if we knew the label of a particular data point.\nThen, which label of the input space if we knew would contribute to the greatest change\nin the current model we have based on the current data we have of the inputs and outputs.\nSo, we have some data on the inputs and outputs and you basically extrapolate to this to the\nbroader question of asking the question saying you could get data of you could get an output\nfor any point in the input space, which point would essentially lead to you making the largest\nchange in the model.\nAnd the idea is to kind of query that that point very specifically the last two sets,\nwhich is the expected error reduction and various reduction use the following approaches\nthe approach is to basically say with error reduction the idea is there some deviation\nof the predication verses actual.\nSo, you can think of it as essentially the residuals in a regression case and you know\nin other in every other case other it is bias or variances for whatever reason you are unable\nto predict you are unable to predict the exact value at a particular location.\nThe question we need to ask our self is and answer to which, point in the input design\nspace could lead to the largest expected error reduction.\nIn that sense its it is fairly close to a uncertainty sampling, but it is not just uncertainty\nsampling is the where it really differs from uncertainty sampling is, uncertainly sampling\nask this question about each data point in the input design space with respect to that\ndata points.\nSo, I go to one data point in the design space and ask the question saying, how much uncertainty\nthere in that point.\nIn terms my predication at that point, where as expected error reduction does not talk\nabout a single point it talks about, how if I got and answer to a question at any particular\npoints.\nSo, I go to a particular point to the input design space and I query the oracle and I\nget an output.\nNow, that output if I now, refit my supervised learning with this extra data point there\nis going to be overall reduction in my residuals across the boat, because this new data point\ncould change the entire regression line fit.\nNow, this new regression line fit will create new residuals and, so I am looking at the\noverall reduction in error between the data points and the fitted model.\nAnd, so I choose to query that data points in the input space and get an output for that\ndata point, which lead to an overall reduction in error.\nNow, the variance reduction approach is a deviation from that it is a deviation in that\nit is you do not look at the error between the fitted model and the data points, But\nyou just look to see reduce the overall variance in the output space.\nNow, that is done probably because it is much easier to do this, but what the core idea\nhere is that you have some variance associated with the outputs.\nAnd you can still continue to reduce generalization error indirectly by minimizing output variance\nand that also can sometimes you do that because it is mathematically a easier you can kind\nof get close forms solution an and that is essentially the core idea.\nSo, I hope this gives you some feel for experimentation and the whole idea of active learning and\nat least some strategies that we could use an experimentation of active learning.\nThank you.\n",
          "document_id": 1139572
        }
      ]
    }
  ]
}