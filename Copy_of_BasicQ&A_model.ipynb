{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanimvsk/Capstone_Project/blob/Source-code/Copy_of_BasicQ%26A_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "8eDHFHc_e7pi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b83188-b0ee-411a-e538-97628f7df11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 15 07:10:33 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4FEBxGe7pl"
      },
      "source": [
        "To start, install the latest release of Haystack with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "I8ZeS67we7pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9954362a-0555-4658-f145-135125963be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farm-haystack[colab]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-fxa056qo/farm-haystack_29612ff22f6243558303ccbcfb713202\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit 50f34372e15fc38a512d0e268714c0feb9d540be\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 66.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.6.3)\n",
            "Collecting azure-ai-formrecognizer>=3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.4/228.4 kB 27.0 MB/s eta 0:00:00\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15\n",
            "  Downloading rapidfuzz-2.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 75.6 MB/s eta 0:00:00\n",
            "Collecting transformers==4.21.2\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 52.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (3.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (2.23.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (8.14.0)\n",
            "Collecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 11.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 5.2 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.3.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 59.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: torch<1.13,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (5.0.0)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 6.5 MB/s eta 0:00:00\n",
            "Collecting huggingface-hub>=0.5.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.5/163.5 kB 20.9 MB/s eta 0:00:00\n",
            "Collecting quantulum3\n",
            "  Downloading quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 84.3 MB/s eta 0:00:00\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.29.0-py3-none-any.whl (16.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 78.9 MB/s eta 0:00:00\n",
            "Collecting elasticsearch<8,>=7.7\n",
            "  Downloading elasticsearch-7.17.6-py2.py3-none-any.whl (385 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.8/385.8 kB 39.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (1.7.3)\n",
            "Collecting posthog\n",
            "  Downloading posthog-2.1.2-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack[colab]) (0.3.5.1)\n",
            "Collecting grpcio==1.47.0\n",
            "  Downloading grpcio-1.47.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 96.8 MB/s eta 0:00:00\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 5.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio==1.47.0->farm-haystack[colab]) (1.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 91.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.2->farm-haystack[colab]) (1.21.6)\n",
            "Collecting azure-core<2.0.0,>=1.23.0\n",
            "  Downloading azure_core-1.26.0-py3-none-any.whl (178 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.9/178.9 kB 21.0 MB/s eta 0:00:00\n",
            "Collecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 11.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.1.1)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (2022.9.24)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0\n",
            "  Downloading jarowinkler-1.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.7/113.7 kB 13.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack[colab]) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.13.1+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 67.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack[colab]) (3.9.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack[colab]) (0.18.1)\n",
            "Collecting alembic<2\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 kB 20.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (3.17.3)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (2022.4)\n",
            "Collecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.3.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 11.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting prometheus-flask-exporter<1\n",
            "  Downloading prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\n",
            "Collecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 11.0 MB/s eta 0:00:00\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (7.1.2)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.1.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (0.4.3)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.5.0)\n",
            "Requirement already satisfied: sqlalchemy<2,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->farm-haystack[colab]) (1.4.41)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.5/182.5 kB 22.9 MB/s eta 0:00:00\n",
            "Collecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.0-py3-none-any.whl (147 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 19.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Collecting backoff<2.0.0,>=1.10.0\n",
            "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack[colab]) (4.9.1)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.2/125.2 kB 15.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 11.8 MB/s eta 0:00:00\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (3.2.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (0.8.10)\n",
            "Collecting urllib3<2,>=1.21.1\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 19.0 MB/s eta 0:00:00\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.0/55.0 kB 7.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (1.0.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 8.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 5.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.2->farm-haystack[colab]) (3.0.9)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.15.0-py3-none-any.whl (60 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.1/60.1 kB 7.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow->farm-haystack[colab]) (1.1.3.post0)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack[colab]) (7.1.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask<3->mlflow->farm-haystack[colab]) (2.0.1)\n",
            "Building wheels for collected packages: sentence-transformers, farm-haystack, langdetect, python-docx, seqeval, tika, databricks-cli, docopt\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=c9dc755ac64e1a45e08e1a1c0a993fc1839e49b0a5bc1da93026ec5479d0d295\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "  Building wheel for farm-haystack (pyproject.toml): started\n",
            "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.10.0rc0-py3-none-any.whl size=689093 sha256=d0c4ac656d8bddd6dbc9ff25b10b2375912a5411b7a5c06fd09d67b10847aa5b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u0k1c8ac/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=8408ca0c8efdec757b4fdb184153fc96f0cb6fca622a1d93c8b24e145730db27\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for python-docx (setup.py): started\n",
            "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=03d46bdef02489af3752367b26fb1e11e61a4f5e387e613d216e08d66224ab16\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=2731806a3a0e8269916092aeef5b7115f93b023678284a2cbc8f4c03f73c5318\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for tika (setup.py): started\n",
            "  Building wheel for tika (setup.py): finished with status 'done'\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=7a1bcaee1aa088f4cbd55a75cc504fc1b6be580fd4e6b2e0cae053a0e4f03e8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "  Building wheel for databricks-cli (setup.py): started\n",
            "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139102 sha256=016b234b8ee63d5eb8490b33be4e9ab13409da4310a7ab4db53472b740bda29b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/73/87/c1e4b2145eb6049bb6c9aaf7ea1e38302b77ca219b6fef5d5c\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=2ee766656212ebc46794ce42ee914770935ec16e9f8e8069bcb03eef9cba0269\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built sentence-transformers farm-haystack langdetect python-docx seqeval tika databricks-cli docopt\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, mmh3, docopt, azure-common, websocket-client, urllib3, smmap, querystring-parser, python-docx, pyjwt, prometheus-client, num2words, langdetect, jarowinkler, isodate, importlib-metadata, gunicorn, grpcio, backoff, requests, rapidfuzz, quantulum3, Mako, gitdb, elasticsearch, tika, seqeval, prometheus-flask-exporter, posthog, huggingface-hub, gitpython, docker, databricks-cli, azure-core, alembic, transformers, msrest, mlflow, sentence-transformers, azure-ai-formrecognizer, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 5.0.0\n",
            "    Uninstalling importlib-metadata-5.0.0:\n",
            "      Successfully uninstalled importlib-metadata-5.0.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.49.1\n",
            "    Uninstalling grpcio-1.49.1:\n",
            "      Successfully uninstalled grpcio-1.49.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 azure-ai-formrecognizer-3.2.0 azure-common-1.1.28 azure-core-1.26.0 backoff-1.11.1 databricks-cli-0.17.3 docker-6.0.0 docopt-0.6.2 elasticsearch-7.17.6 farm-haystack-1.10.0rc0 gitdb-4.0.9 gitpython-3.1.29 grpcio-1.47.0 gunicorn-20.1.0 huggingface-hub-0.10.1 importlib-metadata-4.13.0 isodate-0.6.1 jarowinkler-1.2.3 langdetect-1.0.9 mlflow-1.29.0 mmh3-3.0.0 monotonic-1.6 msrest-0.7.1 num2words-0.5.12 posthog-2.1.2 prometheus-client-0.15.0 prometheus-flask-exporter-0.20.3 pyjwt-2.5.0 python-docx-0.8.11 quantulum3-0.7.10 querystring-parser-1.2.4 rapidfuzz-2.7.0 requests-2.28.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 seqeval-1.2.2 smmap-5.0.0 tika-1.24 tokenizers-0.12.1 transformers-4.21.2 urllib3-1.26.12 websocket-client-1.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-fxa056qo/farm-haystack_29612ff22f6243558303ccbcfb713202\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "E0XB9G3Ae7pn"
      },
      "source": [
        "## Logging\n",
        "\n",
        "We configure how logging messages should be displayed and which log level should be used before importing Haystack.\n",
        "Example log message:\n",
        "INFO - haystack.utils.preprocessing -  Converting data/tutorial1/218_Olenna_Tyrell.txt\n",
        "Default log level in basicConfig is WARNING so the explicit parameter is not necessary but can be changed easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TVbVUMYke7pq"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEB-TR6Ke7px"
      },
      "source": [
        "## Document Store\n",
        "\n",
        "Haystack finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of `DocumentStore` include `ElasticsearchDocumentStore`, `FAISSDocumentStore`,  `SQLDocumentStore`, and `InMemoryDocumentStore`.\n",
        "\n",
        "**Here:** We recommended Elasticsearch as it comes preloaded with features like [full-text queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html), [BM25 retrieval](https://www.elastic.co/elasticon/conf/2016/sf/improved-text-scoring-with-bm25), and [vector storage for text embeddings](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/dense-vector.html).\n",
        "\n",
        "**Alternatives:** If you are unable to setup an Elasticsearch instance, then follow the [Tutorial 3](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/03_Basic_QA_Pipeline_without_Elasticsearch.ipynb) for using SQL/InMemory document stores.\n",
        "\n",
        "**Hint**: This tutorial creates a new document store instance with Wikipedia articles on Game of Thrones. However, you can configure Haystack to work with your existing document stores.\n",
        "\n",
        "### Start an Elasticsearch server locally\n",
        "You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (e.g. in Colab notebooks), then you can manually download and execute Elasticsearch from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXzMopRne7p0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fc018e-8b9f-41cc-a94a-740c74217401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.utils.doc_store:Tried to start Elasticsearch through Docker but this failed. It is likely that there is already an existing Elasticsearch instance running. \n"
          ]
        }
      ],
      "source": [
        "# Recommended: Start Elasticsearch using Docker via the Haystack utility function\n",
        "from haystack.utils import launch_es\n",
        "\n",
        "launch_es()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-omFEPSe7p3"
      },
      "source": [
        "### Start an Elasticsearch server in Colab\n",
        "\n",
        "If Docker is not readily available in your environment (e.g. in Colab notebooks), then you can manually download and execute Elasticsearch from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "GTLj3dZie7p5"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "NKNbuGShe7p6"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "172YMxbCe7p7"
      },
      "source": [
        "### Create the Document Store\n",
        "\n",
        "The `ElasticsearchDocumentStore` class will try to open a connection in the constructor, here we wait 30 seconds only to be sure Elasticsearch is ready before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJGz90Mue7p_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFlBgKb7e7qA"
      },
      "source": [
        "Finally, we create the Document Store instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mvDVNUpSe7qA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d255c9f2-88d7-46f7-e79a-3ae70e0f9418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.telemetry:Haystack sends anonymous usage data to understand the actual usage and steer dev efforts towards features that are most meaningful to users. You can opt-out at anytime by calling disable_telemetry() or by manually setting the environment variable HAYSTACK_TELEMETRY_ENABLED as described for different operating systems on the documentation page. More information at https://haystack.deepset.ai/guides/telemetry\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "# Get the host where Elasticsearch is running, default to localhost\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "#from haystack.document_stores import FAISSDocumentStore\n",
        "#from haystack.document_stores import FAISSDocumentStore\n",
        "\n",
        "#document_store = FAISSDocumentStore(embedding_dim=128, faiss_index_factory_str=\"Flat\")"
      ],
      "metadata": {
        "id": "Jnq_0vuHxnmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "U5WHCR15e7qB"
      },
      "source": [
        "## Preprocessing of documents\n",
        "\n",
        "Haystack provides a customizable pipeline for:\n",
        " - converting files into texts\n",
        " - cleaning texts\n",
        " - splitting texts\n",
        " - writing them to a Document Store\n",
        "\n",
        "In this tutorial, we download Wikipedia articles about Game of Thrones, apply a basic cleaning function, and index them in Elasticsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TRCI1OiXe7qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e4ebbdb-49dd-4ab8-c5b0-73f609469fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data File_1.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data FileA_7.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data_3.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data File_4.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data File_6.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw File_2.txt\n",
            "INFO:haystack.utils.preprocessing:Converting /content/input/Raw Data_5.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Document: {'content': 'Welcome to the lecture on the implementation of simple linear regression using RIn this lecture, we are going to see how to implement simple linear regression in R. As apart of this lecture, we are also going to look at how to load the data from a text le, how to plotthe data, how to build a linear regression model and how to interpret the summary of the model?Now, let us see how to load the data. Now you have been given thedata set bonds in the text format, the extension of the file is dot \\\\txt\". Toload the data from the file, we use the function read dot delim.So, read dot delim reads a file in a table format and creates a data frame out of it.The input arguments to the function are file and row dot names. Now file is the name of thefile from which you want to read the data and row dot names are essentially the row ids. Itcan be a vector of names or only a single number corresponding to the column name.Now, assuming that the data is in your current working directory,the command reads as read dot delim. So, within quotes I have bonds dot txt and I am giving rowdot names equal to 1. Now, once the command is executed, an object of bonds is created,which is a data frame. Now let us see how to view the data. View ofbonds will display the data in a tabular format, the snippet below shows the table.We can also view the first few rows of any data set,head and tail functions will help us to do that. Now, head of bonds will give us thefirst 6 rows from the data and tail of bonds will give us the last 6 rows from the data.Now, let us look at the description of the dataset, now the data has 2 variables couponRate and Bid Price. Now, coupon rate refers to the fixed interest rate that the issuerpays to lender. Bid price is the price someone is willing to pay for the bond.Now, we have seen how to load the data and how to view the data. Let us now see whatthe structure of the data is. By structure I mean that each variable and it is data type.We use the function str and the input to the function is a dataframe. Now weexactly want to see whether the variable data type are same as what we expected them to be,if not we need to coerce them to the respective data types.So, now this should ring a bell, because we have learned the function as dot followed bythe name of the data type and we will use this function to coerce it if the variable is notof the desired type. Now for this dataset, I run the function I say str of bonds nowbonds is the name of my data frame.So, the output reads as data frame bonds is of thetype data frame it has 35 observations of 2 variables. The first column being coupon rate,which is of the type numeric and I have the first few values being displayed.The next column Bid Price is also of the type numeric and the firstfew values of the same column are being displayed.Now, let us look at the summary of the data. So, the summary function followed by the nameof the data frame in this case bonds will give us 5 number summary and mean from the data.Now, the first column which is coupon rate, I have the 5 number summary and the mean and Ialso have the 5 number summary and the mean for the second column. So, till now we have seen howto load the data, how to view the data, We have also looked at the structure and the summary.Now, let us see how to visualize the data. So, to visualize the data I use the plotfunction. We have covered the plot function earlier in the visualization in r section,now the input to the plot function are basically x and y. In this case x refersto my coupon rate and y refers to my bid price. So, in order to access the variables,I need to give the name of the data frame followed by a dollar symbol. So,I say access coupon rate from the bonds data and access bid price from the bonds data.So, I can also give a title to my plot. So, inside the parameter main you can specifythe title of your plot, xlab is nothing, but x label. So, I am assigning it as x “CouponRate\" and y label I am assigning it as “Bid Price\". So, the plot is on right hand side.So, the title is bid price versus coupon rate like how we have assigned it on the y axis Ihave bid price and I have labeled it as bid price and on the x axis,I have coupon rate and I have labeled it as coupon rate.Now, we see a linear trend. Now there are some points which are completely outside the rangeof coupon rate. Now let us see if our linear model will help us to identify these points.So, to start with let us build a linear regression model. So, building a linearmodel is done using the function lm. So, the inputs for the function are formula and data,by formula I mean I am regressing dependent variable versus the independent variable.So, how is it translated to a formula? So, I have dependent variable I have a tillet sign followedby the independent variable. So, the tillet sign tells us regress the dependent variable with theindependent variable So, there are 2 ways to build the linear model, let us see how to dothat. The first way tells us linear model which is a function. So, I am accessing the individualvariables which is bid price and coupon rate using the data frame followed by the dollar symbol. So,take bid price from the bonds data and take coupon rate from the bonds data and regress them.There is also another way. So, instead of mentioning the name of the data frameto access the variables, we can directly mention the name of the variable and givedata equal to bonds. So, access these variables from the bonds data. So,assuming our equation is of the form yi hat equal to beta naught hat plus beta onehat xi plus epsilon i. So, epsilon i is the error which will be called as residuals. So,beta naught hat is the intercept and beta one hat is the slope So, hereafter these estimateswill be referred to as intercept and slope. So, now that we have built a linear model andhave saved it as an object bondsmod let us see how to fit the regression line over the plot.We will use a function called ab line and the input for thefunction is bondsmod which is my linear model.Now, we have already gone back and seen how to plot coupon rate and bid price. Now in additionto the plot you need to mention this command. So, ab here refers to the intercept and slope.If your equation is of the form y equal to a plus bx, then a is my intercept and b is my slope.In this case a is beta naught hat and b is beta one hat. So, let us see how the plot looks. So,on my right I have the plot we are now able to see how the regressionline fits. It fits pretty badly and it is also not identify the outliers. So,we can say that regression line is indeed getting affected by these outliers.So, now let us take a look at the model summary. So, I have regressbid price versus coupon rate from the data bonds, you can also use the other command.So, summary is a function the input to the summary function becomes a linear model. So,we have bondsmod as the linear model, this is the first look at the summary. So,this is how it looks when you run the command and this is how it would look in the callzone.So, we have 4 sections of output we have call, we have residual, we have coefficients, and we havesome few heuristics at the bottom. So, now, let us look at each of these and what they mean in depth.Now, call displays the formula which we have used. So, in this case I have used the formulabidprice versus coupon rate so regress bidprice, which is my dependent variablewith my independent variable which is coupon rate and from the data bonds.Now, this is the way for you to check if you have given the right dependentand independent variable. The next section is residual. So,what are residuals are nothing, but difference between the observed and predicted values. So,in our equation earlier we saw we had a parameter called epsilon i. So, that epsilon i correspondsto residuals below the residuals is the 5 number summary for the residual.So, the next section is coefficients. We see 2 rows which is intercept and coupon rate andcertain set of values associated with them. Now these intercept and coupon rate are nothing,but beta 0 hat and beta 1 hat, we earlier saw for our equation y equal to beta 0 hat plusbeta 1 hat into xi plus epsilon I, beta 0 hat is the intercept and beta 1 hat is the slope.Now, let us see what other 4 parameters in the column have to say. Now I havethe first column which is estimate. Now this is nothing, but the estimatefor the slope and intercept parameter. The next column is standard error. So,standard error is the estimated standard deviation associated for the slope and intercept.I have the next column as t value. So, what is t value? It is the ratioof estimate by the standard error and it is also an important criterion for thehypothesis testing. The column after that is the probability.So, it is the probability of seeing a t random variable, which will be greater than theobserved t statistic. So, we can see few stars being indicated at the end. So, what are thesestars. These stars tell us the significance level above, which the null hypothesis will be rejected.So, what is the null hypothesis? The null hypothesis is that the estimates will beequal to 0. At the last line I have an F statistic and a corresponding p-value associated with it.Now the F statistic is again used to test the null hypothesis which is nothing, but slope equal to 0.So, in this lecture we saw how to load a data, how to plot and how to visualize,how to build a linear model and how to interpret the results from the linear model? So, in thenext lecture we will see how to assess our model and we will see if we can improvise our model.Thank you.', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_1.txt'}, 'embedding': None, 'id': '722f2916fcc0b13905f898b697c543ba'}>, <Document: {'content': '\\nDATA SCICENCE FULL COURSE- 10 HOURS -EDUREKA', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data FileA_7.txt'}, 'embedding': None, 'id': '4b6c03d31b098afa8394fe788069cd43'}>, <Document: {'content': \"Undoubtedly, Data Science is the most revolutionary technology of the era. It's all about deriving useful insights from datain order to solve real-world complex problems. Hi all I welcome you to this sessionon Data Science full course that contains everything that you need to know in order to master data science.Now before we get started, let's take a look at the agenda. The first module is an reduction to data sciencethat covers all the basic fundamentals of data science followed by this. We have statistics and probability modulewhere you'll understand the statistics and math behind data science and machine learning algorithms.The next module is the basics of machine learning where will understand what exactly machine learning is the different typesof machine learning the different machine learning algorithms and so on the next module is the supervised learningalgorithms module where we'll start by understanding the most basic With them or which is linear regression.The next module is the logistic regression module where we will see how logistic regression can be used to solveclassification problems. After this we'll discuss about decision trees and we'll see how decision trees can be used to solvecomplex data-driven problems. The next module is random Forest here will understandhow random Forest can be used to solve classification problems and regression problems with the helpof use cases and examples. The next module will be be discussing is the k-nearest neighbor module.We will understand how gain and can be used to solve complex classification problems followed by this.We look at the naive bias module, which is one of the most important algorithms in the Gmail spam detection.The next algorithm is support Vector machine where we will understand how svm's can be usedto draw a hyperplane between different classes of data. Finally. We move on to the unsupervised learning module where wewill understand how genes can be used for clustering. And how you can perform Market Basket analysis by usingAssociation rule mining. The next module is reinforcement learning where we will understand the different conceptsof reinforcement learning along with a couple of demonstrations followed by this bill.Look at the Deep learning module where we will understand what exactly deep learning is what our neural networkswith different types of neural networks. And so on. The last module is the data science interview questions modulewhere we will understand the important concepts of data. Along with a few tips in order to Ace the interview nowbefore we get started make sure you subscribe to Adorama YouTube channel in order to stay updatedabout the most trending Technologies data science is oneof the most in-demand Technologies right now. Now this is probably because we're generating data at an Unstoppable pace.And obviously we need to process and make sense out of this much data. This is exactly where data science comes in in today's session.We'll be talking about data science in depth. So let's move ahead and take a look at today's agenda. We're going to beginwith discussing the various sources of data and how the evolution of technology and introduction of IODand social media have led to the need of data sign next. We'll discuss how Walmart is using insightful patternsfrom their database to increase the potential of their business. After that. We will see what exactly data science is,then we'll move on and discuss who are data scientist is where we will also discuss the various skill sets.Needed to become a data scientist next we can move on to see the various data science job rolessuch as data analyst data architect data engineer and so on after this we will cover the data life cycle where we will discusshow data is extracted processed and finally use as a solution. Once we're done with that. We'll cover the basics of machine learningwhere we'll see what exactly machine learning is and the different types of machine learning next. We will move onto the K means algorithmand we'll discuss a use case of the k-means clustering after which we Discuss the various steps involvedin the k-means algorithm and then we will finally move on to the Hands-On part where we use the k-means algorithm to Cluster moviesbased on their popularity on social media platforms, like Facebook at the end of today's sessionwill also discuss about what a data science certification is and why you should take it up. So guys, there's a lot to cover in today's session.Let's jump into the first topic. Do you guys remember the times when we have telephones and wehad to go to PC your boots in order to make a phone call. Call now those things are very simple because we didn't generate a lot of data.We didn't even store the contacts and our phones or our telephones. We used to memorize phone numbers back then or you know,these have a diary of all our contact but these days we have smartphones with store a lot of data.So there's everything about us in our mobile phones. We have images we have contacts. We have various apps.We have games. Everything is stored on a mobile phones these days similarly the PCS that we use in the earlier times.It used to process very little data. All right, there was A lot of data processing needed because technology was an evolved that much.So if you guys remember we use floppy disk back then and floppy. This was used to store small amounts of data,but later on hard disks were created and those used to store GBS of data. But now if you look around there's dataeverywhere around us. All right, we have a data stored in the cloud. We have data in each and every Appliance at our houses.Similarly. If you look at smart cars these days they're connected to the internet they connected to a mobile phonesand this also generates a lot of data. What we don't realize is that evolution of technology has generated a lot of data.All right. Now initially there was very little data and most of it was even structured only a small partof the data was unstructured or semi-structured. And in those days you could use Simple bi Tools in orderto process all of this data and make sense out of it. But now we have way too much data and order to process this much data.We need more complex algorithms. We need a better process. All right, and this is where data science comes in now guys,I'm not going to get into the depth of data science. Yet I'm sure all of you have heard of iot or Internet of things.Now. Did you guys know that we produce 2.5 quintillion bytes of data each day.And this is only accelerating with the growth of iot. Now iot or Internet of Things is just a fancy termthat we use for network of tools or devices that communicate and transfer data through the internet.So various devices are connected to each other through the internet and they communicate with each other rightnow the communication happens by exchange of data or by. Generation of data now these devices include the vehicles.We drive the include our TVs of coffee machines refrigerators washing machines and almost everything elsethat we use in a daily basis. Now, these interconnected devices produce an unimaginableamount of data guys iot data is measured in zettabytes and one zettabyte is equal to trillion gigabytes.So according to a recent survey by Cisco. It's estimated that by the end of 2019,which is almost here. The iot will generate more than five hundred zettabytesof data per year. And this number will only increase through time. It's hard to imagine data in that much volume,imagine processing analyzing and managing this much of data. It's only going to cause as a migraineso guys having to deal with this much data is not something that traditional bi tools can do.Okay. We no longer can rely on traditional data processing methods. That's exactly why we need data science.It's our only hope right now now let's not get into the details here. Yet moving on. Let's see how social media is adding onto the generation of data. Now the fact that we are all in love with social media. It's actually generating a lot of data for us.Okay. It's certainly one of the fuels for data creation Now all these numbers that you see on the screen are generated every minuteof the day. Okay, and this number is just going to increase so for Instagram it says that approximately 1.7 million pictures uploadedin a minute and similarly on Twitter approximately. A hundred and forty eight thousand tweets are publishedevery minute of the day. So guys imagine in one are how much that would be and then imagine in 24 hours.So guys, this is the amount of data that is generated through social media. It's unimaginable. Imagine processing this muchdata analyzing it and then trying to figure out, you know, the important insights from this much data analyzingthis much data is going to be very hard with traditional tools or traditional methods. That's why data science was introduced data scienceis a simple process that will just extract the useful information from data. All right, it's just going to processand analyze the entire data and then it's just going to extract what is needed now guys apart from social media and iot,there are other factors as well which contribute to data generation these days all our transactions are done online, right?We pay bills online. We shop online. We even buy homes online these days you can even sell your pets on oil excuses.Not only that when we stream music and Watch videos on YouTube all of this is generating a lotof data not to forget. We've also brought Health Care into the internet wall. Now there are various watches like bit fitwhich basically trans our heart rate and it generates data about a health conditions education isalso an online thing right now. That's exactly what you are doing right now. So with the emergence of the internet,we now perform all our activities online. Okay, obviously, this is helping us, but we are unaware of how much data we are generatingwhat can be done with All of this data and what if we could use the data that we generated to our benefit?Well, that's exactly what data science does data science is all about extracting the useful insights from data and usingit to grow your business. Now before we get into the details of data science, let's see how Walmart uses data science to grow that business.So guys Walmart is the world's biggest retailer with over 20,000 stores in just 28 countries.Okay. Now, it's currently building the world's biggest. Good Cloud, which will be able to process two point five petabytesof data every hour now. The reason behind Walmart success is how the user customer datato get useful insights about customers shopping patterns. Now the data analyst and the data scientist at Walmart.They know every detail about their customers. They know that if a customer buys Pop-Tarts, they might also buy cookies, how do they know all of this?Like how do they generate information like this now the user data that they get from their customers.Hours and the analyze it to see what a particular customer is looking for. Now. Let's look at a few caseswhere Walmart actually analyze the data and they figured out the customer needs. So let's consider the Halloweenand the cookie sales example now during Halloween sales Analyst at Walmart took a look at the data.Okay, and he found out that a specific cookie was popular across all Walmart stores.So every Walmart store was selling these cookies very well, but he found out that they would to stores which are not selling.A DOT. Okay. So the situation was immediately investigated and it was found that there was a simple stocking oversight.Okay, because of which the cookies were not put on the shelves for sale. So because this issue was immediately identifiedthey prevented any further loss of sales now another such example, is that true Association rule mining Walmart found outthat strawberry Pop-Tart sales increased by seven times before a hurricane. So a data analyst at Walmart identified the associationbetween ha Hurricane and strawberry pop tarts through data mining now guys. Don't ask me the relationship between Pop-Tartsand Harry Caine, but for some reason whenever there was a hurricane approaching people really wanted to eat strawberry Pop-Tart.So what Walmart did was they place all the strawberry Pop-Tarts? I will check out before a hurricane would occur.So this way the increase sales of the Pop-Tarts Now, where's this is a natural thing. I'm not making it up.You can look it up on the internet. Not only that Walmart is analyzing the data generatedby Social media to find out all the training product so through social media. You can find out the likes and dislikes of a person right?So what Walmart did is they are quite smart the user data generated by social media to find out what products are trendingor what products are liked by customers. Okay an example of this is 1 mod analyze social media data to find outthat Facebook users were crazy about cake pops. Okay, so Walmart immediately took a decisionand they introduced cake pops into the Walmart stores. So guys the only reason Walmart is so successful isbecause the huge amount of data that they get they don't see it as a burden instead. They process this data analyzeit and then you try to draw useful insights from it. Okay, so they invest a lot of money a lot of effortand a lot of time and data analysis. Okay, they spend a lot of time analyzing data in order to find any hidden patterns.So as soon as they find out hidden pattern or association between any two products, these are giving out offersor Started having discount or something along that line. So basically Walmart uses data in a very effective manner the analyzer very, well.They process the data very well and they find out the useful insights that they need in order to get more customers or in order to improve their business.So guys, this was all about how Walmart uses data science now, let's move ahead and look at what is data set nowguys data science is all about uncovering findings from data. It's all about surfacing the hidden insightsthat can help. Ponies to make smart business decisions. So all these hidden insights or these hidden patterns can be used to make better decisionsin a business now an example of this is also Netflix. So Netflix, basically analyzes the movie viewing patternsof users to understand what drives user interest and to see what users want to watch and thenonce they find out they give people what they want. So guys actually data has a lot of power.You should just know how to process this data and how to extract the useful information. From data.Okay. That's what data science is all about. So guys a big question over here is how do data scientists get useful insights from data.So it's all starts with data exploration. Whenever a data scientist comes across any challenging questionor any sort of challenging situation, they become detectives so the investigative leadsand they try to understand the different patterns or the different characteristics of the data. Okay.They try to get all the information that they can from the data and then Then they use it for the betterment of the organizationor the business. Now, let's look at who is a data scientist. So guys the data scientistshas to be able to view data through a quantitative lengths. So guys knowing math is one of the very important skillsof data scientists. Okay. So mathematics is important because in order to find a solution you're going to build a lot of predictive modelsand these predictive models are going to be based on hard math. So you have to be able to understand allthe Underlying mechanics with these models most of the predictive models most of the algorithms require mathematics.Now, there's a major misconception that data science is all about statistics.Now, I'm not saying that statistics is an important. It is very important, but it's not the only type of math that is utilizedin data science. There are actually many machine learning algorithms which are based on linear algebra.So guys overall you need to have a good understanding of math and apart from that data scientist.Eli's technology, so data scientists have to be really good with technology. Okay. So their main work is they utilize all the technologyso that they can analyze these enormous data sets and work with complex algorithms.So all of this requires tools, which are much more sophisticated than Excel so there's data scientist need to be very efficientwith coding languages and few of the core language has associated with data science include SQL python R & sass.It is also important for a data scientist. Be a tactical business consultant. So guys business problems can be on a sword by data scientistsince our data scientists work so closely with data they know everything about the business.If you have a business and you give the entire data set of your business stored data scientist,he know each and every aspect of your business. Okay? That's how data scientists work. They get the entire data set.They study the data set the analyze it and then we see where things are going wrong or what needs to be done more or what?Needs to be excluded. So guys having this business Acumen is just as important as having skillsin algorithms or being good with math and technology. So guys business is also as important asthese other fields now, you know who our data scientist is. Let's look at the skill sets that a data scientist names.Okay, it always starts with Statistics statistics will give you the numbers from the data.So a good understanding of Statistics is very important for becoming a data scientist. You have to be familiar with satisfaction.Contest distributions maximum likelihood estimators and all of that apartfrom that you should also have a good understanding of probability Theory and descriptive statistics.These Concepts will help you make Better Business decisions. So no matter what typeof company or role you're interviewing for. You're going to be expected to know how to use the tools of the trade.Okay. This means that you have to know a statistical programming language like our or Python and also you'll need to know or database.Wiring language like SQL now the main reason why people prefer our and python is because of the number of packagesthat these languages have and these predefined packages have most of the algorithms in them.So you don't have to actually sit down and code the algorithms instead. You can just load one of these packagesfrom their libraries and run it. So programming languages is a must at the minimum. You should know ouror python and a database query language now, let's move on to data extraction and processing.So guys That you have multiple data sources like mySQL database Mongo database.Okay. So what you have to do is you have to extract from such sources and then in order to analyzeand query this database you have to store it in a proper format or a proper structure.Okay, finally, then you can load the data in the data warehouse and you can analyze the data over here.Okay. So this entire process is called extraction and processing. So guys extractionand processing is all about getting data. From these different data sources and then putting it in a formatso that you can analyze it now next is data wrangling and exploration now guys data wrangling is oneof the most difficult tasks in data science. This is the most time-consuming task because data wrangling is all about cleaning the data.There are a lot of instances where the data sets have missing values or they have null valuesor they have inconsistent formats or inconsistent values and you need to understand what to do with such values.This is Data wrangling or data cleaning comes into the picture then after you're done with that. You are going to analyze the data.So where's after data wrangling and cleaning is done. You're going to start exploring. This is where you try to make sense out of the data.Okay, so you can do this by looking at the different patterns in the data the different Trends outliersand various unexpected results in all of that. Next. We have machine learning. So guys if you're a large companyor with huge amounts of data or if you're working at a company. See where the product is data driven,like if you're working in Netflix or Google Maps, then you have to be familiar with machine learning methods, right?You cannot process large amount of data with traditional methods. So that's why you need a machine learning algorithms.So there are few algorithms. Like knok nearest neighbor does random Forest this K means algorithm this support Vector machines,all of these algorithms. You have to be aware of all of these algorithms and let me tell youthat most of these algorithms can be implemented. Using our or python libraries. Okay, you need to have an understandingof machine learning. If you have large amount of data in front of you which is going to be the case for most of the people right nowbecause data is being generated at an Unstoppable Pace earlier in the session we discussedhow much of data is generated. So for now knowing machine learning algorithms and machine learning Concepts is a very required skillif you want to become a data scientist, so if you're sitting for an interview as a data scientist, you will be asked machine learning.Seems you will be asked how good you are with these algorithms and how well you can Implement them. Next we have big data processing Frameworks.So guys, we know that we've been generating a lot of data and most of this data can be structured or unstructured as well.So on such data, you cannot use traditional data processing system. So that's why you needto know Frameworks like Hadoop and Spark. Okay. These Frameworks can be used to handle big data lastly.We have data visualization. So guys data visualization is Is one of the most important partof data analysis, it is always very important to present the data in an understandable and Visually appealing format.So data visualization is one of the skills that data scientists have to master. Okay, if you want to communicate the data with the end usersin a better way then data visualization is a must so guys are a lot of tools which can be used for data visualization tools like Diabloand power bi are few the most popular visualization tools. So with this we sum up the entire skill setthat is needed to become a data scientist apart from this you should also have data-driven problem solving approach.You should also be very creative with data. So now that we know the skills that are needed to become a data scientist.Let's look at the different job roles just data science is a very vast field. There are many job roles under data science.So let's take a look at each role. Let's start off with a data scientist. So there's data scientists have to understand.The challenge is over business and they have to offer the best solution using data analysisand data processing. So for instance if they are expected to perform predictive analysis, they should also be able to identify Trends and patternsthat can have the companies in making better decisions to become a data scientist. You have to be an expert in our Matlab SQL Python and othercomplementary Technologies. It can also help if you have a higher degree in mathematicsor computer engineering next we have data. An analyst so a data analyst is responsiblefor a variety of tasks, including visualization processing of massive amount of data and among them.They have to also perform queries on databases. So they should be aware of the different query languagesand guys one of the most important skills of a data analyst is optimization. This is because they have to create and modify algorithmsthat can be used to pull information from some of the biggest databases without corrupting the dataso to become Be done. You must know Technologies such as SQL our SAS and python.So certification in any of these Technologies can boost your job application. You should also have a good problem solving quality.Next. We have a data architect. So a data architect creates the blueprints for a data managementso that the databases can be easily integrated centralized and protected with a best security measures.Okay. They also ensure that the data Engineers have the best tools and systems to work with So to become a data architect,you have to have expertise and data warehousing data modeling extraction transformation and loan.Okay. You should also be well versed in Hive Pig and Spark now apart from this there are data Engineers.So guys, the main responsibilities of a data engineer is to build and test scalable Big Data ecosystems.Okay, they are also needed to update the existing systems with newer or upgraded versionsand they are also responsible for improving the efficiency. For database now. If you are interested in a career as a data engineer,then technologies that require hands-on experience include Hive nosql are Ruby Java C++ and Matlab,it would also help if you can work with popular data apis and ETL tools next.We have a statistician. So as the name suggests you have to have a sound understandingof statistical theories and data organization. Not only do they extract and offer valuable insights.They also create new. Methodologies for engineers to apply now. If you want to become a statistician then you haveto have a passion for logic. They are also good variety of database systems such as SQL Data Miningand other various machine learning Technologies by that. I mean, you should be good with math and you should alsohave a good knowledge about the weight is database system such as SQL and also the various machine learning Conceptsand algorithms is the most next we have the database administrator. So guys the job profile of a database administratoris Much self-explanatory, they are basically responsible for the proper functioning of all the databasesand they are also responsible for granting permission or the working in services to the employees of the company.They also have to take care of the database backups and recoveries. So some of the skills that are needed to become a database administrator includedatabase backup and Recovery data security data modeling and design next. We have the business analyst now the role of a business analystis a little It different from all of the other data signs job now. Don't get me wrong. They have a verygood understanding of the data oriented Technologies. They know how to handle a lot of data and process itbut they are also very focused on how this data can be linked to actionable business inside.So they mainly focus on business growth. Okay. Now a business analyst acts like a link between the data engineers and the management Executives.So in order to become a business analyst you have to have an understanding of business finances business intelligence.And also I did acknowledge, he's like data modeling data visualization tools and Etcat last we have a data and analytics manager a data and analytics manager is responsible for the data science operations.Now the main responsibilities of a data and analytics manager is to oversee the data science operation.Okay, he's responsible for assigning the duties to the team according to their skills and expertise now their strength should include Technologieslike SAS our SQL. And of course, they should have good management skills apart from that.They must have excellent social skills leadership qualities and and out-of-the-box thinking attitude.And like I said earlier you need to have a good understanding of Technologies. Like pythons as our Java and Etc.So Guys, these were the different job roles in data science. I hope you all found this informative.Now, let's move ahead and look at the data lifecycle. So guys are basically six steps in the data life cycle.It starts with a business requirement. Next is the data acquisition after that you would process the datawhich is called data processing. Then there is data exploration modeling and finally deployment.So guys before you even start on a data science project. It is important that you understand the problem you're trying to solve.So in this stage, you're just going to focus on identifying the central objectives of the project and you will do this by identifying the variablesthat need to be predicted next up. We have data acquisition. Okay. So now that you have your objectives I find it's timefor you to start Gathering the data. So data mining is the process of gathering your data from different sourcesat this stage some of the questions you can ask yourself is what data do I need for my project?Where does it live? How can I obtain it? And what is the most efficient way to storeand access all of it? Next up there is data processing now usually all the datathat you collected is a huge mess. Okay. It's not formatted. It's not structured. It's not cleaned.So if Find any data set that is cleaned and it's packaged well for you, then you've actually won the lotterybecause finding the right data takes a lot of time and it takes a lot of effort and one of the major time-consuming taskin the data science process is data cleaning. Okay, this requires a lot of time. It requires a lot of effortbecause you have to go through the entire data set to find out any missing values or if there are any inconsistent valuesor corrupted data, and you also find the unnecessary data. Over here and you remove that data.So this was all about data processing next we have data exploration. So now that you have sparkling clean set of data,you are finally ready to get started with your analysis. Okay, the data exploration stage is basically the brainstormingof data analysis. So in order to understand the patterns in your data, you can use histogram.You can just pull up a random subset of data and plot a histogram. You can even create interactive visualizations.This is the point where you Dive deep into the data and you try to explore the different modelsthat can be applied to your data next up. We have data modeling. So after processing the data,what you're going to do is you're going to carry out model training. Okay. Now model training is basically about finding a modelthat answers the questions more accurately. So the process of model training involves a lot of steps.So firstly you'll start by splitting the input data into the training data set and the testing data set.Okay, you're going to take the entire data set and you're going to separate it into Two two parts one is the training and one is the testing dataafter that your build a model by using the training data set and once you're done with that, you'll evaluate the trainingand the test data set now to evaluate the training and testing data. So you'll be using seriesof machine learning algorithms after that. You'll find out the model which is the most suitable for your business requirement.So this was mainly data modeling. Okay. This is where you build a model out of your training data setand then you evaluate this model by using the testing data set. You have deployment.So guys a goal of this stage is to deploy the model into a production or maybe a production like environment.So this is basically done for final user acceptance and the users have to validate the performance of the modelsand if there are any issues with the model or any issues with the algorithm, then they have to be fixed in this stage.So guys with this we come to the end of the data lifecycle. I hope this was clear statistics and probability are essentialbecause these disciples form the basic Foundation of all machine learning algorithms deep learning artificial intelligenceand data science. In fact, mathematics and probability is behind everything around us from shapes patternsand colors to the count of petals in a flower mathematics is embedded in each and every aspect of our lives with this in mind.I welcome you all to today's session. So I'm going to go ahead and Scoffs the agenda for todaywith you all now going to begin the session by understanding what is data after that.We'll move on and look at the different categories of data, like quantitative and qualitative data,then we'll discuss what exactly statistics is the basic terminologies in statistics and a coupleof sampling techniques. Once we're done with that. We'll discuss the different types of Statisticswhich involve descriptive and inferential statistics. Then in the next session will mainly be focusingon descriptive statistics here will understand the different measures of center measuresof spread Information Gain and entropy will also understand all of these measures with the help of a use case and finally we'll discusswhat exactly a confusion Matrix is once we've covered the entire descriptive statistics modulewill discuss the probability module here will understand what exactly probability is the different terminologiesin probability will also study the Different probability distributions, then we'll discuss the types of probability which includemarginal probability joint and conditional probability. Then we move on and discuss a use casewhere and we'll see examples that show us how the different types of probability workand to better understand Bayes theorem. We look at a small example. Also, I forgot to mentionthat at the end of the descriptive statistics module will be running a small demo in the our language.So for those of you who don't know much about our I'll be explaining every line in depth, but if you want to have a more in-depth understandingabout our I'll leave a couple of blocks. And a couple of videos in the description boxyou all can definitely check out that content. Now after we've completed the probability module will discussthe inferential statistics module will start this module by understanding what is point estimation.We will discuss what is confidence interval and how you can estimate the confidence interval.We will also discuss margin of error and will understand all of these concepts by looking at a small use case.We'd finally end the inferential Real statistic module by looking at what hypothesis testing is hypothesis.Testing is a very important part of inferential statistics. So we'll end the session by looking at a use casethat discusses how hypothesis testing works and to sum everything up. We'll look at a demothat explains how inferential statistics Works. Alright, so guys, there's a lot to cover today.So let's move ahead and take a look at our first topic which is what is data.Now, this is a quite simple question if I ask any of You what is data? You'll see that it's a set of numbersor some sort of documents that have stored in my computer now data is actually everything.All right, look around you there is data everywhere each click on your phone generates more data than you know,now this generated data provides insights for analysis and helps us make Better Business decisions.This is why data is so important to give you a formal definition data refers to facts and statistics.Collected together for reference or analysis. All right. This is the definition of data in termsof statistics and probability. So as we know data can be collected it can be measured and analyzedit can be visualized by using statistical models and graphs now data is divided into two major subcategories.Alright, so first we have qualitative data and quantitative data. These are the two different types of dataunder qualitative data. We have nominal and ordinal data and under quantitative data.We have discrete and continuous data. Now, let's focus on qualitative data.Now this type of data deals with characteristics and descriptors that can't be easily measuredbut can be observed subjectively now qualitative data is further dividedinto nominal and ordinal data. So nominal data is any sort of datathat doesn't have any order or ranking? Okay. An example of nominal data is gender.Now. There is no ranking in gender. There's only male female or other right?There is no one two, three four or any sort of ordering in gender race is another example of nominal data.Now ordinal data is basically an ordered series of information.Okay, let's say that you went to a restaurant. Okay. Your information is stored in the form of customer ID.All right. So basically you are represented with a customer ID. Now you would have rated their service aseither good or average. All right, that's how no ordinal data is and similarly they'll have a record of other customerswho visit the restaurant along with their ratings. All right. So any data which has some sort of sequenceor some sort of order to it is known as ordinal data. All right, so guys, this is pretty simple to understand now,let's move on and look at quantitative data. So quantitative data basically these He'swith numbers and things. Okay, you can understand that by the word quantitative itself quantitative isbasically quantity. Right Saudis will numbers a deals with anything that you can measure objectively.All right, so there are two types of quantitative data there is discrete and continuous datanow discrete data is also known as categorical data and it can hold a finite number of possible values.Now, the number of students in a class is a finite Number. All right, you can't have infinite numberof students in a class. Let's say in your fifth grade. They have a hundred students in your class. All right, there weren't infinite number but therewas a definite finite number of students in your class. Okay, that's discrete data. Next.We have continuous data. Now this type of data can hold infinite number of possible values.Okay. So when you say weight of a person is an example of continuous datawhat I mean to see is my weight can be 50 kgs or it NB 50.1 kgsor it can be 50.00 one kgs or 50.000 one or is 50.0 2 3 and soon right there are infinite number of possible values, right? So this is what I mean by a continuous data.All right. This is the difference between discrete and continuous data. And also I'd like to mention a few other things over here.Now, there are a couple of types of variables as well. We have a discrete variableand we have a continuous variable discrete variable is also known as a categorical variableor and it can hold values of different categories. Let's say that you have a variable called messageand there are two types of values that this variable can hold let's say that your message can either be a Spam messageor a non spam message. Okay, that's when you call a variable as discrete or categorical variable.All right, because it can hold values that represent different categories of data now continuous variables are basically variablesthat can store infinite number of values. So the weight of a person can be denoted asa continuous variable. All right, let's say there is a variable called weight and it can store infinite number of possible values.That's why we will call it a continuous variable. So guys basically variable is anything that can store a value right?So if you associate any sort of data with a Able, then it will become either discrete variableor continuous variable. There is also dependent and independent type of variables. Now, we won't discuss all of that in death becausethat's pretty understandable. I'm sure all of you know, what is independent variable and dependent variable right?Dependent variable is any variable whose value depends on any other independent variable?So guys that much knowledge I expect or if you do have all right. So now let's move on and look at our next topic which Which iswhat is statistics now coming to the formal definition of statistics statistics is an area of Applied Mathematics,which is concerned with data collection analysis interpretation and presentation now usuallywhen I speak about statistics people think statistics is all about analysisbut statistics has other parts to it it has data collection is also a part of Statistics data interpretation presentation.All of this comes into statistics already are going to use statistical methods to visualize data to collect data to interpret data.Alright, so the area of mathematics deals with understanding how data can be used to solve complex problems.Okay. Now I'll give you a couple of examples that can be solved by using statistics. Okay, let's saythat your company has created a new drug that may cure cancer. How would you conduct a test to confirmthe As Effectiveness now, even though this sounds like a biology problem. This can be solvedwith Statistics already will have to create a test which can confirm the effectiveness of the drumor a this is a common problem that can be solved using statistics. Let me give you another example youand a friend are at a baseball game and out of the blue. He offers you a betthat neither team will hit a home run in that game. Should you take the BET?All right here you just discuss the probability of I know you'll win or lose. All right, this is another problemthat comes under statistics. Let's look at another example. The latest sales data has just come inand your boss wants you to prepare a report for management on places where the company could improve its business.What should you look for? And what should you not look for now? This problem involves a lotof data analysis will have to look at the different variables that are causing your business to go downor the you have to look at a few variables. That are increasing the performance of your modelsand thus growing your business. Alright, so this involves a lot of data analysis and the basic ideabehind data analysis is to use statistical techniques in order to figure out the relationshipbetween different variables or different components in your business. Okay. So now let's move on and look at our next topicwhich is basic terminologies in statistics. Now before you dive deep into statistics,it is important that you understand basic terminologies used in statistics.The two most important terminologies in statistics are population and Sample.So throughout the statistics course or throughout any problem that you're trying to stall with Statistics.You will come across these two words, which is population and Sample Now population is a collectionor a set of individuals or objects or events. Events whose properties are to be analyzed.Okay. So basically you can refer to population as a subject that you're trying to analyze now a sample is justlike the word suggests. It's a subset of the population. So you have to make sure that you choose the samplein such a way that it represents the entire population. All right. It shouldn't Focus add one part of the population instead.It should represent the entire population. That's how your sample should be chosen.So Well chosen sample will contain most of the information about a particular population parameter.Now, you must be wondering how can one choose a sample that best represents the entire population nowsampling is a statistical method that deals with the selection of individual observationswithin a population. So sampling is performed in order to infer statistical knowledge about a population.All right, if you want to understand the different statistics of a population like the mean the median Median the mode or the standard deviationor the variance of a population. Then you're going to perform sampling. All right, because it's not reasonable for you to studya large population and find out the mean median and everything else. So why is sampling performed you might ask?What is the point of sampling? We can just study the entire population now guys, think of a scenariowhere in your asked to perform a survey about the eating habits of teenagers in the US.So at present there are over 42 million teens in the US and this number is growingas we are speaking right now, correct. Is it possible to survey each of these 42 million individualsabout their health? Is it possible? Well, it might be possiblebut this will take forever to do now. Obviously, it's not it's not reasonable to go aroundknocking each door and asking for what does your teenage son eat and all of that right? This is not very reasonable.That's By sampling is used. It's a method wherein a sample of the population is studiedin order to draw inferences about the entire population. So it's basically a shortcut to studyingthe entire population instead of taking the entire population and finding out all the solutions.You just going to take a part of the population that represents the entire population and you're going to perform all your statistical analysisyour inferential statistics on that small sample. All right, and that sample basically here Presents the entire population.All right, so I'm short of made this clear to y'all what is sample and what is population now?There are two main types of sampling techniques that are discussed today. We have probability sampling and non-probabilitysampling now in this video will only be focusing on probability sampling techniquesbecause non-probability sampling is not within the scope of this video. All right will only discuss the probability partbecause we're focusing on statistics and probability, correct. Now again under probability sampling.We have three different types. We have random sampling systematic and stratified sampling.All right, and just to mention the different types of non-probability sampling,'s we have no bald Kota judgment and convenience sampling. All right now guys in this session.I'll only be focusing on probability. So let's move on and look at the different types of probability sampling.So what is probability sampling it is a sampling technique in which samplesfrom a large population are chosen by using the theory of probability.All right, so there are three types of probability sampling. All right first we have the random sampling nowin this method each member of the population has an equal chance of being selected in the sample.All right, so each and every individual or each and every object in the population has an equal John'sof being a part of the sample. That's what random sampling is all about. Okay, you are randomly going to select any individualor any object. So this Bay each individual has an equal chance of being selected.Correct? Next. We have systematic sampling now in systematic sampling every nth record is chosenfrom the population to be a part of the sample. All right. Now refer this imagethat I've shown over here out of these six. Groups every second group is chosen as a sample.Okay. So every second record is chosen here and this is our systematic sampling works.Okay, you're randomly selecting the nth record and you're going to add that to your sample.Next. We have stratified sampling now in this type of technique a stratum is used to form samplesfrom a large population. So what is a stratum a stratum is basically a subsetof the population that shares at One common characteristics. So let's saythat your population has a mix of both male and female so you can create to straightensout of this one will have only the male subset and the other will have the female subset. All right, this is what stratum is.It is basically a subset of the population that shares at least one common characteristics.All right in our example, it is gender. So after you've created a stratum you're going to use random samplingon these stratums and you're going to choose. Choose a final sample. So random sampling meaningthat all of the individuals in each of the stratum will have an equal chance of being selected in the sample.Correct. So Guys, these were the three different types of sampling techniques. Now, let's move on and look at our next topicwhich is the different types of Statistics. So after this, we'll be looking at the more advanced concepts of Statistics,right so far we discuss the basics of Statistics, which is basically what is statistics the Friendsampling techniques and the terminologies and statistics. All right. Now we look at the different types of Statistics.So there are two major types of Statistics descriptive statistics and inferential statistics in today's session.We will be discussing both of these types of Statistics in depth. All right, we'll also be looking at a demowhich I'll be running in the our language in order to make you understand what exactlydescriptive and inferential statistics is soaked. As which is going to look at the basic, so don't worry.If you don't have much knowledge, I'm explaining everything from the basic level. All right, so guys descriptive statistics is a methodwhich is used to describe and understand the features of specific data set by giving a short summary of the data.Okay, so it is mainly focused upon the characteristics of data. It also provides a graphical summary of the data nowin order to make you understand what descriptive statistics is. Let's suppose that you want to gift allyour classmates or t-shirt. So to study the average shirt size of a student in a classroom.So if you were to use descriptive statistics to study the average shirt size of students in your classroom,then what you would do is you would record the shirt size of all students in the class and then you would find out the maximum minimum and averageshirt size of the cloud. Okay. So coming to inferential statistics inferential.Six makes inferences and predictions about a population based on the sample of data taken from the population.Okay. So in simple words, it generalizes a large data set and it applies probabilityto draw a conclusion. Okay. So it allows you to infer data parameters based on a statistical model by using sample data.So if we consider the same example of finding the average shirt size of students in a classin infinite real statistics. We'll take a sample set of the class which is basically a few people from the entire class.All right, you already have had grouped the class into large medium and small. All right in this method you basically builda statistical model and expand it for the entire population in the class. So guys, there was a brief understanding of descriptiveand inferential statistics. So that's the difference between descriptive and inferential now in the next section,we will go in depth about descriptive statistics. Right. So let's discuss more about descriptive statistics.So like I mentioned earlier descriptive statistics is a method that is used to describe and understand the featuresof a specific data set by giving short summaries about the sample and measures of the data.There are two important measures in descriptive statistics. We have measure of central tendency,which is also known as measure of center and we have measures of variability. This is also known as Measures of spreadso measures of center include mean median and mode now what is measuresof center measures of the center are statistical measures that represent the summary of a data set?Okay, the three main measures of center are mean median and mode coming to measures of variabilityor measures of spread. We have range interquartile range variance and standard deviation.All right. So now let's discuss each of these measures. Has in a little more depth startingwith the measures of center. Now, I'm sure all of you know, what the mean is mean is basically the measureof the average of all the values in a sample. Okay, so it's basically the average of allthe values in a sample. How do you measure the mean I hope all of you know how the main is measuredif there are 10 numbers and you want to find the mean of these 10 numbers. All you have to do is you have to add up all the 10 numbersand you have to divide it by 10 then. Represents the number of samples in your data set.All right, since we have 10 numbers, we're going to divide this by 10. All right, this will give us the averageor the mean so to better understand the measures of central tendency. Let's look at an example.Now the data set over here is basically the cars data set and it contains a few variables.All right, it has something known as cars. It has mileage per gallon cylinder type displacementhorsepower and relax. Silver ratio. All right, all of these measures are related to cars.Okay. So what you're going to do is you're going to use descriptive analysis and you're going to analyze each of the variablesin the sample data set for the mean standard deviation median more and so on.So let's say that you want to find out the mean or the average horsepower of the cars among the population of cards.Like I mentioned earlier what you'll do is you'll check the average of all the values. So in this case we will take The sum of the horsepowerof each car and we'll divide that by the total number of cards. Okay, that's exactly what I've done here in the calculation part.So this hundred and ten basically represents the horsepower for the first car.All right. Similarly. I've just added up all the values of horsepower for each of the carsand I've divided it by 8 now 8 is basically the number of cars in our data set.All right, so hundred and three point six two five is what army mean is or the average of horsepower is all right.Now, let's understand what median is with an example. Okay. So to Define median median is basically a measureof the central value of the sample set is called the median. All right, you can see that it is the middle value.So if we want to find out the center value of the mileage per gallon among the populationof cars first, what we'll do is we'll arrange the MGP values in ascendingor descending Order and choose a middle value right in this case since we have eight values, right?We have eight values which is an even entry. So whenever you have even number of data pointsor samples in your data set, then you're going to take the average of the two middle values.If we had nine values over here. We can easily figure out the middle value and you know choose that as a median.But since they're even number of values we are going to take the average of the two middle values. All right.Right. So 22.8 and 23 are my two middle values and I'm taking the meanof those 2 and hence I get twenty two point nine, which is my median. All right, lastly,let's look at how mode is calculated. So what is mode the value that is most recurrentin the sample set is known as mode or basically the value that occurs most often.Okay, that is known as mode. So let's say that we want to find out the most common type of cylinderamong the population of cards. What we have to do is we will check the value which is repeated the most number of times here.We can see that the cylinders come in two types. We have cylinder of Type 4 and cylinder of type 6, right?So take a look at the data set. You can see that the most recurring value is 6 right.We have one two, three four and five. We have five six and we have one two, three.Yeah, we have three four types of lenders and five six types of lenders.So basically we have three four type cylinders and we have five six type cylinders.All right. So our mode is going to be 6 since 6 is more recurrent than 4 so guysthose were the measures of the center or the measures of central tendency. Now, let's move on and look at the measures of the spread.All right. Now, what is the measure of spread a measure of spread? Sometimes also calledas measure of dispersion is Used to describe the variability in a sample or population.Okay, you can think of it as some sort of deviation in the sample. All right, so you measure this with the helpof the different measure of spreads. We have range interquartile range variance and standard deviation.Now range is pretty self-explanatory, right? It is the given measure of how spread apart the valuesin a data set are the range can be calculated as shown in this formula.You basically going to subtract the maximum value in your data set from the minimum value in your data set.That's how you calculate the range of the data. Alright, next we have interquartile range.So before we discuss interquartile range, let's understand. What a quartile is red.So quartiles basically tell us about the spread of a data set by breaking the data set into different quarters.Okay, just like how the median breaks the data into two parts the court is We'll break it into different quarters.So to better understand how quartile and interquartile are calculated. Let's look at a small example.Now this data set basically represents the marks of hundred students ordered from the lowestto the highest scores red. So the quartiles lie in the following ranges the first quartile,which is also known as q1 it lies between the 25th and 26th observation.All right. So if you look at this I've highlighted Add the 25th and the 26th observation.So how you can calculate Q 1 or first quartile is by taking the average of these two values.Alright, since both the values are 45 when you add them up and divide them by two you'll still get 45 now the second quartileor Q 2 is between the 50th and the 51st observation. So you're going to take the average of 58 and 59and you will get a value of 58.5. Now, this is my second quarter the third quartile.Ah Q3 is between the 75th and the 76th observation here. Again, we'll take the average of the two valueswhich is the 75th value and the 76 value right and you'll get a value of 71.All right, so guys this is exactly how you calculate the different quarters. Now, let's look at what is interquartile range.So IQR or the interquartile range is a measure of variability based on dividing a data setinto quartiles now the The interquartile range is calculated by subtracting the q1 from Q3.So basically Q3 minus q1 is your IQ are so your IQR is your Q3 minus q1?All right. Now this is how each of the quartiles are each core tile represents a quarter,which is 25% All right. So guys, I hope all of you are clear with interquartile range and what our quartiles now,let's look at variance covariance is basically a measure that shows How much a random variable the firstfrom its expected value? Okay. It's basically the variance in any variable now variancecan be calculated by using this formula right here x basically represents any data point in your data setn is the total number of data points in your data set and X bar is basically the main of data points.All right. This is how you calculate variance variance is basically a Computing the squares of deviations.Okay. That's why it says s Square there. Now let's look at what is deviation deviation is just the differencebetween each element from the mean. Okay, so it can be calculated by using this simple formulawhere X I basically represents a data point and mu is the mean of the populationor add this is exactly how you calculate the deviation Now population variance and Sample variance are very specific towhether you're calculating the variance in your population data set or in your sample data set.That's the A difference between population and Sample variance. So the formula for population variance is pretty explanatory.So X is basically each data point mu is the mean of the population n is the number of samples in your data set.All right. Now, let's look at sample. Variance Now sample variance is the average of squared differences from the mean.All right here x i is any data point or any sample in your data set X bar is the meanof your sample. All right. It's not the main of your population. Ation, it's the mean of your sample.And if you notice n here is a smaller n is the number of data points in your sample.And this is basically the difference between sample and population variance. I hope that is clear comingto standard deviation is the measure of dispersion of a set of data from its mean.All right, so it's basically the deviation from your mean. That's what standard deviation is now to better understandhow the measures of spread are calculated. Let's look at a small use case. So let's see Daenerys has 20 dragons.They have the numbers nine to five four and so on as shown on the screen,what you have to do is you have to work out the standard deviation or at in order to calculate the standard deviation.You need to know the mean right? So first you're going to find out the mean of your sample set.So how do you calculate the mean you add all the numbers in your data set and divided by the total number of samples in your data setso you get a value of 7. Here then you calculate the rhs of your standard deviation formula.All right. So from each data point you're going to subtract the mean and you're going to square that. All right.So when you do that, you will get the following result. You'll basically get this 425 for 925and so on so finally you will just find the mean of the squared differences. All right.So your standard deviation will come up to two point nine eight three once you take the square root.So guys, it's pretty simple. It's a simple At the magic technique, all you have to do is you have to substitute the valuesin the formula. All right. I hope this was clear to all of you. Now let's move onand discuss the next topic which is Information Gain and entropy now. This is one of my favorite topics in statistics.It's very interesting and this topic is mainly involved in machine learning algorithms,like decision trees and random forest. All right, it's very important for you to know how Information Gain and entropy really work and why they areso essential in building machine learning models. We focus on the statistic parts of Information Gainand entropy and after that we'll discuss a use case. And see how Information Gain and entropy is used in decision trees.So for those of you who don't know what a decision tree is it is basically a machine learning algorithm.You don't have to know anything about this. I'll explain everything in depth. So don't worry. Now. Let's look at what exactly entropyand Information Gain Is Now guys entropy is basically the measure of any sort of uncertainty that is present in the data.All right, so it can be measured by using this formula. So here s is the set of all instances in the data setor all the data items in the data set n is the different type of classes in your data setPi is the event probability. Now this might seem a little confusing to y'all but when we go through the use case,you'll understand all of these terms even better. All right cam. The information gainedas the word suggests Information Gain indicates how much information a particular featureor a particular variable gives us about the final outcome. Okay, it can be measured by using this formula.So again here heads of s is the entropy of the whole data set s SJ is the numberof instances with the J value of an attribute a s is the total number of instances in the data set V is the set of distinct valuesof an attribute a h of s j is the entropy of subsets of instancesand hedge of a comma s is the entropy of an attribute a even though this seems confusing.I'll clear out the confusion. All right, let's discuss a small problem statement where we will understandhow Information Gain and entropy is used to study the significance of a model.So like I said Information Gain and entropy are very important statistical measuresthat let us understand the significance of a predictive model. Okay to get a more clear understanding.Let's look at a use case. All right now suppose we are given a problem statement.All right, the statement is that you have to predict whether a match can be played or Not by studying the weather conditions.So the predictor variables here are outlook humidity wind day is also a predictor variable.The target variable is basically played or a the target variable is the variable that you're trying to protect.Okay. Now the value of the target variable will decide whether or not a game can be played.All right, so that's why The play has two values. It has no and yes, no, meaning that the weather conditions are not good.And therefore you cannot play the game. Yes, meaning that the weather conditions are good and suitablefor you to play the game. Alright, so that was our problem statement. I hope the problem statement is clear to all of you nowto solve such a problem. We make use of something known as decision trees. So guys think of an inverted treeand each branch of the tree denotes some decision. All right, each branch is Is known as the branch knownand at each branch node, you're going to take a decision in such a manner that you will get an outcome at the end of the branch.All right. Now this figure here basically shows that out of 14 observations 9 observations result in a yes,meaning that out of 14 days. The match can be played only on nine days.Alright, so here if you see on day 1 Day 2 Day 8 day 9 and 11.The Outlook has been Alright, so basically we try to plaster a data setdepending on the Outlook. So when the Outlook is sunny, this is our data set when the Outlook is overcast.This is what we have and when the Outlook is the rain this is what we have. All right, sowhen it is sunny we have two yeses and three nodes. Okay, when the Outlook is overcast.We have all four as yes has meaning that on the four days when the Outlook was overcast.We can play the game. All right. Now when it comes to rain, we have three yeses and two nodes.All right. So if you notice here, the decision is being made by choosing the Outlook variableas the root node. Okay. So the root node is basically the topmost node in a decision tree.Now, what we've done here is we've created a decision tree that starts with the Outlook node.All right, then you're splitting the decision tree further depending on other parameters like Sunny overcast and rain.All right now like we know that Outlook has three values. Sunny overcast and brain so let me explain thisin a more in-depth manner. Okay. So what you're doing here is you're making the decision Tree by choosing the Outlook variableat the root node. The root note is basically the topmost node in a decision tree.Now the Outlook node has three branches coming out from it, which is sunny overcast and rain.So basically Outlook can have three values either it can be sunny. It can be overcast or it can be rainy.Okay now these three values Use are assigned to the immediate Branch nodes and for eachof these values the possibility of play is equal to yes is calculated. So the sunnyand the rain branches will give you an impure output. Meaning that there is a mix of yes and no right.There are two yeses here three nodes here. There are three yeses here and two nodes over here,but when it comes to the overcast variable, it results in a hundred percent pure subset.All right, this shows that the overcast baby. Will result in a definite and certain output.This is exactly what entropy is used to measure. All right, it calculates the impurity or the uncertainty.Alright, so the lesser the uncertainty or the entropy of a variable more significant is that variable?So when it comes to overcast there's literally no impurity in the data set. It is a hundred percent pure subset, right?So be want variables like these in order to build a model. All right now, we don't always Ways get lucky and we don't always findvariables that will result in pure subsets. That's why we have the measure entropy. So the lesser the entropy of a particular variable the mostsignificant that variable will be so in a decision tree. The root node is assigned the best attributeso that the decision tree can predict the most precise outcome meaning that on the root note.You should have the most significant variable. All right, that's why we've chosen Outlook or and now some of you might ask me why haven't you chosenovercast Okay is overcast is not a variable. It is a value of the Outlook variable.All right. That's why we've chosen our true cure because it has a hundred percent pure subsetwhich is overcast. All right. Now the question in your head is how do I decide which variableor attribute best Blitz the data now right now, I know I looked at the data and I told you that,you know here we have a hundred percent pure subset, but what if it's a more complex problemand you're not able to understand which variable will best split the data, so guys when it comesto decision tree Information and gain and entropy will help you understand which variable will best split the data set.All right, or which variable you have to assign to the root node because whichever variable is assigned to the root node.It will best let the data set and it has to be the most significant variable. All right. So how we can do this is we need to useInformation Gain and entropy. So from the total of the 14 instances that we saw nineof them said yes and five of the instances said know that you cannot play on that particular day.All right. So how do you calculate the entropy? So this is the formula you just substitute the values in the formula.So when you substitute the values in the formula, you will get a value of 0.9940. All right.This is the entropy or this is the uncertainty of the data present in a sample.Now in order to ensure that we choose the best variable for the root node. Let us look at all the possible combinationsthat you can use on the root node. Okay, so these are All the possible combinationsyou can either have Outlook you can have windy humidity or temperature. Okay, these are four variablesand you can have any one of these variables as your root note. But how do you selectwhich variable best fits the root node? That's what we are going to see by using Information Gain and entropy.So guys now the task at hand is to find the information gain for each of these attributes.All right. So for Outlook for windy for humidity and for temperature, we're going to find out the information.Nation gained all right. Now a point to remember is that the variable that results in the highest Information Gain must be chosenbecause it will give us the most precise and output information. All right. So the information gain for attribute windy will calculatethat first here. We have six instances of true and eight instances of false.Okay. So when you substitute all the values in the formula, you will get a value of zero point zero four eight.So we get a value of You 2.0 for it. Now. This is a very low value for Information Gain.All right, so the information that you're going to get from Windy attribute is pretty low. So let's calculate the information gainof attribute Outlook. All right, so from the total of 14 instances, we have five instances with say Sunny for instances,which are overcast and five instances, which are rainy. All right for Sonny. We have three yesesand to nose for overcast we have Or the for as yes for any we have three years and two nodes.Okay. So when you calculate the information gain of the Outlook variable will get a valueof zero point 2 4 7 now compare this to the information gain of the windy attribute.This value is actually pretty good. Right we have zero point 2 4 7 which is a pretty good valuefor Information Gain. Now, let's look at the information gain of attribute humidity now over here.We have seven instances with say hi and seven instances with same. Right and under the high Branch node.We have three instances with say yes, and the rest for instances would say no similarlyunder the normal Branch. We have one two, three, four, five six seven instances would say yesand one instance with says no. All right. So when you calculate the information gainfor the humidity variable, you're going to get a value of 0.15 one. Now.This is also a pretty decent value, but when you compare it to the Information Gain, Of the attribute Outlook it is less right now.Let's look at the information gain of attribute temperature. All right, so the temperature can hold repeat.So basically the temperature attribute can hold hot mild and cool. Okay under hot.We have two instances with says yes and two instances for no under mild. We have four instances of yes and two instances of noand under col we have three instances of yes and one instance of no. All right.When you calculate the information gain for this attribute, you will get a value of zero point zero to nine,which is again very less. So what you can summarize from here is if we look at the information gain for each of these variable will seethat for Outlook. We have the maximum gain. All right, we have zero point two four seven,which is the highest Information Gain value and you must always choose a variable with the highest Information Gain to split the dataat the root node. So that's why we assign The Outlook variable at the root node.All right, so guys. I hope this use case was clear. If any of you have doubts. Please keep commenting those doubts now,let's move on and look at what exactly a confusion Matrix is the confusion Matrix is the last topicfor descriptive statistics read after this. I'll be running a short demo where I'll be showing youhow you can calculate mean median mode and standard deviation variance and all of those valuesby using our okay. So let's talk about confusion Matrix now guys.What is the confusion Matrix now don't get confused. This is not any complex topic now confusion.Matrix is a matrix that is often used to describe the performance of a model.Right? And this is specifically used for classification models or a classifierand what it does is it will calculate the accuracy or it will calculate the performance of your classifierby comparing your actual results and Your predicted results. All right. So this is what it looks like to prositof true- and all of that. Now this is a little confusing. I'll get back to what exactly true positiveto negative and all of this stands for for now. Let's look at an example and let's try and understand what exactly confusion Matrix is.So guys. I made sure that I put examples after each and every topic because it's important you understand the Practical part of Statistics.All right statistics has literally nothing to do with Theory you need to understand how Calculationsare done in statistics. Okay. So here what I've done is let's look at a small use case.Okay, let's consider that your given data about a hundred and sixty-five patient's out of which hundred and five patients have a diseaseand the remaining 50 patients don't have a disease. Okay. So what you're going to do is you will build a classifierthat predicts by using these hundred and sixty five observations your feed all of these 165 observations to your classifierand It will predict the output every time a new patients detail is fed to the classifier right nowout of these 165 cases. Let's say that the classifier predicted.Yes hundred and ten times and no 55 times. Alright, so yes basically stands for yes.The person has a disease and no stands for know. The person has not have a disease. All right, that's pretty self-explanatory.But yeah, so it predicted that a hundred and ten times. Patient has a disease and 55 times thatnor the patient doesn't have a disease. However in reality only hundred and five patientsin the samples have the disease and 60 patients who do not have the disease, right?So how do you calculate the accuracy of your model? You basically build the confusion Matrix?All right. This is how the Matrix looks like and basically denotes the total number of observationsthat you have which is 165 in our case actual denotes the actual usein the data set and predicted denotes the predicted values by the classifier.So the actual value is no here and the predicted value is no here. So your classifier was correctly ableto classify 50 cases as no. All right, since both of these are no so 50it was correctly able to classify but 10 of these cases it incorrectly classified meaningthat your actual value here is no but you classifier predicted it as yes or athat's why this And over here similarly it wrongly predicted that five patients do not have diseaseswhereas they actually did have diseases and it correctly predicted hundred patients,which have the disease. All right. I know this is a little bit confusing. But if you look at these values no,no 50 meaning that it correctly predicted 50 values NoYes means that it wrongly predicted. Yes for the values are it was supposed to predict.No. All right. Now what exactly is? Is this true positive to negative and all of that?I'll tell you what exactly it is. So true positive are the cases in which we predicted a yesand they do not actually have the disease. All right, so it is basically this valuealready predicted a yes here, even though they did not have the disease.So we have 10 true positives right similarly true- is we predicted knowand they don't have the disease meaning that this is correct. False positive is be predicted.Yes, but they do not actually have the disease. All right. This is also known as type 1 error falls- is we predicted.No, but they actually do not have the disease. So guys basically false negative and true negatives are basicallycorrect classifications. All right. So this was confusion Matrix and I hope this concept is clear again guys.If you have doubts, please comment your doubt in the comment section. So guys that was descriptive statistics now,Before we go to probability. I promised all that will run a small demo in our all right,we'll try and understand how mean median mode works in our okay, so let's do that first.So guys again what we just discussed so far was descriptive statistics. All right, next we're going to discuss probabilityand then we'll move on to inferential statistics. Okay in financial statistics is basically the second type of Statistics.Okay now to make things more clear of you, let me just zoom in.So guys it's always best to perform practical implementations in order to understand the concepts in a better way.Okay, so here will be executing a small demo that will show you how to calculate the mean median mode variance standard deviationand how to study the variables by plotting a histogram. Okay. Don't worry. If you don't know what a histogram is.It's basically a frequency plot. There's no big signs behind it. Alright, this is a very simple demobut it also forms a foundation that everything. Machine learning algorithm is built upon.Okay, you can say that most of the machine learning algorithms actually all the machine learning algorithms and deep learning algorithms havethis basic concept behind them. Okay, you need to know how mean median mode and all of that is calculated.So guys am using the our language to perform this and I'm running this on our studio. For those of you who don't know our language.I will leave a couple of links in the description box. You can go through those videos. So what we're doing is we are randomly generated.Eating numbers and Miss storing it in a variable called data, right? So if you want to see the generated numbersjust to run the line data, right this variable basically stores all our numbers.All right. Now, what we're going to do is we're going to calculate the mean now. All you have to do in our is specify the word meanalong with the data that you're calculating the mean of and I was assigned this whole thing into a variable called meanJust hold the mean value of this data. So now let's look at the meanfor that abuser function called print and mean.All right. So our mean is around 5.99. Okay. Next is calculating the median.It's very simple guys. All you have to do is use the function median or write and pass the data as a parameter to this function.That's all you have to do. So our provides functions for each and everything. All right statistics is very easy when it comes to Rbecause R is basically a statistical language. Okay. So all you have to do is just name the functionand that function is Ready in built in your art. Okay, so your median is around 6.4.Similarly. We will calculate the mode. All right. Let's run this function.I basically created a small function for calculating the mode. So guys, this is our mode meaningthat this is the most recurrent value right now. We're going to calculate the variance and the standard deviation for that.Again. We have a function in are called as we're all right. All you have to do is pass the data to that function.Okay, similarly will calculate the standard deviation, which is basically the square root of your varianceright now will Rent the standard deviation, right? This is our standard deviation value.Now. Finally, we will just plot a small histogram histogram is nothing but it's a frequency plot already inshow you how frequently a data point is occurring. So this is the histogram that we've just created it's quite simple in ourbecause our has a lot of packages and a lot of inbuilt functions that support statistics.All right. It is a statistical language that is mainly used by data scientists or by dataand analysts and machine learning Engineers because they don't have to student code these functions.All they have to do is they have to mention the name of the function and pass the corresponding parameters.So guys that was the entire descriptive statistics module and now we will discuss about probability.Okay. So before we understand what exactly probability is, let me clear out a very common misconception peopleoften tend to ask me this question. What is the relationship between statistics and probability?So probability and statistics are related fields. All right. So probability is a mathematical method usedfor statistical analysis. Therefore we can say that a probability and statistics are interconnectedbranches of mathematics that deal with analyzing the relative frequency of events.So they're very interconnected feels and probability makes use of statistics and statistics makes useof probability or a they're very interconnected Fields. So that is the relationship between said It is six and probability.Now. Let's understand what exactly is probability. So probability is the measureof How likely an event will occur to be more precise. It is the ratioof desired outcome to the total outcomes. Now, the probability of all outcomes always sum up to 1 the probability will alwayssum up to 1 probability cannot go beyond one. Okay. So either your probability can be 0 or it can be 1or it can In the form of decimals like 0.5 to or 0.55 or it can be in the form of 0.5 0.7 0.9.But it's valuable always stay between the range 0 and 1 okay, another famous example of probability is rollinga dice example. So when you roll a dice you get six possible outcomes, right? You get one two, three four and five six phases of a dies now each possibilityonly has one outcome. So what is the probability that on rolling a dice? You will get 3 the probability is 1 by 6 rightbecause there's only one phase which has the number 3 on it out of six phases.There's only one phase which has the number three. So the probability of getting 3when you roll a dice is 1 by 6 similarly. If you want to find the probability of gettinga number 5 again, the probability is going to be 1 by 6. All right. So all of this will sum up to 1.All right, so guys, this is exactly what Ability is it's a very simple concept.We all learnt it in 8 standard onwards right now. Let's understand the different terminologiesthat are related to probability. Now that three terminologies that you often come across when we talk about probability.We have something known as the random experiment. Okay. It's basically an experiment or a process for whichthe outcomes cannot be predicted with certainty. All right. That's why you use probability.You're going to use probability in order to predict the outcome with Some sort of certainty sample space is the entire possible set of outcomesof a random experiment and event is one or more outcomes of an experiment.So if you consider the example of rolling a dice now, let's say that you want to find out the probabilityof getting a to when you roll the dice. Okay. So finding this probability is the random experimentthe sample space is basically your entire possibility. Okay. So one two, three,four five six Is are there and out of that you need to find the probability of getting a 2 right?So all the possible outcomes will basically represent your sample space gives a 1 to 6 are all your possible outcomes.This represents your sample space now event is one or more outcome of an experiment.So in this case my event is to get a tattoo when I roll a dice, right? So my event is the probability of getting a towhen I roll a dice, so guys, this is basically what random experiment samples.All space and event really means alright now, let's discuss the different types of events.There are two types of events that you should know about there is disjoint and non disjoint events.Disjoint events are events that do not have any common outcome. For example,if you draw a single card from a deck of cards, it cannot be a king and a queen correct it can either be kingor it can be Queen now a non disjoint events are events that have common out.For example a student can get hundred marks in statistics and hundred marks in probability.All right, and also the outcome of a ball delivered can be a no ball and it can be a 6 right.So this is what non disjoint events are or n? These are very simple to understand right now.Let's move on and look at the different types of probability distribution. All right, I'll be discussingthe three main probability distribution functions. I'll be talking about probability density.Aaron normal distribution and Central limit theorem. Okay probability density function also knownas PDF is concerned with the relative likelihood for a continuous random variable to take on a given value.Alright, so the PDF gives the probability of a variable that lies between the range A and B.So basically what you're trying to do is you're going to try and find the probability of a continuous random variableover a specified range. Okay. Now this graph denotes the PDF of a continuous variable.Now this graph is also known as the bell curve right? It's famously called the bell curve because of its shape and the three important propertiesthat you need to know about a probability density function. Now the graph of a PDF will be continuousover a range this is because you're finding the probability that a continuous variable lies between the ranges A and B,right the second property. Is that the area bounded by By the curve of a density functionand the x-axis is equal to 1 basically the area below the curve is equal to 1 all right,because it denotes probability again the probability cannot arrange more than one it has to bebetween 0 and 1 property number three is that the probability that our random variable assumes a value between Aand B is equal to the area under the PDF bounded by A and B. Okay. Now what this means,is that the probability You is denoted by the area of the graph. All right, so whatever value that you get here,which basically one is the probability that a random variable will lie between the range A and B.All right. So I hope all of you have understood the probability density function. It's basically the probability of finding the valueof a continuous random variable between the range A and B. All right.Now, let's look at our next distribution, which is normal distribution now.Normal distribution, which is also known as the gaussian distribution is a probability distributionthat denotes the symmetric property of the mean right meaning that the idea behind this function.Is that the data near the mean occurs more frequently than the data away from the mean.So what it means to say is that the data around the mean represents the entire data set.Okay. So if you just take a sample of data around the mean it can represent the entire data set now similarto Probability density function the normal distribution appears as a bell curve right nowwhen it comes to normal distribution. There are two important factors. All right, we have the mean of the populationand the standard deviation. Okay, so the mean and the graph determines the location of the center of the graph,right and the standard deviation determines the height of the graph. Okay. So if the standard deviation is large the curve is goingto look something like this. All right, it'll be short and wide. I'd and if the standard deviation is small the curveis tall and narrow. All right. So this was it about normal distribution. Now, let's look at the central limit theorem.Now the central limit theorem states that the sampling distribution of the mean of any independent random variable will be normalor nearly normal if the sample size is large enough now, that's a little confusing.Okay. Let me break it down for you now in simple terms if we had a large populationand be Why did it in too many samples, then the mean of all the samples from the population will be almost equal to the meanof the entire population right? Meaning that each of the sample is normally distributed.Right? So if you compare the mean of each of the sample, it will almost be equal to the mean of the population.Right? So this graph basically shows a more clear understanding of the central limit theorem red you can see each sample hereand the mean of each sample. Oil is almost along the same line, right?Okay. So this is exactly what the central limit theorem States now the accuracy or the resemblance to the normal distribution dependson two main factors, right? So the first is the number of sample points that you consider. All right,and the second is the shape of the underlying population. Now the shape obviously depends on the standard deviationand the mean of a sample, correct. So guys the central limit theorem basically statesthat eats Bill will be normally distributed in such a way that the mean of each sample will coincide with the meanof the actual population. All right in short terms. That's what central limit theorem States. All right, and this holds true only for a large data set mostlyfor a small data set and there are more deviations when compared to a large data set is because ofthe scaling Factor, right? The small is deviation in a small data set will change the value vary drastically,but in a large data set a small deviation will not matter at all. Now, let's move.Vaughn and look at our next topic which is the different types of probability. This is a important topicbecause most of your problems can be solved by understanding which type of probability should I use to solve this problem?Right? So we have three important types of probability. We have marginal joint and conditional probability.So let's discuss each of these now the probability of an event occurring unconditionedon any other event is known as marginal. Or unconditional probability.So let's say that you want to find the probability that a card drawn is a heart.All right. So if you want to find the probability that a card drawn is a heart The Profit will be 13 by 52since there are 52 cards in a deck and there are 13 hearts in a deck of cards.Right and there are 52 cards in a total deck. So your marginal probability will be 13 by 52.That's about marginal probability. Now, let's understand what is joint probability. And now joint probabilityis a measure of two events happening at the same time. Okay, let's say that the two events are A and B.So the probability of event A and B occurring is the intersection of A and B.So for example, if you want to find the probability that a card is a four and a red that would be joint probability.All right, because you're finding a card that is 4 and the card has to be red in color. So for the answer to this would be to Biceps you dobecause we have 1/2 in heart and we have 1/2 and diamonds, correct. So both of these are red and color therefore.Our probability is to by 52 and if you further down it is 1 by 26, right?So this is what joint probability is all about moving on. Let's look at what exactly conditional probability is.So if the probability of an event or an outcome is based on the occurrence of a previous event or an outcome.Then you call it as a conditional probability. Okay. So the conditional probability of an event B is the probabilitythat the event will occur given that an event a has already occurred. Right?So if a and b are dependent events, then the expression for conditional probability is given by this.Now this first term on the left hand side, which is p b of a is basically the probabilityof event B occurring given that event a has already occurred.So like I said, if a and b are dependent events than this is the expression but if a and b are independent events,and the expression for conditional probability is like this, right? So guys P of A and B of B is obviously the probabilityof a and probability of B right now, let's move on now in orderto understand conditional probability joint probability and marginal probability.Let's look at a small use case. Okay now basically we're going to Take a data setwhich examines the salary package and training undergone my candidates. Okay. Now in this there are 60 candidates a without trainingand forty five candidates, which have enrolled for Adder Acres training right. Now the task here is you have to assess the trainingwith a salary package. Okay. Let's look at this in a little more depth. So in total,we have hundred and five candidates out of which 60 of them have not enrolled Frederick has trainingand 45 of them have enrolled for a deer Acres. Inning. All right. This is the small survey that was conductedand this is the rating of the package or the salary that they got right? So if you read through the data,you can understand there were five candidates without Eddie record training who got a very poor salary package.Okay. Similarly, there are 30 candidates with Ed Eureka training who got a good package, right?So guys, basically you're comparing the salary package of a person depending on whether or not they've enrolled for a A core training right?This is our data set. Now. Let's look at our problem statement find the probabilitythat a candidate has undergone editor Acres training quite simple, which type of probability is this.This is marginal probability. Right? So the probability that a candidate has undergone Edge rakers training isobviously 45 divided by a hundred and five since 45 is the number of candidates with Eddie record rainingand hundred and five is the total number of candidates, so you Value of approximately 0.4 to orI that's the probability of a candidate that has undergone a Judaica straining next question find the probabilitythat a candidate has attended edger a constraining and also has good package.Now. This is obviously a joint probability problem, right? So how do you calculate this now?Since our table is quite formatted we can directly find that people who have gotten a good packagealong with Eddie record raining or 30, right? So out of hundred and five people 30 peoplehave education training and a good package, right? They specifically asking for people with Ado Rekha training remember that right?The question is find the probability that a candidate has attended editor Acres training and also has a good package.Alright, so we need to consider two factors that is a candidate who's addenda deaderick has trainingand who has a good package. So clearly that number is 30 30 divided by total number of candidates,which is 1 0 Five, right. So here you get the answer clearly. Next we have find the probabilitythat a candidate has a good package given that he has not undergone training.Okay. Now this is clearly conditional probability because here you're defining a condition you're sayingthat you want to find the probability of a candidate who has a good package given that he's not undergone.Any training, right? The condition is that he's not undergone any training. All right. So the number of peoplewho have not undergone training are 60 and out of that five of them have got a good package, right?So that's why this is Phi by 60 and not 5 by hundred and five because here they have clearly mentioned hasa good package given that he has not undergone training. You have to only consider peoplewho have not undergone training, right? So only five people who have not undergone training have gottena good package, right? So 5 divided by 60 you get a probability of around 208which is pretty low, right? Okay. So this was all about the different types of probability.Now, let's move on and look at our last Topic in probability, which is base theorem.Now guys Bayes theorem is a very important concept when it comes to statistics and probability.It is majorly used in knife bias algorithm. Those of you who aren't aware. Now I've bias isa supervised learning classification algorithm and it is mainly Used in Gmail spam filtering,right a lot of you might have noticed that if you open up Gmail, you'll see that you have a folder called spam rightor that is carried out through machine learning and the algorithm used there is knife bias, right?So now let's discuss what exactly the Bayes theorem is and what it denotes the bias theorem is usedto show the relation between one conditional probability and it's inverse. All right, basically Nothing,but the probability of an event occurring based on prior knowledge of conditionsthat might be related to the same event. Okay. So mathematically the bell's theoremis represented like this, right like shown in this equation. The left-hand term is referred to as the likelihood ratio,which measures the probability of occurrence of event B, given an event a okay on the left hand side iswhat is known as the posterior right is referred to as posterior. Are which means that the probabilityof occurrence of a given an event B, right? The second term is referred to as the likelihood ratioor a this measures the probability of occurrence of B, given an event a now P of a is also known as the priorwhich refers to the actual probability distribution of A and P of B is again, the probability of B, right.This is the bias theorem in order to better understand the base theorem. Let's look at a small example.Let's say that we Three balls we have about a bowel be and bouncy okay barley contains two blue ballsand for red balls bowel be contains eight blue balls and for red balls baozi contains one blue balland three red balls. Now if we draw one ball from each Bowl,what is the probability to draw a blue ball from a bowel a if we know that we drew exactly a totalof two blue balls right if you didn't Understand the question. Please. Read it.I shall pause for a second or two. Right. So I hope all of you have understood the question.Okay. Now what I'm going to do is I'm going to draw a blueprint for you and tell you how exactly to solve the problem.But I want you all to give me the solution to this problem, right? I'll draw a blueprint. I'll tell you what exactly the steps arebut I want you to come up with a solution on your own right the formula is also given to you. Everything is given to you.All you have to do is come up with the final answer. Right? Let's look at how you can solve this problem.So first of all, what we will do is Let's consider a all right, let a be the event of picking a blue ball from bag in and letX be the event of picking exactly two blue balls, right because these are the two eventsthat we need to calculate the probability of now there are two probabilities that you need to consider here.One is the event of picking a blue ball from bag a and the other is the event of picking exactly two blue balls.Okay. So these two are represented by a and X respectively Lee so what we want is the probability of occurrenceof event a given X, which means that given that we're picking exactly two blue balls,what is the probability that we are picking a blue ball from bag? So by the definition of conditional probability,this is exactly what our equation will look like. Correct. This is basically a occurrence of event a given an event Xand this is the probability of a and x and this is the probability of X alone, correct?And what we need to do is we need to find these two probabilities which is probability of a and X occurring togetherand probability of X. Okay. This is the entire solution. So how do you find P probabilityof X this you can do in three ways. So first is white ball from a either white from beor read from see now first is to find the probability of x x basically represents the eventof picking exactly two blue balls. Right. So these are the three ways in which it is possible.So you'll pick one blue ball from bowel a and one from bowel be in the second case.You can pick one from a and another blue ball from see in the third case.You can pick a blue ball from Bagby and a blue ball from bagsy. Right? These are the three ways in which it is possible.So you need to find the probability of each of this step two is that you need to find the probability of aand X occurring together. This is the sum of terms 1 and 2. Okay, this isbecause in both of these events, we are picking a ball from bag, correct. So there is find out this probability and letme know your answer in the comment section. All right. We'll see if you get the answer right? I gave you the entire solution to this.All you have to do is substitute the value right? If you want a second or two, I'm going to pause on the screen so that you can go through thisin a more clear away. Right? Remember that you need to calculate two.Tease the first probability that you need to calculate is the event of picking a blue ballfrom bag a given that you're picking exactly two blue balls. Okay, II probability you need to calculateis the event of picking exactly two blue bonds. All right. These are the two probabilities.You need to calculate so remember that and this is the solution. All right, so guys make sure you mention your answersin the comment section for now. Let's move on and Look at our next topic, which is the inferential statistics.So guys, we just completed the probability module right now. We will discuss inferential statistics,which is the second type of Statistics. We discussed descriptive statistics earlier.Alright, so like I mentioned earlier inferential statistics also known as statistical inference is a branch of Statisticsthat deals with forming inferences and predictions about a population based on a sample of data.Are taken from the population. All right, and the question you should ask is how does one form inferences or predictions on a sample?The answer is you use Point estimation? Okay. Now you must be wondering what is point estimation one estimation is concernedwith the use of the sample data to measure a single value which serves as an approximate valueor the best estimate of an unknown population parameter. That's a little confusing. Let me break it down to you for Campingin order to calculate the mean of a huge population. What we do is we first draw out the sample of the populationand then we find the sample mean right the sample mean is then used to estimate the population mean this is basically Point estimate,you're estimating the value of one of the parameters of the population, right? Basically the mainyou're trying to estimate the value of the mean. This is what point estimation is the two main termsin point estimation. There's something known as as the estimator and the something known as the estimate estimator is a function of the samplethat is used to find out the estimate. Alright in this example. It's basically the sample mean right so a functionthat calculates the sample mean is known as the estimator and the realized value of the estimator is the estimate right?So I hope Point estimation is clear. Now, how do you find the estimates? There are four common ways in which you can do this.The first one is method of Moment you'll what you do is you form an equation in the sample data setand then you analyze the similar equation in the population data set as well like the population mean population variance and so on.So in simple terms, what you're doing is you're taking down some known facts about the populationand you're extending those ideas to the sample. Alright, once you do that, you can analyze the sample and estimate moreessential or more complex values right next. We have maximum likelihood.But this method basically uses a model to estimate a value. All right. Now a maximum likelihood is majorly based on probability.So there's a lot of probability involved in this method next. We have the base estimator this works by minimizingthe errors or the average risk. Okay, the base estimator has a lot to do with the Bayes theorem.All right, let's not get into the depth of these estimation methods. Finally. We have the best unbiased estimators in this method.There are seven unbiased estimators that can be used to approximate a parameter.Okay. So Guys these were a couple of methods that are used to find the estimatebut the most well-known method to find the estimate is known as the interval estimation.Okay. This is one of the most important estimation methods or at this is where confidence interval also comes into the picture rightapart from interval estimation. We also have something known as margin of error. So I'll be discussing all of this.In the upcoming slides. So first let's understand. What is interval estimate? Okay, an interval or range of values,which are used to estimate a population parameter is known as an interval estimation, right?That's very understandable. Basically what they're trying to see is you're going to estimate the value of a parameter.Let's say you're trying to find the mean of a population. What you're going to do is you're going to build a rangeand your value will lie in that range or in that interval. All right. So this way your output is going to be more accuratebecause you've not predicted a point estimation instead. You have estimated an intervalwithin which your value might occur, right? Okay. Now this image clearly showshow Point estimate and interval estimate or different. So where's interval estimate is obviously more accuratebecause you're not just focusing on a particular value or a particular pointin order to predict the probability instead. You're saying that the value might bewithin this range between the lower confidence limit and the upper confidence limit.All right, this is denotes the range or the interval. Okay, if you're still confused about interval estimation,let me give you a small example if I stated that I will take 30 minutes to reach the theater.This is known as Point estimation. Okay, but if I stated that I will take between 45 minutesto an hour to reach the theater. This is an example of Will estimation all right.I hope it's clear. Now now interval estimation gives rise to two important statistical terminologies one is known as confidence intervaland the other is known as margin of error. All right. So there's it's important that you pay attentionto both of these terminologies confidence interval is one of the most significant measuresthat are used to check how essential machine learning model is. All right. So what is confidence interval confidence interval isthe measure of your confidence that the interval estimated contains the population parameter or the population meanor any of those parameters right now statisticians use confidence interval to describe the amountof uncertainty associated with the sample estimate of a population parameter now guys,this is a lot of definition. Let me just make you understand confidence interval with a small example.Okay. Let's say that you perform a survey and you survey a group of cat owners.The see how many cans of cat food they purchase in one year. Okay, you testyour statistics at the 99 percent confidence level and you get a confidence intervalof hundred comma 200 this means that you think that the cat owners by between hundred to two hundred cans in a year and alsosince the confidence level is 99% shows that you're very confident that the results are, correct.Okay. I hope all of you are clear with that. Alright, so your confidence interval here will bea hundred and two hundred and your confidence level will be 99% Right? That's the difference between confidence intervaland confidence level So within your confidence interval your value is going to lie and your confidence level will showhow confident you are about your estimation, right? I hope that was clear. Let's look at margin of error.No margin of error for a given level of confidence is a greatest possible distancebetween the Point estimate and the value of the parameter that it is estimating you can saythat it is a deviation from the actual point estimate right. Now. The margin of error can be calculatedusing this formula now zc her denotes the critical value or the confidence intervaland this is X standard deviation divided by root of the sample size.All right, n is basically the sample size now, let's understand how you can estimate the confidence intervals.So guys the level of confidence which is denoted by C is the probability that the interval estimate contains a population parameter.Let's say that you're trying to estimate the mean. All right. So the level of confidence is the probabilitythat the interval estimate contains a population parameter. So this interval between minus Z and zor the area beneath this curve is nothing but the probability that the interval estimate contains a population parameter.You don't all right. It should basically contain the value that you are predicting right.Now. These are known as critical values. This is basically your lower limit and your higher limit confidence level.Also, there's something known as the Z score now. This court can be calculated by using the standard normal table, right?If you look it up anywhere on Google you'll find the z-score table or the standard normal table get to understandhow this is done. Let's look at a small example. Okay, let's say that the level of Vince is 90% This meansthat you are 90% confident that the interval contains the population mean. Okay, so the remaining 10% which is out of hundred percent.The remaining 10% is equally distributed on these Dale regions. Okay, so you have 0.05 here and 0.05 over here, right?So on either side of see you will distribute the other leftover percentage now these these scores are calculated from the tableas I mentioned before. All right one. N64 5 is get collated from the standard normal table.Okay. So guys how you estimate the level of confidence. So to sum it up. Let me tell you the steps that are involvedin constructing a confidence interval first. You'll start by identifying a sample statistic.Okay. This is the statistic that you will use to estimate a population parameter. This can be anything like the meanof the sample next you will select a confidence level now the confidence level describes the uncertaintyof a Sampling method right after that you'll find something known as the margin of error, right?We discuss margin of error earlier. So you find this based on the equation that I explained in the previous slide,then you'll finally specify the confidence interval. All right. Now, let's look at a problem statementto better understand this concept a random sample of 32 textbook prices is taken from a local College Bookstore.The mean of the sample is so so and so and the sample standard deviation isThis use a 95% confident level and find the margin of error for the mean priceof all text books in the bookstore. Okay. Now, this is a very straightforward question. If you want you can read the question again.All you have to do is you have to just substitute the values into the equation.All right, so guys, we know the formula for margin of error you take the Z scorefrom the table. After that we have deviation Madrid's 23.4 for rightand that's standard deviation and n stands for the number of samples here. The number of samples is 32 basically 32 textbooks.So approximately your margin of error is going to be around 8.1 to this is a pretty simple question.All right. I hope all of you understood this now that you know, the idea behind confidence interval.Let's move ahead to one of the most important topics in statistical inference,which is hypothesis testing, right? So Sigelei statisticians use hypothesis testingto formally check whether the hypothesis is accepted or rejected. Okay, hypothesis.Testing is an inferential statistical technique used to determine whether there is enough evidence in a data sample to inferthat a certain condition holds true for an entire population. So to understandthe characteristics of a general population, we take a random sample, and we analyze the properties of the sample right we test.Whether or not the identified conclusion represent the population accurately and finally we interpret their results nowwhether or not to accept the hypothesis depends upon the percentage value that we get from the hypothesis.Okay, so to better understand this, let's look at a small example before that. There are few steps that are followed in hypothesis,testing you begin by stating the null and the alternative hypothesis. All right. I'll tell you what exactly these terms areand then you formulate. Analysis plan right after that you analyze the sample dataand finally you can interpret the results right now to understand the entire hypothesis testing.We look at a good example. Okay now consider for boys Nick jean-boband Harry these boys were caught bunking a class and they were asked to stay back at schooland clean the classroom as a punishment, right? So what John did is he decided that four of them would take turns to clean their classrooms.He came up with a plan of writing each of their names on chits and putting them in a bout now every day.They had to pick up a name from the bowel and that person had to play in the clock, right? That sounds pretty fair enough now it is been three daysand everybody's name has come up except John's assuming that this event is completely randomand free of bias. What is a probability of John not treating right or is the probabilitythat he's not actually cheating this can Solved by using hypothesis testing.Okay. So we'll Begin by calculating the probability of John not being picked for a day.Alright, so we're going to assume that the event is free of bias. So we need to find out the probabilityof John not cheating right first we'll find the probability that John is not picked for a day, right?We get 3 out of 4, which is basically 75% 75% is fairly high.So if John is not picked for three days in a row the Probability will drop down to approximately 42% Okay.So three days in a row meaning that is the probability drops down to 42 percent.Now, let's consider a situation where John is not picked for 12 days in a rowthe probability drops down to Tea Point two percent. Okay, that's the probabilityof John cheating becomes fairly high, right? So in order for statisticians to come to a conclusion,they Define what is known as the threshold value. Right considering the above situationif the threshold value is set to 5 percent. It would indicate that if the probability lies below 5% then John is cheatinghis way out of detention. But if the probability is about threshold value then John it just lucky and his name isn't getting picked.So the probability and hypothesis testing give rise to two important components of hypothesis testing,which is null hypothesis and alternative hypothesis. Null. Hypothesis is based.Basically approving the Assumption alternate hypothesis is when your result disapproves the Assumption right thereforein our example, if the probability of an event occurring is less than 5% which it is then the event is biased hence.It proves the alternate hypothesis.Undoubtedly machine learning is the most in-demand technology in today's market.It's applications. From Seth driving cause to predicting deadly diseases such as ALS the high demandfor machine learning skills is the motivation behind today's session. So let me discuss the agenda with you first.Now, we're going to begin the session by understanding the need for machine learning and why it is important after that.We look at what exactly machine learning is and then we'll discuss a couple of machine learning definitions.Once we're done with that. We'll look at the machine learning process and how you can solve a problem by using Usingthe machine learning process next we will discuss the types of machine learning which includes supervised unsupervisedand reinforcement learning. Once we're done with that. We'll discuss the different types of problemsthat can be solved by using machine learning. Finally. We will end this session by looking at a demowhere we'll see how you can perform weather forecasting by using machine learning. All right, so guys,let's get started with our first topic. So what is the importance or what is the need for machine learning now?Since the technical Revolution, we've been generating an immeasurable amount of data as for researchwith generating around 2.5 quintillion bytes of data every single dayand it is estimated that by 2020 1.7 MB of data will be created every secondfor every person on earth. Now that is a lot of data right now. This data comes from sources such asthe cloud iot devices social media and all of that. Since all of us are very interestedin the internet right now with generating a lot of data. All right, you have no idea how much data we generatethrough social media all the chatting that we do and all the images that we post on Instagram the videosthat we watch all of this generates a lot of data. Now how does machine learning fit into all of thissince we're producing this much data, we need to find a method that can analyze process and interpret this much data.All right, and we need to find a method. That can make sense out of data. And that method is machine learning.Now the lot of talk tire companies and data driven company such as Netflix and Amazonwhich build machine learning models by using tons of data in order to identify any profitable opportunities.And if they want to avoid any unwanted risk it make use of machine learning. Alright, so through machine learning You can predict riskYou can predict profits you can identify opportunities, which will help you grow your business. Business so now I'll show youa couple of examples of where in machine learning is used. All right, so I'm sure all of you have been watch on Netflix.Now the most important thing about Netflix is its recommendation engine. All right.Most of Netflix's Revenue comes from its recommendation engine. So the recommendation enginebasically studies the movie viewing patterns of its users and then recommends relevant movies to them.All right, it recommends movies depending on users interests. Depending on the typeof movies the user watches and all of that. Alright, so that is how Netflix uses machine learning.Next. We have Facebook's Auto tagging feature. Now the logic behind Facebook'sAuto tagging feature is machine learning and neural networks. I'm not sure how many of you know this but Facebookmakes use of deepmind face verification system, which is based on machine learning natural language processingand neural networks. So deep mine basically studies the facial features in an image and it tag your friends and family.Another such example is Amazon's Alexa now Alexa is basically an advanced level virtual assistantthat is based on natural language processing and machine learning. Now, it can do more than just play music for you.All right, it can book your Uber it can connect with other I/O devices that your house it can track your health.It can order food online and all of that. So data, and machine learning are basically the main factorsbehind Alex has power another such example is the Google spam filter.So guys Gmail basically makes use of machine learning to filter out spam messages.If any of you just open your Gmail inbox, you'll see that there are separate sections.There's one for primary this social the spam and the Joe general made now basically Gmail makes useof machine learning algorithms and natural language processing to an Is emails in real timeand then classify them as either spam or non-spam now, this is another famous application of machine learning.So to sum this up, let's look at a few reasons. Why machine learning is so important.So the first reason is obviously increase in data generation. So because of excessive production of data,we need a method that can be used to structure and lies and draw useful insights from data.This is where machine learning comes as in it uses data to solve problems and find solutionsto the most complex tasks faced by organizations. Another important reason is that it improves decision-making.So by making use of various algorithms machine learning can be used to make Better Business decisions.For example machine learning is used to forecast sales. It is used to predict any downfalls in the stock market.It is used to identify risks anomalies and so on now the next reason Is it uncovers patternsand Trends in data finding hidden patterns and extracting key insights from data is the most essential partof machine learning. So by building predictive models and using statistical techniques machine learningallows you to dig beneath the surface and explore the data at a minut scale now understanding dataand extracting patterns manually will take a lot of days. Now, if you do this through machine learning algorithms,you can perform such computations. Nations in less than a second. Another reason isthat it's solved complex problems. So from detecting genes that are linked to deadly ALS diseaseis to building self-driving cars and building phase detection systems machine learningcan be used to solve the most complex problems. So guys now that you know,why machine learning is so important. Let's look at what exactly machine learning is.The term machine learning was first coined by Arthur Samuel in the year 1959 now looking backthat your was probably the most significant in terms of technological advancements.There is if you browse through the net about what is machine learning you'll get at least a hundred different definitions.Now the first and very formal definition was given by Tom and Mitchell now,the definition says that a computer program is set to learn from experience ewith respect to some class. Of caste and performance measure P if its performance at tasks in Das measured by P improves with experience e all right. Now I know this is a little confusing.So let's break it down into simple words. Now in simple terms machine learning is a subsetof artificial intelligence which provides machines the ability to learn automaticallyand improve from experience without being explicitly programmed to do so in the sense.It is the practice of getting machines to solve problems by gaining the ability to think but wait nowhow can a machine think or make decisions? Well, if you feel a machine a good amount of data,it will learn how to interpret process and analyze this data by using machine learning algorithm.Okay. Now guys, look at this figure on top. Now this figure basically shows how a machine learning algorithmor how the machine learning process really works. So the machine learning Begins by feeding the machine lotsand lots of data okay by using this data. The machine is trained to detect hidden insights and Trends.Now these insights are then used to build a machine learning model by using an algorithmin order to solve a problem. Okay. So basically you're going to feed a lot of data to the machine.The machine is going to get trained by using this data. It's going to use this data and it's going to draw useful insightsand patterns from it, and then it's going to build a model by Using machine learning algorithms.Now this model will help you predict the outcome or help you solve any complex problem or any business problem.So that's a simple explanation of how machine learning works. Now, let's move on and look at some of the most commonly used machine learning terms.So first of all, we have algorithm. Now, this is quite self-explanatory. Basically algorithm is a set of rulesor statistical techniques, which are used to learn patterns from data now an algorithm is The logic behind a machine learning model.All right, an example of a machine learning algorithm is linear regression. I'm not sure how many of you have heard of linear regression.It's the most simple and basic machine learning algorithm. All right. Next we have model now model is the main componentof machine learning. All right. So model will basically map the input to your outputby using the machine learning algorithm and by using the data that you're feeding the machine.So basically the model is a representation of the entire machine learning process.So the model is basically fed input which has a lot of data and then it will output a particular resultor a particular outcome by using machine learning algorithms. Next we have something known as predictor variable.Now predictor variable is a feature of the data that can be used to predict the output.So for example, let's say that you're trying to predict the weight of a person dependingon the person's height and their age. All right. So over here the predictor variables are your heightand your age because you're using height and age of a person to predict the person's weight.Alright, so the height and the A's are the predictor variables now, Wait on the other hand is the responseor the target variable. So response variable is a feature or the output variable that needs to be predicted by using the predictor variables.All right, after that we have something known as training data. So guys the data that is fed to a machine learning model is always splitinto two parts first. We have the training data and then we have the testing data now trainingdata is basically used to build the machine learning model. So usually training data is much larger.Than the testing data because obviously if you're trying to train the machine then you're going to feed it a lot more data.Testing data is just used to validate and evaluate the efficiency of the model.Alright, so that was training data and testing data. So Guys, these were a few terms that I thought you should knowbefore we move any further. Okay. Now, let's move on and discuss the machine learning process.Now, this is going to get very interesting because I'm going to give you an example and make you understand how the machine learning.process works So first of all, let's define the different stages or the different steps involved in the machine learning process.So machine learning process always begins with defining the objective or defining the problemthat you're trying to solve next is is data Gathering or data collection. Now the data that you need to solve this problemis collected at this stage. This is followed by data preparation or data processing after that.You have data exploration and Analysis. Isis and the next stage is building a machine learning model.This is followed by model evaluation. And finally you have prediction or your output.Now, let's try to understand this entire process with an example. So our problem statement here is to predict the possibilityof rain by studying the weather conditions. So let's say that you're given a problem statementand you're asked to use a machine learning process to solve this problem statement. So let's get started.Alright, so the first step is to Find the objective of the problem statement. Our objective here is to predict the possibilityof rain by studying the weather conditions. Now in the first stage of a machine learning process.You must understand what exactly needs to be predicted. Now in our case the objective is to predict the possibilityof rain by studying weather conditions, right? So at this stage, it is also essential to take mental notes on what kindof data can be used to solve this problem or the type of approach that you can follow to get.Get to the solution. All right, a few questions that are worth asking during this stage iswhat are we trying to predict? What are the Target features or what are the predictor variables?What kind of input data do we need? And what kind of problem are we facing? Is it a binary classification problem or is ita clustering problem now, don't worry. If you don't know what classification and clustering is I'll be explaining thisin the upcoming slides. So guys this was the first step of a machine learning process, which is Define the Double the problem.All right. Now, let's move on and look at step number two. So step number two is basically data collectionor data Gathering now at this stage. You must be asking questions such as what kind of datais needed to solve the problem is the data available and if it is available, how can I get the data?Okay. So once you know the type of data that is required, you must understand how you can derive this data data collectioncan be done manually or by web scraping, but if you're a beginner Nor and you're just looking to learnmachine learning you don't have to worry about getting the data. OK there are thousands of data resources on the web.You can just go ahead and download the datasets from websites such as kaggle. Okay, now coming back to the problemat hand the data needed for weather forecasting includes measures such as humidity level temperature pressure localitywhether or not you live in a hill station and so on so guys such data must be collectedand stored for analysis. Now the next stage in machine learning is preparing your datathe data you collected is almost never in the right format. So basically you'll encounter a lot of inconsistenciesin the data set. Okay, this includes missing values redundant variables duplicate valuesand so on removing such values is very important because they might lead to wrongful computationsand predictions. So that's why at this stage you must can the entire data set for any inconsistencies.You have to fix them at this stage. Now. The next step is exploratory data analysis.Now data analysis is all about diving deep into data and finding all the hidden data Mysteries.Okay. This is where you become a detective. So edu or exploratory data analysis is like a brainstormingof machine learning data exploration involves understanding the patterns and the trends in your data.So at this stage all the useful insights are drawn and all the correlations. Turns between the variables are understood.So you might ask what sort of correlations are you talking about? For example in the case of predicting rain fall.We know that there is a strong possibility of rain if the temperature has fallen low. Okay.So such correlations have to be understood and mapped at this stage. Now. This stage is followed by stage number 5,which is building a machine learning model. So all the insights and the patterns that you deriveduring data exploration are used to build the machine learning. So this stage always Begins by splitting the data setinto two parts training data and the testing data. So earlier in the session. I already told you what trainingand testing data is now the training data will be used to build and analyze the model and the logic of the modelwill be based on the machine learning algorithm that is being implemented. Okay. Now in the case of predicting rainfallsince the output will be in the form of true or false we can use a classification algorithm like logistically.Regression now choosing the right algorithm depends on the type of problem. You're trying to solve the data set you haveand the level of complexity of the problem. So in the upcoming sections will be discussing different types of problems that can be solved by using machine learning.So don't worry. If you don't know what classification algorithm is and what logistic regression in.Okay. So all you need to know is at this stage, you'll be building a machine learning model by using machine learning algorithmand by using the training data set the next But in on machine learning process is model evaluationand optimization. So after building a model by using the training data set it is finally time to put the model to a test.Okay. So the testing data set is used to check the efficiency of the model and how accurately it can predict the outcome.So once you calculate the accuracy any improvements in the model have to be implemented in this stage.Okay, so methods like parameter tuning and cross-validation can be used to improve the The performanceof the model this is followed by the last stage, which is predictions. So once the model is evaluatedand improved it is finally used to make predictions. The final output can be a categorical variableor it can be a continuous quantity in our case for predicting the occurrence of rainfall the output will be a categorical variablein the sense. Our output will be in the form of true or false. Yes or no. Yes, basically representsthat is going to rain and no will represent that. It wondering okay as simple as that,so guys that was the entire machine learning process.A linear regression is one of the easiest algorithm in machine learning. It is a statistical model that attempts to show the relationshipbetween two variables. So the linear equation, but before we drill down to linear regression algorithm in depth,I'll give you a quick overview of today's agenda. So we'll start a session with a quick overview of what is regressionas linear regression is one of a type of regression algorithm. Once we learn about regression,its use case the various types of it next. We'll learn about the algorithm from scratch where I liveTo its mathematical implementation first, then we'll drill down to the coding part and Implement linear regression using pythonin today's session will deal with linear regression algorithm using least Square method checketts goodness of fitor how close the data is to the fitted regression line using the R square method and then finallywhat we'll do well optimized it using the gradient descent method in the last part on the coding session.I'll teach you to implement linear regression using Python and the coding session. Would be divided into two parts the first part would consistof linear regression using python from scratch where you will use the mathematical algorithmthat you have learned in this session. And in the next part of the coding session will be using scikit-learn for direct implementationof linear regression. All right. I hope the agenda is clear to you guys are like so let's begin our session with what is regression.Well regression analysis is a form of predictive modeling technique which investigates the relationship between a dependent and independent.Able a regression analysis involves graphing a line over a set of data pointsthat most closely fits the overall shape of the data or regression shows the changesin a dependent variable on the y-axis to the changes in the explanatory variable on the x-axis fine.Now you would ask what are the uses of regression? Well, they are major three uses of regression analysisthe first being determining the strength of predicator, 's the regression might be used to identify the strength of the effectthat the independent. Variables have on the dependent variable. For example, you can ask question. Like what is the strength of relationship between salesand marketing spending or what is the relationship between age and income second is forecastingan effect in this the regression can be used to forecast effects or impact of changes.That is the regression analysis help us to understand how much the dependent variable changes with the changein one or more independent variable fine. For example, you can ask question like how Additionalseal income will I get for each thousand dollars spent on marketing third is Trend forecastingin this the regression analysis to predict Trends and future values. The regression analysis can be used to getPoint estimates in this you can ask questions. Like what will be the price of Bitcoin and next six months, right?So next topic is linear versus logistic regression by now. I hope that you know, what a regression is.So let's move on and understand its type. So there are various kinds of regression like linear. Session logistic regression polynomial regressionand others. All right, but for this session will be focusing on linear and logistic regression.So let's move on and let me tell you what is linear regression. And what is logistic regression then what we'll do we'll compare both of them.All right. So starting with linear regression in simple linear regression. We are interested in things like y equal MX plus C.So what we are trying to find is the correlation between X and Y variable this meansthat every value of X has a corresponding value of y in it if it is continuous.I like however in logistic regression we are not fitting our data to a straight line like linear regression insteadwhat we are doing. We are mapping Y versus X to a sigmoid function in logistic regression.What we find out is is y 1 or 0 for this particular value of x so thus we are essentially deciding true or false valuefor a given value of x fine. So as a core concept of linear regression You can saythat the data is modeled using a straight line where in the case of logistic regression the data is model using a sigmoid function.The linear regression is used with continuous variables on the other hand the logistic regression.It is used with categorical variable the output or the prediction of a linear regression is the value of the variableon the other hand the output of production of a logistic regression is the probability of occurrence of the event.Now, how will you check the accuracy and goodness of fit in case of linear regression? We are various methods.Take measured by loss r squared adjusted r squared Etc while in the case of logistic regression youhave accuracy precision recall F1 score, which is nothing but the harmonic mean of precisionand recall next is Roc curve for determining the probability threshold for classificationor the confusion Matrix Etc. There are many all right. So summarizing the difference between linear and logistic regression.You can say that the type of function you are mapping to is the main point of difference between linearand regression a linear regression Maps a continuous X2 a continuous fion the other hand a logistic regression Maps a continuous x to the bindery why so we can use logistic regression to make categoryor true false decisions from the data find so let's move on ahead. Next is linear regression selection criteria,or you can say when will you use linear regression? So the first is classification and regression capabilities regression models predicta continuous variable such as the Don't a day or predict the temperature of a city their Relianceon a polynomial like a straight line to fit a data set poses a real challenge when it comes towards building a classification capability.Let's imagine that you fit a line with the training points that you have now imagine you add some more data points to it.But in order to fit it, what do you have to do? You have to change your existing model that is maybe you have to change the threshold itself.So this will happen with each new data point you add to the model, hence. The linear regression is not good for classification.All's fine. Next is data quality each missing value removes one data point that could optimize the regressionin simple linear regression. The outliers can significantly disrupt the outcome just for now. You can know that if you remove the outliers your modelwill become very good. All right. So this is about data quality. Next is computational complexity a linear regression is oftennot computationally expensive as compared to the decision tree or the clustering algorithm the orderof complexity for n training example and X features. Usually Falls in either Big O of x square or big of xn next is comprehensibleand transparent the linear regression are easily comprehensible and transparent in nature.They can be represented by a simple mathematical notation to anyone and can be understood very easily.So these are some of the criteria based on which you will select the linear regression algorithm.All right. Next is where is linear regression used first is evaluating Trends and sales estimate.Well linear regression can be used in Business to evaluate Trends and make estimates or focused for example,if a company sales have increased steadily every month for past few years then conducting a linear analysison the sales data with monthly sales on the y axis and time on the x axis.This will give you a line that predicts the upward Trends in the sale after creating the trendline the company could use the slopeof the lines too focused sale in future months. Next is analyzing. The impact of price changes will linear regressioncan be To analyze the effect of pricing on consumer behavior. For instance. If a company changesthe price on a certain product several times, then it can record the quantity itself for each price leveland then perform a linear regression with sold quantity as a dependent variable and price as the independent variable.This would result in a line that depicts the extent to which the customer reduce their consumption of the productas the prices increasing. So this result would help us in future pricing decisions. Next is assessment of risk and fine.Financial services and insurance domain. Well linear regression can be used to analyze the risk,for example health insurance company might conduct a linear regression algorithm how it can do it can do it by plotting the number of claimsper customer against its age and they might discover that the old customers then to make more health insurance claim.Well the result of such analysis might guide important business decisions. All right, so by now you have just a rough idea ofwhat linear regression algorithm as like, What it does where it is used when you should use it early now,let's move on and understand the algorithm and depth. So suppose you have independent variable on the x-axisand dependent variable on the y-axis. All right suppose. This is the data point on the x axis.The independent variable is increasing on the x axis. And so does the dependent variable on the y-axis?So what kind of linear regression line you would get you would get a positive linear regression line. All right as the slope would be positive.Next is suppose. You have an independent variable on the x-axis which is increasing and on the other hand the dependent variable on the y-axisthat is decreasing. So what kind of line will you get in that case? You will get a negative regression line.In this case as the slope of the line is negative. And this particular line that is line of y equal MXplus C is a line of linear regression which shows the relationship between independent variable and dependent variableand this line is only known as line of linear regression. Okay? So let's add some data points to our graph.So these are some observation or data points on our graphs. Let's plot some more. Okay.Now all our data points are plotted now our task is to create a regression line or the best fit line.All right now once our regression line is drawn now, it's the task of production now suppose.This is our estimated value or the predicted value and this is our actual value. Okay.So what we have to do our main goal is to reduce this error. That is to reduce the distance between the estimatedor the predicted value and the actual value. The best fit line would be the one which had the least erroror the least difference in estimated value and the actual value. All right, and other words we have to minimize the error.This was a brief understanding of linear regression algorithm soon. We'll jump towards mathematical implementation.All right, but for then let me tell you this suppose you draw a graph with speed on the x-axisand distance covered. On the y axis with the time demeaning constant, if you plot a graph between the speed travelby the vehicle and the distance traveled in a fixed unit of time, then you will get a positive relationship.All right. So suppose the equation of line as y equal MX plus C. Then in this case Y is the distance traveledin a fixed duration of time x is the speed of vehicle m is the positive slope of the line and see is the y-intercept of the line.All right suppose the distance remaining constant. You have to plot a graph between the Rid of the vehicleand the time taken to travel a fixed distance then in that case you will get a line with a negative relationship.All right, the slope of the line is negative here the equation of line changes to y equal minus of MX plus Cwhere Y is the time taken to travel a fixed distance X is the speed of vehicle m is the negative slopeof the line and see is the y-intercept of the line. All right. Now, let's get back to our independent and dependent variable.So in that term why is our dependent variable and That is our independent variable.Now, let's move on and see the mathematical implementation of the things. Alright, so we have xequal 1 2 3 4 5 let's plot them on the x-axis. So 0 1 2 3 4 5 6 alike and we have y as 3 4 2 4 5.All right. So let's plot 1 2 3 4 5 on the y-axis now, let's plot our coordinates 1 by 1 so x equal 1 and y equal 3,so We have here x equal 1 and y equal 3. So this is the point 1 comma 3 so similarlywe have 1 3 2 4 3 2 4 4 & 5 5. All right. So moving on ahead.Let's calculate the mean of X and Y and plot it on the graph. All right, so mean of X is 1plus 2 plus 3 plus 4 plus 5 divided by 5. That is 3. All right, similarly mean of Y is 3 plus 4 plus 2plus 4 plus 5 that is 18. So it in divided by 5. That is nothing but 3.6 aligned so nextwhat we'll do we'll plot our mean that is 3 comma 3 .6 on the graph. Okay. So there's a point 3 comma 3 .6see our goal is to find or predict the best fit line using the least Square Method All right.So in order to find that we first need to find the equation of line, so let's find the equation of our regression line.All right. So let's suppose this is our regression line y equal MX plus C.Now. We have an equation of line. So all we need to do is find the value of M and seewhere m equals summation of x minus X bar X Y minus y bar upon the summation of xminus X bar whole Square don't get confused. Let me resolve it for you. All right. So moving on ahead as a part of formula.What we are going to do will calculate x minus X bar. So we have X as 1 minus X bar as 3 so 1 minus 3that is minus 2 next. We have x equal to minus its mean 3that is minus 1 similarly. We have 3 minus 3 is 0 4 -3 1 5 - 3 2 alight so x minus X bar. It's nothing but the distance of all the pointthrough the line y equal 3 and what does this y minus y bar implies it impliesthat distance of all the point from the line x equal 3 .6 fine. So let's calculate the value of y minus y bar.So starting with y equal 3 - value of y. A bar that is 3.6.So it is three minus 3.6 how much - of 0.6 next is 4 minus 3.6 that is 0.4 next to minus 3.6that is minus of 1 point 6 next is 4 minus 3.6 that is 0.4 again,5 minus 3.6 that is 1.4. Alright, so now we are done with Y minus y bar fine now nextwe will calculate x minus X bar whole Square Let's calculate x minus X bar whole Square.So it is minus 2 whole square. That is 4 minus 1 whole square. That is 1 0 squared is 0 1 Square 1 2 square for fine.So now in our table we have x minus X bar y minus y bar and x minus X bar whole Square.Now what we need. We need the product of x minus X bar X Y minus y bar.Alright, so let's see the product of x minus X bar X Y minus y bar that is minus of 2 x minus of 0.6.That is one. Point 2 minus of 1 x 0 point 4 that is minus of 0 point 4 0 xminus of 1.6. That is 0 1 multiplied by zero point four that is 0.4.And next 2 multiplied by 1 point for that is 2.8. All right.Now almost all the parts of our formula is done. So now what we need to do is get the summationof last two columns. All right, so the summation of x minus X bar whole square is 10and the summation of x minus X bar. X Y minus y bar is 4 so the value of M will be equalto 4 by 10 fine. So let's put this value of m equals zero point 4 and our line y equal MX plus C.So let's file all the points into the equation and find the value of C. So we have y as 3.6 remember the mean by m as 0.4which we calculated just now X as the mean value of x that is 3 and we have thein as 3 point 6 equals 0 point 4 x 3 plus C. Alrightthat is 3.6 equal 1 Point 2 plus C. So what is the value of C that is 3.6 minus 1 Point 2.That is 2 point 4. All right. So what we had we had m equals zero point four seeas 2.4 and then finally when we calculate the equation of the regression line what we get is y equal zero point four times of Xplus two point four. So there is the regression line. Like so there's how you're plotting your points.This is your actual point. All right. Now for given m equals zero point four and SQL 2.4.Let's predict the value of y for x equal 1 2 3 4 & 5. So when x equal 1 the predicted valueof y will be zero point four x one plus two point four that is 2.8.Similarly when x equal to predicted value of y will be zero point 4 x 2 plus 2 point 4 that equals to 3 point.Two similarly x equal 3 y will be 3 point 6 x equal 4 y will be 4 point 0x equal 5 y will be four point four. So let's plot them on the graph and the line passing through all these predicting pointand cutting y-axis at 2.4 as the line of regression. Now your task is to calculate the distance between the actualand the predicted value and your job is to reduce the distance. All right, or in other words, you have to reduce the error between the actualand the predicted. The line with the least error will be the line of linear regression or regression line and it will also be the best fit line.Alright, so this is how things work in computer. So what it do it performs a number of iterationfor different values of M for different values of M. It will calculate the equation of linewhere y equals MX plus C. Right? So as the value of M changes the line is changing so iteration will start from one.All right, and it will perform a number of iteration so after Every iteration what it will do it will calculate the predicted valueaccording to the line and compare the distance of actual value to the predicted value and the value of Mfor which the distance between the actual and the predicted value is minimum will be selectedas the best fit line. All right. Now that we have calculated the best fit line now,it's time to check the goodness of fit or to check how good a model is performing. So in order to do that,we have a method called R square method. So what is this R square? Well r-squared value is a statistical measure ofhow close the data are to the fitted regression line in general. It is considered that a high r-squared value model is a good model,but you can also have a lower squared value for a good model as well or a higher Squad value for a modelthat does not fit at all. All right. It is also known as coefficient of determination or the coefficient of multiple determination.Let's move on and see how a square is calculated. So these are our actual values plotted on the graph.We had calculated the predicted values of Y as 2.8 3.2 3.6 4.0 4.4.Remember when we calculated the predicted values of Y for the equation Y predicted equals 0 1 4 xof X plus two point four for every x equal 1 2 3 4 & 5 from there.We got the power. Good values of Phi. All right. So let's plot it on the graph. So these are point and the line passingthrough these points are nothing but the regression line. All right. Now, what you need to do isyou have to check and compare the distance of actual - mean versus the distance of predicted - mean.Alright. So basically what you are doing you are calculating the distance of actual value to the mean to distance of predicted value to the mean.All right, so there is nothing but a square in mathematically you can represent our school. Whereas summation of Y predicted values minus ybar whole Square divided by summation of Y minus y bar whole Square where Y is the actual value y p is the predicted valueand Y Bar is the mean value of y that is nothing but 3.6. Remember, this is our formula.So next what we'll do we'll calculate y minus y bar. So we have y is 3y bar as 3 point 6 so we'll calculateit as 3 minus 3.6 that is nothing but minus of 0.6 similarly for y equals 4 and Y Bar equal 3.6.We have y minus y bar as zero point 4 then 2 minus 3.6. It has 1 point 6 4 minus 3.6 againzero point four and five minus 3.6 it is 1.4. So we got the value of y minus y bar.Now what we have to do we have to take it Square. So we have minus of 0.6 Square as 0.36 0.4 Square as 0.16 -of 1.6 Square as 2.56 0.4 Square as 0.16 and 1.4 squaredis 1.96 now is a part of formula what we need. We need our YP minus y BAR value.So these are VIP values and we have to subtract it from the No, right. So 2 .8 minus 3.6 that is minus 0.8.Similarly. We will get 3.2 minus 3.6 that is 0.4 and 3.6 minus 3.6that is 0 for 1 0 minus 3.6 that is 0.4. Then 4 .4 minus 3.6 that is 0.8.So we calculated the value of YP minus y bar now, it's our turn to calculate the value of y b minusy bar whole Square next. We have - of 0.8 Square as 0.64 - of Point four square as 0.160 Square0 0 point 4 Square as again 0.16 and 0.8 Square as 0.64.All right. Now as a part of formula what it suggests it suggests me to take the summation of Y Pminus y bar whole square and summation of Y minus y bar whole Square. All right. Let's see.So on submitting y minus y bar whole Square what you get is five point two and summation of Y P minusy bar whole Square you get one point six. So the value of R square can be calculated as1 point 6 upon 5.2 fine. So the result which will get is approximately equal to 0.3.Well, this is not a good fit. All right, so it suggests that the data points are far away from the regression line.Alright, so this is how your graph will look like when R square is 0.3when you increase the value of R square to 0.7. So you'll see that the actual value would like closer to the regression linewhen it reaches to 0.9 it comes. More clothes and when the value of approximately equalsto 1 then the actual values lies on the regression line itself, for example, in this case.If you get a very low value of R square suppose 0.02. So in that case what you'll see that the actual values arevery far away from the regression line, or you can say that there are too many outliers in your data.You cannot focus anything from the data. All right. So this was all about the calculation of R square now,you might get a question like are low values of Square always bad. Well in some field it is entirely expected that I askwhere value will be low. For example any field that attempts to predict human behavior such as psychologytypically has r-squared values lower than around 50% through which you can conclude that humans are simply harderto predict the under physical process furthermore. If you are squared value is low, but you have statistically significant predictors,then you can still draw important conclusion about how changes in the predicator values associated.Oh sated with the changes in the response value regardless of the r-squared the significant coefficient still represent the mean changein the response for one unit of change in the predicator while holding other predators in the model constant,obviously this type of information can be extremely valuable. All right. All right.So this was all about the theoretical concept now, let's move on to the coding part and understand the code in depth.So for implementing linear regression using python, I will be using Anaconda with jupyter notebook installed on it.So I like there's a jupyter notebook and we are using python 3.01 it alright, so we are going to use a data set consistingof head size and human brain of different people. All right. So let's import our data set percent matplotlib and line.We are importing numpy as NP pandas as speedy and matplotlib and from matplotlib.We are importing pipe out of that as PLT. Alright next we will import our data had brain dot CSVand store it in the data variable. Let's execute the Run button and see the armor. But so this asterisk symbol it symbolizesthat it still executing. So there's a output or dataset consists of two thirty seven rowsand four columns. We have columns as gender age range head size in centimeter Cubeand brain weights and Graham fine. So there's our sample data set that is how it looks it consists of all these data set.So now that we have imported our data, so as you can see they are 237 values in the training setso we can find a linear. Relationship between the head size and the Brain weights. So now what we'll do we'll collect X & Ythe X would consist of the head size values and the Y would consist of brain with values. So collecting X and Y. Let's execute the Run.Done next what we'll do we need to find the values of b 1 or B not or you can say m and C.So we'll need the mean of X and Y values first of all what we'll do we'll calculate the mean of X and Y so mean xequal NP dot Min X. So mean is a predefined function of Numb by similarly meanunderscore y equal NP dot mean of Y, so what it will return if you'll return the mean values of Ynext we'll check the total number of values. So m equals. Well length of X. Alright,then we'll use the formula to calculate the values of b 1 and B naught or fnc. All right, let's execute the Run button and seewhat is the result. So as you can see here on the screen we have got b 1 as 0 point 2 6 3 +B not as three twenty five point five seven. Alright, so now that we have a coefficient.So comparing it with the equation y equal MX plus C. You can say that brain weight equalszero point 2 6 3 X Head size plus three twenty five point five seven so you can saythat the value of M here is 0.26 3 and the value of C. Here is three twenty five point five seven.All right, so there's our linear model now, let's plot it and see graphically.Let's execute it. So this is how our plot looks like this model is not so bad.But we need to find out how good our model is. So in order to find it the many methodslike root means Square method the coefficient of determination or the a square method. So in this tutorial,I have told you about our score method. So let's focus on that and see how good our model is.So let's calculate the R square value. All right here SS underscore T is the total sum of square SS.Our is the total sum of square of residuals and R square as the formula is 1 minus total sumof squares upon total sum of square of residuals. All right next when you execute it,you will get the value of R square as 0.63 which is pretty very good. Now that you have implemented simple linear regression modelusing least Square method, let's move on and see how will you implement the model using machine learning librarycalled scikit-learn. All right. So this scikit-learn is a simple machine. Young Library in Python welding machine learning model arevery easy using scikit-learn. So suppose there's a python code.So using the scikit-learn libraries your code shortens to this length like so let's execute the Run button and see youwill get the same our to score as Well, this was all for today's discussion.Most of the entities in this world are related in one way or another at times finding relationship between entitiescan help you take valuable business decisions today. I'm going to talk about logistic regression,which is one such approach towards predicting relationships. Now, let us see what all we are going to cover in today's training.So we'll start off the session by getting a quick introduction to what is regression. Then we'll see the different types of regressionand we'll be discussing the what and by of logistic regression. So in this part, we'll discuss what exactly it is.It is used why it is used and all those things moving ahead will compare linear regressionversus logistic regression along with the various real-life use cases and finally towards the end. I will be practicallyimplementing logistic regression algorithm. So let's quickly start off with the very first topicwhat is regression. The regression analysis is a predictive modeling technique. So it always involves predictions.So in this session, we'll just talk about predictive analysis and not prescriptive analysis. Now why becauseif descriptive analysis you Need to have a good base and a stronghold on the predictive part first.Now, it estimates relationship between the dependent variable and an independent variable. So for those of youwho are not aware of these terminologies, let me give you a quick summary of it. So dependent variable is nothing but a variablewhich you want to predict now, let's say I want to know what will be the sales on 26th of this month.So sales becomes a dependent variable or you can see the target variable. Now this dependent variableor Target variable are going to depend on a lot of actors. The number of products you sold till dateor what is the season out there? Is there the availability of product or how is the product quality and all these things?So these are the NeverEnding factors which are nothing but the different features that leads to sail so these variables are called as an independent variableor you can say the predictor now if you look at the graph over here, we have some values of X and we have values of Y nowas you can see over here if X increases the value of by also increases so let me explain you thiswith an example. Let's say we have until the value of x which is six point seven five and somebody asked you.What was the value of y when the value of x is 7 so the way that you can do it or how regression comes into the picture isby fitting a straight line by all these points and getting the value of M and C. So this is straight line guysand the formula for the straight line is y is equal to MX plus C. So using this we can try to predict the value of y so hereif you notice the X variable can increase as much as it can but the Y variable will increase according to xso Why is basically dependent on your X variable? So for any arbitrary value of x You can predict the valueof y and this is always done through regression. So that is how regression is useful. Now regression is basically classified into three typesyour linear regression, then your logistic regression and polynomial regression. So today we will be discussing logistic regression.So let's move forward and understand the what and by of logistic regression. Now this algorithm is most widely usedwhen the dependent variable or you can see the output is in the binary. A format. So here you need to predict the outcomeof a categorical dependent variable. So the outcome should be always discreet or categoricalin nature Now by discrete. I mean the value should be binary or you can say you just have two values it can either be 0or 1 it can either be yes or a no either be true or false or high or low. So only these can be the outcomes so the valuewhich you need to create it should be discrete or you can say categorical in nature. Whereas in linear regression.We have the value of by or you can see Val you need to predict within a range that is how there's a difference between linear regressionand logistic regression. We must be having question. Why not linear regression now guys in linear regression the value of by or the value,which you need to predict is in a range, but in our case as in the logistic regression, we just have two values it can be either 0or it can be one. It should not entertain the values which is below zero or above one. But in linear regression,we have the value of y in the range so here in order to implement logic regression we need To clip this partso we don't need the value that is below zero or we don't need the value which is above 1 so since the value of y will be between only 0 and 1that is the main rule of logistic regression. The linear line has to be clipped at 0 and 1 now.Once we clip this graph it would look somewhat like this. So here you're getting the curve which is nothing but three different straight lines.So here we need to make a new way to solve this problem. So this has to be formulated into equation.And hence we come up with logistic regression. So here the outcome is either 0 Or one which is the main rule of logistic regression.So with this our resulting curve cannot be formulated. So hence our main aim to bring the values to 0and 1 is fulfilled. So that is how we came up with large stick regression now here once it gets formulated into an equation.It looks somewhat like this. So guys, this is nothing but an S curve or you can say the sigmoid curve a sigmoid function curve.So this sigmoid function basically converts any value from minus infinity to Infinity to your discrete values,which a Logitech regression wants or it Can say the values which are in binary format either 0 or 1.So if you see here the values as either 0 or 1 and this is nothing but just a transition of it,but guys there's a catch over here. So let's say I have a data point that is 0.8. Now, how can you decidewhether your value is 0 or 1 now here you have the concept of threshold which basically divides your line.So here threshold value basically indicates the probability of either winning or losing so here by winning.I mean the value is equal. One and by losing I mean the values equal to 0 but how does it do that?Let's have a data point which is over here. Let's say my cursor is at 0.8. So here I checkwhether this value is less than the threshold value or not. Let's say if it is more than the threshold value.It should give me the result as 1 if it is less than that, then should give me the result is zero. So here my threshold value is 0.5.I need to Define that if my value let's is 0.8. It is more than 0.5. Then the value shall be rounded of two one.One and let's say if it is less than 0.5. Let's I have a value 0.2 then should reduce it to zero.So here you can use the concept of threshold value to find output. So here it should be discreet.It should be either 0 or it should be one. So I hope you caught this curve of logistic regression.So guys, this is the sigmoid S curve. So to make this curve we need to make an equation.So let me address that part as well. So let's see how an equation is formed to imitate this functionality so over here,we have an equation of a straight. Line, which is y is equal to MX plus C. So in this case, I just have only one independent variable but let's sayif we have many independent variable then the equation becomes m 1 x 1 plus m 2 x 2 plus m 3 x 3 and so on till M NX n now,let us put in B and X. So here the equation becomes Y is equal to b 1 x 1 plus beta 2 x 2 plus b 3 x 3 and so ontill be nxn plus C. So guys equation of the straight line has a range from minus infinity to Infinity.Yeah, but in our case or you can say largest equation the value which we need to predict or you can saythe Y value it can have the range only from 0 to 1. So in that case we need to transform this equation.So to do that what we had done we have just divide this equation by 1 minus y so now Y is equalto 0 so 0 over 1 minus 0 which is equal to 1 so 0 over 1 is again 0and if we take Y is equals to 1 then 1 over 1 minus 1 which is 0 so 1 over 0 is infinity.So here are my range is now. Between 0 to Infinity, but again, we want the range from minus infinity to Infinity.So for that what we'll do we'll have the log of this equation. So let's go ahead and have the logarithmic of this equation.So here we have this transform it further to get the range between minus infinity to Infinity so over here we have log of Yover 1 minus 1 and this is your final logistic regression equation. So guys, don't worry. You don't have to write this formula or memorizethis formula in Python. You just need to call this function which is logistic regression and Everything will be automatically for you.So I don't want to scare you with the maths in the formulas behind it. But it is always good to know how this formula was generated.So I hope you guys are clear with how logistic regression comes into the picture next. Let us see what are the major differencesbetween linear regression was a logistic regression the first of all in linear regression, we have the valueof y as a continuous variable or the variable between need to predict are continuous in nature. Whereas in logistic regression.We have the categorical variable so here the value which you need to Should be discrete in nature. It should be either 0or 1 or should have just two values to it. For example, whether it is raining or it is not rainingis it humid outside or it is not humid outside. Now, how's it going to snow and it's not going to snow.So these are the few example, we need to predict where the values are discrete or you can just predictwhere this is happening or not. Next linear equation solves your regression problems. So here you have a concept of independent variableand a dependent variable. So here you can calculate the value of y which you need to Plate it. Using the value of x.So here your y variable or you can see the value that you need to predict are in a range. But whereas in logistic regression,you have discrete values. So logistic regression basically solves a classification problem so it can basically classify it and it can just give you resultwhether this event is happening or not. So I hope it is pretty much Clear till now next in linear regression.The graph that you have seen is a straight line graph so over here, you can calculate the value of ywith respect to the value of x where as in logistic regression. Glad that we got was a Escobar.You can see the sigmoid curve. So using the sigmoid function You can predict your y values.So I hope you guys are clear with the differences between the linear regression and logistic regression moving the a little seethe various use cases where in logistic regression is implemented in real life. So the very first is weather prediction nowlargest aggression helps you to predict your weather. For example, it is used to predict whether it is raining or not whether it is sunny.Is it cloudy or not? So all these things things can be predicted using logistic regression. Where as you need to keep in mindthat both linear regression and logistic regression can be used in predicting the weather. So in that case linear regression helps you to predictwhat will be the temperature tomorrow whereas logistic regression will only tell you which is going to rain or not or whether it's cloudy or not,which is going to snow or not. So these values are discrete. Whereas if you apply linear regression, you will predicting things like what is the temperature tomorrowor what is the temperature day after tomorrow and all those thing? So these are the slight? Is between linear regressionand logistic regression the moving ahead. We have classification problem. So python performs multi-class classification,so here it can help you tell whether it's a bird. It's not a board. Then you classify different kind of mammals.Let's say whether it's a dog or it's not a dog similarly, you can check it for reptile whether it's a reptile or not a reptile.So in logistic regression, it can perform multi-class classification. So this point I've already discussedthat it is using classification problems next. It also helps you to determine the illnesses. Where so let me take an example.Let's say a patient goes for a routine check up in hospital. So what doctor will do it, it will perform various tests on the patient and we'll checkwhether the patient is actually a law or not. So what will be the features so doctor can check the sugar levelthe blood pressure then what is the age of the patient? Is it very small or is it the old person then?What is the previous medical history of the patient and all of these features will be recorded by the doctorand finally, dr. Checks the patient data and Data - the outcome of Illness and the severity of illness.So using all the data of a doctor can identify whether a patient is ill or not. So these are the various use casesin which you can use logistic regression now, I guess enough of theory part. So let's move ahead and see some of the Practical implementationof logistic regression so over here, I be implementing two projects when I have the data setof a Titanic so over here will predict what factors made people more likely to survive the sinkingof the Titanic ship anime. Second project will see the data analysis. On the SUV cars so over here.We have the data of the SUV cars who can purchase it and what factors made people more interested in buying SUV.So these will be the major questions as to why you should Implement logistic regression and what output will you get by it?So let's start by the very first project that is Titanic data analysis. So some of you might knowthat there was a ship called as Titanic with basically hit an iceberg and sank to the bottom of the ocean and it was a big disaster at that timebecause it was the first voyage of the ship. It was supposed to be really really strongly built and oneof the best ships of that time. So it was a big disaster of that time. And of course there is a movie about this as well.So many of you might have washed it. So what we have we have data of the passengers those who survived and thosewho did not survive in this particular tragedy. So what you have to do you have to look at this data and analyze which factors would have been contributedthe most to the chances of a person survival on the ship or not. So using the logistic regression, we can predictwhether the person survived or the person died. Now apart from this we also have a look with the various features along with that.So first it is explore the data set so over here, we have the index value then the First Columnis passenger ID, then my next column is survived so over here, we have two values a 0 and a 1 so 0 standsfor did not survive and one stands for survive. So this column is categorical where the values are discrete next.We have passenger class so over here, we have three values 1 2 and 3. So this basically tells you that whether a I thinka stabbing in the first class second class or third class. Then we have the name of the passenger. We have the six or you can see the gender of the passengerwhere the passenger is a male or female. Then we have the age we have the Sip SP. So this basically means the number of siblingsor the spouses aboard the Titanic so over here, we have values such as 1 0 and so on then we haveParts apart is basically the number of parents or children aboard the Titanic so over here,we also have some values then we I have the ticket number. We have the fear. We have the cabin number and we have the embarked column.So in my inbox column, we have three values we have SC and Q. So s basically standsfor Southampton C stands for Cherbourg and Q stands for Queenstown. So these are the featuresthat will be applying our model on so here we'll perform various steps and then we'll be implementing logistic regression.So now these are the various steps which are required to implement any algorithm. So now in our case we are implementinglogistic regression, so, Very first step is to collect your data or to import the libraries that are used for collecting your dataand then taking it forward then my second step is to analyze your data so over here, I can go to the various fields and then I can analyze the data.I can check did the females or children survive better than the males or did the rich passenger survived morethan the poor passenger or did the money matter as in who paid more to get into the shapewith the evacuated first? And what about the workers does the worker survived or what is the survival rate?If you were the worker in the ship and not just a traveling passenger, so all of these are very very interesting questionsand you would be going through all of them one by one. So in this stage, you need to analyze our dataand explore your data as much as you can then the third step is to Wrangle your data now data wrangling basically means cleaning your data so over here,you can simply remove the unnecessary items or if you have a null values in the data set. You can just clear that data and then you can take it forward.So in this step you can build your model using the train data. And then you can test it using a test so over here you will be performing a splitwhich basically split your data set into training and testing data set and find you will check the accuracy.So as to ensure how much accurate your values are. So I hope you guys got these five steps that you're going to implement in autistic regression.So now let's go into all these steps in detail. So number one. We have to collect your data or you can say import the libraries.So it may show you the implementation part as well. So I just open my jupyter notebook and I just Implement all of these steps.It's side-by-side. So guys this is my jupyter notebook first. Let me just rename jupyter notebook to let's sayTitanic data analysis. Now our first step was to import all the librariesand collect the data. So let me just import all the libraries first. So first of all, I'll import pandas.So pandas is used for data analysis. So I'll say input pandas as PD then I will be importing numpy.So I'll say import numpy as NP so numpy is a library in Python which basically stands for numerical Pythonand it is widely used to perform any scientific computation. Next. We will be importing Seaborn.So c 1 is a library for statistical brought think so. Say import Seaborn as SNS.I'll also import matplotlib. So matplotlib library is again for plotting.So I'll say import matplotlib dot Pi plot as PLT now to run this library in jupyter Notebook all I haveto write in his percentage matplotlib in line. Next I will be importing one module as well.So as to calculate the basic mathematical functions, so I'll say import mats. So these are the librariesthat I will be needing in this Titanic data analysis. So now let me just import my data set. So I will take a variable.Let's say Titanic data and using the pandas. I will just read my CSV or you can see the data set.I like the name of my data set that is Titanic dot CSV. Now. I have already showed you the data set so over here.Let me just print the top 10 rows. So for that I will just say I take the variable Titanic data dot headand I'll say the top ten rules. So now I'll just run this so to run these fellows have to press shift + enteror else you can just directly click on this cell so over here. I have the index. We have the passenger ID, which is nothing.But again the index which is starting from 1 then we have the survived column which has a category. Call values or you can say the discrete values,which is in the form of 0 or 1. Then we have the passenger class. We have the name of the passenger 6 8and so on so this is the data set that I will be going forward with next let us bring the number of passengerswhich are there in this original data set for that. I'll just simply type in print. I'll say a number of passengers.And using the length function, I can calculate the total length. So I'll say length and inside this I will be passing this variable because Titanic data,so I'll just copy it from here. I'll just paste it dot index and next set me just bring this one.So here the number of passengers which are there in the original data set we have is 891so around this number were traveling in the Titanic ship so over here, my first step is donewhere you have just collected data imported all the libraries and find out the total number of passengers,which are Titanic so now let me just go back to presentation and let's see. What is my next step. So we're done with the collecting data.Next step is to analyze your data so over here, we will be creating different plots to check the relationshipbetween variables as in how one variable is affecting the other so you can simply explore your data set by making useof various columns and then you can plot a graph between them. So you can either plot a correlation graph.You can plot a distribution curve. It's up to you guys. So let me just go back to my jupyter notebook and let me analyze some of the data.Over here. My second part is to analyze data. So I just put this in headed to now to put this in here to I just have to goand code click on mark down and I just run this so first let us plot account plot where you can pay between the passengerswho survived and who did not survive. So for that I will be using the Seabourn Library so over here I have imported Seaborn as SNSso I don't have to write the whole name. I'll simply say SNS dot count plot.I say axis with the survive and the data that I'll be using is the Titanic data or you can say the nameof variable in which you have store your data set. So now let me just run this so who were here as you can see I have survived column on my xaxis and on the y axis. I have the count. So 0 basically stands for did not survive and one stands for the passengerswho did survive so over here, you can see that around 550 of the passengers who did not survive and they were around 350 passengerswho only survive so here you can basically compute. There are very less survivors than on survivors.So this was the very first floor now that is not another plot to compare the sex as to whetherout of all the passengers who survived and who did not survive. How many were men and how many were femaleso to do that? I'll simply say SNS dot count plot.I add the Hue as six so I want to know how many females and how many male survivethen I'll be specifying the data. So I'm using Titanic data set and let me just run this you have done a mistakeover here so over here you can see I have survived column on the x-axis and I have the count on the why now.So here your view color stands for your male passengers and orange stands for your female. So as you can see here the passengerswho did not survive that has a value 0 so we can see that. Majority of males did not survive and if we see the peoplewho survived here, we can see the majority of female survive. So this basically concludes the gender of the survival rate.So it appears on average women were more than three times more likely to survive than men next.Let us plot another plot where we have the Hue as the passenger class so over here we can see which class at the passenger was traveling inwhether it was traveling in class one two, or three so for that I just tried the same command.I'll say SNS dot count plot. I keep my x-axis assubtly I'll change my you to passenger class. So my variable named as PE class.And the data said that I'll be using is Titanic data. So this is my result so over here you can see I have blue for first-class orangefor second class and green for the third class. So here the passengers who did not survive a majorly of the third classor you can say the lowest class or the cheapest class to get into the dynamic and the people who did survive majorly belong to the higher classes.So here 1 & 2 has more eyes than the passenger who were traveling in the third class. So here we have concluded that the passengerswho did not survive a majorly of third class. Us all you can see the lowest class and the passengers who were traveling in first and second classwould tend to survive more next. I just got a graph for the age distribution over here. I can simply use my data.So we'll be using pandas library for this. I will declare an array and I'll pass in the column. That is age.So I plot and I want a histogram so I'll say plot da test.So you can notice over here that we have more of young passengers, or you can see the children between the ages 0 to 10and then we have the average people and if you go ahead Lester would be the population.So this is the analysis on the age column. So we saw that we have more young passengers and more mediocre eight passengers,which are traveling in the Titanic. So next let me plot a graph of fare as well. So I'll say Titanic data.I say fair. And again, I got a histogram so I'll say haste.So here you can see the fair size is between zero to hundred now. Let me add the bin size.So as to make it more clear over here, I'll say Ben is equals to let's say 20 and I'll increase the figure size as well.So I'll say fixed size. Let's say I'll give the dimensions as 10 by 5.So it is bins. So this is more clear now next. It is analyzed the other columns as well.So I'll just type in Titanic data and I want the information as to what all columns are left.So here we have passenger ID, which I guess it's of no use then you have see how many passengers survivedand how many did not we also see the analysis on the gender basis. We saw when the female tend to survive moreor the maintain to survive more then we saw the passenger class where the passenger is traveling in the first class second classor third class. Then we have the name. So in name, we cannot do any analysis. We saw the sex we saw the age as well.Then we have sea bass P. So this stands for the number of siblings or the spouses which Are aboard the Titanic so let us do this as well.So I'll say SNS dot count plot. I mentioned X SC SP.And I will be using the Titanic data so you can see the plot over here so over here youcan conclude that. It has the maximum value on zero so you can conclude that neither children nor a spouse wason board the Titanic now second most highest value is 1 and then we have various values for 2 3 4 and so on nextif I go above the store this column as well. Similarly can do four parts. So next we have partso you can see the number of parents or children which were aboard the Titanic so similarly can do. As well then we have the ticket number.So I don't think so. Any analysis is required for Ticket. Then we have fears of a we have already discussed asin the people would tend to travel in the first class. You will be the highest view then we have the cable numberand we have embarked. So these are the columns that will be doing data wrangling on so we have analyzed the dataand we have seen quite a few graphs in which we can conclude which variable is better than anotheror what is the relationship the whole third step is my data wrangling so data wrangling basicallymeans Cleaning your data. So if you have a large data set, you might be having some null valuesor you can say Nan values. So it's very important that you remove all the unnecessary items that are present in your data set.So removing this directly affects your accuracy. So I'll just go ahead and clean my data by removing all the n n values and unnecessary columns,which has a null value in the data set the next time you're performing data wrangling.Supposed to fall I check whether my data set is null or not. So I'll say Titanic data, which is the name of my data set and I'll say is null.So this will basically tell me what all values are null and will return me a Boolean result. So this basically checks the missing dataand your result will be in Boolean format as in the result will be true or false so Falls mean if it is not null and prove meansif it is null, so let me just run this. Over here you can see the values as false or true.So Falls is where the value is not null and Drew is where the value is none. So over here you can see in the cabin column.We have the very first value which is null so we have to do something on this so you can seethat we have a large data set. So the counting does not stop and we can actually see the some of it.We can actually print the number of passengers who have the Nan value in each column. So I'll say Titanic underscore data is nulland I want the sum of it all. Same thought some so this is basically print the number of passengerswho have the n n values in each column so we can see that we have missing values in each column that is 177.Then we have the maximum value in the cave in column and we have very Less in the Embark column.That is 2 so here if you don't want to see this numbers, you can also plot a heat map and then you can visually analyze it let me just dothat as well. So I'll say SNSD heat map.And save I take labels. False Choice run this as we have already seenthat there were three columns in which missing data value was present. So this might be age so over here almost 20%of each column has a missing value. Then we have the cabling columns. So this is quite a large valueand then we have two values for embark column as well. Add a see map for color coding.So I'll say see map. So if I do thisso the graph becomes more attractive so over here yellow stands for Drew or you can say the values are null.So here we have computed that we have the missing value of H. We have a lot of missing values in the cabin columnand we have very less value, which is not even visible in the Embark column as well. So to remove these missing values,you can either replace the values and you can put in some dummy values to it or you can simply drop the column.So here let us suppose pick the age column. So first, let me just plot a box plot and they will analyze with having a column as H.So I'll say SNS dot box plot. I'll say x is equals to passenger class.So it's p class. I'll say Y is equal to H and the data set that I'll be using is Titanic side.So I'll say three times goes to Titanic data. You can see the edge in first class and second class tends to be more older ratherthan we have it in the third class. Well that depends on The Experience how much you earn or might be there any number of reasons so here we concludedthat passengers who were traveling in class one and class two a tend to be older than what we have in the class 3so we have found that we have some missing values in EM. Now one way is to either just drop the columnor you can just simply fill in some values to them. So this method is called as imputation nowto perform data wrangling or cleaning it is for spring the head of the data set. So I'll say tightening knot head.So it's Titanic. Data, let's say I just want the five rows. So here we have survived which is again categorical.So in this particular column, I can apply logic to progression. So this can be my y value or the valuethat you need to predict. Then we have the passenger class. We have the name. Then we have ticket number.We're taping so over here. We have seen that in keeping. We have a lot of null values or you can say that any invalidwhich is quite visible as well. So first of all, we'll just drop this column for dropping it. I'll just say Titanic underscore data.And I'll simply type in drop and the column which I need to draw so I have to drop the cable column.I mention the access equals to 1 and I'll say in place also to true.So now again, I just print the head and let us see whether this column has been removed from the data set or not.So I'll say Titanic dot head. So as you can see here, we don't have given column anymore.Now, you can also drop the na values. So I'll say Titanic data dot dropall the any values or you can say Nan which is not a number and I will say in place is equal to True its Titanic.So over here, let me again plot the heat map and let's say for the values we should before showing a lot of null values.Has it been removed or not. So I'll say SNS dot heat map. I'll pass in the data set.I'll check it is null. I'll say why tick labels is equal to false.And I don't want color coding. So again I say false. So this will basically help me to checkwhether my values has been removed from the data set or not. So as you can see here, I don't have any null values.So it's entirely black now. You can actually know the some as well. So I'll just go above So I'll just copy this partand I just use the sum function to calculate the sum. So here the tells me that data set is clean asin the data set does not contain any null value or any Nan value. So now we have R Angela data.You can see cleaner data. So here we have done just one step in data wrangling that is just removing one column out of it.Now you can do a lot of things you can actually fill in the values with some other values or you can just calculate the meanand then you can just fit in the null values. But now if I see my data set, so I'll say Titanic data dot head.But now if I see you over here I have a lot of string values. So this has to be converted to a categorical variablesin order to implement logistic regression. So what we will do we will convert this to categorical variableinto some dummy variables and this can be done using pandas because logistic regression just take two values.So whenever you apply machine learning you need to make sure that there are no string values present because it won't be taking these as your input variables.So using string you don't have to predict anything but in my case I have the survived columns 2210 how many?People tend to survive and how many did not so CEO stands for did not survive and one stands for survive.So now let me just convert these variables into dummy variables. So I'll just use pandas and a say PD not get dummies.You can simply press tab to autocomplete and say Titanic data and I'll pass the sixso you can just simply click on shift + tab to get more information on this. So here we have the type data frameand we have the passenger ID survived and passenger class. So if Run this you'll see that 0 basically stands for not a female and one standfor it is a female similarly for male 0 Stanford's not made and one Stanford may now we don't require both these columnsbecause one column itself is enough to tell us whether it's male or you can say female or not.So let's say if I want to keep only male I'll say if the value of mail is 1 so it is definitely a maid and is not a female.So that is how you don't need both of these values. So for that I just remove the First Column,let's say a female so I'll say drop first. Andrew it has given me just one columnwhich is male and has a value 0 and 1. Let me just set this as a variable hsx soover here I can say sex dot head and just want to see the first five rows.Sorry, it's dot. So this is how my data looks like now here. We have done it for sex.Then we have the numerical values in age. We have the numerical values in spouses. Then we have the ticket number.We have the pair and we have embarked as well. So in Embark the values are in. C and Q so here also we can apply this get dummy function.So let's say I will take a variable. Let's say embark. I'll use the pandas Library.I'll enter the column name that is embarked.Let me just print the head of it. So I'll say Embark dot head so over here. We have c q and s now here also we can drop the First Columnbecause these two values are enough with the passenger is either traveling for Q. That is Q in stone S4 sound timeand if both the values are 0 then definitely the passenger is from Cherbourg. That is the third valueso you can again drop the first value. So I'll say drop and true.Let me just run this. So this is how my output looks like now similarly you can do it for The class as well.So here also we have three classes one two, and three so I'll just copy the whole statement.So let's say I want the variable name. Let's say PCL. I'll pass in the column namethat is PE class and I'll just drop the First Column. So here also the values will be 1 2 or 3and I'll just remove the First Column. So here we just left with two and three so if both the values are 0 then definitelythe passengers travelling in the first class now, we have made the values as categorical now,my next step would be to concatenate all these new rules into a data set. We can see Titanic data using the pandas will just concatenateall these columns. So I'll Superior. One cat and then say if we have to concatenate sex,we have to concatenate embarked and PCL and then I will mention the access to one.I'll just run this can you to print the head so over here you can see that these columns have been added over here.So we have the mail column with basically tells where the person is male or it's a female then we have the Embarkwhich is basically q and s so if it's traveling from Queenstown value would be one else it would be 0 and If both of these values are zeroed,it is definitely traveling from Cherbourg. Then we have the passenger class as 2 and 3.So the value of both these is 0 then passengers travelling in class one. So I hope you got thistill now now these are the irrelevant columns that we have it over here so we can just drop these columns will dropin PE class the embarked column and the sex column. So I'll just typein Titanic data dot drop and mention the columns that I want to drop. So I say I even read the passenger IDbecause it's nothing but just the index value which is starting from one. So I'll drop this as well then I don't want name as well.So I'll delete name as well. Then what else we can drop we can drop the ticket as well.And then I'll just mention the axis. I'll say in place is equal to True.Okay. So now my column name starts uppercase. So these has been dropped now,let me just bring my data set again. So this is my final leader said guys, we have the survived column which has the value 0and 1 then we have the passenger class or we forgot to drop this as well. So no worries.I'll drop this again.So now let me just run this. So over here we have the survive. We have the age.We have the same SP. We have the part. We have Fair mail and these we have just converted.So here we have just performed data angle. You can see clean the data and then we have just converted the values of genderto male then embarked to q and s and the passenger Class 2 2 & 3. So this was all about my data wranglingor just cleaning the data then my next up is training and testing your data. So here we will split the data set into train subsetand test steps. And then what we'll do we'll build a model on the train data and then predict the output on your test data set.So let me just go back to Jupiter and it is implement this as well over here. I need to train my data set.So I just put this indeed heading 3. So over here, you need to Define your dependent variableand independent variable. So here my Y is the output for you can say the value that you need to predict so over here,I will write Titanic data. I'll take the column which is survive. So basically I have to predict this columnwhether the passenger survived or not. And as you can see we have the discrete outcome, which is in the form of 0 and 1 and rest all the things wecan take it as a features or you can say independent variable. So I'll say Titanic data.Not drop so we just simply drop the survive and all the other columns will be my independent variable.So everything else as a features which leads to the survival rate. So once we have defined the independent variableand the dependent variable next step is to split your data into training and testing subset. So for that we will be using SK loan.I just type in from sklearn dot cross validation. import train test plate Now hereif you just click on shift and tab, you can go to the documentation and you can just see the examples over here.I second class to open it and then I just go to examples and see how you can split your data.So over here you have extra next test wide range why test and then using this train test plateletand just passing your independent variable and dependent variable and just Define a size and a random straight to it.So, let me just copy this and I'll just paste over here. Over here we will train testthen we have the dependent variable train and test and using the split function will pass in the independentand dependent variable and then we'll set a split size. So let's say I'll put it up 0.3.So this basically means that your data set is divided in 0.3 that is in 70/30 ratio, and then I can add any random straight to it.So let's say I'm applying one this is not necessary. If you want the same result as that of mine,you can add the random shape. So this will basically take exactly the same sample every Next I have to train and predict by creating a model.So here logistic regression will graph from the linear regression. So next I'll just type in from SK loan dot linear model import logistic regression.Next I'll just create the instance of this logistic regression model. So I'll say log model is equals to largest aggression now.I just need to fit my model. So I'll say log model dot fit and I'll just pass in my ex train.and white rain Alright, so here it gives me all the detailsof logistic regression. So here it gives me the class way dual fit intercept and all those things then what I need to do,I need to make prediction. So I'll take a variable and checked addictions and I'll pass on the model to it.So I'll say log model dot predict and I'll pass in the value that is X test.So here we have just created a model fit that model and then we had made predictions. So now to evaluate how my model has been performing.So you can simply calculate the accuracy or you can also calculate a classification report.So don't worry guys. I'll be showing both of these methods. So I'll say from sklearn dot matrix input classification report.It's all here are used as fiction report. And inside this I'll be passing in white testand the predictions. So guys this is my classification report.So over here, I have the Precision. I have the recall. We have the advanced code and then we have support.So here we have the value of decision as 75 72 and 73 which is not that bad nowin order to calculate the accuracy as well. You can also use the concept of confusion Matrix.So if you want to print the confusion Matrix, I will simply say from sklearn dot matrix import confusion Matrix first of all,and then we just print this So how my function has been imported successfully so I'll say confusion Matrix.And again passing the same variables which is why test and predictions. So I hope you guys already know the concept of confusion Matrix.So I just tell you in a brief what confusion Matrix is all about? So confusion Matrix is nothing but a 2 by 2 Matrixwhich has a four outcomes. This basically tells us that how accurate your values are. So here we have the column as predicted.No predicted. Why? And we have actual no and then actually yes.So this is the concept of confusion Matrix. So here let me just fade in these values which we have just calculated.So here we have 105. 105 2125 and 63 So as you can see here,we have got four outcomes now 105 is the value where a model has predicted.No, and in reality. It was also a no so where we have predicted know an actual know similarly.We have 63 as a predicted. Yes. So here the model predicted. Yes, and actually also it was a yes.So in order to calculate the accuracy, you just need to add the sum of these two values and divide the whole by the some.So here these two values tells me where the order has actually predicted the correct output.This value is also called as true- This is called as false positive. This is called as true positiveand this is called a false negative. Now in order to calculate the accuracy. You don't have to do it manually.So in Python, you can just import accuracy score function and you can get the results from that.So I'll just do that as well. So I'll say from sklearn dot-matrix import accuracy scoreand I'll simply print the accuracy and we'll pass in the same variables. That is why it is and predictions so over.Here, it tells me the address. He has 78 which is quite good so over here if you want to do itmanually, we have 2 plus these two numbers, which is 105 263. So this comes out to almost 168 and then you have to divideby the sum of all the phone numbers. So 105 plus 63 plus 21 plus 25,so this gives me a result of to 1/4. So now if you divide these two number, you'll get the same accuracythat is 78 percent or you can say point seven eight. So that is how you can calculate the See,so now let me just go back to my presentation. I let's see what all we have covered till now. So here we have first plate our data into trainand test subset then we have build a model on the train data and then predicted the output on the test data setand then my fifth step is to check the accuracy. So here we have calculator accuracy to almost 78 percentwhich is quite good. You cannot say that accuracy is bad. So here it tells me how accurate your results are so him accuracy score definesthat and hence got a good accuracy. So now moving ahead. Let us see the second project that is SUV data analysis.So in this a car company has released new SUV in the market and using the previous data about the sales of their SUV.They want to predict the category of people who might be interested in buying this. So using the logistic regression,you need to find what factors made people more interested in buying this SUV. So for this let us hear data set where I have user ID.I have gender as male and female then we have the age we have the estimated. Melody and then we have the purchased column.So this is my discreet column or you can see the categorical column. So here we just have the value that is 0 and 1 and this column we need to predictwhether a person can actually purchase a SUV or Not. So based on these factors, we will be decidingwhether a person can actually purchase a SUV or not. So we know the salary of a person we know the ageand using these we can predict whether person can actually purchase SUV or not. So, let me just go to my jupyter notebookand it is Implement a logistic regression. So guys, I I will not be going through all the details of data cleaning and analyzing the part start part.I'll just leave it on you. So just go ahead and practice as much as you can. Alright, so the second project is SUV predictions.So first of all, I have to import all the libraries so I say import numpy as NP and similarly.I'll do the rest of it.Alright, so now let me just print the head of this data set. So this we have already seen that we have columns as user ID.We have gender. We have the H we have the salary and then we have to calculate whether person can actually purchase a SUV or not.So now let us just simply go on to the algorithm part. So we'll directly start off with the logistic regressionon how you can train a model. So for doing all those things, we first need to Define your independent variableand dependent variable. So in this case, I want my ex at is an independent variable is a data set.I lock so here I will be specifying all the School and basically stands for that and in the columns,I want only two and three dot values. So here we should fetch me all the rowsand only the second and third column which is age and estimated salary. So these are the factors which will be used to predict the dependent variablethat is purchase. So here my dependent variable is purchase and independent variable is of age and salaryso I'll say Lena said dot I love I'll have all the rows and add just one fourth column.That is my purchased column. You don't values. All right, so I just forgot when one square bracket over here.Alright, so over here. I have defined my independent variable and dependent variable. So here my independent variable is age and salaryand dependent variable is the column purchase. Now, you must be wondering what is this? I lock function.So I look function is basically an index of a panda's data frame and it is used for integer based indexingor you can also say selection by index now, let me just bring these independent variablesand dependent variable. If I bring the independent variable I have age as well as a salary next.Let me print the dependent variable as well. So over here you can see I just have the values in 0and 1 so 0 stands for did not purchase next. Let me just divide my data set into training and test subset.So I'll simply write in from sklearn dot cross plate not cross-validation.Import drain test next I'll just press shift + Tab and over here.I'll go to the examples and just copy the same line. So I'll just copy this.As move the points now, I want to text size to be let's see 25, so I have divided the train in tested in 75/25 ratio.Now, let's say I'll take the random set of 0 So Random State basically ensures the same resultor you can say the same samples taken whenever you run the code. So let me just run this now.You can also scale your input values for better performing and this can be done using standard scalar.So let me do that as well. So I'll say from sklearn Dot pre-processing.Import standard scale now. Why do we scale it now? If you see a data set we are dealing with large numbers.Well, although we are using a very small data set. So whenever you're working in a prod environment, you'll be working with large data set wewill be using thousands and hundred thousands of you pulls so they're scaling down will definitely affect the performance by a large extent.So here let me just show you how we can scale down these input values and then the pre-processing contains all your methods & functionality,which is Required to transform your data. So now let us scale down for test as well as a training data set.So else First Make an instance of it. So I'll say standard scalar. Then I have Extreme sasc Dot fit fit underscore transform.I'll pass in my Xtreme video.And similarly I can do it for test wherein I'll pass the X test. All right.Now my next step is to import logistic regression. So I'll simply apply logistic regressionby first importing it. So I'll say from sklearn sklearn the linear model import logistic regression over here.I'll be using classifier. So I said classifier dot is equals to logistically aggression so over here,I just make an instance of it. So I'll say logistic regression and over here. I just pass in the random state,which is 0 No, I simply fit the model.And I simply passing next rain and white rain. So here it tells me all the detailsof logistic regression. Then I have to predict the value. So I'll say why I prayed it's equal to classifier.Then predict function and then I just pass in X test. So now we have created the model.We have scaled down our input values. Then we have applied logistic regression. We have predicted the valuesand now we want to know the accuracy. So now the accuracy first we need to import accuracy scores.So I'll say from sklearn dot matrix input accuracy schooland using this function we can calculate the accuracy or you can manually do that by creating a confusion Matrix.So I'll just pass. my lightest and my y predicted All right, so over here I get the accuracyas 89% So we want to know the accuracy in percentage. So I just have to multiply it by a hundred and if I run thisso it gives me 89% So I hope you guys are clear with whatever I have taught you today. So here I have taken my independent variable as ageand salary and then we have calculated that how many people can purchase SUV and then we have calculated our model by checkingthe accuracy so over here we get the accuracies 89 which is great. Alright guys that is it for today.So I'll Scoffs what all we have covered in today's training. First of all, we had a quick introduction to what is regressionand where the regression is actually use then we have understood the types of regression and then got into the detailsof what and why of logistic regression of compared linear was in logistic regression. We have also seen the various use caseswhere you can Implement logistic regression in real life and then we have picked up two projects that is Titanic data analysisand SUV prediction so over here we have seen how we can collect your data analyze your data then perform.Modeling on that data train the data test the data and then finally have calculated the accuracy.So in your SUV prediction, you can actually analyze clean your data and you can do a lot of thingsso you can just go ahead pick up any data set and explore it as much as you can open your eyes and seearound you will find dozens of applications of machine learning which you are using and interacting with in your daily life peedbe using the phase detection. And Facebook are getting the recommendation for similar products from Amazon machine learningis applied almost everywhere. So hello and welcome all to this YouTube session will learn about how to build a decision tree.This session is designed in a way that you get most out of it. Alright. So this decision tree is a type of classification algorithmwhich comes under these supervised learning technique. So before learning about decision tree, I'll give you a short introduction to classificationwhere we'll learn about. What is classification what I'd say, Various types where it is used or what I'd see use cases now,once you get your fundamental clear will jump to the decision tree part under this. First of all, I will teach you to mathematicallycreate a decision tree from scratch then once you get your Concepts clear, we'll see how you can write a decision tree classifierfrom scratch in Python using the card algorithm. All right. I hope the agenda is scared you guys what is classification?I hope every one of you must have used Gmail. So how do you think the male is getting classified as Spamor not spam mail. Well, there's nothing but classification So What It Is Well classification is the processof dividing the data set into different categories or groups by adding label. In other way, you can saythat it is a technique of categorizing the observation into different category. So basically what you are doing is you are takingthe data analyzing it and on the basis of some condition you finely divided into various categories.Now, why do we classify it? Well, we classify it to perform predictive analysis on it. Like when you get the mailthe machine predicts it to be a Spam or not spam mail and on the basis of that prediction it add the irrelevant or spam mailto the respective folder in general this classification. Algorithm handle questions. Like is this data belongs to a category or B category?Like is this a male or is this a female something like that now the question arises where will you use it?Well, you can use this of protection order to check whether the transaction is genuine or not suppose I am using.A credit card here in India now due to some reason I had to fly to Dubai now. If I'm using the credit card over there,I will get a notification alert regarding my transaction. They would ask me to confirm about the transaction.So this is also kind of predictive analysis as the machine predicts that something fishy is in the transaction as very for our ago.I made the transaction using the same credit card and India and 24 hour later. The same credit card is being used for the payment in Dubai.So the Machine predicts that something fishy is going on in the transaction. So in order to confirm it it sends you a notification alert.All right. Well, this is one of the use case of classification you can even use it to classify different itemslike fruits on the base of its taste color size overweight a machine. Well trained using the classification algorithmcan easily predict the class or the type of fruit whenever new data is given to it. Not just the fruit.It can be any item. It can be a car. It can be a house. It can be a I'm bored or anything.Have you noticed that while you visit some sites or you try to login into some you get a picture capture for that rightwhere you have to identify whether the given image is of a car or its of a pole or not? You have to select it for example that 10 imagesand you're selecting three Mages out of it. So in a way you are training the machine right you are tellingthat these three are the picture of a car and rest are not so who knows you are training at for something big right?So moving on ahead. Let's discuss the types. S of classification online. Well, there are several different waysto perform the same tasks like in order to predict whether a given person is a male or a female the machine had to be trained first.All right, but there are multiple ways to train the machine and you can choose any one of them just for Predictive Analytics.There are many different techniques but the most common of them all is the decision tree, which we'll cover in depth in today's session.So as a part of classification algorithm we have decision tree random Forest name buys k-nearest neighbor.Logistic regression linear regression support Vector machines and so on there are many. Alright, so let me give you an idea about fewof them starting with decision tree. Well decision tree is a graphical representation of all the possible solutionto a decision the decisions which are made they can be explained very easily. For example here is a task,which says that should I go to a restaurant or should I buy a hamburger you are confused on that.So for that what you will do, you will create a dish entry for it starting with the root node will be first of all,you will check whether you are hungry or not. All right, if you're not hungry then just go back to sleep.Right? If you are hungry and you have $25 then you will decide to go to restaurant. And if you're hungry and you don't have $25,then you will just go and buy a hamburger. That's it. All right. So there's about decision tree now moving on ahead.Let's see. What is a random Forest. Well random Forest build multiple decision treesand merges them together to get a more accurate and stable production. All right, most of the time random Forest is trainedwith a bagging method. The bagging method is based on the idea that the combinationof learning model increases the overall result. If you are combining the learning from different modelsand then clubbing it together what it will do it will Increase the overall result fine.Just one more thing. If the size of your data set is huge. Then in that case one single decision tree would leadto our Offutt model same way like a single person might have its own perspective on the complete population as a population is very huge.Right? However, if we implement the voting system and ask different individual to interpret the data,then we would be able to cover the pattern in a much meticulous way even from the diagram.You can see that in section A we have Howard large training data set what we do. We first divide our training data setinto n sub-samples on it and we create a decision tree for each cell sample. Now in the B partwhat we do we take the vote out of every decision made by every decision tree. And finally we Club the vote to getthe random Forest dition fine. Let's move on ahead. Next.We have neighbor Buys. So named by is is a classification technique, which is based on Bayes theorem.It assumes that It's of any particular feature in a class is completely unrelated to the presenceof any other feature named buys is simple and easy-to-implement algorithm and due to a Simplicitythis algorithm might out perform more complex model when the size of the data set is not large enough.All right, a classical use case of name bias is a document classification. And that what you do you determinewhether a given text corresponds to one or more categories in the text case,the features used might be the presence or absence. Absence of any keyword. So this was about Nev from the diagram.You can see that using neighbor buys. We have to decide whether we have a disease or not. First what we do we check the probabilityof having a disease and not having the disease right probability of having a disease is 0.1while on the other hand probability of not having a disease is 0.9. Okay first, let's seewhen we have disease and we go to the doctor. All right, so when we visited the doctorand the test is positive Adjective so probability of having a positive test when you're having a disease is 0.8 0 and probabilityof a negative test when you already have a disease that is 0.20. This is also a false negative statement as the testis detecting negative, but you still have the disease, right? So it's a false negative statement.Now, let's move ahead when you don't have the disease at all. So probability of not having a disease is 0.9.And when you visit the doctor and the doctor is like, yes, you have the disease. But you already know that you don't have the disease.So it's a false positive statement. So probability of having a disease when you actually know there is no disease is 0.1 and probabilityof not having a disease when you actually know there is no disease. So and the probability of it is around 0.90 fine.It is same as probability of not having a disease in the test is showing the same results a true positive statement.So it is 0.9. All right. So let's move on ahead and discuss about kn n algorithm.So this KNN algorithm or the k-nearest neighbor, it stores all the available casesand classifies new cases based on the similarity measure the K in the KNN algorithm as the nearest neighbor,we wish to take vote from for example, if k equal 1 then the object is simply assigned to the classof that single nearest neighbor from the diagram. You can see the difference in the image when k equal 1 k equal 3 and k equal 5, right?Well the And systems are now able to use the k-nearest neighbor for visual pattern recognization to scanand detect hidden packages in the bottom bin of a shopping cart at the checkout if an object is detectedwhich matches exactly to the object listed in the database. Then the price of the spotted product could evenautomatically be added to the customers Bill while this automated billing practice is not usedextensively at this time, but the technology has been developed and is available for use if you want you can just use It and yeah,one more thing k-nearest neighbor is also used in retail to detect patterns in the credit carduses many new transaction scrutinizing software application use Cayenne algorithmsto analyze register data and spot unusual pattern that indicates a species activity.For example, if register data indicates that a lot of customers information is being entered manually ratherthan to automated scanning and swapping then in that case. This could indicate that the employees were using the register.Are in fact stealing customers personal information or if I register data indicates that a particular good is being returnedor exchanged multiple times. This could indicate that employees are misusing the return policyor trying to make money from doing the fake returns. Right? So this was about KNN algorithmsince our main focus for this session will be on decision tree. So starting with what is decision tree,but first, let me tell you why did we choose the Gentry to start with? Well, these decision tree are really very easy.Easy to read and understand it belongs to one of the few models that are interpretable where you can understand exactlywhy the classifier has made that particular decision right? Let me tell you a fact that for a given data set.You cannot say that this algorithm performs better than that. It's like you cannot say that the Asian trees better than a buysor name biases performing better than decision tree. It depends on the data set, right? You have to apply hit and trial method with allthe algorithms one by one and then compare the The model which gives the best result as a modelwhich you can use at for better accuracy for your data set. All right. So let's start with what is decision tree.Well a decision tree is a graphical representation of all the possible solution to our decision based on certain conditions.Now, you might be wondering why this thing is called as decision tree. Well, it is called sobecause it starts with the root and then branches off to a number of solution just like a tree right even the trees.Starts from a roux and it starts growing its branches once it gets bigger and bigger similarly in a decision tree.It has a roux which keeps on growing with increasing number of decision and the conditions now,let me tell you a real life scenario. I won't say that all of you, but most of you must have used it.Remember whenever you dial the toll-free number of your credit card company. It redirects you to his intelligent computerised assistantwhere it asks you questions like, press one for English or press 2 for Henry, press 3 for this press 4 for that.Great now once you select one now again, it redirects you to a certain set of questions like press 1 for this press 1 for thatand similarly, right? So this keeps on repeating until you finally get to the right person, right?You might think that you are caught in a voicemail hell but what the company was actually doing it was just using a decision tree to get you to the right person.I lied. I'd like you to focus on this particular image for a moment on this particular slide. You can see I image where the task is.Should I accept a new job offer? Or not. All right, so you have to decide that for that what you did you created a decision tree startingwith the base condition or the root node. Was that the basic salary or the minimum salary should be $50,000if it is not $50,000. Then you are not at all accepting the offer. All right. So if your salary is greater than $50,000,then you will further check whether the commute is more than one hour or not. If it is more than one are you will just decline the offerif it is less than one hour, then you are getting closer to accepting the job offer. Photo what you will do you will checkwhether the company is offering free coffee or not. Right if the company is not offering the free coffee,then you will just declined off and if it is offering the free coffee and yeah, you will happily accept the offer right thereare just an example of a decision tree. Now, let's move ahead and understand a decision tree.Well, here is a sample data set that I will be using it to explain you about the decision tree. Alright in this data set each row is an exampleand the first two columns provide features. Attributes that describes the data and the last columngives the label or the class we want to predict and if you like you can just modify this data by adding additional featuresand more example and our program will work in exactly the same way fine. Now this data set is pretty straightforwardexcept for one thing. I hope you have noticed that it is not perfectly separable. Let me tell you something more about that asin the second and fifth examples, they have the same features, but different labels, both are Yellow as a Colour and diameter as three,but the labels are mango and lemon right? Let's move on and see how our decision tree handles this case.All right, in order to build a tree will use a decision tree algorithm called card this card algorithmstands for classification and regression tree algorithm online. Let's see a preview of how it works.All right to begin with We'll add a root note for the tree and all the nodes receive a listof rows as input and the root will receive the entire. Training data set now each node will ask true and false questionabout one other feature. And in response to that question will split or partition the data set into two different subsetsthese subsets then become input to child node. We are to the tree and the goal of the question is to finally unmix the labelsas we proceed down or in other words to produce the purest possible distribution of the labels at each node.For example, the input of this node contains only one single type of label. So we See that it's perfectly unmixed.There is no uncertainty about the type of label as it consists of only grapes righton the other hand the labels in this node are still mixed up. So we would ask another question to further drill it down.Right but before that we need to understand which question to ask and when and to dothat we need to conduct by how much question helps to unmix the label and we can quantify the amount of uncertaintyat a single node using a metric. Called gini impurity and we can quantify how much a question reducesthat uncertainty using a concept called Information Gain will use these to select the best question to ask at each point.And then what we'll do we'll iterate the steps will recursively build the tree on each of the new node will continue dividing the datauntil they are no further question to ask and finally we reach to our Leaf. Alright, alright.So this was about decision tree. So in order to create a decision tree, first of all what you have to do you have to identifyA different set of questions that you can ask to a tree like is this color green and what will be these question?These questions will be decided by your data set like as this colored green is the diameter greater than equal to 3 is the color yellowright questions resembles to your data set remember that? All right. So if my color is green,then what it will do it will divide into two parts. First. The Green Mango will be in the true while on the false.We have lemon and the Mac. All right if the color is green or the diameter. Meter is greater than equal to 3or the color is yellow Asian tree terminologies. So starting with root node root node is a base nodeof a tree the entire tree starts from a root node. In other words. It is the first node of a tree it representsthe entire population or sample and this entire population is further segregatedor divided into two or more homogeneous set fine. Next is the leaf node.Well Leaf node is the one when you reach at the The tree right that is you cannot further segregated downto any other level that is the leaf node. Next is splitting splitting is dividing your root nodeor node into different sub part on the basis of some condition. All right, then comes the branch or the sub tree.Well, this Branch or subtree gets formed when you split the tree suppose when you split a root node,it gets divided into two branches or two subtrees. Right? Next is the concept of pruning.Well you can Say that pruning is just opposite of splitting what we are doing here. We are just removing the sub node of a decision treewill see more about pruning later in this session. All right, let's move on ahead. Next is parent or child node.Well, first of all root node is always the parent node and all other nodes associatedwith that is known as chalky node. Well, you can understand it in a way that all the top nodebelongs to a parent node and all the bottom node, which are derived from a top node is a child node.Node producing a further note is a child node and the node which is producing it as a parent nodesimple concept, right? It's use the cart algorithm and design a tree manually. So first of allwhat you will do you decide which question to ask and when so how will you do that? So let's first of all visualize the decision tree.So there's the decision tree which will be creating manually or like first of all, let's have a look at the data set.You have Outlook temperature humidity and windy as your different attribute on the basis of that you have to predictthat whether you can play or not. So which one among them should you pick first answer determinethe best attribute that classifies the training data? All right. So how will you choose the best attributeor how does a tree decide where to split or how the tree will decide its root node? Well before we move onand split a tree there are some terminologies that you should know. All right, first being the gini index.So what is this gini index? The gini index is the measure of impurity or Purity used in building a day.Gentry and cart algorithm. All right. Next is Information Gain this Information Gain isthe decrease in entropy after data set is split on the basis of an attribute constructing a decision tree is all about finding an attributethat Returns the highest Information Gain. All right, so you will be selecting the node that would give you the highest Information Gain.Alright next is reduction in variance. This reduction in variance is an algorithm, which is used for continuous Target variableor regression problems the split With lower variance is selected as a criteria to let the population see in general term.What do you mean by variance? Variance is how much your data is wearing? Right? So if your data is less impure or is more purethan in that case the variation would be less as all the data almost similar, right? So there's also a way of setting a tree the splitwith lower variance is selected as the criteria to split the population. Alright. Next is the chi Square C Square.It is an algorithm which is used to find out these statistical significance between the Is between sub nodes and the parent nodes fine.Let's move ahead. Now. The main question is how will you decide the best attributefor now just understand that you need to calculate something known as Information Gain the attributewith the highest Information Gain is considered the best. Yeah. I know your next question might be like, what is this information again?But before we move on and see what exactly Information Gain Is let me first introduce youto a term called entropy because this term will be used in calculating the Information Gain. Mmmmmm.Well entropy is just a metric which measures the impurity of something or in other words, you can say that as the first step to dobefore you solve the problem of a decision tree as I mentioned is something about impurity. So let's move on and understand what is impurity suppose.You are a basket full of apples and another Bowl which is full of same label, which says Apple nowif you are asked to pick one item from each basket and ball then the probability of getting the appleand it's correct label is 1 so in this case, You can see that impurities zero. All right.Now what if there are four different fruits in the basket and four different labels in the bowl,then the probability of matching the fruit to a label is obviously not one. It's something less than that.Well, it could be possible that I picked banana from the basket and when I randomly picked the label from the ball,it says a cherry any random permutation and combination can be possible. So in this case I'd say that impurities is nonzero.I hope the concept of impurities care. Are so coming back to entropy as I said entropy is the measure of impurityfrom the graph on your left. You can see that as the probability is zero or one that has either they are highly impureor they are highly pure than in that case the value of entropy is zero. And when the probability is 0.5,then the value of entropy is maximum. Well, what is impurity impurities the degreeof Randomness how random data is so if the data is completely pure in that case the randomness equals 0 orif the Dies completely Empire even in that case the value of impurity will be zero question.Like why is it that the value of entropy is maximum at 0.5 might arise in a mine, right?So let me discuss about that. Let me derive at mathematically as you can see here on the slide,the mathematical formula of entropy is - of probability of yes, let's move on and seewhat this graph has to say mathematically suppose s is our total sample space and it's divided into two parts.Yes, and no. No, like in our data set the result for playing was divided into two parts. Yes or no,which we have to predict either we have to play or not. Right? So for that particular case, you can Define the formula of entropy as entropyof total sample space equals negative of probability of e is multiplied by logof probability of years with a base 2 minus probability of no X log of probabilityof no with base to where s is your total sample space and P of v s is the probabilityof E. And be of known as the probability of no, well, if the number of yes equal number of knowthat is probability of s equals 0.5 right since you have equal number of yes,and no so in that case value of entropy will be one just put the value over there.All right. Let me just move to the next slide. I'll show you this. Alright next is if it contains all Yes,or all know that is probability of a sample space is either 1 or 0 then in that case entropy will be equal to 0Let's see the mathematically one by one. So let's start with the first conditionwhere the probability was 0.5. So this is our formula for entropy, right?So there's our first case right which we discuss the art when the probability of vs equal probability of nodethat is in our data set. We have equal number of yes, and no. All right. So probability of yes equal probability of noand that equals 0.5 or in other words, you can say that yes plus no equal to Total sample.He's all right, since the probability is 0.5. So when you put the valuesin the formula you get something like this and when you calculate it, you will get the entropy of the total sample space as one.All right. Let's see for the next case. What is the next case either you have totally usor you have totally know so if you have total, yes, let's see the formula when we have totally as soyou have all yes and 0 no fine. So probability of e s equal 1 and yes.Yes as the total sample space obviously. So in the formula when you put that thing up here,you get entropy of sample space equal negative X of 1 multiplied by log of 1 as the value of log 1 equals 0.So the total thing will result to 0 similarly is the case with no even in that case, you will get the entropy of total sample space as 0so this was all about entropy. All right. Next is what is Information Gain?Well Information Gain what it does is it measures the reduction in entropy? It decides which attributesshould be selected as the decision node. If s is our total collection than Information Gain equals entropy,which we calculated just now that - weighted average X entropy of each feature.Don't worry. We'll just see how it to calculate it with an example. Let's manually build a decision treefor our data set. So there's our data set which consists of 14 different instances out of which we have nine.Yes and five know I like so we have the formula for entropy just put over that since 9 years.So total probability of e s equals 9 by 14 and total probability of no equals Phi by 14and when you put up the value and calculate the result, you will get the value of entropy as 0.94.All right. So this was your first step that is compute the entropy for the entire data set only now,you have to select that out of Outlook temperature humidity and windy, which of the node should you select as the root nodebig question right? I will Decide that this particular node should be chosen at the base note. And on the basis of that only I will be creatingthe entire tree. I will select that. Let's see. So you have to do it one by one you have to calculate the entropyand Information Gain for all of the different nodes. So starting with Outlook. So Outlook hasthree different parameters Sunny overcast and rainy. So first of all select how many number of yearsand no are there in the case of Sunny like when it is sunny how many number of years and how many number of knows?Are there so in total we have to yes and three Nos and case of sunny in case of overcast.We have all yes. So if it is overcast then we will surely go to play. It's like that. Alright and next it is rainy then total numberof vs equal 3 and total number of no equals 2 fine next what we do we calculate the entropyfor each feature for here. We are calculating the entropy when Outlook equals Sunny.First of all, we are assuming that Outlook is our root node and for that we are calculating the Can gain for it.All right. So in order to calculate the Information Gain remember the formula it was entropy of the total sample space -weighted average X entropy of each feature. All right. So what we are doing here, we are calculating the entropy of Outlookwhen it was sunny. So total number of yes, when it was Sonny was to and total number of knowthat was three fine. So let's put up in the formula since the probability of yes is 2 by 5and the probability of no is 3 by 5. So you will get something like this. All right. So you are getting the entropyof sunny as zero point nine seven one fine. Next we will calculate the entropy for overcastwhen it was overcast. Remember it was all yes, right. So the probability of e is equal 1 and when you put overthat you will get the value of entropy as 0 fine and when it was rainy rainy has 3s and to nose.So probability of e s in case of Sonny's 3 by 5 and probability of know in case of Sonny's 2 by 5and when you add the You of probability of vs and probability of note the formula you get the entropyof sunny as zero point nine seven one point. Now, you have to calculate how much information you are getting from Outlookthat equals weighted average. All right. So what was this weighted average total number of yearsand total number of no fine. So information from Outlook equals 5 by 14 from where does this 5 came over?We are calculating the total number of sample space within that particular Outlook when it was sunny, right?So in case of Sunny there was two years and three NOS. All right. So weighted average for Sonny would be equal to 5 by 14.All right, since the formula was five by 14 x entropy of each feature. All right, soas calculated the entropy for Sonny is zero point nine seven one, right?So what we'll do we'll multiply five by 14 with 0.97 one, right?Well, this was the calculation for information when Outlook equal sunny, but Outlook even equals overcast and rainy.In that case, what we'll do again similarly will calculate for everything for overcast and sunnyfor overcast weighted averages for by 14 x its entropy. That is 0 and for Sonny it is same 5i 14-3.Yes and two nodes X its entropy that is zero point nine seven one. And finally we'll take the sum of all of them which equalsto 0.693 right next. We will calculate the information gained thiswhat we did earlier was Malaysian taken from Outlook. Now. We are calculating. What is the information?We are gaining from Outlook right. Now this Information Gain that equals to Total entropy minus the informationthat is taken from Outlook. All right. So total entropy we had 0.94 -information we took from Outlook as 0.693. So the value of information gained from Outlook resultsto zero point two four seven. All right. So next what we have to do. Let's assume that Wendy is our root node.So Wendy consists of two parameters false and true. Let's see how many yearsand how many nodes are there in case of true and false. So when Wendy has Falls as its parameter,then in that case, it has six years and two nodes and when it as true as its parameter,it has 3 S and 3 nodes. All right. So let's move ahead and similarly calculate the information taken from Wendyand finally calculate the information gained from Wendy. Alright, so first of all, what we'll do we'll calculate the entropy of each feature.ER starting with windy equal true. So in case of true we had equal number of yesand equal number of know. We'll remember the graph when we had the probability as 0.5 as total number of yearsequal total number of know and for that case the entropy equals 1 so we can directly write entropy of roomwhen it's windy is one as we had already proved it when probability equals 0.5 the entropy is the maximumthat equals to 1. All right. Next is entropy of false when it is Vending. I like so similarly just put the probability of yesand no in the formula and then calculate the result since you have six years and to nose.So in total, you'll get the probability of yes 6 by 8 and probability of no as 2 by 8.All right, so when you will calculate it, you will get the entropy of false as zero point eight one one.Alright now, let's calculate the information from windy. So total information collected from Windyequals information taken when Wendy equal true plus Action taken when Wendy equal false.So we'll calculate the weighted average for each one of them and then we'll sum it up to finally get the total information taken from windy.So in this case, it equals to 8 by 14 multiplied by 0.8 1 1 plus 6 by 14 x 1.What is this? 8 it is total number of yes, and no in case when when D equals false, right?So when it was false, so total number of BS that equals to 6 and total more of know that equal to 2that some UPS to 8. Alright, so that is why the waiter. Resul results to Aid by 14 similarly information takenwhen windy equals true equals to 3 plus 3 that is 3 S and 3 no equal 6 divided by total numberof sample space that is 14 x 1 that is entropy of true. All right.So it is 8 by 14 multiplied by 0.8 1 1 plus 6 by 14 x onewhich results to 0.89 to this is information taken from Windy.All right. Now how much information you are gaining from Wendy? So for that what you will do,so total information gained from Windy that equals to Total entropy - information taken from Windy.All right, that is 0.94 - 0.89 to that equals to zero point zero four eight.So 0.048 is the information gained from Windy. Similarly. We calculated for the rest too.So for Outlook as you can see, the information was 0.693, and it's Information Gain was zero point two four seven incase of temperature the information was around. Zero point nine one one and the Information Gainthat was equal to 0.02 9 in case of humidity. The information gained was 0.15 to and in the case of windy.The information gained was 0.048. So what we'll do we'll select the attribute with the maximum fine.Now, we are selected Outlook as our root node, and it is further subdivided into three different parts Sunny overcast and rain,so in case of overcast we have seen that it consists of all ears so we can consider it as a Leaf node,but in case of sunny and rainy it's doubtful as it consists of both. Yes and both knowso you need to recalculate the things right again for this node. You have to recalculate the things.All right, you have to again select the attribute which is having the maximum Information Gain. All right, so there ishow your complete tree will look like. All right. So, let's see when you can play so you can playwhen Outlook is overcast. All right in that case. You can always play if the Outlook is sunny.You will further drill. Time to check the humidity condition. All right, if the humidity is normal, then you will playif the humidity is high then you won't play right when the Outlook predicts that it's raining then further you will checkwhether it's windy or not. If it is a week went then you will go and offer play but if it has strong wind,then you won't play right? So this is how your entire decision tree would look like at the end. Now comes the concept of pruning say isthat what should I do to play? Well you have to do pruning pruning will decide how you will play.Say what is this pruning? Well, this pruning is nothing but cutting down the nodes and order to get the optimal solution.All right. So what pruning does it reduces the complexity? All right, as are you can see on the screen that it showing only the resultfor yes that is it showing all the result which says that you can play before we drill down to a practical sessiona common question might come in your mind. You might think that our tree based model better than linear model right?You can think like if I can Was a logistic regression for classification problem and linear regression for regression problem.Then why there is a need to use the tree. Well, many of us have this question in their mind and well there's a valid question too.Well actually as I said earlier, you can use any algorithm. It depends on the type of problem.You're solving let's look at some key factor, which will help you to decide which algorithm to use andwhen so the first point being if the relationship between dependent and independent variable as well approximated by By a linear model,then linear regression will outperform tree base model second case if there is a high non-linearityand complex relationship between dependent and independent variables at remodel will outperform a classical regression modelin third case. If you need to build a model which is easy to explain to people a decision tree modelwill always do better than a linear model as the decision tree models are simpler to interpret then linear regression.All right. Now let's move on ahead and see how you can write it as Gentry classifier from scratchand python using the cart algorithm. All right for this. I will be using jupyter notebook with python 3.0 installed on it.Alright, so let's open the Anaconda and the jupyter notebook. Where is that? So this is our Anaconda Navigatorand I will directly jump over to jupyter notebook and hit the launch button. I guess everyone knows that jupyter.Notebook is a web-based interactive Computing notebook environment where you can run your python codes.So my Jupiter notebook it opens on my Local Host w89 1 so I will be using this jupyter notebookin order to write my decision tree classifier using python for this decision tree classifier. I have already written the set of codes.Let me explain you just one by one. So we'll start with initializing our training data set.So there's our sample data set for which each row is an example. The last column is a labeland the first two columns are the features. If you want you can add some more features an example for your practice interesting fact isthat This data set is design and way that the second and fifth example have almost the same features, but they have different labels.All right, so let's move on and see how the tree handles this case as you can see here. Both of them II and the fifth column have the same features.What did different is just their label? Right? So let's move ahead. So this is our training data set next what we are doing weare adding some column labels. So they are used only to print the trees fine. So what we'll do we'll add header to the columnslike the First Column is of Close second is of diameter and third is a label column. All right, nextwhat we'll do we'll Define a function as unique values in which will pass the rows and the columns.So this function what it will do it will find the unique values for a column in the data set.So there's an example for that. So what we are doing here, we are passing training data Hazard rowand column number as 0 so what we are doing we are finding unique values in terms of color.And in this since the row is training data and the column is 1 so what you are doing here, so we are finding the you Values in terms of diameter fine.So this is just an example next what we'll do we'll Define a function as class count and we'll pass the rows into it.So what it does, it counts the number of each type of example within data set. So in this functionwhat you are basically doing we are counting the number of each type for example in the data set or what we are doing we are counting the unique valuesfor the label in the data set as a sample. You can see here we can pass that entire training data set to this particular function as class underscore countwhat it will do it will find all the different types of Label within the training data set as you can see here the unique label consistsof mango grape and lemon. So next what we'll do. We'll Define a function is numeric and we'll passa value into it. So what it will do it will just test if the value is numeric or not and it will return if the value is an integer or a float.For example, you can see is numeric. We are passing 7 so it is an integer so it will return in value and if we are passing red,it's not a numeric value, right? So moving on ahead where you define a class named as question,so This question does this question is used to partition the data set. This class voted does it just records a column number?For example 0 for color a light and a column value for example, green next what we are doing we are defining a match methodwhich is used to compare the feature value in the example to the feature values stored in the question.Let's see how first of all what you are doing. We are defining an init function and inside that we are passing the self columnand the value as parameter. So next what we do we Define a function as match what it Does it compares the feature valuein an example to the feature value in this question when next we'll Define a function as re PR,which is just a helper method to print the question in a readable format next what we are doing we are defining a function partition.Well, this function is used to partition the data set each row in the data set it checks if it match the question or notif it does so it adds it to the true rose or if not then it adds to the false Rose. All right, for example,as you can see, it's partition the training data. Based on whether the roses are red or not here.We are calling the function question and we are passing a value of zero and read to it. So what did we do it will assign all the red roseto True underscore Rose and everything else will be assigned to false underscore rose fine.Next. What we'll do we'll Define a gini impurity function and inside that will pass the list of rows.So what it will do it will just calculate the gini impurity for the list of rows.Next what we are doing here. We defining a function as Information Gain. So what this Information Gain function does it calculatesthe information game using the uncertainty of the starting node - the weighted impurity of the child node.The next function is find the best plate. Well, this function is used to find the best question to askby iterating over every feature of value and then calculating the Information Gain. But the detail explanation on the code,you can find the code in the description given below. All right next we'll define a class as leavefor classifying the data. It holds a dictionary of glass like mango for how many times it appears in the row from the training datathat reaches the sleeve. Alright, next is the decision node. So this decision node, it will ask a question.This holds a reference to the question and the two child nodes on the base of it. You are deciding which node to add further to which branch.Alright so next. What we are doing we are defining a function of build tree and inside that we are passing our number of rows.So this is the function that is used to build the tree. So initially what we did we Define all the various functionthat we'll be using in order to build a tree. So let's start by partitioning the data set for each unique attribute,then we'll calculate the information gain and then return the question that produces the highest gain and on the basis of that will split the tree.So what we are doing here, we are partitioning the data set calculating the Information Gain. And then what this is returning it is returning the questionthat is producing the highest gain. All right. Now if gain equals 0 return Leaf Rose,so what it will do. So if you are getting no for the gain that is gain equals 0 then in that casesince no further question could be asked so what it will do it will return a leaf fine now trueor underscore Rose or false underscore Rose equal partition with rose and the question.So if we are reaching till this position, then you have already found. A feature of value which will be used to partition the data set thenwhat you will do you will recursively build the true branch and similarly recursively build the false Branch.So return Division and Discord node and side that will be passing question to branch and false front.So what it will do it will return a question node. Alice question owed this recalls the best featureor the value to ask at this point fine. Now that we have built our tree next what we'll do we'll Define a print underscore tree functionwhich will be used to print the tree fine. So finally what we are doing in this particular functionthat we are printing our tree next is the classify function which will use it to decide whether to follow the true Branch or the false branchand then compared to the feature values stored in the node to the example. We are considering and lastwhat we'll do we'll finally print the production at Leaf. So let's execute it and see okay,so there's our testing data. All right. So we printed all Leafas well now that we have trained our algorithm with our training data set now it's time to test it.So there's our testing data set. So let's finally execute it and see what is the result.So this is the result you will get so first question, which is asked by the algorithm is is diameter greaterthan equal to 3 if it is true, then it will further ask if the color is yellow again,if it is true, then it will predict mango as one and lemon with one.And in case it is false, then it will just predict the mango. Now. This was the true part.Now next coming to diameter is not greater than or equal to 3 then in that case it's falseand what it will do it will just predict the grape fine. Okay. So this was all about the coding part now,let's conclude this session. But before concluding let me just show you one more thing. Now, there's a scikit-learn algorithm cheat sheet,which explains you which algorithm you should use and when all right, let's build in a decision tree format.Let's see how it is built. So first condition it will check whether you have 50 samples or not. If your samples are greater than 50,then we'll move ahead if it is less than 50, then you need to collect more data if you sample is greater than 50,then you have to decide whether you want to predict a category or not. If you want to predict a category,then further you will see that whether you have labeled data or not. If you have label data, then that would be a classificationalgorithm problem. If you don't have the label data, then it would be a clustering problem. Now if you don't want to Category then what?Do you want to predict predict a quantity? Well, if you want to predict a quantity, then in that case, it would be a regression problem.If you don't want to predict a quantity and you want to keep looking further, then in that case, you should go for dimensionality reduction problems and stillif you don't want to look and the predicting structure is not working. Then you have tough luck for that.I hope this doesn't recession clarifies all your doubt over decision tree algorithm.Let's begin this tutorial by looking at the topics that we'll be covering today.So first of all, we'll start Away by getting a brief introduction of random forest and then we'll goas to see why we actually need random Forest right? Why not anything else but actually random Forest.So once we understand it's need at first place, then we'll go on to learn more about what is random forestand we'll also look at various. Examples of random Forest so that we get a very clear understanding of it.So for the will also delve inside in to understand the working of random Forest as tohow exactly random Forest Works will also watch out the random Forest algorithm step by step,right so that you are able to write any piece of code any domain specific algorithm on your own now,I personally believe that any learning is really incomplete. If it's not put into applicationso for its completion will also Implement random forest in r with a very simple use case that is diabetes prevention.So let's get started with the introduction then. No, random Forest is actually one of the classifierswhich is used for solving classification problems. Now since some of you might not be really awareof what classification is. So let's quickly understand classification first,and then we'll try to related to the random Forest. So basically classification is a machine learning techniquein which you already have predefined categories under which you can classify your data.So it's nothing but to supervised learning model where you already have a data based on which you can trainyour machine, right? So your machine actually learns from this data. So whatever all that predefined datathat you already have it actually works as a fuel for your machine, right?So let's say for an example ever wonderedhow your Gmail gets to know about the spam emails and filters it out from the rest of the genuine emails any guesses.All right. I'll give you a hint try to think something on the line that what would it actually look for what can bethe possible parameters based on which you can decide or read. This is a genuine email or this is a spam email.So there are certain parameters that your classifier will actually look for like The subject lineor the text or the HTML tags and also the IP address of the source from where is this mail gettingfrom so it will analyze all these variables and then it will classify them into this Pamor the genuine folder. So let's say for an example if your subject line States like mador cute or pretty and some other absurd keywords. Your classifier is smart enoughand it's trained in such a manner that it will Get to know. All right, this is a spam email and itwill automatically filter it out from your genuine emails. So that is how you classify it works basically,so that's pretty much about the classification now, let's move forward and see what always can be therethrough which you can actually perform classification. So we have three classifiersnamely decision tree random forest and a base, right so speaking briefly about Season 3 at firstso decision tree actually splits your entire data set in this structure of a treeand it makes decision at every node and hence called decision tree. So no big bang theory, right?So you have certain data set. There are certain nodes at each node. It will for the split into the child nodesand at each node. It will make a decision. So final decision will be in the form of positiveand negative, right? So let's say for an example you want to purchase a car, right?So what all will be the parameters? Let's say I have a go and I want to purchase a carand I will keep certain parameters in my mind. That would be what exactly is my income.What is my budget? What is the particular brand that I want to go for? What is the mileage of the car?What is the cylinder capacity of the car and so on and so forth, right? So I'll make my decision based on.All these parameters, right and that is how you make decisions and further.If you really want to know more about decision tree as to how it exactly works. You can also check out our decision tree tutorial as well.So let's begin now to the random Forest now. So Random Forest isn't in simple classifier.Actually now, let's understand what this war in symbol means.So in simple methods actually. Use multiple machine learning algorithms to obtainbetter predictive performance. So particularly talking about random Forest So Random forests usesmultiple decision trees for prediction, right? So you are in assembling a lot of decision trees to come upto your final outcome. As you can also look here in the image that your entire data set is actually for the splitinto three subsets, right and each subset for Leads to a particular decision tree.So here you have three decision trees and each decision tree will lead to certain outcome.Now what random Forest will do is it will compile the results from all the decision treesand then it will lead to a final outcome. Right? So it's compiled a section of all the multiple decision trees.That's all about the random Forest now, let's see what's lies there in a pace, right?So naive Bayes is very famous classifier, which is made on a very famous rule called Bayes theorem.You might have studied about Nee Bayes theorem in your 10 standard as well. So let's just see what Bayes theorem describes.So based on actually describes the probability of an event based on certain prior knowledge of conditionsthat might be related to the event, right? So for example, if cancer is related to age, right,so then person's age can be used to more accurately assess probability of having a cancerthan without having the knowledge of age. So if you know the age then it will become handy in addictingthe occurrence of cancer for a particular person. Right? So the outcome of first event here is actually affectingyour final outcome, isn't it? Yeah. So this is how naive Bayes classifier actually works.So that was all to give an overview of Nave Bayes classifier.And this were pretty much about the types of classifiers now,we'll try to find out the answer to this particular question as to why we need random Forest fine.So like human beings learn from the past experiences. So unlike human beings a computer does not haveexperiences then how does machine takes decisions? Where does it learn from?Um, well a computer system actually learns from the data which representssome past experiences of an application domain. So now let's see how random Forest helps in building up in learning modelwith a very simple use case of credit risk detection. Now needless to saythat credit card companies have a very nested interest in identifying Financial transactionsthat are illegitimate and criminal in nature. And also I would like to mention this pointthat according to the Federal Reserve payment study Americans used credit cards to payfor twenty six point two million purchases in 2012, and the estimated loss due to unauthorized transactionsthat here was us six point 1 billion dollars now in the banking industry measuring risk is very criticalbecause the stakes are too high. So the overall goal is actually to figure out Outwho all can be fraudulent before too much Financial damage has been done.So for this a credit card company receives thousands of applications for new cardsand each application contains information about an applicant, right?So so here as you can see that from all those applicationswhat we can actually figure out is that predictor variables. Like what is the marital status of the person?What is the gender of the person? The age of the person and the statuswhich is actually whether it is a default pair or a non-default pair. So default payments are basically when paymentsare not made in time and according to the agreement signed by the cardholder. So now that account is actually set to be in the default.So you can easily figure out the history of the particular card holder from this then we can also lookat the time of payment whether he has been a regular pair or not. Regular one, what is the source of incomefor that particular person? And so and so forth. So to minimize lossthe back actually needs certain decision rule to predict whether to approve a particular loan of that particular person or not.Now here is where the random Forest actually comes into the picture right now.Let's see how random Forest can actually help us in this particular scenario.Now, we have taken randomly two parameters. Out of all the predictive variablesthat we saw previously now, we have taken two predictor variables here.The first one is the income and the second one is the H right and similarly parallelit to decision trees have been implemented upon those predicted variables and let's first assume the caseof the income variable, right? So here we have divided our income into three categoriesthe first one being the person earning over 35,000. And dollars second from 15 to 35 thousand dollars the third one runningin the range of 0 to 15 thousand dollars. Now if a person is earning over $35,000,which is a pretty good income pretty decent. So now we'll check out for the credit history.Now the here the probability is that if a person is earning a good amount then there is very low riskthat he won't be able to pay back already earning good. So the It isthat his application of loan will get approved. Right? So there is actually low risk or moderate risk,but there's no real issue of high risk as such we can approve the applicants request here.Now, let's move on and watch out for the second category where the person is actually earning from 15 to 35 thousand dollars right now here the person mayor may not pay back. So in such scenarios will look for the credit. History as to what has been his previous history.Now if his previous history has been bad like he has been a default. ER in the previous transactions will definitely not considerapproving his request and he will be at the high risk in which is not good for the bank.If the previous history of that particular applicant is really good then wewill just to clarify our doubt will consider another pair. Dress. Well, that will be on depth.I have his already in really high depth then the risks again increases and there are chancesthat he might not pay repay in the future. So here will not accept the request of the personhaving high dipped if the person is in the low depth and he has been a good pair in his past history.Then there are chances that he might be back and we can consider approving the request of this particular applicant.And let's look at the third category, which is a person earning from 0 to 15 thousand dollars.Now, this is something which actually raises I broke and this person will actually liein the category of high risk. All right. So the probability isthat his application of loan would probably get rejected now, we'll get one final outcome from this income parameter, right?Now let us look at our second variable that is age which will lead into the second decision tree.Now. Let us say if the person is Young, right? So now we will look forward to if it is a student nowif it is a student then the chances are high that he won't be able to repay back because he has no learning Source, right?So here the risks are too high and probability is that his application of loan will get rejected fine.Now if the person is Young And he's not a student then we'll probably go on and look for another variable.That is pan balance. Now. Let's look if the bank balance is less than 5 lakhs. So again the risk arises and the probabilitiesthat his application of loan will get rejected. Now if the person is Young is not a studentand his bank balance of greater than 5 lakhs is got a pretty good and stable and balanced then the probabilitiesthat his zone of application will get approved. Of not let us take another scenarioif he's a senior, right? So if he is a senior will probably go and check out for this credit history.How well has he been in his previous transactions? What kind of a person he is likewhether he's a defaulter or is Ananda falter now if he is a very fair kind of personin his previous transactions then again the risk arises and the probability of his applicationgetting rejected actually increases right now. If he has been an excellent person asper his transactions in the previous history. So now again here there is least riskand the probabilities that his application of loan will get approved. So now here these two variables income and age have ledto two different decision trees. Right and these two different decision trees actually ledto two different results. Now what random forest does is it will actually compilethese two different results from these two different. Decision trees and then finally, it will lead to a final outcome.That is how random Forest actually works. Right? So that is actually the motive of the random Forest.Now let us move forward and see what is random Forest right?You can get an idea of the mechanism from the name itself random forests. So a collection of trees is a fortressthat's why I called for is probably and here also the trees are actually because being trained on subsetswhich are being selected at random. And therefore they are called random forests.So a random forests is a collection or an in symbol of decision.Eat straight head a decision trees actually built using the whole data set considering all features,but actually in random Forest only a fraction of the number of rows is selected and that too at random and a particular number of features,which are actually selected at random are trained upon and that is how the decision trees are built upon.Right? So similarly number of decision trees will be grown and each decision tree will result in two.With a certain final outcome and random Forest will do nothing, but actually just compiled the resultsof all those decision trees to bring up the final result. As you can see in this particular figurethat a particular instance actually has resulted into three different decision trees right sonar treeone results into a final outcome called Class A and tree to results into class B. Similarly tree three results into class PSo Random Forest will compile the results of all these decision trees. And it will go by the goal of the majority voting nowsince head to decision trees have actually voted into the favor of the Class B that is decision tree two,and three therefore the final outcome will be in the favor of the Class B. And that is how random Forest actually works upon.Now one really beautiful thing about this particular algorithm is that it is one of the versatile algorithmswhich is capable of Performing both regression as well as classification.Now, let's try to understand random Forest further with a very beautiful example or a this is my favorite one.So let's say you want to decide if you want to watch edge of tomorrow or not, right?So in this particular scenario, you will have two different actions to work Bond either.You can just straight away go to your best friend asked him about or read. Whether should I go for Edge of Tomorrow?And what will I like this movie or you can ask a bunch? Your friends and take their opinion considerationand then based on the final results. You can go out and watch Edge of Tomorrow, right?So now let's just take the first scenario. So where you go to your best friend asked aboutwhether you should go out to watch edge of tomorrow or not. So your friend will probably ask you certain questionslike the first one being here Jonah. So so let's say your friend asks youif you really like The Adventurous kind of movies or not. So you say yes, definitely I would love to watch it Venture kind of movie.So the probabilities that you will like edge of tomorrow as well. Since it's of Tomorrow is also a movie of Adventureand sci-fi kind of Jonah, right? So let's say you do not like the adventure John a movie.So then again the probability reduces that you might really not like edge of Morrow right.So from here you can come to a certain conclusion right? Let's say your best friend puts you into another situationwhere he'll ask you or a do you like Emily plant? And you see definitely I like Emily Bluntand then he puts another question to you. Do you like Emily Blunt to be in the main leadand you say yes, then again, the probability arises that you will definitely like edge of tomorrow aswell because Edge of Tomorrow is Has the Emily plant in the main lead cast soand if you say oh I do not like Emily Blunt then again, the probability reducesthat you would like Edge of Tomorrow to write. So this is one waywhere you have one decision tree and your final outcome. Your final decision will be based on your one decision tree,or you can see your final outcome will be based on just one friend. No, definitely not really convinced.You want to consider the options of your other friends also so that you can make very precise and crispdecision right you go out and you approach some other bunch of friends of yours.So now let's say you go to three of your friends and you ask them the same questionwhether I would like to watch Age of Tomorrow or not. So you go out and approach three or four friends friend one friend twin friend three.Now, you will consider each of their Sport and then you will your decision now will be dependenton the compiled results of all of your three friends, right? Now here, let's say you go to your first friendand you ask him whether you would like to watch it if tomorrow not and your first friend puts you to one question.Did you like Top Gun? And you say yes, definitely I did like the movie Top Gun and the probabilitiesthat you would like edge of tomorrow as well because topgun is actually a military action drama,which is also Tom Cruise. So now again the probability Rises that yes, you will like edge of tomorrow as well andIf you say no I didn't like Top Gun then again. The chances are that you wouldn't like Edge of Tomorrow, right?And then another question that he puts you across is that do you really like to watch action movies?And you say yes, I would love to watch them. Then again. The chances are that you would like to watch Edge of Tomorrow.So from your friend when you can come to one conclusion, I hear since the ratio of liking the movieto don't like is actually 2 is to 1 so the final result. Actually, you would like Edge of Tomorrow.Now you go to your second friend and you ask the same question. So now you are second friend asks you did you like farand away when we went out and did the last time when we washed it and you say no I really didn't like far and awaythen you would say then you are definitely going to like Edge of Tomorrow. Why does so because far and away is actuallysince most of whom might not be knowing it so far in a ways Johner of romance and it revolves around a girland a guy Guy falling in love with each other and so on. So the probability isthat you wouldn't like edge of tomorrow. So he ask you another question. Did you like Bolivianand to really like to watch Tom Cruise? And you say Yes, again. The probability isthat you would like to watch Edge of Tomorrow. Why because Oblivion again is a science fictioncasting Tom Cruise full of strange experiences. And where Tom Cruise is the savior of the masses.Kind well, that is the same kind of plot in edge of tomorrow as well.So here it is pure yes that you would like to watch edge of tomorrow. So you getanother second decision from your second friend. Now you go to your third friend and ask him soprobably our third friend is not really interesting in having any sort of conversation with you say just simply asks you did youlike Godzilla and you say no I didn't like Godzilla's we say definitely you wouldn't likeEdge of Tomorrow why so because Godzilla is also actually Fiction moviefrom the adventure Jonah. So now you have got three results from three different decision trees from three different friends.Now you compile the results of all those friends and then you make a final call that yes,would you like to watch edge of tomorrow or not? So this is some very real time and very interesting examplewhere you can actually Implement random Forest into ground reality.Now let us look at various domains where random Forest is actually used. So because of its diversity random Forest is actually usedin various diverse to means like so beat banking beat medicine beat land use beat marketing name itand random Forest is there so in banking particularly random Forest is being actually used to make it outwhether the applicant will be a default a pair or it will be Older oneso that it can accordingly approve or reject the applications of loan, right?So that is how random Forest is being used in banking talking about medicine.Random. Forest is widely used in medicine field to predict beforehand. What is the probabilityif a person will actually have a particular disease or not? Right? So it's actually used to look at the various disease Trends.Let's say you want to figure out what is the probability that a person will have diabetes or not?It and so what would you do? It'd probably look at the medical history of the patient and then you will see.All right. This has been the glucose concentration. What was the BMI? What was the insulin levelsin the patient in the past previous three months. What is the age of this particular person and do it'll make a different decision trees based on each oneof these predictor variables and then you'll finally compiled the results of all those variables and then you will make a final decision.As to whether the person will have diabetes in the near future or not.That is how random Forest will be used in medicine sector now move. Random Forest is also actually used to find out the land use.For example, I want to set up a particular industry in certain area. So what would I probably look for a look for?What is the vegetation over there? What is the Urban population over there? Right and how much is the distancefrom the nearest modes of Transport like from the bus station or the railway station and accordingly.I will split my parameters and I will make decision on each one of these parameters and finally I'll compile my decision of allthese parameters in that will be my final outcome. So that is how I am finally going to predictwhether I should put my industry at this particular location or not. Right?So these three examples have actually been of majorly Classification problembecause we are trying to classify whether or not with actually trying to answer this questionwhether or not right now, let's move forward and look how marketing is revolving around random Forest.So particularly in marketing we try to identify the customer churn.So this is particularly the regression kind of problem right now how let's see so customer churnis nothing but actually the number of people which are actually on the number. Of customers who are losing out.So we're going out of your market. Now you want to identify what will be your customer churn in near future.So you'll most of them e-commerce Industries are actually using this like Amazon Flipkart Etc.So they particularly look at your each Behavior as to what has been your past history.What has been your purchasing history. What do you like based on your activity around certain things around certain adsaround certain discounts? And I'm certain kind of materials right if you would like a particular top your activity will be morearound that particular top. So that is how they track each and every particular moveof yours and then they try to predict whether you will be moving out or not. So that is how they identify the customer churn.So these all are various domains where random Forest is used. And this is not the only list so there arenumerous other examples, which actually Lee are using random forests that makes it so special actually.Now, let's move forward and see how random Forest actually works. Right. So let us start with the random Forest algorithm first.Let's just see it step by step as to how random Forest algorithm works.So the first step is to actually select certain M features from T. Where m is less than T.So here T is the total number of the predictor variables that you have in your data set and out of those total predictor variables.You will select some random Lisa. Um few features out of those now why we are actually selectinga few features only. The reason is that if you will select all the predictive variablesor the total predictor variables then each of your decision tree will be sameso we model is not actually learning something new. It is learning the same previous thing because all those decision trees will be similar rightif you actually split your predicted variables and you select randomly a few predicted variables.Need let's say there are 14 total number of variables and out of those who randomly pick just three right?So every time you will get a new decision tree, so there will be a variety right?So the classification model will be actually much more intelligent than the previous one.Now. It has got very yet experiences. So definitely it will make different decisions each time.And then when you will compile all those different decisions, it will be a new more. Are accurate and efficient result, right?So the first important step is to select certain number of features out of all the features now,let's move on to the second step. Let's say for any node D. Now. The first step is to calculate the best plate at that point.So, you know that decision tree how decision trees actually implemented soyou pick up a the most significant variable right? And then you will split that particular node.For the child nodes, that is how the split takes place, right? So you will do it for M number of variablesthat you've selected. Let's say you have selected three so you will implement the split at all.Those three nodes in one particular decision tree, right the third step is split up the nodeinto two daughter nodes. So now you can split your root note into as many notesas you want to but here we'll split our node into 2.2 notes as to this or that so it will be an answer.In terms of this or that right at fourth step will be to repeat all these three stepsthat we've done previously and we'll repeat all this splitting until we have reached all the N number of nodes, right?So we need to repeat until we have reached till the leaf nodes of a decision tree that ishow we will do it right now after these four steps. We will have our one decision tree.But random Forest is actually about Decision trees. So here our fifth step will come into the picturewhich will actually repeat all these previous steps for D number of times now hit these the the numberof decision trees. Let's say I want to implement five decision trees. So my first step will be to implementall the previous steps 5 times. So the head the eye tration is 4/5 number of times right now.Once I have created these five decision trees still my task is not completed.Pleat yet. Now. My final task will be to compile the results of all these five different decision treesand I will make a call in the majority voting right here. As you can see in this picture.I had in different instances. Then I created indifferent decision trees.And finally, I will compile the result of all these n different decision trees and I will take my call on the majority voting right.So whatever my majority vote says It will be my final result. So this is basically an overview of the random Forest algorithmhow it actually works. Let's just have a look at this example to getmuch better understanding of what we have learnt. So let's say I have this data setwhich consists of four different instances, right? So basically it consists of the weather informationof previous 14 days right from D1 tildy 14, and this basically Outlook humidity and Win,this basically gives me the weather condition of those 14 days. And finally I have playwhich is my target variable weather match did take place on that particular day or not right.Now. My main goal is to find out whether the match will actually take placeif I have following these weather conditions with me on any particular day.Let's say the Outlook is rainy that day and humidity is high and the wind is very weak.So now I need to predict whether I will be able to play The match that they are not all right. So this is a problem statement fine.Now, let's see how random Forest is used in this to sort it out now here the first step is to actuallysplit my entire data set into subsets here. I have split my entire 14 variables into furthersmaller subsets right now these subsets may or may not overlap like there is certain overlapping between d 1till D3 and D3 till D6. Fine, so there is an overlapping of D3. So it might happenthat there might be overlapping so you need not really worry about the overlapping but you have to make surethat all those subsets are actually different right? So here I have taken three different subsetsmy first sub set consists of D1 till D3 Mexican subset consists of D3till D6 and methods subset consists of D7 tildy. Now now I will first be focusing on my first subset now here,let's say that particular day the out It was overcast fine. If yes, it was overcast then the probabilitiesthat the match will take place. So overcast is basically when your weather is too cloudy.So if that is the condition then definitely the match will take place and let's say it wasn't overcast.Then you will consider these second most probable option that will be the wind and we will make a decision based on this nowwhether wind was weak or strong if wind was weak, then you And play the match else you would not. So now the final outcome out of this decision tree will be PlayBecause here the ratio between the play and no play is to is to 1 so we get to a certain decision from a first decision tree.Now, let us look at the second subset now since second subset has different number of variables.So that is why this decision trees absolutely different from what we saw in our four subsets.So let's say if it was overcast then you will play the match. If it isn't the overcastand you would go and look out for humidity now further, it will get split into two whether it was high or normal.Now, we'll take the first case if the humidity was high and wind was week.Then you will play the match else if humidity was high but wind was too strong,then you would not go out and play the match right now. Let us look at the second dot to node of humidityif the humidity was Oil and the wind was weak then you will definitely go out and play the matchas you want go out and play the match. So here if you look at the final result,then the ratio of placed no play is 3 is to 2 then again. The final outcome is actually play, right?So from second subset, we get the final decision of play now, let us look at our third subsetwhich consists of D7 till D9 here if again the overcast is yes,then you will A match it's you will go and check out for humidity. And if the humidityis really high then you won't play the match and you will play the match again the probabilityof playing the matches. Yes, because the ratio of no play is Twist one, right?So three different subsets three different decision trees three different outcomesand one final outcome after compiling all the results from these three different decision trees are so I hopethis gives a better perspective a bit understanding of random Forest like how it really works.All right. So now let's just have a look at various features of random Forest Ray. So the first and the foremost feature isthat it is one of the most accurate learning algorithms, right? So why it is sobecause single decision trees are actually prone to having high varianceor Hive bias and on the contrary actually. Random Forest it averages the entire varianceacross the decision trees. So let's say if the variances say X4 decision tree,but for random Forest, let's say we have implemented n number of decision trees parallely.So my entire variance gets averaged to upon and my final variance actually becomes X upon n sothat is how the entire variance actually goes down as compared to other algorithms.Thumbs right now second most important feature is that it works? Well for both classification and regression problemsand by far I have come across this is one and the only algorithm which works equally well for both of them.Beh classification kind of problem or a regression kind of problem, right?Then it's really runs efficient on large databases. So basically it's really scalable.Even if you work for the lesser amount of database or if you work for really huge volume of data, right?So that's a very good part about it. Then the fourth most important point is that it requires almost no input preparation.Now, why am I saying this is because it has got certain implicit methods,which actually take care. And remove all the outliers and all the missing data and you really don't have to take careabout all that thing while you are in the stages of input preparations. So Random Forest is all here to take careof everything else and next. Is it performs implicit feature selection, right?So while we are implementing multiple decision trees, so it has got implicit methodwhich will automatically pick up some random features. Result of all your parameters and then it will goon and implementing different decision trees. So for example, if you just give one simple commandthat all right, I want to implement 500 decision trees no matter how so Random Forest will automatically take careand it will Implement all those 500 decision trees and those all 500 decision trees will be differentfrom each other and this is because it has got implicit methods which will automatically collect different parameters.Has itself out of all the variables that you have right, then it can be easily grown in parallel why it is sobecause we are actually implementing multiple decision trees and all those decision trees are runningor all those decisions trees are actually getting implemented parallely. So if you say I want thousand trees to be implemented.So all those thousand trees are getting implemented parallely. So that is how the computation time reduces.Right, and the last point is that it has got methods for balancing errorin unbalanced it as it's now what exactly unbalanced data sets are let me just give you an example of that.So let's say you're working on a data set fine and you create a random forest model and get90% accuracy immediately. Fantastic you think right. So now you start diving deep you go a little Little deeperand you discovered that ninety percent of that data actually belongs to just one class tan your entire data setyour entire decision is actually biased to just one particular class.So Random Forest actually takes care of this thing and it is really not biasedtowards any particular decision tree or any particular variable or any class. So it has got methods which looks after itand they Does all the balance of errors in your data sets? So that's pretty muchabout the features of random forests.K-nearest neighbor is a simple algorithm which uses entire data set in its training phase when our prediction is required for unseen data.What it does is it searches through the entire training data set for kaymu similar instances and the data with the most similar instance is finallyreturned as the prediction. So hello. Oh and welcome all to this YouTube session and in today's session will be dealing with KNN algorithm.So without doing any further, let's move on and discuss agenda for today's session. So we'll start our session with what is KNwhere I'll brief you about the topic and we'll move ahead to see what its popular use casesor how the industry is using KN for their benefit. Once we are done with it. We will drill down to the working of algorithmand while learning the algorithm you will also understand the significance of K, or what does this case standsfor in the nearest neighbor algorithm? Then we'll see how the prediction is made using Canon algorithm manually or mathematically.All right. Now once we are done with the theoretical concept will start the Practical or the demo session where we'll learnhow to implement KNN algorithm using python. So let's start our session. So starting with whatis KNN algorithm will k-nearest neighbor is a simple algorithm that stores all the available casesand classify the new data or case based on a similarity measure. It suggests that if you are similar to your neighbors,then you have one of them right for example, if apple looks more similar to banana orangeor Melon rather than a monkey rat or a cat that most likely Apple belong to the group of fruits.All right. Well in general Cayenne is used in Search application where you are looking for similar itemsthat is when your task is some form of fine items similar to this one. Then you call this search as a Cayenne in search.But what is this KN KN? Well this K denotes the number of nearest neighbor which are voting class of the new dataor the testing data. For example, if k equal 1 then the Sting data are given the same labelas a close this example in the training set similarly. If k equal 3 the labels are the three closes classes are checkedand the most common label is assigned to then testing data. So this is what a KN KN algorithm means so moving on ahead.Let's see some of the example of scenarios where KN is used in the industry. So, let's see the industrial applicationof KNN algorithm starting with recommender system. Well the biggest use case of cayenne and search is a recommender system.Thus recommended system is like an automated. Good form of a shop counter guy when you asked him for a product not only shows you the productbut also suggest you or displays your relevant set of products, which are related to the item. You're already interested in buying this KNN algorithmapplies to recommending products like an Amazon or for recommending media, like in case of Netflix or even for recommending advertisementto display to a user if I'm not wrong almost all of you must have used Amazon for shopping, right?So just to tell you more than 35% of amazon.com revenue is generated by its recommendation engine.So what's the strategy Amazon uses recommendation as a targeted marketing tool in both the email campaigns around mostof its website Pages Amazon will recommend many products from different categories based on what you have browserand it will pull those products in front of you which you are likely to buy like the frequently bought together optionthat comes at the bottom of the product page to tempt you into buying the combo. Well, this recommendation has just one main goalthat is increase average order value or to upsell and cross-sell customers by providing product suggestions.Eastern items in the shopping cart or based on the product. They're currently looking at on site.So next industrial application of KNN algorithm is concept search or searching semantically similar documentsand classifying documents containing similar topics. So as you know, the data on the Internet is increasing exponentiallyevery single second. There are billions and billions of documents on the internet each document on the internet contains multiple Concepts,that could be a potential concept. Now, this is a situation where the main problem is to Extract concept from a set of documentsas each page could have thousands of combination that could be potential Concepts an average document could havemillions of concept combined that the vast amount of data on the web. Well, we are talking about an enormous amountof data set and Sample. So what we need is we need to find a concept from the enormous amount of data set and samples, right?So for this purpose, we will be using KNN algorithm more advanced example could include handwriting detection like an OCRor image recognization or even video. Organization. All right. So now that you know various use casesof KNN algorithm. Let's proceed and see how does it work. So how does a KNN algorithm work?Let's start by plotting these blue and orange point on our graph. So these Blue Points the belong to class Aand the orange ones they belong to class B. Now you get a star as a new pony and your task is to predictwhether this new point it belongs to class A or it belongs to the class B. So to start the production the very first thingthat you have to do is select the Value of K. Just as I told you KN KN algorithm refers to the numberof nearest neighbors that you want to select. For example, in this case k equal to 3.So what does it mean it means that I am selecting three points which are the least distance to the new pointor you can say I am selecting three different points which are closest to the star. Well at this point of time you can askhow will you calculate the least distance? So once you calculate the distance, you will get one blueand two orange points which are closest to this star now. Since in this case as we have a majority of orange points,so you can say that for k equal 3D star belongs to class B, or you can say that the star is more similar to the orange pointsmoving on ahead. Well, what if k equal to 6 well for this case, you have to look for six different pointswhich are closest to this star. So in this case after calculating the distance, we find that we have four blue pointsand two Orange Point which are closest to the star now as you can see that the blue points are in majority,so you Can say that for k equals 6 this star belongs to class A or the star is more similar to Blue Points.So by now, I guess you know how a KNN algorithm work and what is the significance of gain KNN algorithm.So how will you choose the value of K? So keeping in mind this case the most important parameterin KNN algorithm. So, let's see when you build a k nearest neighbor classifier. How will you choose a value of K?Well, you might have a specific value of K in mind or you could divide up your data and use something like cross-validation technique to test several valuesof K in order. To determine which works best for your data, for example, if n equal 2,000 cases then in that case the optimal valueof K lies somewhere in between 1 to 19. But yes, unless you try it you cannot be sure of it.So, you know how the algorithm is working on a higher level. Let's move on and see how things are predicted using KNN algorithm.Remember I told you the KNN algorithm uses the least distance measure in order to find its nearest neighbors.So, let's see how these distance is calculated. Well, there are several distance measure which can be used.So to start with Will mainly focus on euclidean distance and Manhattan distance in this session.So what is this euclidean distance? Well, this euclidean distance is defined as the square rootof the sum of difference between a new point x and an existing Point why so for example here we have Point P1 and P2 PointT. 1 is 1 1 and point p 2 is 5 for so what is the euclidean distance between both of them?So you can see that euclidean distance is a direct distance between two points. So what is the distance between the point P1 and P2so we can calculate it as 5 minus 1 whole square plus 4 minus 1 whole squareand we can route it over which results to 5. So next is the Manhattan distance.Well, this Manhattan distance is used to calculate the distance between real Vector using this some of their absolute differencein this case the Manhattan distance between the point P1 and P2 is Mode of 5 minus 1plus mod value of 4 minus 1 which results to 3 plus 4 that is 7 so this slide shows the differencebetween euclidean and Manhattan distance from point A to point B. So euclidean distance is nothing but the director the least possible distance between A and B. Whereas the Manhattan distance is a distance between Aand B measured along the axis at right angle. Let's take an example and see how things are predicted using KNN algorithmor how the cannon algorithm is working. Suppose we have a data set which consists of height weightand T-shirt size of some customers. Now when a new customer come we only have his height and weight as the information now our task is to predict.What is the T-shirt size of that particular customer so for this will be using the KNN algorithm.So the very first thing what we need to do, we need to calculate the euclidean distance. So now that you have a new data of height 160 one centimeterand weight are 61 kg. So the very first thing that we'll do is we'll calculate the euclidean distance. Stance which is nothing but the square rootof 160 1 minus 158 whole square plus 61 minus 58 whole square and square root of that is 4.24.Let's drag and drop it. So these are the various euclidean distance of other points. Now, let's suppose k equal to 5 then the algorithmwhat it does is it searches for the five customer closest to the new customer that is most similar to the new data in termsof its attribute for k equal 5. Let's find the top five minimum euclidian distance. So these are the distancewhich we are going to use Two three four and five. So let's rank them in the order first.This is second. This is third then this one is for again. This one is 5 so there is our order.So for k equal 5 we have for t-shirts which commanders size M and one t-shirt which comes under size lso obviously best guess for the best protection for the T-shirt size of height 160 one centimeterand wait 60 1 kg is M. Or you can say that a new customer Fittin to size M. Well this was all about Body theoretical session,but before we drill down to the coding part, let me just tell you why people call KN as a lazy learner.Well Cannon for classification is a very simple algorithm, but that's not why they are called lazy KN is a lazy learnerbecause it doesn't have a discriminative function from the training data. But what it does it memorizes the training data,there is no learning phase of the model and all of the work happens at the time. Your prediction is requested.So as such there's the reason why KN is often referred to us lazy learning algorithm. So this was all about Or detail reticle session now,let's move on the coding part. So for the Practical implementation of the Hands-On part, I'll be using the IRS data set.So this data set consists of 150 observation. We have four features and one class label the four features includethe sepal length sepal width petal length and the petrol head whereas the class label decides which flower belongsto which category. So this was the description of the data set, which we are using now, let's move on and see what are the stepby step solution to perform a KNN algorithm. So first we'll start by handling the The data what we have to do we have to open the data setfrom the CSV format and split the data set into train and test part next we'll take the similaritywhere we have to calculate the distance between two data instances. Once we calculate the distance next.We'll look for the neighbor and select K Neighbors which are having the least distance from a new point. Now once we get our neighbor,then we'll generate a response from a set of data instances. So this will decide whether the new Point belongs to class A or Class B.Finally, we'll create the accuracy function and in the end. We'll tie it all together in the main function.So let's start with our code for implementing KNN algorithm using python. I'll be using jupyter notebook python 3.0 installed on it.Now, let's move on and see how can an algorithm can be implemented using python. So there's my jupyter notebook,which is a web-based interactive Computing notebook environment with python 3.0 installed on itso that the launched its launching so there's our jupyter notebook and we'll be riding our python codes on it.So the first thing that we need to do is load our file, our data is in CSV format without a header lineor any code we can open the file the open function and read the data line using the reader functionin the CSV module. So let's write a code to load our data file. Let's execute the Run button.So once you execute the Run button, you can see the entire training data set as the output next.We need to split the data into a training data set that KN can use to make prediction and a test data setthat we can use to evaluate the accuracy of The module so we first need to convert the flower measurethat were loaded as string into numbers that we can work. Next. We need to split the data set randomly to train and test ratioof 67 is 233 for test is to train as a standard ratio, which is used for this purpose.So let's define a function as load data set that loads a CSV with the provided file named and split it randomly into trainingand test data set using the provided split ratio. So this is our function load data set which is using filenamesthat ratio training data set and testing data set. As its input. All right. So let's execute the Run button and check for any errors.So it's executed with zero errors. Let's test this function. So there's our training set testing set load data set.So this is our function load data set on inside that we are passing. Our file is data with a split ratio of 0.66and training data set and test data set. Let's see what our training data set and test data set its dividing into so it's giving a countof training data set and testing data set. The total number of training data set as split into is 97 and total numberof Test data set we have is 53. So total number of training data set we have here is 97 and totalnumber of test data set we have here is 53. All right. Okay. So our function load data set is performing.Well, so let's move on to step two which is similarity. So in order to make prediction, we need to calculate the similarity betweenany two given data instances. This is needed so that we can locate the kamo similar data instancesin the training data set are in turn make a prediction given that all for flour measurement are numeric and have same unit.We can directly use the euclidean distance measure. This is nothing but the square root of the sum of squared differences between two erasof the number given that all the for flower measurements are numeric and have same unit. We can directly use the euclidean distance measurewhich is nothing but the square root of the sum of squared difference between two arrays or the number additionally we want to controlwhich field to include in the distance calculation. So specifically we only want to include first for attribute.So our approach will be to limit the euclidean distance to a fixed length. All right. So let's define our euclidean function.So these are euclidean distance function which takes instance one instance to and length as parameters instance one and instance two are the two pointsof which you want to calculate the euclidean distance, whereas this length and denote that how many attributes you want to include.Okay. So there's our euclidean function. Let's execute it. It's executing fine without any errors.Let's test the function suppose the data one or the first instance consists of the data point us to to to and it belongs to class A. Aand data to consist of four for four and it belongs to class P. So when we calculate the euclidean distanceof data one to data to and what we have to do we have to consider only first three features of them.All right. So let's print the distance as you can see here. The distance comes out to be three point four six four.All right. So this is nothing but the square root of 4 minus 2 whole Square. So this distance is nothing but the euclidean distanceand it is calculated as square root of 4 minus 2 whole square plus 4 minus 2 whole square that is nothing but 3 times or 4 minus 2 whole That is 12 +square root of 12 is nothing but 3.46 for all right. So now that we have calculated the distance now,we need to look for K nearest neighbors. Now that we have a similarity measure we can use it to collectthe kamo similar instances for a given unseen instance. Well, this is a straightforward process of calculating the distance for all the instancesand selecting a subset with the smallest distance value. And now what we have to do we have to select the smallest distance values.So for that will be defining a function as get neighbors. So for that what we will be doing will be defining a functionas get neighbors what it will do it will return the K most similar Neighbors From the training set for a given test instance.All right. So this is how our get nabal function look like it takes training data set and test instance and K as its input here.The K is nothing but the number of nearest neighbor you want to check for. All right. So basically what you'll be gettingfrom this get Mabel's function is K different points having least euclidean distance from the test instance. All right, let's execute it.So the function executed without any errors. So let's test our function. Suppose the training data set includes the data like 2 to 2and it belongs to class A and other data includes four four four and it belongs to class P and our testing instances five five five or now.We have to predict whether this test instance belongs to class A or it belongs to class be. All right for k equal 1 we have to predictits nearest neighbor and predict whether this test instance it will belong to class A or will it belong to class be?All right. So let's execute the Run button aligned. So an executing the Run button you can see that we have output is 4 4 4and B. Be a new instance 5 5 5 is closest to point 4 4 4 which belongs to class be?All right. Now once you have located the most similar neighbor for a test instance next task is to predict a response basedon those neighbors. So how we can do that. Well, we can do this by allowing each neighbor to vote for the class attributeand take the majority vote as a prediction. Let's see how we can do that. So we have a function as getresponse with takes neighbors as the input.Well, this neighbor was nothing but the output of this get me / function. The output of get neighbor function will be fedto get response. All right, let's execute the Run button. It's executed. Let's move ahead and test our function get response.So we have a neighbor as one one one. It belongs to class A to to to it belongs to class a33. It belongs to class B.So this response what it will do it will store the value of get response by passing this neighbor value.All right. So what we want to check is we want to predict whether that test instance five five five.It belongs to class A or Class B. Be when the neighbors are 1 1 1 a 2 2 A + 3 3 p.So let's check our response now that we have created all the different function which are required for a KNN algorithm.So important main concern is how do you evaluate the accuracy of the prediction and easy way to evaluate the accuracy of the modelis to calculate a ratio of the total correct prediction to all the prediction made. So for this I will be defining functionas get accuracy and inside that I'll be passing my test data set and the predictions get accuracy function.Check it. Executed without any error. Let's check it for a sample data set.So we have our test data set as 1 1 1 which belongs to class A 2/2 which again belongs to class 3 3 3 which belongs to class Band my predictions is for first test data. It predicted latter belongs to class A which is truefor next it predicted that belongs to class E, which is again to and for the next again it predictive that it belongs to class A which is false in this casecause the test data belongs to class be. All right. So in total we have to correct prediction out of three.All right. Right. So the ratio will be 2 by 3, which is nothing but 66.6. So our accuracy rate is 66.6.So now that you have created all the function that are required for KNN algorithm. Let's compile them into one single main function.Alright, so this is our main function and we are using Iris data set with a split of 0.67 and the value of K is 3 Let's see.What is the accuracy score of this check how accurate are modulus so in training data set,we have 113 values and then the test data set we have Seven values. These are the predicted and the actual valuesof the output. Okay. So in total, we got an accuracy of ninety seven point two nine percent, which is really very good.Alright, so I hope the concept of this KNN algorithm is here devised in a world full of machine learningand artificial intelligence surrounding almost everything around us classification and prediction is oneof the most important aspects of machine learning. So before moving forward, let's have a Look at the agenda.I'll start of this video by explaining you guys. What exactly is Nave biased then we'll understandwhat is space theorem which serves as a logic behind the name pass algorithm moving forward.I'll explain the steps involved in the neighb as algorithm one by one and finally, I'll finish off this video with a demo on the Nave bassusing the sklearn package noun a bass is a simple but surprisingly powerful algorithmfrom predictive analysis. It is a classification technique based on base. him with an assumptionof Independence among predictors it comprises of two parts, which is knaveand bias in simple terms neighbors classifier assumes that the presence of a particular featurein a class is unrelated to the presence of any other feature, even if this features depend on each otheror upon the existence of the other features, all of these properties independently contributeto the probability whether a fruit is an apple or an orange or a banana, so That is why itis known as naive now naive based model is easy to build and particularly useful for very large data setsin probability Theory and statistics based theorem, which is already known as the base lawor the base rule describes the probability of an event based on prior knowledge of the conditionsthat might be related to the event now pasted here m is a way to figure out conditional probability.The conditional probability is the probability of an event happening given that it has some relationship.One or more other events, for example, your probability of getting a parking space is connectedto the time of the day. You park where you park and what conventions are you going on at that timeBayes theorem is slightly more nuanced in a nutshell. It gives you an actual probability of an event giveninformation about the tests. Now, if you look at the definition of Bayes theorem, we can see that given a hypothesis Hand the evidence e-base term states that the relationship between the E of the hypothesis before getting the evidence,which is the P of H and the probability of the hypothesis after getting the evidence that is p of H given e is defined as probabilityof e given H into probability of H divided by probability of e it's rather confusing, right?So let's take an example to understand this theorem. So suppose I have a deck of cards andif a single card is drawn from the deck of playing cards, the probability that the card is a king is for by 52since there are four Kings in a standard deck of 52 cards. Now if King is an event, this card is a king.The probability of King is given as 4 by 52 that is equal to 1 by 13.Now if the evidence is provided for instance someone looks Such as the card that the single card is a face card the probabilityof King given that it's a face can be calculated using the base theorem by this formula.Now since every King is also a face card the probability of face given that it's a king is equal to 1and since there are three phase cards in each suit. That is the chat king and queen.The probability of the face card is equal to 12 by 52. That is 3 by 30.No using base certain we can find out the probability of King given that it's a face.So our final answer comes to 1 by 3, which is also true. So if you have a deck of cards,which has having only faces now, there are three types of phases which are the chat king and queen.So the probability that it's the king is 1 by 3. Now. This is the simple example of how based on works nowif we look at the proof as in how this paste Serum evolved. So here we have probability of a given Band probability of B given a now for a joint probability distribution over the sets A and B,the probability of a intersection B, the conditional probability of a given B is defined as the probabilityof a intersection B divided by probability of B, and similarly probability of B, given a is defined as probability of B intersectiona divided by probability of a now we Equate probability of a intersection pand probability of B intersection a as both are the same thing now from this methodas you can see, we get our final base theorem proof, which is the probability of a given b equals probability of B,given a into probability of P divided by the probability of a now while this is the equationthat applies to any probability distribution over the events A and B. It has a particular nice interpretation in casewhere a is represented as the hypothesis h H and B is representedas some observed evidence e in that case the formula is pof H given e is equal to P of e given H into probability of H divided by probability of e now this relatesthe probability of hypothesis before cutting the evidence, which is p of H to the probability of the hypothesis after getting the evidencewhich is p of H given e for this reason P of H is known as the prior probabilitywhile P of It's given e is known as the posterior probability and the factor that relates the two is known as the likelihood ratio Now usingthis term space theorem can be rephrased as the procedure probability equals. The prior probability times the likelihood ratio.So now that we know the maths which is involved behind the Bayes theorem. Let's see how we can implement this in real life scenario.So suppose we have a data set. Set in which we have the Outlook the humidity and we need to find outwhether we should play or not on that day. So the Outlook can be sunny overcast rainand the humidity are high normal and the wind are categorized into two phases which are the weak and the strong winds.The first of all will create a frequency table using each attribute of the data set.So the frequency table for the Outlook looks like this we have Sunny overcast and rainy the frequency tableof humidity looks like this. And a frequency table of when looks like this we have strongand weak for wind and high and normal ranges for humidity. So for each frequency table,we will generate a likelihood table now now the likelihood table contains the probabilityof a particular day suppose we take the sunny and we take the play as yesand no so the probability of Sunny given that we play yes is 3 by 10, which is 0.3 the probability of X,which is the probability of Sunny He is equal to 5 by 14. Now. These are all the termswhich are just generated from the data which we have here. And finally the probability of yes is 10 out of 14.So if we have a look at the likelihood of yes given that it's a sunny we can see using Bayes theorem.It's the probability of Sunny given yes into probability of yes divided by the probability of Sunny.So we have all the values here calculated. So if you put that in our base serum equation,we get the likelihood of Is a 0.59 similarly the likelihoodof no can also be calculated here is 0.40 now similarly. We are going to create the likelihood tablefor both the humidity and the win there's a for humidity the likelihood for yes given the humidityis high is equal to 0.4 to and the probability of playing knowgiven the Venice High is 0.58 the similarly for table wind.The probability of e is given that the wind is week is 0.75 and the probability of no giventhat the win is week is 0.25 now suppose we have of daywhich has high rain which has high humidity and the wind is weak. So should we play or not?That's all for that. We use the base theorem here again the likelihood of yes on that day is equalto the probability of Outlook rain given that it's a yes into probability. Of humidity given that say yes,and the probability of when that is we given that it's we are playing yes into the probability of yes,which equals to zero point zero one nine and similarly the likelihood of know on that day is equalto zero point zero one six. Now if we look at the probability of yes for that day of playing we just need to divide itwith the likelihood some of both the yes and no so the probability of playing tomorrow,which is yes is .5. Whereas the probability of not playing is equal to 0.45.Now. This is based upon the data which we already have with us.So now that you have an idea of what exactly is named by as how it works and we have seenhow it can be implemented on a particular data set. Let's see where it is used in the industry.The started with our first industrial use case, which is news categorized. It's move on to them or we can usethe term text classification to broaden the spectrum of this algorithm news in the web are rapidly growing in the era of Information Agewhere each new site has its own different layout and categorization for grouping news.Now these heterogeneity of layout and categorization cannot always satisfy individual users need to remove these heterogeneityand classifying the news articles. Owing to the user preference is a formidable task companiesuse web crawler to extract useful text from HTML Pages the news articlesand each of these news articles is then tokenized now these tokens are nothing but the categories of the news nowin order to achieve better classification result. We remove the less significant Words, which are the stop was from the documentsor the Articles and then we apply the Nave base classifier for classifying the news contents based on the news.Now this is by far one of the best examples of Neighbors classifier,which is Spam filtering. Now. It's the Nave Bayes classifier are a popular statistical technique for email filtering.They typically use bag-of-words features to identify at the spam emailand approach commonly used in text classification as well. Now it works by correlating the use of tokens,but the spam and non-spam emails and then the Bayes theorem, which I explainedearlier is used to calculate the probability that an email is or not a Spam so named by a Spam filtering isa baseline technique for dealing with Spam that container itself to the emails need of an individual userand give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filteringwith its roots in the 1990s particular words have particular probabilities of occurring in spam.And in legitimate email as well for instance most emails users will frequently encounter the world lotteryor the lucky draw a spam email, but we'll sell them see it in other emails. The filter doesn't know these probabilities in advanceand must be friends. So it can build them up to train the filter. The user must manually indicatewhether a new email is Spam or not for all the words in each straining email. The filter will adjust the probabilitythat each word will appear in a Spam or legitimate. All in the database now after training the word probabilities also knownas the likelihood functions are used to compute the probability that an email with a particular set of words as in belongsto either category each word in the email contributes the email spam probability. This contribution is called the posterior probabilityand is computed again using the base 0 then the email spam probability is computed over all the verse in the emailand if the total exceeds a certain threshold say Or 95% the filter will Mark the email as spam.Now object detection is the process of finding instances of real-world objects such as faces bicyclesand buildings in images or video now object detection algorithm typically use extracted featuresand learning algorithm to recognize instance of an object category here again,a bias plays an important role of categorization and classification of object now medical area.This is increasingly voluminous amount of electronic data, which are becoming more and more complicated.The produced medical data has certain characteristics that make the analysis very challenging and attractiveas well among all the different approaches. The knave bias is used. It is the most effective and efficient classificationalgorithm and has been successfully applied to many medical problems empirical comparisonof knave bias versus five popular classifiers on Medical data sets showsthat may bias is well suited for medical application and has high performance in most of the examine medical problems.Now in the past various statistical methods have been used for modeling in the area of disease diagnosis.These methods require prior assumptions and are less capable of dealing with massive and complicated nonlinear and dependent data oneof the main advantages of neighbor as approach which is appealing to Physicians is that all the available information is used?To explain the decision this explanation seems to be natural for medical diagnosis and prognosis.That is it is very close to the way how physician diagnosed patients now weather is oneof the most influential factor in our daily life to an extent that it may affect the economy of a countrythat depends on occupation like agriculture. Therefore as a countermeasure to reduce the damagecaused by uncertainty in whether Behavior, there should be an efficient way to print the weather nowwhether projecting has Challenging problem in the meteorological department since ears even after the technology skilland scientific advancement the accuracy and production of weather has never been sufficient evenin current day this domain remains as a research topic in which scientists and mathematicians are working to produce a modelor an algorithm that will accurately predict the weather now a bias in approach based model is created bywhere procedure probabilities are used to calculate the likelihood of each class label for input.Data instance and the one with the maximum likelihood is considered as the resulting output now earlier.We saw a small implementation of this algorithm as well where we predicted whether we should play or not based on the data,which we have collected earlier. Now, this is a python Library which is known as scikit-learn it helps to build in a biasand model in Python. Now, there are three types of named by ass model under scikit-learn Library.The first one is the caution. It is used in classification and it Assumes that the feature follow a normal distribution.The next we have is multinomial. It is used for discrete counts. For example, let's say we have a text classification problemand here we consider bernouli trials, which is one step further and instead of word occurring in the document.We have count how often word occurs in the document you can think of it as a number of times outcomes number is observedin the given number of Trials. And finally we have the bernouli type.Of Naples, the binomial model is useful if your feature vectors are binary bag of words modelwhere the once and the zeros are words occur in the document and the verse which do not occurin the document respectively based on their data set. You can choose any of the given discussed model here,which is the gaussian the multinomial or the bernouli.So let's understand how this algorithm works. And what are the different steps? One can take to create a bison model and use knave biasto predict the output so here to understand better. We are going to predict the onset of diabetes Nowthis problem comprises of 768 observations of medical detailsfor Pima Indian patients. The record describes instantaneous measurement takenfrom the patient such as the age the number of times pregnant and the blood work groupnow all the patients are women aged 21 and Old and all the attributes are numeric and the unit's varyfrom attribute to attribute. Each record has a class value that indicate whether the patient suffered on onset of diabeteswithin five years or the measurements. Now, these are classified as zero.Now, I've broken the whole process down into the following steps. The first step is handling the data in which we loadthe data from the CSV file and split it into training and test data sets. The second step is summarizing the data.In which we summarize the properties in the training data sets so that we can calculate the probabilities and make predictions.Now the third step comes is making a particular prediction. We use the summaries of the data set to generate a single prediction.And after that we generate predictions given a test data set and a summarize training data sets.And finally we evaluate the accuracy of the predictions made for a test data set as the percentage correct out of all the predictions madeand finally We tied together and form. Our own model of nape is classifier.Now. The first thing we need to do is load our data the data is in the CSV format without a header lineor any codes. We can open the file with the open function and read the data lines using the read functionsin the CSV module. Now, we also need to convert the attributes that were loaded as strings into numbersso that we can work with them. So let me show you how this can be implemented now for that you need to Tall pythonon a system and use the jupyter notebook or the python shell.Hey, I'm using the Anaconda Navigator which has all the things required to do the programming in Python.We have the Jupiter lab. We have the notebook. We have the QT console. Even we have a studio as well.So what you need to do is just install the Anaconda Navigator it comes with the pre installed python also,so the moment you click launch on The jupyter Notebook. It will take you to the Jupiter homepagein a local system and here you can do programming in Python. So let me just rename it as by my India diabetes.So first, we need to load the data set. So I'm creating here a function load CSV now before that.We need to import certain CSV the math and the random method. So as you can see,I've created a load CSV function which will take the pie my Indian diabetes data dot CSV file using the CSV dot reader methodand then we are converting every element of that data set into float originally all the Ants are in string,but we need to convert them into floor for our calculation purposes. Now next we need to split the data into training data setsthat nay bias can use to make the prediction and this data set that we can use to evaluate the accuracy of the model.We need to split the data set randomly into training and testing data set in the ratio of usuallywhich is 70 to 30, but for this example, I am going to use 67 and 33 now 70 and 30 is a Ratio for testing algorithmsso you can play around with this number. So this is our split data set function.Now the Navy base model is comprised of summary of the data in the training data set. Now this summary is then used while making predictions.Now the summary of the training data collected involves the mean the standard deviation of each attribute by class value now, for example,if there are two class values and seven numerical attributes, then we need a meanand the standard deviation for each of these seven attributes and the class value which makes The 14 attribute summariesso we can break the preparation of this summary down into the following sub tasks which are the separating data by class calculating meancalculating standard deviation summarizing the data sets and summarizing attributes by class.So the first task is to separate the training data set instances by class value so that we can calculate statistics for each class.We can do that by creating a map of each class value to a list of instances that belong to the class.Class and sort the entire dataset of instances into the appropriate list. Now the separate by class function just the same.So as you can see the function assumes that the last attribute is the class value the function returns a map of class value to the listof data instances next. We need to calculate the mean of each attribute for a class value.Now, the mean is the central middle or the central tendency of the data and we use it as a middleof our gaussian distribution when Calculating the probabilities. So this is our function for mean now.We also need to calculate the standard deviation of each attribute for a class value. The standard deviation is calculated as a square rootof the variance and the variance is calculated as the average of the squared differencesfor each attribute value from the mean now one thing to note that here is that we are using n minus one methodwhich subtracts one from the number of attributes values when calculating the variance.The now that we have the tools to summarize the data for a given list of instances, we can calculate the mean and standard deviationfor each attribute. Now that's if function groups the values for each attribute across our data instances into their own listsso that we can compute the mean and standard deviation values for each attribute. The next comes the summarizing attributes by class.We can pull it all together by first separating our training data sets into instances growth by class then calculating the summariesfor each a To be with now. We are ready to make predictions using the summaries prepared from our training data making predictions involvescalculating the probability that a given data instance belong to each class then selecting the classwith the largest probability as a prediction. Now we can divide this whole method into four taskswhich are the calculating gaussian probability density function calculating class probability making a predictionand then estimating the accuracy now to calculate the gaussian probability density function.We use the gaussian function to estimate the probability of a given attribute value given the node meanand the standard deviation of the attribute estimated from the training data. As you can see the parameters are xmean and the standard deviation now in the calculate probability function, we calculate the exponent first then calculate the main divisionthis lets us fit the equation nicely into two lines. Now, the next taskis calculating the class properties now that we had can calculate the probability of an attributebelonging to a class. We can combine the probabilities of all the attributes values for a data instanceand come up with a probability of the entire. Our data instance belonging to the class. So now that we have calculated the class properties.It's time to finally make our first prediction now, we can calculate the probability of the data instance belongto each class value and we can look for the largest probability and return the associated classand for that we are going to use this function to predict which uses the summaries and the input Vector which is basically all the probabilitieswhich are being input for a particular label now finally we can An estimate the accuracyof the model by making predictions for each data instances in our test data for that.We use the cat predictions method. Now this method is used to calculate the predictions based upon the test data setsand the summary of the training data set. Now, the predictions can be compared to the class values in our test data setand classification accuracy can be calculated as an accuracy ratio between the zeros and the hundred percent.Now the get accuracy method will calculate this accuracy ratio. Now finally to sum it all up.We Define our main function we call all these methods which we have defined earlier one by one to getthe Courtesy of the model which we have created. So as you can see, this is our main function in which we have the file name.We have defined the split ratio. We have the data set. We have the training and test data set.We are using the split data set method next. We are using the summarized by class function usingthe get prediction and the get accuracy method as well. So guys as you can see the output of this one gives usthat we are splitting the seven sixty eight rows into 514 which is the training and 254which is the test data set rows and the accuracy of this model is 68% Now we can play with the amount of trainingand test data sets which are to be used so we can change the split ratio to seventies.238 is 220 to get different sort of accuracy. So suppose I change the split ratio from 0.67 20.8.So as you can see, we get the accuracy of 62 percent. So splitting it into 0.67 gave us a better resultwhich was 68 percent. So this is how you can Implement Navy bias caution classifier.These are the step by step methods which you need to do in case of using the Nave Bayes classifier,but don't worry. We do not need to write all this many lines of code to make a model this with The Sacketts.And I really comes into picture the scikit-learn library has a predefined methodor as say a predefined function of neighbor bias, which converts all of these lines,of course into merely just two or three lines of codes. So, let me just open another jupyter notebook.So let me name it as sklearn a pass. Now here we are going to use the most famous data setwhich is the iris dataset. Now, the iris flower data set is a multivariatedata set introduced by the British statistician and biologists Roland Fisher and based on this fish is linear discriminant model this data setbecame a typical test case for many statistical classification techniques in machine learning.So here we are going to use the caution NB model, which is already available in the sklearn.As I mentioned earlier, there were three types of Neighbors which are the question multinomial and the bernouli.So here we are going to use the caution and be model which is already present in the sklearn library,which is the cycle learn Library. So first of all, what we need to do is import the sklearn data setsand the metrics and we also need to import the caution and be Nowonce all these libraries are lowered we need to load the data set which is the iris dataset.The next what we need to do is fit a Nave by a small to this data set. So as you can see we have so easily defined the modelwhich is the gaussian NB which contains all the programming which I just showed you earlier all the methodswhich are taking the input calculating the mean the standard deviation separating it bike lastand finally making predictions. Calculating the prediction accuracy. All of this comes under the caution and be methodwhich is inside already present in the sklearn library. We just need to fit it according to the data setwhich we have so next if we print the model we see which is the gaussian NB model.The next what we need to do is make the predictions. So the expected output is data set dot Targetand the projected is using the pretend model and the model we are using is the cause in NB here.Here now to summarize the model which created we calculate the confusion Matrixand the classification report. So guys, as you can see the classification to providewe have the Precision of Point Ninety Six, we have the recall of 0.96.We have the F1 score and the support and finally if we print our confusion Matrix,as you can see it gives us this output. So as you can see using the gaussian and we method just putting it in the modeland using any of the data. Fitting the model which you created into a particular data setand getting the desired output is so easy with the scikit-learn library.So guys, this is it. I hope you understood a lot about the nape Bayes classifier how it is usedwhere it is used and what are the different steps involved in the classification techniqueand how the scikit-learn makes all of those techniques very easy to implement in any data set which we have.As we M or support Vector machine is one of the most effective machine learning classifierand it has been used in various Fields such as face recognition cancer classification and so on today's sessionis dedicated to how svm works the various features of svm and how it is used in the real world.So without any further due let's take a look at the agenda for today. We're going to begin the sessionwith an introduction to machine learning and the different types of machine learning. Next we'll discusswhat exactly support Vector machines are and then we'll move on and see how svm worksand how it can be used to classify linearly separable data will also briefly discuss abouthow nonlinear svm's work and then we'll move on and look at the use case of svm in colon cancer classificationand finally we'll end the session by running a demo where we'll use svm to predict whether a patient is suffering from a heart disease or not.Okay, so that was the agenda. Let's get stood with our first topic. So what is machine learning machine learning is a scienceof getting computers to act by feeding them data and letting them learn a few tricks on their own.Okay, we're not going to explicitly program the machine instead. We're going to feed it data and let it learnthe key to machine learning is the data machines learn just like us humans. We humans need to collect informationand data to learn similarly machines must also be fed data in order to learn and make decisions.Let's say that you want a machine to predict the value of a stock. All right in such situations.You just feed the machine with relevant data after which you develop a model which is used to predict the value of the stock.NOW one thing to keep in mind is the more data you feed the machine the better it will learn and make more accurate predictions obviously machinelearning is not so simple in order for a machine to analyze and get useful insights from data.It must process and study the data by running different. Algorithms on it. All right. And today we'll be discussing about one of the most widelyused algorithm called the support Vector machine. Okay. Now that you have a brief idea about what machine learning is,let's look at the different ways in which machines Lon first. We have supervised learning in this typeof learning the machine learns under guidance. All right, that's why it's called supervised learningnow at school. Our teachers guided us and taught us similarly in supervised learning machineslearn by feeding them labeled data. Explicitly telling them. Hey, this is the input and this ishow the output must look. Okay. So guys the teacher in this case is the training data.Next we have unsupervised learning here. The data is not labeled and there is no guide of any sort.Okay, the machine must figure out the data set given and must find hidden patterns in order to make predictionsabout the output an example of unsupervised learning is an adult's like you and me.We don't need a guide to help us with our daily activities. They figured things out on our own without any supervision.All right, that's exactly how I'm supervised learning work. Finally. We have reinforcement learning.Let's say you were dropped off at an isolated island. What would you do now initially you would panicand you'll be unsure of what to do where to get food from How To Live and all of that but after a while you will have to adapt you must learnhow to live in the island adapt to the changing climate learn what to eat and what not to eat.You're basically following the hit and trial. Because you're new to the surrounding and the only way to learn is experience and then learnfrom your experience. This is exactly what reinforcement learning is. It is a learning method wherein an agent interactswith its environment by producing actions and discovers errors or words. Alright, and once it gets trained it gets ready to predictthe new data presented to it. Now in our case the agent was you basically stuckon the island and the environment was the island. All right? Okay now now let's move on and seewhat svm algorithm is all about. So guys svm or support Vector machine is a supervised learning algorithm,which is mainly used to classify data into different classes now unlike most algorithms svm makes use of a hyperplanewhich acts like a decision boundary between the various classes in general svm can be used to generatemultiple separating hyperplanes so that the data is divided into segments.Okay and each These segments will contain only one kind of data. It's mainly used for classification purpose wearing you want to classifyor data into two different segments depending on the features of the data. Now before moving any further,let's discuss a few features of svm. Like I mentioned earlier svm is a supervised learning algorithm.This means that svm trains on a set of labeled data svm studies the label training dataand then classifies any new input data depending on what it learned in the training.In Phase a main advantage of support Vector machine is that it can be used for both classificationand regression problems. All right. Now even though svm is mainly known for classification the svrwhich is the support Vector regressor is used for regression problems. All right, so svm can be used both for classification.And for regression. Now, this is one of the reasons why a lot of people prefer svm because it's a very good classifier and along with that.It is also used for regression. Another feature is the svm kernel functions svm can be usedfor classifying nonlinear data by using the kernel trick the kernel trick basically meansto transform your data into another dimension so that you can easily draw a hyperplanebetween the different classes of the data. Alright, nonlinear data is basically datawhich cannot be separated with a straight line. Alright, so svm can even be used on nonlinear data sets.You just have to use a kernel functions to do this. All right, so Guys, I hope you all are clear with the basic concepts of svm.Now. Let's move on and look at how svm works so guys an order to understand how svm Works let's consider a small scenario nowfor a second pretend that you own a firm. Okay, and let's say that you have a problem and you want to set up a fence to protect your rabbitsfrom the pack of wolves. Okay, but where do you build your fence one way to get around?The problem is to build a classifier based on the position of the rabbits and words in your Faster.So what I'm telling you is you can classify the group of rabbits as one group and draw a decision boundary between the rabbitsand the world. All right. So if I do that and if I try to draw a decision boundarybetween the rabbits and the Wolves, it looks something like this. Okay. Now you can clearly build a fence along this linein simple terms. This is exactly how SPM work it draws a decision boundary,which is a hyperplane between any two classes in order to separate them or class. Asif I them now,I know you're thinking how do you know where to draw a hyperplane the basic principle behind svm is to draw a hyperplanethat best separates the two classes in our case the two glasses of the rabbits and the Wolves.So you start off by drawing a random hyperplane and then you check the distance between the hyperplaneand the closest data points from each glove these closes on your is data points to the hyperplane are known as support vectors and that'swhere the name comes from support. Active machine. So basically the hyperplane is drawnbased on these support vectors. So guys an Optimum hyperplane will have a maximum distance from each of these support vectors.All right. So basically the hyper plane which has the maximum distance from the support vectors is the most optimal hyperplaneand this distance between the hyperplane and the support vectors is known as the margin.All right. So to sum it up svm is used to classify data by using a hyper plane suchthat the distance distance between the hyperplane and the support vectors is maximum. So basically your margin has to be maximum.All right, that way, you know that you're actually separating your classes or add because the distance between the two classes is maximum.Okay. Now, let's try to solve a problem. Okay. So let's say that I input a new data point.Okay. This is a new data point and now I want to draw a hyper plane such that it best separates the two classes.Okay, so I start off by drawing a hyperplane like this and then I check the distance between Hyper planeand the support vectors. Okay, so I'm trying to check if the margin is maximum for this hyperplane,but what if I draw a hyper plane which is like this? All right. Now I'm going to check the support vectors over here.Then I'm going to check the distance from the support vectors and with this hyperplane, it's clear that the margin is more rightwhen you compare the margin of the previous one to this hyperplane. It is more. So the reason why I'm choosing this hyperplane isbecause the distance between the support vectors and the hi Hyperplane is maximum in this scenario.Okay, so guys this is how you choose a hyperplane. You basically have to make sure that the hyper plane has a maximum.Margin. All right, it has two best separate the two classes. All right. Okay so far it was quite easy.Our data was linearly separable which means that you could draw a straight line to separate the two classes.All right, but what will you do? If the data set is like this you possibly can't draw a hyper plane like this.All right. It doesn't separate the two. At all, so what do you do in such situations now earlier in the session I mentionedhow a kernel can be used to transform data into another dimension that has a clear dividing margin between the classes of data.Alright, so kernel functions offer the user this option of transforming nonlinear spaces into linear ones.Nonlinear data set is the one that you can't separate using a straight line. All right, in order to deal with such data sets you're goingto Ants form them into linear data sets and then use svm on them. Okay. So simple trick would be to transform the two variablesX and Y into a new feature space involving a new variable called Z. All right, so guys so far we were plotting our dataon two dimensional space. Correct? We will only using the X and the y axis so we had only those two variables X and Y nowin order to deal with this kind of data a simple trick would be to transform the two variables Xand I into a new feature space involving a new variable called Z. Ok, so we're basically visualizing the dataon a three-dimensional space. Now when you transform the 2D space into a 3D space,you can clearly see a dividing margin between the two classes of data right now. You can go ahead and separate the two classesby drawing the best hyperplane between them. Okay, that's exactly what we discussed in the previous slides.So guys, why don't you try this yourself dry drawing a hyperplane, which is the most Optimum. For these two classes.All right, so guys, I hope you have a good understanding about nonlinear svm's now. Let's look at a real world use case of support Vector machines.So guys s VM as a classifier has been used in cancer classificationsince the early 2000s. So there was an experiment held by a group of professionalswho applied svm in a colon cancer tissue classification. So the data set consistedof about 2,000 transmembrane protein samples and Only about 50 to 200 genes samples were inputInto the svm classifier Now this sample which was input into the svm classifier had both colon cancer tissue samplesand normal colon tissue samples right now. The main objective of this study was to classify Gene samplesbased on whether they are cancerous or not. Okay, so svm was trained using the 50 to 200 samplesin order to discriminate between non-tumor from tumor specimens. So the performance of The svm classifierwas very accurate for even a small data set. All right, we had only 50 to 200 samples.And even for the small data set svm was pretty accurate with its results. Not only that its performance was comparedto other classification algorithm like naive Bayes and in each case svm outperform naive Bayes.So after this experiment it was clear that svm classify the data more effectivelyand it worked exceptionally good with small data sets.Let's go ahead and understand what exactly is unsupervised learning. So sometimes the given data is unstructured and unlabeledso it becomes difficult to classify the data into different categories. So unsupervised learning helps to solve this problem.This learning is used to Cluster the input data in classes on the basis of their statistical properties.So example, we can cluster Different Bikes based upon the speed limit their accelerationor the average. Average that they are giving so and suppose learning is a type of machine learning algorithmused to draw inferences from data sets consisting of input data without labels responses.So if you have a look at the workflow or the process flow of unsupervised learning, so the training data is collection of informationwithout any label. We have the machine learning algorithm and then we have the clustering malls.So what it does is that distributes the data into different clusters and again if you provide any Lebanon new data,it will make a prediction and find out to which cluster that particular data or the data set belongsto or the particular data point belongs to so one of the most important algorithms in unsupervised learning is clustering.So let's understand exactly what is clustering. So a clustering basically is the process of dividing the data setsinto groups consisting of similar data points. It means grouping of objects basedon the information found in the data describing the objects or their relationships,so So clustering malls focus on and defying groups of similar records and labeling records according to the groupto which they belong now. This is done without the benefit of prior knowledge about the groupsand their creator districts. So and in fact, we may not even know exactly how many groups arethere to look for. Now. These models are often referred to as unsupervised learning models,since there's no external standard by which to judge the malls classification performance.There are no right or wrong answers to these model and if we talk about why clustering is usedso the goal of clustering is to determine the intrinsic growth in a set of unlabeled data sometime.The partitioning is the goal or the purpose of clustering algorithm is to make senseof and exact value from the last set of structured and unstructured data.So that is why clustering is used in the industry. And if you have a look at the various use casesof clustering in Industry so first of all, it's being used in marketing. So discovering distinct groupsin customer databases such as customers who make a lot of long distance calls customerswho use internet more than cause they're also using insurance companies for like identifying groupsof Corporation insurance policy holders with high average claim rate Farmers crash cops,which is profitable. They are using C Smith studies and Define probability areas of oil or gas exploration based.Don't cease make data and they're also used in the recommendation of movies.If you'd say they are also used in Flickr photos. They also used by Amazon for recommending the product which category it lies in.So basically if we talk about clustering there are three types of clustering. So first of all, we have the exclusive clusteringwhich is the hard clustering so here and item belongs exclusively to one cluster not several clustersand the datapoint belong exclusively to one cluster. ER so an example of this is the k-means clustering soclaiming clustering does this exclusive kind of clustering so secondly, we have overlapping clusteringso it is also known as soft clusters in this and item can belong to multiple clusters as its degree of associationwith each cluster is shown and for example, we have fuzzy or the c means clusteringwhich has been used for overlapping clustering and finally we have the hierarchical clusteringso When two clusters have a parent-child relationship or a tree-like structure,then it is known as hierarchical cluster. So as you can see here from the example, we have a parent-child kindof relationship in the cluster given here. So let's understand what exactly is K means clustering.So today means clustering is an Enquirer them whose main goal is to group similar elements of data points into a clusterand it is a process by which objects are classified into a predefined number of groupsso that they They are as much just similar as possible from one group to another groupbut as much as similar or possible within each group now if you have a look at the algorithm working here,you're right. So first of all, it starts with and defying the number of clusters,which is K that I can we find the centroid we find that distance objects to the distance objectto the centroid distance of object to the centroid. Then we find the grouping based on the minimum distance.Past the centroid Converse if true then we make a cluster false. We then I can't find the centroid repeatall of the steps again and again, so let me show you how exactly clustering was with an example here.So first we need to decide the number of clusters to be made now another important task here ishow to decide the important number of clusters or how to decide the number of classes will get into that later.So first, let's assume that the number of clusters we have decided. It is three. So after that then we provide the centroidsfor all the Clusters which is guessing and the algorithm calculates the euclidean distanceof the point from each centroid and assize the data point to the closest cluster now euclidean distance.All of you know is the square root of the distance the square root of the square of the distance.So next when the centroids are calculated again, we have our new clusters for each data point then again the distance from the points.To the new classes are calculated and then again the points are assigned to the closest cluster.And then again, we have the new centroid scattered and now these steps are repeateduntil we have a repetition the centroids or the new centralized are very close to the very previous ones.So until unless our output gets repeated or the outputs are very very close enough.We do not stop this process. We keep on calculating the euclidean distance of all the points to the centroid.It's then we calculate the new centroids and that is how K means clustering Works basically,so an important part here is to understand how to decide the value of K or the number of clustersbecause it does not make any sense. If you do not know how many classes are you going to make?So to decide the number of clusters? We have the elbow method. So let's assume firstof all compute the sum squared error, which is sse4 some value of a for example.Take two four six and eight now the SSE which is the sum squared error is defined as a sumof the squared distance between each number member of the cluster and its centroid mathematically andif you mathematically it is given by the equation which is provided here. And if you brought the key against the SSE,you will see that the error decreases as K gets large not this is because the number of cluster increasesthey should be smaller. So the Distortion is also smaller know. The idea of the elbow method is to choose the K at whichthe SSE decreases abruptly. So for example here if we have a look at the figure given here.We see that the best number of cluster is at the elbow as you can see here the graph here changes abruptlyafter the number four. So for this particular example, we're going to use for as a number of cluster.So first of all while working with k-means clustering there are two key points to know first of all,Be careful about where you start so choosing the first center at random during the second center.That is far away from the first center similarly choosing the NIH Center as far away as possible from the closestof the of the other centers and the second idea is to do as many runs of k-means each with different random starting pointsso that you get an idea of where exactly and how many clusters you need to make and where exactly the centroid liesand how the data is getting converted. Divorced now k-means is not exactly a very good method.So let's understand the pros and cons of k-means clustering. We know that k-means is simple and understandable.Everyone learns to the first go the items automatically assigned to the Clusters.Now if we have a look at the cons, so first of all one needs to define the number of clusters,there's a very heavy task asks us if we have three four or if we have 10 categories, and if you do not knowwhat the number of clusters are going to be. It's very difficult for anyone. You know to guess the number of clusters not all the itemsare forced into clusters whether they are actually belong to any other cluster or any other category.They are forced to rely in that other category in which they are closest to this against happens because of the numberof clusters with not defining the correct number of clusters or not being able to guess the correct number of clusters.So and for most of all, it's unable to handle the noisy data and the outliners because anyways machine learning engineers and date.Our scientists have to clean the data. But then again it comes down to the analysiswhat they're doing and the method that they are using so typically people do not clean the datafor k-means clustering or even if the clean there's sometimes a now see noisyand outliners data which affect the whole model so that was all for k-means clustering.So what we're going to do is now use k-means clustering for the movie datasets,so, Have to find out the number of clusters and divide it accordingly.So the use case is that first of all, we have a data set of five thousand movies.And what we want to do is grip them if the movies into clusters based on the Facebook likes,so guys, let's have a look at the demo here. So first of all, what we're going to do is import deep copy numpy pandasSeaborn the various libraries, which we're going to use now and from my proclivities in the use ply plot.And we're going to use this ggplot and next what we're going to do is import the data set and lookat the shape of the data set. So if you have a look at the shape of the data set we can see that it has 5043 rose with 28 columns.And if you have a look at the head of the data set we can see it just 5043 data points,so George we going to do is place the data points in the plot we take the director Facebook likesand we have a look at the data columns face number in post carstotal Facebook likes director Facebook likes. So what we have done herenow is taking the director Facebook likes and the actor three Facebook likes, right.So we have five thousand forty three rows and two columns Now using the k-means from sklearnwhat we're going to do is import it. First we're going to import k-means from scale and Dot cluster.Remember guys eschaton is a very important library in Python for machine learning.So and the number of cluster what we're going to do is provide as five now this again,the number of cluster depends upon the SSE, which is the sum of squared errors all the we're going to use the elbow method.So I'm not going to go into the details of that again. So we're going to fit the data into the k-means to fit andif you find the cluster, Us than for the k-means and printed. So what we find is is an array of five clustersand Fa print the label of the k-means cluster. Now next what we're going to do is plot the datawhich we have with the Clusters with the new data clusters, which we have found and for this we're going to use the CC Bondand as you can see here, we have plotted that car. We have plotted the datainto the grid and you can see here we have five clusters. So probably what I would say isthat the cluster 3 and the cluster zero are very very close.So it might depend see that's exactly what I was going to say. Is that initially the main Challengeand k-means clustering is to define the number of centers which are the K. So as you can see herethat the third Center and the zeroth cluster the third cluster and the zeroth cluster up very very close to each other.So guys It probably could have been in one another cluster and the another disadvantage was that we do not exactly knowhow the points are to be arranged. So it's very difficult to force the data into any other clusterwhich makes our analysis a little different works fine. But sometimes it might be difficult to codein the k-means clustering now, let's understand what exactly is c means clustering.So the fuzzy see means is an extension of the k-means clustering the popular simple.Clustering technique so fuzzy clustering also referred as soft clustering is a formof clustering in which each data point can belong to more than one cluster.So k-means tries to find the heart clusters where each point belongs to one cluster.Whereas the fuzzy c means discovers the soft clusters in a soft cluster any point can belongto more than one cluster at a time with a certain Affinity value towards each 4zc means assigns the degree of membership,which Just from 0 to 1 to an object to a given cluster. So there is a stipulation that the sum of Z membershipof an object to all the cluster. It belongs to must be equal to 1 so the degree of membershipof this particular point to pull of these clusters as 0.6 0.4. And if you add up we get 1so that is one of the logic behind the fuzzy c means so and and this Affinity is proportional to the distancefrom the point to the center of a cluster now then again We have the pros and cons of fuzzy see means.So first of all, it allows a data point to be in multiple cluster. That's a pro. It's a more neutral representation of the behaviorof jeans jeans usually are involved in multiple functions. So it is a very good type of clusteringwhen we're talking about genes First of and again, if we talk about the cons again,we have to Define c which is the number of clusters same as K next. We need to determine the membership cutoff value also,so that takes a lot of I'm and it's time-consuming and the Clusters are sensitive to initial assignment of centroid.So a slight change or deviation from the center's it's going to result in a very different kind of, you know,a funny kind of output with that from the fuzzy see means and one of the major disadvantage of c means clustering isthat it's this a non-deterministic algorithm. So it does not give you a particular output asin such that's that now let's have a look at At the throat type of clustering which is the hierarchical clustering.So hierarchical clustering is an alternative approach which builds a hierarchy from the bottom upor the top to bottom and does not require to specify the number of clusters beforehand.Now, the algorithm works as in first of all, we put each data point in its own cluster and if I the closest to Clusterand combine them into one more cluster repeat the above step till the data points are in a single cluster.Now, there are two types of hierarchical clustering one is I've number 80 plus string and the other one is division clustering.So a cumulative clustering bills the dendogram from bottom level while the division clustering it starts all the data pointsin one cluster the fruit cluster now again hierarchical clustering also has some sort of pros and cons.So in the pros don't know Assumption of a particular number of cluster is required and it may correspond to meaningful tax anomalies.Whereas if we talk about the cons once a decision is made to combine two clusters. It cannot be undone and oneof the major disadvantage of these hierarchical clustering is that it becomes very slow. If we talked about very very large data sets and nowadays.I think every industry are using last year as it's and collecting large amounts of data.So hierarchical clustering is not the act or the best method someone might need to go for so there'sthat Hello everyoneand welcome to this interesting session on a prairie algorithm. Now many of us have visited retails shops such asWalmart or Target for our household needs. Well, let's say that we are planning to buy a new iPhone from Target.What we would typically do is search for the model by visiting the mobile section of the stove and then select the productand head towards the billing counter. But in today's world the goal of the organization is to increase the revenue.Can this be done by just pitching one? I worked at a time to the customer. Now. The answer to Is is clearly no hence organization beganmining data relating to frequently bought items. So a Market Basket analysis is one of the key techniquesused by large retailers to uncover associations between items now examples could be the customerswho purchase Bread have a 60 percent likelihood to also purchase Jam customerswho purchase laptops are more likely to purchase laptop bags as well. They try to find outassociations between different items and products that can be sold together which gives assisting in the right product placement.Typically, it figures out what products are being bought together and organizations can place products in a similar manner,for example, people who buy bread also tend to buy butter, right and the marketing teamat retail stores should Target customers who buy bread and butter and provide an offer to themso that they buy a But item suppose X so if a customer buys breadand butter and sees a discount offer on X, he will be encouraged to spend more and buy the eggsand this is what Market Basket analysis is all about. This is what we are going to talk about in this session,which is Association rule Mining and the a prayer real Corinth now Association rule can be thought of asan if-then relationship just to elaborate on that. We have come up with a rule supposeif an item a is Been bought by the customer. Then the chances of Item B being picked by the customer to underthe same transaction ID is found out you need to understand here that it's not a cash reality rather.It's a co-occurrence pattern that comes to the force. Now, there are two elements to this rule first if and second is the then nowif is also known as antecedent. This is an item or a group of itemsthat are typically found in the item set and the later one. Is called the consequent this comes along as an itemwith an antecedent group or the group of antecedents a purchase. Now if we look at the image here a arrow B,it means that if a person buys an item a then he will also buy an item b or he will most probably by an item B.Now the simple example that I gave you about the bread-and-butter and the x is just a small example,but what if you have thousands and thousands of items if you go to any proof additional data scientistwith that data, you can just imagine how much of profit you can make if the data scientist provides you with the right examplesand the right placement of the items, which you can do and you can get a lot of insights.That is why Association rule mining is a very good algorithm which helps the business make profit.So, let's see how this algorithm works. So Association rule mining is all about building the rulesand we have just seen one rule that If you buy a then there's a slight possibilityor there is a chance that you might buy be also this type of a relationship in which we can find the relationshipbetween these two items is known as single cardinality, but what if the customerwho bought a and b also wants to buy C or if a customer who bought a b and c also wants to buy D. Thenin these cases the cardinality usually increases and we can have a lot of combination around.These data and if you have around 10,000 or more than 10,000 dataor items just imagine how many rules you're going to create for each product.That is why Association rule mining has such measures so that we do not end up creating tens of thousands of rules.Now that is where the a priori algorithm comes in. But before we get into the a priori algorithm,let's understand. What's the maths behind it. Now there are three types of matrices.Which help to measure the association? We have support confidence and lift.So support is the frequency of item a or the combination of item ARB.It's basically the frequency of the items, which we have bought and what are the combination of the frequency of the item.We have bought. So with this what we can do is filter out the items, which have been bought less frequently.This is one of the measures which is support now what confidence tells us so conference.Gives us how often the items NB occur together given the number of times a occur.Now this also helps us solve a lot of other problems because if somebody is buying aand b together and not buying see we can just rule out see at that point of time. So this solves another problem isthat we obviously do not need to analyze the process which people just by barely.So what we can do is according to the sages we can Define our minimum support and confidence and when youhave set Values we can put this values in the algorithm and we can filter out the data and wecan create different rules and suppose even after filtering you have like five thousand rules.And for every item we create these 5,000 rules. So that's practically impossible.So for that we need the third calculation, which is the lift so lift is basically the strength of any Rule now,let's have a look at the denominator of the formula given here and if you see Here,we have the independent support values of A and B. So this gives us the independent occurrence probability of A and B.And obviously there's a lot of difference between the random occurrence and Association andif the denominator of the lift is more what it means is that the occurrenceof Randomness is more rather than the occurs because of any association.So left is the final verdict where we know whether we have to spend time. On this particular rule what we have got here or not.Now, let's have a look at a simple example of Association rule mining. So suppose.We have a set of items a b c d and e and a set of transactions T1 T2 T3 T4and T5 and as you can see here, we have the transactions T1 in which we have ABC T to a CD t3b CDT for a d e and T5 BCE.Now what we generally do is create. At some rules or Association rules such as a gives Tor C gives a a gift C B and C gives a what this basically means isthat if a person buys a then he's most likely to buy D. And if a person by C, then he's most likely to buy a andif you have a look at the last one, if a person buys B and C is most likely to buy the itema as well now if we calculate the support confidence and lift using these rulesas you can see here in the table, we have the rule. And the support confidence handle lift values.Let's discuss about a prairie. So a priori algorithm uses the frequent itemsetsto generate the association Rule and it is based on the concept that subset of a frequent itemsets must also bea frequent item set itself. Now this raises the question what exactly is a frequent item set.So a frequent item set is an item set whose support value is greater than the threshold value just now we discussedthat the marketing team according to the says have a minimum threshold value for the confidence as well as the support.So frequent itemsets is that animset who support value is greater than the threshold value already specified example,if A and B is a freaker item set Than A and B should also be frequent itemsets individually.Now, let's consider the following transaction to make the things such as easier suppose.We have transactions 1 2 3 4 5 and these Items out there. So T 1 has 1 3 & 4 T 2 has 2 3 and 5 T3 has1 2 3 5 T 4 to 5 and T 5 1 3 & 5 now the first step is to build a listof items sets of size 1 by using this transactional data. And one thing to note here is that the minimum support countwhich is given here is to Let's suppose it's too so the first step is to create item setsof size 1 and calculate their support values. So as you can see here. We have the table see onein which we have the item sets 1 2 3 4 5 and the support values if you remember the formula of support,it was frequency divided by the total number of occurrence. So as you can see here for the itemsthat one the support is 3 as you can see here that item set one up here s and t 1 T 3 and T 5.So as you can see, it's frequency is 1 2 & 3 now as you can see the item setfor has a support of one as it occurs only once in Transaction one but the minimum support value is 2that's why it's going to be eliminated. So we have the final table which is the table F1,which we have the item sets 1 2 3 and 5 and we have the support values 3 3 4 & 4 now the next step isto create Adam sets of size 2 and calculate their support values now all the combination of the item sets in the F1,which is the final table in which it is carded the for are going to be used for this iteration.So So we get the table c 2. So as you can see here, we have 1 2 1 3 1 5 2 3 2 5 & 3 5 nowif you calculate the support here again, we can see that the item set 1 comma 2 has a support of onewhich is again less than the specified threshold. So we're going to discard that so if we have a lookat the table f 2 we have 1 comma 3 1 5 2 3 2 5 & 3 5 again,we're going to move forward and create the atoms. That of size 3 and calculate this support values.Now all the combinations are going to be used from the item set F to for this particular iterations.Now before calculating support values, let's perform proning on the data set. Now what is pruning nowafter the combinations are being made we device c 3 item sets to check if there is another subset whose support is lessthan the minimum support value. That is what frequent items that means. So if you have a look here the item sets.We have is 1 2 3 1 2 1 3 2 3 4 the first onebecause as you can see here if we have a look at the subsets of one two, three, we have 1 comma 2 as well,so we are going to discard this whole item set same goes for the second one.We have one to five. We have 1/2 in that which was discarded in the previous set or the previous step.That's why we're going to discard that also which leaves us with only two factors,which is 1 3 5 8. I'm set and the two three five and the support for this is 2and 2 as well. Now if we create the table C for using four elements,we going to have only one item set, which is 1 2 3 and 5 and if you have a look at the table here the transaction table one,two, three and five appears only one. So the support is one and since C for the support of the whole table C4 is less than 2 so we're going to stop here and return to the previous item set that It is 3 3so the frequent itemsets have 1 3 5 and 2 3 5 now let's assume our minimum confidence value is 60 percent for that.We're going to generate all the non-empty subsets for each frequent itemsets. Now for I equals 1 comma 3 comma 5 which is the item set.We get the subset one three one five three five one three and five similarlyfor 2 3 5 we get to three to five three five two three. and five now this rule statesthat for every subset s of I the output of the rule gives something like s gives i2sthat implies s recommends I of s and this is only possible if the support of I divided by the support of s is greaterthan equal to the minimum confidence value now applying these rules to the item set of F3 we get rule 1 which is 1 3gives 1 comma 3 comma 5 and 1/3 3 it means 1 and 3 gives 5so the confidence is equal to the support of 1 comma 3 comma fire driver supportof 1 comma 3 that equals 2 by 3 which is 66% and which is greater than the 60 percent.So the rule 1 is selected now if we come to rule 2 which is 1 comma 5 it gives 1 comma 3 comma 5 and 1 5it means if we have 1 & 5 it implies. We also going to have three know.Calculate the confidence of this one. We're going to have support 1 3 5 whereby support 1/5which gives us a hundred percent which means rule 2 is selected as well. But again if you have a look at rule 506 over here similarly,if it's select 3 gives 1 3 5 & 3 it means if you have three, we also get one and five.So the confidence for this comes at 50% Which is less than the given 60 percent Target.So we're going to reject this Rule and same. Goes for the rule number six. Now one thing to keep in mind here isthat all those are rule 1 and Rule 5 look a lot similar they are not so it really dependswhat's on the left hand side of the arrow. And what's on the right-hand sides of the arrow. It's the if-then possibility.I'm sure you guys can understand what exactly these rows are and how to proceed with this rules.So, let's see how we can implement the same in Python, right? So for that what I'm going to do is create a new python.and I'm going to use the chapter notebook. You're free to use any sort of ID.I'm going to name it as a priority. So the first thing what we're going to do is we will be usingthe online transactional data of retail store for generating Association rules. So firstly what we need to do is get the pandas and ml x10 libraries imported and read the file. So as you can see here,we are using the online retail dot xlsx format file and from ml extant.We're going to import a prairie and Association rules at all comes under MX 10.So as you can see here, we have the invoice the stock quote the description the quantitythe invoice data unit price customer ID and the country now next in this step. What we're going to do is do data cleanupwhich includes removing the spaces from some of the descriptions. And drop the rules that do not have invoicenumbers and remove the great grab transactions because that is of no use to us.So as you can see here at the output in which we have like five hundred and thirty two thousand rowswith eight columns. So after the cleanup, we need to consolidate the items into one transaction per rowwith each product for the sake of keeping the data set small. We are only looking at the sales for France.So as you can see here, we have excluded all the other says we're just looking at the sales for France.Now. There are a lot of zeros in the data. But we also need to make sure any positive values are converted to 1and anything less than zero is set to 0 so as you can see here, we are still 392 Rose.We're going to encode it and see. Check again. Now that you have structured the data properly in this step.What we're going to do is generate frequent itemsets that have support at least seven percent,but this number is chosen so that you can get close enough and generated rules with the corresponding support confidence and lift.So go ahead you can see here. The minimum support is 0.71 of what if we add another constrainton the rules such as the lift is greater than 6 and the conference is greater than 0.8.So as you can see here, we have the left-hand side and the right-hand side of the association rule, which is the antecedent and the consequence.We have the support. We have the confidence to lift the leverage and the conviction. So guys, that's it for this session.That is how you create Association rules using the API. Real gold tone which helps a lot in the marketing business.It runs on the principle of Market Basket analysis, which is exactly what big companies like Walmart.You have Reliance and Target to even Ikea does it and I hope you gotto know what exactly is Association rule mining what is lift confidence and support and how to create Association rules.So guys reinforcement learning. Dying is a part of machine learning where an agent is put in an environmentand he learns to behave in this environment by performing certain actions. Okay, so it basically performs actions and it either getsa rewards on the actions or it gets a punishment and observing the reward which it gets from those actions reinforcement learning is allabout taking an appropriate action in order to maximize the reward in a particular situation.So guys in supervised learning the training data comprises of the input and the expected outputAnd so the model is trained with the expected output itself, but when it comes to reinforcement learning,there is no expected output here. The reinforcement agent decides what actions to take in order to perform a given task in the absenceof a training data set. It is bound to learn from its experience itself. Alright. So reinforcement learning is all about an agentwho's put in an unknown environment and he's going to use a hit and trial method in order to figure out the environment and then come upwith an outcome. Okay. Now, let's look at it. Reinforcement learning within an analogy. So consider a scenario where in a baby is learninghow to walk the scenario can go about in two ways. Now in the first case the baby starts walkingand makes it to the candy here. The candy is basically the reward it's going to get so since the candy is the end goal the baby is happy.It's positive. Okay, so the baby is happy and it gets rewarded a set of candies now another way in which this could go isthat the baby starts walking but Falls due to some hurdle in between The baby gets hot and it doesn't get any candy and obviously the baby is sad.So this is a negative reward. Okay, or you can say this is a setback. So just like how we humans learn from our mistakes by trialand error reinforcement learning is also similar. Okay, so we have an agent which is basically the baby and a rewardwhich is the candy over here. Okay, and with many hurdles in between the agent is supposedto find the best possible path to read through the reward. So guys. I hope you all are clear with the reinforcement learning now,let's look at At the reinforcement learning process. So generally a reinforcement learning system hastwo main components, right? The first is an agent and the second one is an environment. Now in the previous case,we saw that the agent was the baby and the environment was the living room where in the baby was crawling.Okay. The environment is the setting that the agent is acting on and the agent over hererepresents the reinforcement learning algorithm. So guys the reinforcement learning process startswhen the environment sends a state to the And then the agent will take some actions basedon the observations in turn the environment will send the next state and the respective reward back to the agent.The agent will update its knowledge with the reward returned by the environment and it usesthat to evaluate its previous action. So guys this Loop keeps continuing until the environment sends a terminal state which meansthat the agent has accomplished all his tasks and he finally gets the reward. Okay. This is exactlywhat was depicted in this scenario. So the agent keeps climbing up ladders until he reaches his reward to understand this better.Let's suppose that our agent is learning to play Counter Strike. Okay. So let's break it down now initially the RL agentwhich is basically the player player 1. Let's say it's a player one who is trying to learn how to play the game.Okay. He collects some state from the environment. Okay. This could be the first date of Counter-Strike now basedon the state the agent will take some action. Okay, and this action can be anything that causes a result.So if the Almost left or right it's also considered as an action. Okay, so initially the action is going to be randombecause obviously the first time you pick up Counter-Strike, you're not going to be a master at it. So you're going to try with different actionsand you just want to pick up a random action in the beginning. Now the environment is going to give a new state.So after clearing that the environment is now going to give a new state to the agent or to the player.So maybe he's across th one now. He's in stage 2. So now the player will get a rewardour one from the environment. Because it cleared stage 1. So this reward can be anything. It can be additional points or coins or anything like that.Okay. So basically this Loop keeps going on until the player is dead or reaches the destination.Okay, and it continuously outputs a sequence of States actions and rewards. So guys, this was a small example to show youhow reinforcement learning process works. So you start with an initial State and once a player clothes that state he gets a rewardafter that the environment will give another stage to the player. And after it clears that state it's going to get another awardand it's going to keep happening until the player reaches his destination. All right, so guys, I hope this is clear now,let's move on and look at the reinforcement learning definitions. So there are a few Concepts that you should be awareof while studying reinforcement learning. Let's look at those definitions over here. So first we have the agent now an agent is basicallythe reinforcement learning algorithm that learns from trial and error. Okay, so an agent takes actions like For example a soldierin Counter-Strike navigating through the game. That's also an action. Okay, if he moves left right or if he shoots at somebodythat's also an action. Okay. So the agent is responsible for taking actions in the environment.Now the environment is the whole Counter-Strike game. Okay. It's basically the world through which the agentmoves the environment takes the agents current state and action as input and it Returns the agency reward and its next state as output.Alright next we have action now all the possible. Steps that an agent can take are called actions.So like I said, it can be moving right left or shooting or any of that. Alright, then we have state now state isbasically the current condition returned by the environment. So whichever State you are in if you are in state 1 or if you're in stateto that represents your current condition. All right. Next we have reward a reward is basically an instant returnfrom the environment to appraise Your Last Action. Okay, so it can be anything like coinsor it can be audition. Two points. So basically a reward is given to an agentafter it clears the specific stages. Next we have policy policies basically the strategythat the agent uses to find out his next action based on his current state policy is just the strategy with which youapproach the game. Then we have value. Now while you is the expected long-term returnwith discount so value in action value can be a little bit confusing for you right now, but as we move further,you'll understand what I'm talking. Kima okay. So value is basically the long-term return that you get with discount.Okay discount. I'll explain in the furthest lines. Then we have action value now action value is also known as Q value.Okay. It's very similar to Value except that it takes an extra parameter, which is the current action.So basically here you'll find out the Q value depending on the particular action that you took.All right. So guys don't get confused with value and action value. We look at examples in the further slides and you will understand this better.Okay. So guys make sure that you're familiar with these terms because you'll be seeing a lot of these terms in the further slides.All right. Now before we move any further, I'd like to discuss a few more Concepts. Okay. So first we will discuss the reward maximization.So if you haven't already realized it the basic aim of the RL agent is to maximize the reward now,how does that happen? Let's try to understand this in depth. So the agent must be trained in such a waythat he takes the best action so that the reward is Because the end goal of reinforcement learningis to maximize your reward based on a set of actions. So let me explain this with a small game nowin the figure you can see there is a fox there's some meat and there's a tiger so our agent is basically the fox and his end goalis to eat the maximum amount of meat before being eaten by the tiger now since the fox is a clever fellow he eats the meatthat is closer to him rather than the meat which is closer to the tiger. Now this is because the closer he is to the tiger thehigher our his chances of getting killed. So because of this the rewards which are near the tiger,even if they are bigger meat chunks, they will be discounted. So this is exactly what discounting meansso our agent is not going to eat the meat chunks which are closer to the tiger because of the risk. All right now,even though the meat chunks might be larger. He does not want to take the chances of getting killed. Okay. This is called discounting.Okay. This is where you discount because it improvise and you just eat the meat which are closer to you instead of taking risksand eating the meat which are The to your opponent. All right. Now the discounting of reward Works basedon a value called gamma will be discussing gamma in our further slides but in short the value of gamma is between 0 and 1.Okay. So the smaller the gamma the larger is the discount value. Okay. So if the gamma value is lesser,it means that the agent is not going to explore and he's not going to try and eat the meat chunkswhich are closer to the tiger. Okay, but if the gamma value is closer to 1 it means that our agent is actually We're going to exploreand it's going to dry and eat the meat chunks which are closer to the tiger. All right, now, I'll be explaining this in depth in the further slides.So don't worry if you haven't got a clear concept yet, but just understand that reward maximization is a very important stepwhen it comes to reinforcement learning because the agent has to collect maximum rewards by the end of the game.All right. Now, let's look at another concept which is called exploration and exploitation. So exploration like the name suggests isabout exploring and capturing. More information about an environment on the other hand exploitation isabout using the already known exploited information to heighten the rewards. So guys consider the fox and tiger examplethat we discussed now here the fox eats only the meat chunks which are close to him, but he does not eat the meat chunkswhich are closer to the tiger. Okay, even though they might give him more Awards. He does not eat themif the fox only focuses on the closest rewards, he will never reach the big chunks of meat.Okay, this is what exploitation is the about you just going to use the currently known informationand you're going to try and get rewards based on that information. But if the fox decides to explore a bit,it can find the bigger award which is the big chunks of meat. This is exactly what exploration is.So the agent is not going to stick to one corner instead. He's going to explore the entire environment and tryand collect bigger rewards. All right, so guys, I hope you all are clear with exploration and exploitation.Now, let's look at the markers decision process. So guys this is basically a mathematical approachfor mapping a solution in reinforcement learning in a way. The purpose of reinforcement learning is to solvea Markov decision process. Okay. So there are a few parameters that are used to get to the solution.So the parameters include the set of actions the set of states the rewards the policythat you're taking to approach the problem and the value that you get. Okay, so to sum it up the agent must takean action a to transition from a start state. The end State s while doingso the agent will receive a reward are for each action that he takes. So guys a seriesof actions taken by the agent Define the policy or it defines the approach and the rewardsthat are collected Define the value. So the main goal here is to maximize the rewardsby choosing the optimum policy. All right. Now, let's try to understand this with the help of the shortest path problem.I'm sure a lot of you might have gone through this problem when you are in college. So guys look at the graph over here.So our aim here is to find the shortest path between a and d with minimum possible cost.So the value that you see on each of these edges basically denotes the cost. So if I want to go from a to c it's going to cost me 15 points.Okay. So let's look at how this is done. Now before we move and look at the problem in this problem the set of states are denoted by the nodes,which is ABCD and the action is to Traverse from one node to the other. So if I'm going from a Bethat's an action similarly a to see that's an action. Okay, the reward is basically the costwhich is represented by each Edge over here. All right. Now the policy is basically the path that I choose to reach the destination.So let's say I choose a seed be okay that's one policy in order to get to D and choosing a CDwhich is a policy. Okay. It's basically how I'm approaching the problem. So guys here you can start off at node aand you can take baby steps to your destination now initially you're Clueless. So you can just take the next possible node,which is visible to you. So guys if you're smart enough, you're going to choose a to see instead of ABCD or ABD.All right. So now if you are at nodes see you want to Traverse to note D. You must again choose a wise pathor red you just have to calculate which path has the highest cost or which path will give you the maximum rewards.So guys, this is a simple problem. We just drank to calculate the shortest path between aand d by traversing through these nodes. So if I travels from a CD it gives me the maximum reward.Okay, it gives me 65 which is more than any other policy would give me okay. So if I go from ABD,it would be 40 when you compare this to a CD. It gives me more reward. So obviously I'm going to go with a CB.Okay, so guys was a simple problem in order to understand how Markov decision process works.All right, so guys, I want to ask you a question. What do you think? I did hear did I perform explorationor did I perform exploitation? Now the policy for the above example is of exploitationbecause we didn't explore the other nodes. Okay. We just selected three notes and we Traverse through them. So that's why this is called exploitation.We must always explore the different notes so that we can find a more optimal policy. But in this case, obviously a CD has the highest rewardand we're going with a CD, but generally it's not so simple. There are a lot of nodes there hundreds of notes to Traverseand they're like 50 60 policies. Okay, 50 60 different policies. So you make sure you explore.All the policies and then decide on an Optimum policy which will give you a maximum reward.So guys before we perform the Hands-On part. Let's try to understand the math behind our demo.Okay. So in our demo will be using the Q learning algorithm which is a type of reinforcement learning algorithm.Okay, it's simple, it just means that if you take the best possible actions to reach your goal or to get the most rewards.All right, let's try to understand this with an example. So guys, this is exactly what be running in In our demo,so make sure you understand this properly. Okay. So our goal here is we're going to place an agentin any one of the rooms. Okay. So basically these squares you see here our rooms. OK 0 is a roomfor is a room three is a room one is a room and 2:05 is also a room. It's basically a way outside the building.All right. So what we're going to do is we're going to place an agent in any one of these rooms and the goal is to reach outside the building.Okay outside. The building is room number five. Okay, so these are These spaces are basically doors,which means that you can go from zero to four. You can go from 4 to 3 3 to 1 1 to 5and similarly 3 to 2, but you can't go from 5 to 2 directly. All right, so there are certain set of roomsthat don't get connected directly. Okay. So like of mentioned here each room is numbered from 0 to 4,and the outside of the building is numbered as five and one thing to note here is Room 1 and roomfor directly lead to room number five. All right. So room number one and four will directly lead outto room number five. So basically our goal over here is to get to room number five.Okay to set this room as a goal will associate a reward value to each door. Okay.Don't worry. I'll explain what I'm saying. So if you re present these rooms in a graph this ishow the graph is going to look. Okay. So for example from true, you can go to three and then three two,one one two five which will lead us to our goal these arrows represent the link between the dose.No, this is quite understandable now. Our next step is to associate a reward valueto each of these doors. Okay, so the rooms that are directly connected to our end room,which is room number five will get a reward of hundred. Okay. So basically our room number one will have a reward five now.This is obviously because it's directly connected to 5 similarly for will also be associated with a reward of hundredbecause it's directly connected to 5. Okay. So if you go out from for it will lead to five now the other know.Roads are not directly connected to 5. So you can't directly go from 0 to 5. Okay. So for this will be assigning a reward of zero.So basically other doors not directly connected to the Target room have a zero reward.Okay now because the doors are to weigh the two arrows are assigned to each room. Okay, you can see two arrows assigned to each room.So basically zero leads to four and four leads back to 0 now. We have assigned 0 0 over herebecause 0 does not directly lead to five but one directly leads to Five and that's why you can see a hundred over here similarlyfor directly leads to our goal State and that's why we were signed a hundred over here and obviously five two five is hundred as well.So here all the direct connections to room number five are rewarded hundred and all the indirect connectionsare awarded zero. So guys in q-learning the end goal is to reach the state with the highest rewardso that the agent arrives at the goal. Okay. So let me just explain this graph to youin detail now these These rooms over here labeled one, two, three to five they represent the state an agent is in soif I stay to one It means that the agent is in room number one similarly the agents movementfrom one room to the other represents the action. Okay. So if I say one two, three, it represents an action.All right. So basically the state is represented as node and the action is represented by these arrows.Okay. So this is what this graph is about these nodes represent the rooms and these Arrows represent the actions.Okay. Let's look at a small example. Let's set the initial state to 0. So my agent is placed in room number two,and he has to travel all the way to room number five. So if I set the initial stage to to he can travel to State 3.Okay from three he can either go to one or you can go back to to or you can go to for if he chooses to go tofor it will directly take him to room number 5, okay, which is our end goal and even if he goes from room number3 2 1 it will take him to room number. High five, so this is how our algorithm works is going to drivers different rooms.In order to reach the Gold Room, which is room number 5. Now, let's try and depict these rewardsin the form of a matrix. Okay, because we'll be using this our Matrix or the reward Matrix to calculate the Q valueor the Q Matrix. Okay. We'll see what the Q value is in the next step. But for now, let's see how this reward Matrix is calculated.Now the - ones that you see in the table, they represent the null values. Now these -1 basically meansthat Wherever there is no link between nodes. It's represented as minus 1 so 02 0 is minus 1 0 to 1 there is no link. Okay, there's no direct link from 0 to 1.So it's represented as minus 1 similarly 0 to 2 or 2. There is no link. You can see there's no line over here.So this is also minus 1, but when it comes to 0 to 4, there is a connection and we have numbered 0because the reward for a state which is not directly connected to the goal is zero, but if you lookat this 1 comma 5 which is is basically traversing from Node 1 to node 5, you can see the reward is hundred.Okay, that's basically because one and five are directly connected and five is our end goal.So any node which will directly connected to our goal state will get a reward of hundred. Okay.That's why I've put hundred over here similarly. If you look at the fourth row over here. I've assigned hundred over here.This is because from 4 to 5 that is a direct connection. There's a direct connection which gives them a hundred reward.Okay, you can see from 4 to 5. There is a direct link. Okay, so from room number for to room number five you can go directly.That's why there's a hundred reward over here. So guys, this is how the reward Matrix is made. Alright, I hope this is clear to you all.Okay. Now that we have the reward Matrix. We need to create another Matrix called The Q Matrix.OK here, you'll store or the Q values that will calculate now this Q Matrix basicallyrepresents the memory of what the agent has learned through experience. Okay. So once he traverses from one room to the final room,whatever he's learned. It is stored in this Q Matrix. Okay, in order for him to remember that the next time he travels this we use this Matrix.Okay. It's basically like a memory. So guys the rows of the Q Matrix will represent the current stateof the agent The Columns will represent the possible actions and to calculate the Q value use this formula.All right, I'll show you what the Q Matrix looks like, but first, let's understand this formula. Now this Q valuewill calculating because we want to fill in the Q Matrix. Okay. So this is basically a Matrix over here initially,it's all 0 but as the agent Traverse is from different nodes to the destination node.This Matrix will get filled up. Okay. So basically it will be like a memory to the agent. He'll know that okay,when he traversed using a particular path, he found out that his value was maximum or as a reward was maximum of year.So next time he'll choose that path. This is exactly what the Q Matrix is. Okay. Let's go back now guys,don't worry about this formula for now because we'll be implementing this formula in an example.In the next slide. Okay, so don't worry about this formula for now, but here just remember that this Q basically represents the Q Matrix the r representsthe reward Matrix and the gamma is the gamma value which I'll talk about shortly and here you just finding out the maximum from the Q Matrix.So basically the gamma parameter has a range from 0 to 1 so you can have a value of 0.1 0.3 0.5 0.8 and all of that.So if the gamma is closer to zero it means That the agent will consider only the immediate rewards which meansthat the agent will not explore the surrounding. Basically, it won't explore different rooms. It will just choose a particular roomand then we'll try sticking to it. But if the value of gamma is high meaning that if it's closer to one the agent will consider future Awardswith greater weight. This means that the agent will explore all the possible approachesor all the possible policies in order to get to the end goal. So guys, this is what I was talking about when Imention ation and exploration. All right. So if the gamma value is closer to 1 it basically meansthat you're actually exploring the entire environment and then choosing an Optimum policy.But if your gamma value is closer to zero, it means that the agent will only stick to a certain set of policiesand it will calculate the maximum reward based on those policies. Now next. We have the Q learning algorithmthat we're going to use to solve this problem. So guys now this is going to look very confusing to y'all.So let me just explain In this with an example. Okay. We'll see what we're actually going to run in our demo.We will do the math behind it. And then I'll tell you what this Q learning algorithm is. Okay, you'll understand it as I'm showing you the example.So guys in the Q learning algorithm the agent learns from his experience. Okay, so each episode,which is basically when the agents are traversing from an initial room to the end goal is equivalent to one training sessionand in every training session the agent will explore the environment it will Receive some rewarduntil it reaches the goal state which is five. So there's a purpose of training is to enhance the brain of our agent.Okay only if he knows the environment very well, will he know which action to take and this is why we calculate the Q Matrix okay in Q Matrix,which is going to calculate the value of traversing from every state to the end state from every initial roomto the end room. Okay, so when we calculate all the values or how much reward we're getting from each policythat we We know the optimum policy that will give us the maximum reward. Okay, that's why we have the Q Matrix.This is very important because the more you train the agent and the more Optimum your output will be so basically herethe agent will not perform exploitation instead. He'll explore around and go back and forth through the different roomsand find the fastest route to the goal. All right. Now, let's look at an example. Okay. Let's see how the algorithm works.Okay. Let's go back to the previous slide and Here it says that the first step is to set the gamma parameter.Okay. So let's do that. Now the first step is to set the value of the learning parameter, which is gamma and we have randomly set itto zero point eight. Okay. The next step is to initialize the Matrix Q 2 0 Okay.So we've set Matrix Q 2 0 over here and then we will select the initial stage Okay, the third step is select a random initial State and herewe've selected the initial State as room number one. Okay. So after you initialize the matter Q as a zero Matrixfrom room number one, you can either go to room number three or number five. So if you look at the reward Matrix can seethat from room number one, you can only go to room number three or room number five. The other values are minus 1 here,which means that there is no link from 1 to 0 1 2 1 1 2 2 and 1 to 4.So the only possible actions from room number one is to go to room number 3 and to go to room number five.All right. Okay. So let's select room number five, okay. So from room number one, you can go to 3 and 5 and we have randomly selected five.You can also select three but for example, let's select five over here. Now from Rome five, you're going to calculate the maximum Q valuefor the next state based on all possible actions. So from number five, the next state can be room number one four or five.So you're going to calculate the Q value for traversing 5 to 1 5 2 4 5 2 5 and you're going to find outwhich has the maximum Q value and that's how you're going. Compute the Q value. So let's Implement our formula.Okay, this is the q-learning formula. So right now we're traversing from room number one to room number 5.Okay. This is our state. So here I've written Q 1 comma 5. Okay one represents our current statewhich is room number one. Okay. Our initial state was room number one and we are traversing to room number five.Okay. It's shown in this figure room number 5 now for this we need to calculate the Q value next in our formula.It says the reward Matrix State and action. So the reward Matrix for 1 comma 5 let's look at 1 comma5 1 comma 5 corresponds to a hundred. Okay, so I reward over here will be hundred sor 1 comma 5 is basically hundred then you're going to add the gamma value. Now the gamma value will be initialized itto zero point eight. So that's what we have written over here. And we're going to multiply it with the maximum valuethat we're going to get for the next date based on all possible actions. Okay. So from 5, the next state is 1 4 and 5.So if Travis from five to one that's what I've written over here 5 to 4. You're going to calculate the Q value of Fire 2 4 & 5 to 5.Okay. That's what I mentioned over here. So Q 5 comma 1 5 comma 4 and 5 comma 5 are the next possible actionsthat you can take from State V. So r 1 comma 5 is hundred. Okay, because from the reward Matrix,you can see that 1 comma 5 is hundred 0.8 is the value of gamma after that.We will calculate Q of 5 comma 1 5 comma 4 and 5 comma 5 Like I mentioned earlierthat we're going to initialize Matrix Q as zero Matrix So based setting the value of 0because initially obviously the agent doesn't have any memory of what is happening. Okay, so he just starting from scratch.That's why all these values are 0 so Q of 5 comma 1 will obviously be 0 5 comma 4 would be 0and 5 comma 5 will also be zero and to find out the maximum between these it's obviously 0.So when you compute this equation, you will get hundred so the Q value of 1 comma 5 isSo if I agent goes from room number one to room number five, he's going to have a maximum rewardor Q value of hundred. All right. Now in the next slide you can see that I've updated the value of Q of 1 comma 5.Okay, it said 200. All right now similarly, let's look at another example so that you understand this better.So guys, this is exactly what we're going to do in our demo. It's only going to be coded. Okay. I'm just explaining our code right now.I'm just telling you the math behind it. Alright now, let's look at another example. Example OK this time.We'll start with a randomly chosen initial State. Let's say that we've chosen State 3.Okay. So from room 3, you can either go to room number one two, or four randomly will select room numberone and from room number one, you're going to calculate the maximum Q value for the next state based on all possible actions.So the possible actions from one is to go to 3 and to go to 5 now if you calculate the Q value using this formula,so let me explain this to you once again now, 3 comma 1 basically represents that we're in room number three and we are goingto room number one. Okay. So this represents our action? Okay. So we're going from 3 to 1which is our action and three is our current state next we will look at the rewardof going from 3 to 1. Okay, if you go to the reward Matrix 3 comma 1 is 0 okay. Now this isbecause there's no direct link between three and five. Okay, so that's why the reward here is zero. So the value here will be 0after that we have the gamma value, which is zero point. Eight and then we're going to calculate the Q Maxof 1 comma 3 and 1 comma 5 out of these whichever has the maximum value we're going to use that.Okay, so Q of 1 comma 3 is 0. All right 0 you can see here 1 comma 3 is 0and 1 comma 5 if you remember we just calculated 1 comma 5 in the previous slide.Okay 1 comma 5 is hundred. So here I'm going to put a hundred. So the maximum here is hundred.So 0.8 in 200 will give us c t so that's the Q value. Going to get if you Traverse from three two one.Okay. I hope that was clear. So now we have Travers from room number three to room number one with the reward of 80.Okay, but we still haven't reached the end goal which is room number five. So for our next episode the state will be room.Number one. So guys, like I said, we'll repeat this in a loop because room number one is not our end goal.Okay, our end goal is room number 5. So now we need to figure out how to get from room number one to room number 5.So from room number one, you can either either go to three or five. That's what I've drawn over here. So if we select five we know that it's our end goal.Okay. So from room number 5, then you have to calculate the maximum Q value for the next possible actions.So the next possible actions from five is to go to room number one room number four or room number five.So you're going to calculate the Q value of 5 to 1 5 2 4 & 5 2 5 and find out which is the maximum Q valuehere and you're going to use that value. All right. So let's look at the formula now now again, we're in room number one and Want to goto room number 5. Okay, so that's exactly what I've written here Q 1 comma 5 next is the reward Matrix.So reward of 1 comma 5 which is hundred. All right, then we have added the gamma value which is 0.8.And then we're going to find the maximum Q value from 5 to 1 5 2 4 & 5 to 5.So this is what we're performing over here. So 5 comma 1 5 comma 4 and 5 comma 5 are all 0 this isbecause we initially set all the values of the Q Matrix as 0 so you get Hundred over here and the Matrix Remains the Samebecause we already had calculated Q 1 comma 5 so the value of 1 comma 5 is already fed to the agent.So when he comes back here, he knows our okay. He's already done this before now. He's going to try and Implement another method.Okay is going to try and take another route or another policy. So he's going to try to go from different roomsand finally land up in room number 5, so guys, this is exactly how our code runs. We're going to Traverse through each and every nodebecause we want an Optimum ball. See, okay. An Optimum policy is attained only when you Traverse through all possible actions.Okay. So if you go through all possible actions that you can perform only then will you understand which is the best actionwhich will lead us to the reward. I hope this is clear now, let's move on and look at our code.So guys, this is our code and this is executed in Python and I'm assuming that all of you have a good background in Python.Okay, if you don't understand python very well. I'm going to leave a link in the description. You can check out that video on Pythonand then maybe come back to this later. Okay, but I'll be explaining the code to you anyway, but I'm not going to spend a lot of time explaining eachand every line of code because I'm assuming that you know python. Okay. So let's look at the first line of code over here.So what we're going to do is we're going to import numpy. Okay numpy is basically a python libraryfor adding support for large multi-dimensional arrays and matrices and it's basically for computingmathematical functions. Okay so first Want to import that after that we're going to create the our Matrix.Okay. So this is the our Matrix next we're going to create a q Matrix and it's a 6 into 6 Matrixbecause obviously we have six states starting from 0 to 5. Okay, and we are going to initialize the value to zero.So basically the Q Matrix is going to be initialized to zero over here. All right, after that we're setting the gamma parameter to 0.8.So guys you can play with this parameter and you know move it to 0.9 or movement logo to 0.8.Okay, you can see see what happens then then we'll set an initial stage. Okay initial stage is set as 1 after that.We're defining a function called available actions. Okay. So basically what we're doing here issince our initial state is one. We're going to check our row number one. Okay, this is our own number one.Okay. This is wrong number zero. This is zero number one and so on. So we're going to check the row number one and we're goingto find the values which are greater than or equal to 0 because these values basically The nodes that we can travel to nowif you select minus 1 you can Traverse 2-1. Okay, I explained this earlier the - one represents all the nodes that we can travel to but wecan travel to these nodes. Okay. So basically over here a checking all the values which are equal to 0or greater than 0 these will be our available actions. So if our initial state is one we can travel to other stateswhose value is equal to 0 or greater than 0 and this is stored in this variable called.All available act right now. This will basically get the available actions in the current state.Okay. So we're just storing the possible actions in this available act variable over here. So basically over heresince our initial state is one we're going to find out the next possible States we can go to okaythat is stored in the available act variable. Now next is this function chooses at random which actionto be performed within the range. So if you remember over here, so guys initially we are in stage number.Okay are available actions is to go to stage number 3 or stage number five. Sorry room number 3 or room number 5.Okay. Now randomly, we need to choose one room. So for that using this line of code, okay.So here we are randomly going to choose one of the actions from the available act this available act.Like I said earlier stores all our possible actions. Okay from the initial State. Okay.So once it chooses an action is going to store it in next action, so guys this action will Presentthe next available action to take now next is our Q Matrix. Remember this formula that we used.So guys this formula that we use is what we are going to calculate in the next few lines of code.So in this block of code, which is executing and Computing the value of Q. Okay, this is our formula for computing the valueof Q current state Karma action. Our current state Karma action gamma into the maximum value.So here basically we're going to calculate the maximum index meaning that To be going to checkwhich of the possible actions will give us the maximum Q value read if you rememberin our explanation over here this value over here Max Q or five comma 1 5 comma 4 and 5 comma 5 we hadto choose a maximum Q value that we get from these three. So basically that's exactly what we're doing in this line of code,the calculating the index which gives us the maximum value after we finish Computing the value of Q will justhave to update our Matrix. After that, we'll be updating the Q value and will be choosing a new initial State.Okay. So this is the update function that is defined over here. Okay. So I've just called the function over here.So guys this whole set of code will just calculate the Q value. Okay. This is exactly what we did in our examples after that.We have the training phase. So guys remember the more you train an algorithm the better it's going to learn.Okay so over here I have provided around 10,000 titrations. Okay. So my range is 10 thousand iterations meaningthat my age It will take 10,000 possible scenarios and in go to 10,000 titrations to find out the best policy.So you're exactly what I'm doing is I'm choosing the current state randomly after that. I'm choosing the available action from the current state.So either I can go to stage 3 or straight five then I'm calculating the next action and then I'm finally updating the valuein the Q Matrix and next. We just normalize the Q Matrix. So sometimes in our Q Matrix the value might exceed.Okay, let's say it. Heated to 500 600 so that time you want to normalize The Matrix. Okay, we want to bring it down a little bit.Okay, because larger numbers we won't be able to understand and computation would be very hard on larger numbers.That's why we perform normalization. You're taking your calculated value and you're dividing it with the maximum Q value in 200.All right, so you are normalizing it over here. So guys, this is the testing phase. Okay here you will just randomly set a current state and youwant given any other data because you've already trained our model. Okay, you're To give a Garden State thenyou're going to tell your agent that listen you're in room. Number one. Now. You need to go to room number five.Okay, so he has to figure out how to go to room number 5 because we have trained him now. All right. So here we have set the current state to oneand we need to make sure that it's not equal to 5 because 5 is the end goal. So guys this is the same Loop that we executed earlier.So we're going to do the same I trations again now if I run this entire code, let's look at the result.So our current state here we've chosen as one. Okay and And if we go back to our Matrix,you can see that there is a direct link from 1 to 5, which means that the route that the agent should take is one to five.Okay directly. You should go from 1 to 5 because it will get the maximum reward like that. Okay.Let's see if that's happening. So if I run this it should give me a direct path from 1 to 5.Okay, that's exactly what happened. So this is the selected path so directly from one to fiveit went and it calculated the entire Q Matrix. Works for me. So guys this is exactly how it works.Now. Let's try to set the initial stage as that's a to so if I set the initial stage as to and if I try to run the code,let's see the path that it gives so the selected path is 2 3 4 5 now chose this pathbecause it's giving us the maximum reward from this path. Okay. This is the Q Matrix that are calculatedand this is the selected path. All right, so guys with this we come to the end of this demo.So basically what we did was we just placed an agent in a room random room and we ask it to Traversethrough and reach to the end room, which is room number five. So basically we trained our agent and we made surethat it went through all the possible paths. to calculate the best path the for a robotand environment is a place where it has been put to use. Now. Remember this reward is itself the agent for examplean automobile Factory where a robot is used to move materials from one place to another now the task we discussed just nowhave a property in common. Now, these tasks involve and environment and expect the agent to learn from the environment.Now, this is where traditional machine learning phase and hence the need for reinforcement learning now,it is good to have Establish overview of the problem that is to be solved using the Q learningor the reinforcement learning. So it helps to define the main components of a reinforcement learning solution.That is the agent environment action rewards and States. So let's suppose we are to builda few autonomous robots for an automobile building Factory. Now, these robots will help the factory personalby conveying them the necessary parts that they would need in order to pull the car. Now these different parts are locatedat Nine different positions within the factory warehouse the car part include the chassisWheels dashboard the engine and so on and the factory workers have prioritized the locationthat contains the body or the chassis to be the topmost but they have provided the priorities for other locations as well,which will look into the moment. Now these locations within the factory look somewhat like this.So as you can see here, we have L1 L2 L3 all of these stations. Now one thing you might notice herethat there are little obstacle prison in between the locations. So L6 is the top priority locationthat contains the chassis for preparing the car bodies. Now the task is to enable the robotsso that they can find the shortest route from any given location to another location on their own.Now the agents in this case are the robots the environment is the automobile factory warehouse the let's talkabout the state's the states. Are the location in which a particular robot is presentin the particular instance of time which will denote it states the machines understand numbersrather than let us so let's map the location codes to number. So as you can see here, we have map location l 1 to this t 0 L 2 and 1and so on we have L8 as state 7 + L line at state.So next what we're going to talk about are the actions. So in our example, the action will be the direct location that a robot can.Call from a particular location, right consider a robot that is a tel to location and the Direct locationsto which it can move our L5 L1 and L3. Now the figure here may come in handy to visualize this nowas you might have already guessed the set of actions here is nothing but the set of all possible statesof the robot for each location the set of actions that a robot can take will be different.For example, the set of actions will change if the robot is. An L1 rather than L2. So if the robot is in L1,it can only go to L 4 and L 2 directly now that we are done with the states and the actions.Let's talk about the rewards. So the states are basically zero one two, three four and the actions are also 0 12 3 4 up till 8:00. Now, the rewards now will be given to a robot. If a locationwhich is the state is directly reachable from a particular location. So let's take an example suppose l Lane isdirectly reachable from L8. Right? So if a robot goes from LA to align and vice versa,it will be rewarded by one and if a location is not directly reachable from a particular equation.We do not give any reward a reward of 0 now the reward is just a number and nothing else it enables the robots to make senseof the movements helping them in deciding what locations are directly reachable and what are not now with this Q. Wecan construct a reward table which contains all the required. Use mapping between all possible States.So as you can see here in the table the positions which are marked green have a positive reward.And as you can see here, we have all the possible rewards that a robot can get by moving in between the different states.Now comes an interesting decision. Now remember that the factory administrator prioritized L6to be the topmost. So how do we incorporate this fact in the above table now, this is done by associating the topmost priority locationwith a very high reward. The usual ones so let's put 999 in the cell L 6 comma and six now the tableof rewards with a higher reward for the topmost location looks something like this. We have not formally defined all the vital componentsfor the solution. We are aiming for the problem discussed now, we will shift gears a bit and study someof the fundamental concepts that Prevail in the world of reinforcement learning and q-learning the first of all we'll startwith the Bellman equation now consider the following Square. Rooms, which is analogousto the actual environment from our original problem. But without the barriers now suppose a robot needs to goto the room marked in the green from its current position a using the specified Direction.Now, how can we enable the robot to do this programmatically one idea would be introduced some kind of a footprintwhich the robot will be able to follow now here a constant value is specified in each of the rooms,which will come along the robots way if it follows the directions by Fight about now in this wayif it starts at location a it will be able to scan through this constant value and will move accordinglybut this will only work if the direction is prefix and the robot always starts at the location a now consider the robot startsat this location rather than its previous one. Now the robot now sees Footprintsin two different directions. It is therefore unable to decide which way to go in order to get the destination which is the Green Room.It happens. Primarily because the robot does not have a way to remember the directions to proceed.So our job now is to enable the robot with a memory. Now, this is where the Bellman equation comes into play.So as you can see here, the main reason of the Bellman equation is to enable the reward with the memory.That's the thing we're going to use. So the equation goes something like this V of s gives maximum a r of s comma a plus gamma of vs -where s is a particular state Which is a room is the Action Moving between the rooms as -is the state to which the robot goes from s and gamma is the discount Factor now we'll get into it in a momentand obviously R of s comma a is a reward function which takes a state as an action a and outputs the reward now Vof s is the value of being in a particular state which is the footprint now we consider all the possible actionsand take the one that yields the maximum value. Now there is one constraint.However regarding the value footprint that is the room marked in the yellow just below the Green Room.It will always have the value of 1 to denote that is one of the nearest room adjacent to the green room.Now. This is also to ensure that a robot gets a reward when it goes from a yellow room to The Green Room.Let's see how to make sense of the equation which we have here. So let's assume a discount factor of 0.9as remember gamma is the discount value or the discount Factor. So let's Take a 0.9.Now for the room, which is Mark just below the one or the yellow room, which is the Aztec Mark for this room.What will be the V of s that is the value of being in a particular state? So for this V of s would be somethinglike maximum of a will take 0 which is the initial of our s comma.Hey plus 0.9 which is gamma into 1 that gives us zero point nine now here the robotwill not get any reward for Owing to a state marked in yellow hence the IR s comma a is 0 herebut the robot knows the value of being in the yellow room. Hence V of s Dash is one following thisfor the other states. We should get 0.9 then again, if we put 0.9 in this equation,we get 0.81 then zero point seven to nine and then we again reached the starting point.So this is how the table looks with some value Footprints computer. From the Bellman equation nowa couple of things to notice here is that the max function has the robot to always choose the statethat gives it the maximum value of being in that state now the discount Factor gamma notifies the robotabout how far it is from the destination. This is typically specified by the developer of the algorithm.That would be installed in the robot. Now, the other states can also be given their respective valuesin a similar way. So as you can see here the boxes Into the green one have one andif we move away from one we get 0.9 0.8 1 0 1 7 to 9.And finally we reach 0.66 now the robot now can precede its way through the Green Room utilizing these value Footprints eventif it's dropped at any arbitrary room in the given location now, if a robot Lance up in the highlighted Sky Blue Area,it will still find two options to choose from but eventually either of the parties.It's will be good enough for the robot to take because Auto V the value Footprints are not only that out.Now one thing to note is that the Bellman equation is one of the key equations in the world of reinforcement learning and Q learning.So if we think realistically our surroundings do not always work in the way we expect there is always a bitof stochastic City involved in it. So this applies to robot as well. Sometimes it might so happenthat the robots Machinery got corrupted. Sometimes the robot makes come across some hindrance on its waywhich may not be known to it beforehand. Right and sometimes even if the robot knowsthat it needs to take the right turn it will not so how do we introduce this to cast a cityin our case now here comes the Markov decision process now consider the robot is currently in the Red Roomand it needs to go to the green room. Now. Let's now consider the robot has a slight chanceof dysfunctioning and might take the left or the right or the bottom. On instead updating the upper turn in order to getto The Green Room from where it is now, which is the Red Room. Now the question is, how do we enable the robot to handle this when it is outin the given environment right. Now, this is a situation where the decision making regarding which turn is to be taken is partly randomand partly another control of the robot now partly random because we are not surewhen exactly the robot mind dysfunctional and partly under the control of the robot because it is still Making a decisionof taking a turn right on its own and with the help of the program embedded into it. So a Markov decision processis a discrete time stochastic Control process. It provides a mathematical framework for modelingdecision-making in situations where the outcomes are partly random and partly under control of the decision maker.Now we need to give this concept a mathematical shape most likely an equationwhich then can be taken further now you might be Price that we can do this with the helpof the Bellman equation with a few minor tweaks. So if we have a look at the original Bellman equation V of X is equal to maximumof our s comma a plus gamma V of s stash what needs to be changed in the above equationso that we can introduce some amount of Randomness here as long as we are not surewhen the robot might not take the expected turn. We are then also not sure in which room it might end upin which is nothing but the room it. Moves from its current room at this pointaccording to the equation. We are not sure of the S stash which is the next state or the room,but we do know all the probable turns the reward might take now in order to incorporate eachof this probabilities into the above equation. We need to associate a probability with eachof the turns to quantify the robot if it has got any experts it is chance of taking this turn nowif we do, so We get PS is equal to maximum of our s comma a plus gammainto summation of s - PS comma a comma s stash into V of his stash now the PS a--and a stash is the probability of moving from room s to establish with the action aand the submission here is the expectation of the situation that the robot in curse,which is the randomness now, let's take a look at this example here. So when We associate the probabilities to eachof these Stones. We essentially mean that there is an 80% chance that the robot will take the upper turn.Now, if you put all the required values in our equation, we get V of s is equal to maximum of our of s comma a +comma of 0.8 into V of room up plus 0.1into V of room down 0.03 into a room of V of from leftplus 0.03 into Vo Right now note that the value Footprints will not change due to the factthat we are incorporating stochastic Ali here. But this time we will not calculate those values Footprints instead.We will let the robot to figure it out. Now up until this point. We have not considered about rewarding the robotfor its action of going into a particular room. We are only watering the robot when it gets to the destination now,ideally there should be a reward for each action the robot takes to help it better as Assess the quality of the actions,but there was need not to be always be the same but it is much better than having some amountof reward for the actions than having no rewards at all. Right and this idea is known as the living penalty in reality.The reward system can be very complex and particularly modeling sparse rewards is an active areaof research in the domain of reinforcement learning. So by now we have got the equationwhich we have a so what? To do is now transition to Q learning. So this equation gives us the value of goingto a particular State taking the stochastic city of the environment into account. Now, we have also learned very briefly about the ideaof living penalty which deals with associating each move of the robot with a reward so Q learning processesand idea of assessing the quality of an action that is taken to move to a state rather thandetermining the possible value of the state which is being moved to So earlier we had 0.8into V of s 1 0.03 into V of S 2 0 point 1 into Vof S 3 and so on now if you incorporate the idea of assessing the qualityof the action for moving to a certain state so the environment with the agent and the quality of the action will look something like this.So instead of 0.8 V of s 1 will have q of s 1 comma a one will have qof S 2 comma 2 You of S3 not the robot now has four different states to choosefrom and along with that. There are four different actions also for the current state it is in sohow do we calculate Q of s comma a that is the cumulative quality of the possible actionsthe robot might take so let's break it down. Now from the equation V of s equals maximum a RS comma a +comma summation s - PSAs stash - into V of s -if we discard the maximum function we have is of a plus gamma into summation pand v now essentially in the equation that produces V of s we are considering all possible actionsand all possible States from the current state that the robot is in and then we are taking the maximum value causedby taking a certain action and the equation produces a value footprint,which is for just one possible action. In fact if we can think of it as the qualityof the action so Q of s comma a is equal to RS comma a plus gamma of summation p and v nowthat we have got an equation to quantify the quality of a particular action. We are going to make a little adjustmentin the equation we can now say that we of s is the maximum of all the possible valuesof Q of s comma a right. So let's utilize this fact and replace V of s Stash as a functionof Q so q s comma a becomes R of s comma a + comma of summation PSAs -and maximum of the que es - a - so the equationof V is now turned into an equation of Q, which is the quality.But why would we do that now? This is done to ease our calculations because now we have only one function Q,which is also the core of the Programming language. We have only one function Q to calculate an R of s commaa is a Quantified metric which produces reward of moving to a certain State.Now, the qualities of the actions are called The Q values and from now on we will refer to the value Footprintsas the Q values an important piece of the puzzle is the temporal difference. Now temporal difference is the componentthat will help the robot calculate the Q values which respect to the change. Changes in the environment over time.So consider our robot is currently in the mark State and it wants to move to the Upper State.One thing to note that here is that the robot already knows the Q value of making the actionthat is moving through the Upper State and we know that the environment is stochastic in nature and the rewardthat the robot will get after moving to the Upper State might be different from an earlier observation.So how do we capture this change the real difference? We calculate the new Q as My a with the same formulaand subtract the previous you known qsa from it. So this will in turn give us the new QA now the equationthat we just derived gifts the temporal difference in the Q values which further helps to capture the random changesin the environment which may impose now the new q s comma a is updated as the followingso Q T of s comma is equal to QT minus 1 s comma a plus Alpha TD.ET of a comma s now here Alpha is the learning rate which controlshow quickly the robot adapts to the random changes imposed by the environment the qts comma is the current state q valueand a QT minus 1 s comma is the previously recorded Q value. So if we replace the TDS comma a with its full form equation,we should get Q T of s comma is equal to QT - 1 of s comma y plus Alphainto our of S comma a plus gamma maximum of q s Dash a dash minus QTminus 1 s comma a now that we have all the little pieces of q line together.Let's move forward to its implementation part. Now, this is the final equation of q-learning, right?So, let's see how we can implement this and obtain the best path for any robot to take now to implement the algorithm.We need to understand the warehouse. Ian and how that can be mapped to different states.So let's start by reconnecting the sample environment. So as you can see here, we have L1 L2 L3 to align and as you can see here,we have certain borders also. So first of all, let's map each of the above locations in the warehousetwo numbers or the states so that it will ease our calculations, right? So what I'm going to do is create a new Python 3 filein the jupyter notebook and I'll name it as learning Numb, butokay, so let's define the states. But before that what we need to do is import numpybecause we're going to use numpy for this purpose and let's initialize the parameters.That is the gamma and Alpha parameters. So gamma is 0.75, which is the discount Factor whereas Alpha is 0.9,which is the learning rate. Now next what we're going to do is Define the states and mapit to numbers. So as I mentioned earlier l 1 is Zero and online. We have defined the states in the numerical form.Now. The next step is to define the actions which is as mentioned above represents the transitionto the next state. So as you can see here, we have an array of actions from 0 to 8.Now, what we're going to do is Define the reward table. So as you can see here is the same Matrixthat we created just now that I showed you just now now if you understood it correctly,there isn't any real Barrel limitation as depicted in the image, for example, the transitional for tell one is allowedbut the reward will be 0 to discourage that path or in tough situation. What we do is add a minus 1 thereso that it gets a negative reward. So in the above code snippet as you can see here,we took each of the It's and put once in the respective state that are directly reachable from the certain State.Now. If you refer to that reward table, once again, which we created the above or reconstruction will be easy to understandbut one thing to note here is that we did not consider the top priority location L6 yet.We would also need an inverse mapping from the state's back to its original locationand it will be cleaner when we reach to the other depths of the algorithms. So for that what we're going to do is Have the inversemap location state to location. We will take the distinct State and location and convert it back.Now. What will do is will not Define a function get optimal which is the get optimal route,which will have a start location and an N location.Don't worry the code is back. But I'll explain you each and every bit of the code. It's not the get optimal root function will take two argumentsthe starting location in the warehouse and the end location in the warehouse recipe lovely and it will return the optimal routefor reaching the end location from the starting location in the form of an ordered list containing the letters.So we'll start by defining the function by initializing the Q values to be all zeros.So as you can see here we have Even the Q value has to be 0 but before that what we need to do is copy the reward Matrix to a new one.So this the rewards new and next again, what we need to do is get the ending State correspondingto the ending location. And with this information automatically will set the priority of the given ending stay to the highest onethat we are not defining it now, but will automatically set the priority of the given ending State as nine nine nine.So what we're going to do is initialize the Q values to be 0 and in the Learning process what you can see here.We are taking I in range 1000 and we're going to pick up a state randomly.So we're going to use the MP dot random randint and for traversing through the neighbor locationin the same maze we're going to iterate through the new reward Matrix and get the actions which are greater than 0 and after thatwhat we're going to do is pick an action randomly from the list of the playable actions in years to the next statewill going to compute the temporal difference, which is TD, which is the rewards plus gamma into the queue of next stateand will take n p dot ARG Max of Q of next 8 minus Q of the current state.We going to then update the Q values using the Bellman equation as you can see here. We have the Bellman equationand we're going to update the Q values and after that we're going to initialize the optimal routewith a starting location now here we do not know what the next location yet.So initialize it with a value of the starting location, which Again is the random location.So we do not know about the exact number of iteration needed to reach to the final location.Hence while loop will be a good choice for the iteration. So when you're going to fetch the starting State fetchthe highest Q value penetrating to the starting State we go to the index or the next state,but we need the corresponding letter. So we're going to use that state to location function.We just mentioned there and after that we're going to update the starting location for the The next iterationand finally we'll return the root. So let's take the starting location of n lineand and location of L while and see what part do we actually get?So as you can see here we get Airline l8l 5 L2 and L1. And if you have a look at the image here,we have if we start from L9 to L1. We got L8 L5 L 2 l 1 l 8l v L2 L1that would He does the maximum value of the maximum reward for the robot.So now we have come to the end of this Q learning session and I hope you got to knowwhat exactly is Q learning with the analogy all the way starting from the number of rooms and I hope the example which I took the analogywhich I took was good enough for you to understand q-learning understand the Bellman equationhow to make quick changes to the Bellman equation and how to create the reward table the cue.Will and how to update the Q values using the Bellman equation, what does alpha do what does karma do?\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data FileA_7.txt'}, 'embedding': None, 'id': '14b52ebaac2575c690b1388fba8f5392'}>, <Document: {'content': \"DATA SCIENCE COURSE 3HOURS VIDEO\\nHello everyone and welcome to this interesting session on data science full course. So before we begin let's have a quick look at the agenda of thissession so first of all I'll be starting off by explaining you guys about the evolution of data how it led to the growth of data science, machine learning,AI and all the different aspects of data. Then we'll have a quick introduction to data science, understand what exactly it is then we'll move forward to the datascience careers and the salary and understand what are the different job profiles in the data science career path how to become a data scientist dataanalyst or a machine learning engineer. Then we'll move on to the first and the foremost part of data science which is statistics and after completing statisticswe'll move on to machine learning where we'll understand what exactly is machine learning what are the different types of machine learning and how are they usedand where are they used the different algorithms and next we'll understandwhat is deep learning and how deep learning is different from machine learning, what is the relationship between AI, machine learning and deeplearning in terms of data science and understand how exactly neural network works, how to create a neural network and much more, So let's begin our session now.Data is increasingly shaping the systems that we interact with every day, whether you are searching something on Google using Siri or browsing your Facebookfeed you are consuming the result of data analysis. It is increasing at a very alarming rate where we are generating 2.5 quintillion bytes of it every day.Now that's a lot of data and considering there are more than 3 billion Internet users in the world a quantity that has tripled in the last 12 yearsand 4.3 billion cell phone users that's a heck lot of data and this rapid growthhas generated opportunity for new professionals who can make sense out of this data. Now given its transformation ability it's no wonder that so many dataarrays with jobs have been created in the past few years like data analysts, data scientists, machine learning engineers, artificial intelligenceengineers and much more. And before we dwell into the details of all of these different professionals, let's understand exactlywhat data science is. So data science also known as the relevant science is aninterdisciplinary field about scientific methods, processes and systems to extractknowledge or insights from data in various forms. It's structured or unstructured. It is the study of where information comes from what itrepresents and how it can be turned into a valuable resource in the creation of business and IT strategies. So data science employs many techniques andtheories from fees like mathematics, statistics, information science as well as computer science, and can be applied to small data sets also yet most peoplethink data science is when you are dealing with big data or large amounts of data. So this brings the question which job profile is suitable for you, isit the data analysts, the data scientist or the machine learning engineer. Nowdata scientist has been called the sexiest java 21st century nonetheless data science is a hot and growing field so before we drill into the data sciencelet's discuss all of these profiles one by one and see what this roles are and howthey work in the industries so read a science career usually starts with mathematics and stats as the base which brings up the force profile in our datascience career path which is a data analyst so Idina analyst delivers value to the companies by taking information about specific topics and theninterpreting analyzing and presenting the finding in comprehensive reports now many different types of businesses use data analysts to help as experts dataanalysts are often called on to use the skills and tools provide competitive analysis and identify trends within the industry's most entry-level professionalinterested in going into Data related jobs start off as data analyst qualifying for this role is as simple as it gets all you need is a bachelor'sdegree in computer science mathematics and a good statistical knowledge strong technical skills would be a plus and can give you an edge over most otherapplicants so next we have data scientists there are several definitions available on data scientists but in simple wordsscientist is one who practices the art of data science the highly popular term data scientist was coined by DJ Patton and Jeff hammer backer data scientistsare those who crack complex data problems with a strong expertise in certain scientific disciplines they work with several elements related tomathematics statistics computer science and much more now data scientists are usually business analysts or data analysts with a difference it is aposition for specialists and you can specialize in different types of skills like speech analytics text analytics which is the natural language processingimage processing video processing medicine simulation material simulationnow each of these specialists roles are very limited in number and hence thevalue of such a specialist is immense now if we talk about AI or machine learning ingenious so machine learning engineers are sophisticated programmerswho develop machines and systems that can learn and apply knowledge without specific direction artificial intelligence is the goal of a machinelearning engineer they are computer programmers but their focus goes beyond specifically programming machines to perform specific tasks now they createprograms that will enable machines to take actions without being specifically directed to perform those tasks so now if we have a look at the salary trendsof all of these professionals so starting with a data analyst the average salary in the u.s. is around 83,000 dollars or it's almost close to eightyfour thousand dollars whereas in India it's around four lakh and four thousandrupees per annum. Now coming to data scientist the average salary is ninety one thousand dollars nine eleven point five thousand dollars and in India it isalmost seven lakh rupees and finally four ml in ten years the average salaryin the u.s. is around one hundred and eleven thousand dollars whereas in India is around seven lakh and twenty thousand dollars so as you can see the radiusscientist an ml ingenious position are a certain higher position which requires certain degree of expertise in that field so that's the reason why there isa difference in the salary of all the three professionals so if you have a look at the road map of becoming any one of these professionso what first one needs to do is own a bachelor's degree now this bachelor's degree can be in either computer science mathematicsinformation technology statistics finance or even economics now aftercompleting a bachelor's degree the next comes is fine-tuning the technical skills during the technical skills is one of the most important parts in theroadmap where you learn all the statistical methods and packages you either learn are Python essays languages which are very important you learn aboutdata warehousing business intelligence data cleaning visualization reporting techniques walking knowledge of Hadoop and MapReduce is very very important andif you talk about machine learning techniques it is one of the most important parts of the data science career now apart from these technicalskills there are also some business skills which are very much required so this involves analytical problem-solving effective communication creativethinking as well as industry knowledge now after fine-tuning your technical skills and developing all the business skills you have the options of eithergoing for a job or either going for a master's degree or certification programs now I might suggest as you go for a master's degree as just coming outof the BTech world and having the technical skills is not enough so you need to have a certain level of expertise in the field so it's better togo for any masters or PhD programs which are in computer science statistics ormachine learning you can also go for big data certifications and you can also go for industry certifications regarding the data analysis machine learning orthe data science it so happens that arica also provides a machine learning data analysis as well as a data science certification training they havemaster's program which are equivalent to a master's degree which you get from a certain University so do check it out guys I'll leave the link to all of thesein the description box below and after you have completed the master's degree what comes is working on the projects which are related to this field so it'sbetter if you work on machine learning deep learning or data ethics projects that will give you an edge over other competitors while applying for a jobscenario so a certain level of expertise in the field is also required and thisis how you will succeed in the rate of science career path there are certain skills which are required which I was talking about earlier the technicalskills and the non technical skills now if you talk about the skills which are required to become all of these professions so they are mostly the sameso for any data analyst first of all you need to have analytical skills which involves Maths having good knowledge of matrix multiplications the Fouriertransformations and all next we have communication skills so come looking for a data analyst require someone who has the good communication skills who canexplain all of their technical terms to non-technical teams such as marketing or the sales team another important skill required is critical thinking you needto think in certain directions and gain insights from the data so that's one of the most important part of a data analysts job obviously you need to payattention on the details so as a minor shift or the deviation in the result or in the calculation what you say the analysis might result in some sort ofloss of the company it's not necessarily to create a loss but it's better to avoid any kind of deviation from the results so paying attention to thedetail is very very important and then again we talk about the mathematical skills knowing about all the types of differentiations and integrations isgoing to help a lot because you know a lot of machine learning algorithms as I would say are mostly mathematical terms or mathematical functions so having goodknowledge of mathematics is also required apart from this the usage of technical tools such as Python are we have essays you need to know about thebig data ecosystem how it works the HDFS how to extract data create a pipeline you know about JavaScript a little and if you talk about the skills of datascientist it's almost the same having analytical and statistics knowledge now another important part here is to know the machine learning algorithms as itplays an important role in the data science career from solving skills obviously now another important aspect if you talk about the skill whichdiffers from that of a data analyst is only deep learning so deep learning I'lltalk about deep learning later in the second half or the later part of the video so having a good knowledge of deep learning and the various frameworks suchas tensorflow PI torch you have piano all of this is very required for data scientists and again business communication as Imentioned earlier is very much required because as you know these are one of the technical roles most technical roles in the industries and the output of theseroles or what I would say the output of what these professions - is not that much technical is more business oriented so they have to explain all of thesefindings to either the non-technical teams the sales the marketing and againyou need the technical tools and the skills now for machine learning engineer obviously programming languages having good knowledge of our Python C++ or Javait's very much required you need to know about calculus and statistics as I mentioned earlier learning about mattresses integration now anotherimportant skill here is signal processing so a lot of times machine learning engineers have to work on robots and signal processing they workon human-like robots they work on robotics which mimic human behavior so alot of signal processing techniques are also required in this field applied mathematics as I mentioned earlier and again neural networks it is one of thebase of artificial intelligence which is being used and again we have natural language processing so as you know we have personal assistants like Siri andCortana and they work on language processing and not just language processing you have audio processing as well as video processing so that theycan interact with a real environment and provide a certain answer to a particular question so these were the skills I would say for all of these three rolesnext if we have a look at the peripherals of data science so first ofall we have statistics needless to say there are programming languages we have short read integrations then we have machine learning which is a big part ofdata science and then again we have big data so let's start with statisticswhich is the first area of data science or I should say the first milestone which we should cover so for statistics let's understand firstwhat exactly is data so data in general terms refers to facts and statistics collected together for reference or analysis when working withstatistic it's important to recognize the different types of data so data can be broadly classified into numerical categorical and ordinal now data with noinherent order or ranking such as a gender or race is called nominal data soas you can see in the type 1 we have male female male female that is nominal data now data with an ordered series iscalled ordinal data so as you can see here we have an ordered series where we have the customer IDs and the rating scale no data with only two optionsseries is called binary data now in this type of data there are only two options like either yes or no or true or false or 1 or 0 so as you can see here we havecustomer ID and in the owner or car column we have either yes or no now thetypes of data we just discussed under law describe the quality of something in size appearance value or something such kind of data is broadly classified intoqualitative data now data which can be categorized into a classification datawhich is based upon counts there is only a finite number of values possible and the values cannot be subdivided meaningfully is called discrete data soas you can see here in our example we have organization and the number of products so this cannot be subdivided into number of sub products right and ifyou talk about data which can be measured on a continuum or a scale no data which can have almost any numeric value and can be subdivided into finerand finer increments is called continuous data so as you can see here in patient ID we have weight of the patient it is 6.5 kgs now kgs can besubdivided into grams and milligrams and final refinement is also possible nowthis type of data that can be measured by the quantity of something rather than its quality is called quantitative data now that we have honest with thedifferent types of data qualitative and quantitative it's time to understand the types of variables we have now there are majorly two types of variables dependentand independent variables so if you want to know whether caffeine affects your appetite the presence or the absence of the amount ofcaffeine would be the independent variable and how hungry you are would be the dependent variables so in statistics dependent variable is the outcome of anexperiment as you change the independent variable you watched what happens to the dependent variable whereas if you talk about independent variable a variablethat is not affected by anything that you or the researcher does usually plotted on the x-axis now the next step after knowing aboutthe datatypes and the variables is to know about population and sampling and that comes into experimental research now in experimental research the aim isto manipulate an independent variable and then examine the effect that this change has on a dependent variable now since it is possible to manipulate theindependent variable experimental research has the advantage of enabling a researcher to identify a cause and effect between the variables wellsuppose there are 100 volunteers at the hospital and a doctor needs to check the working of a particular medicine which has been cleared by the government sothe doctor divides those hundred patients into two groups of 50 and then asked one group to take one type of medicine and the other group to not takeany medicine at all and then after of me then compare the results and in non experimental research the researcher does not manipulate the independentvariable this is not to say that it is impossible to do so but it will either be impractical or it will be unethical to do so so for example a researcher maybe interested in the effect of illegal recreational drug views which is the independent variable on certain types of behavior which is the dependent variablehowever why is possible it would be unethical to ask an individual to takeillegal drugs in order to study what effects this hat on certain behaviors itis always good to go for experimental research rather than non experimental research so next in our session we have population and sampling those are two ofthe most important terms in statistics so let's understand these terms so instatistic the term population is the entire pool from which a sample is drawn statistician also speak of a population of objects or events or procedures orobservation including such things as the quantity of the number of vehicle owned by a penny person now population is thus anaggregate of creatures things cases and so on and a population commonly containstoo many individuals to study conveniently an investigation is often restricted to one or most samples drawn from it now a world chosen sample willcontain most of the information about a particular population parameter but the relationship between the sample and the population must be such as to allow trueinferences to be made about a population from that sample for that we have different types of sampling techniques so in probabilities there are samplingmethods which are classified either as probability or non probability so in probability sampling each member of the population has a known nonzeroprobability of being selected probably the methods include random sampling systematic sampling and stratified sampling whereas in nonprobabilitysampling members are selected from a population in some non-random manner butthese includes convenience sampling judgement sampling quota sampling and snowball sampling while sampling is important there is another term which isknown as sampling error so sampling error is a degree to which a sample might differ from the population when inferring to a population results arereported plus or minus the sampling error now in probability sampling there are three terms which are random sampling systematic sampling andstratified sampling so talking about random sampling probability of each member of the population to be chosen has equal chance of being selected suchtype of sampling is random sampling never talk about systematic sampling it is often used instead of random sampling and it is also called the NEP nameselection technique now pay attention to the name called Anette name so after therequired sample size has been calculated every NS record is selected from the list of the population member now it's only advantage over Anna's havingtechnique is its simplicity now the final type of sampling is a stratified sampling so a stratum is a subset of the population that shares at least onecommon characteristics the researcher first hand you fires irrelevant stratums and there actual representations in the populationbefore analysis so now that we know how our data is and what kind of sampling is done let's have a look at the measure of center which helps describe to whatextent this pattern holds for a specific numerical value so as you can see in measure of center we have three terms which are the mean median and mode andI'm sure everyone must be aware of all of these terms I'll not get into the details of these terms what's more important is to knowabout the measure of spreads now a measure of spread sometime called a measure of dispersion is used to describe the variability in the sampleor population it is usually used in conjunction with a measure of Center tendency such as the mean or median provide an overall description of a setof data now if you talk about deviation it is the difference between each X Iand the mean for a sample population which is known as the deviation about the mean whereas variance is based on deviation and entails computing squaresof deviation so as you can see here we have the formula for the variance whichis the difference between the mean and the particular data point squared and divided by the total number of data points and it's summation standarddeviation is basically the under root of variance so as you can see the formula is the same just we have the under root over the variance so that was standevasion and variance another topic in probability and statistics is kunis soskewness is a measure of symmetry or more precisely the lack of symmetry soas you can see here we have left skewed symmetric non symmetric left skewed we have right skewed so normally distributed curves are the mostsymmetric curves we'll talk about normal distribution later so after skewness what we need to know about is the confusion matrix nowconfusion matrix represent a tabular representation of actual versus the predicted values now this help us find the accuracy of the model when we arecreating any machine learning or the team learning model to find the accuracy what we do is plot a confusion matrix so what you need to do is you can calculatethe accuracy of your model with adding the true positives and the true negative and dividing it with the true positives plus true negatives plus false positiveplus false negatives that will give you the accuracy of the model so as you can see in the image we have good bad for predicted as well as actual and as youcan see here the true positive D and the true negative a are the two areas wherewe have created it it was good and the actual value was good in true negative awe have the predicted it was bad and the actually it's bad so model which getsthe higher true positive and true negatives are the ones which have the higher accuracy so that's what confusion matrix are for now the next term and avery important term in statistics is probability so probability is the measure of how likely something will occur it is the ratio of desiredoutcomes to the total outcomes now if I roll a dice there are six total possibilities one two three four five and sixnow each possibility has one outcome so each has a probability of one out of sixnow for instance the probability of getting a number two is one out of six since there is only a single two on the dice now when talking about theprobability distribution techniques or the terminologies there are three possible terms which are the probability density function normal distribution andthe central limit theorem so probability density function it is the equation describing a continuous probability distribution so it is usually referredas PDF now if we talk about normal distribution so the normal distribution is a probability distribution that associates the normal random variable Xwith a cumulative probability the normal distribution is defined by the following equation so as you can see here Y is 1 by Sigma into the square root of 2 pi 2whole multiplied by E raised to power minus X minus mu whole square divided by 2 Sigma square where X is a random normal variable mu is the mean and Sigmais the standard deviation now the central limit theorem states that the sampling distribution of the mean of any independent random variable will benormal or nearly normal if the sample size is large enough now accuracy or theresemblance to normal distribution depends on however two factors the first one is a number of sample points taken and second is the shape of theunderlying population now enough about statistics if you want to know more about statistics and if you want to get in-depth knowledge over statistics youcan refer to our statistics for data science video I'll leave the link to that video in the description box so that video talks about statistics andprobability in a more depth movie then I explained here so I will talk about thep-value is the hypotheses what all are required or any data science project solet's move on to our next part of data science learning which is learning paths which is the machine learning so let's understand what exactly is machinelearning so machine learning is an application of artificial intelligence that provides systems the ability to automatically learn and improve fromexperience without being explicitly programmed now getting computers toprogram themselves and also teaching them to make decisions using data wherewriting software is a bottleneck let the data do the work instead now machine learning is a class of algorithms which is data driven that is unlike normalalgorithms it is the data that does what the good answer is so if we have a lookat the various features of machine learning so first of all it uses the data to detect patterns in a data set and adjust the program actionsaccordingly it focuses on the development of computer programs that can teach themselves to grow and change when exposed to new data so it's notjust the old data on which it has been trained so whenever a new data is entered the program changes accordingly it enables computers to find hiddeninsights using iterative algorithms without being explicitly programmed either so machine learning is a method of data analysis that automatesanalytical model building now let's understand how exactly it Wells so if we have a look at the diagram which is given here we have traditionalprogramming on one side we have machine learning on the other so first of all in traditional program what we used to do was provide the data provide the programand the computer used to generate the output so things have changed now so in machine learning what we do is provide the data and we provide a predictedoutput to the machine now what the machine does is learns from the data find hidden insights and creates a model now it takes the output data also againand it reiterates and trains and grows accordingly so that the model getsbetter every time it's a strain with the new data or the new output so the first and the foremost application of machine learning in the industry I would like toget your attention towards is the navigation or the Google Maps so GoogleMaps is probably the app we use whenever we go out and require assistant in directions and traffic right the other day I was traveling to another city andtook the expressway and the math suggested despite the havoc traffic you are on the fastest route no but how does it know that well it's a combination ofpeople currently using the services the historic data of that fruit collected over time and a few tricks acquired from the other companies everyone using mapsis providing their location their average speed the route in which they are traveling which in turn helps Google collect massive data about the trafficwhich may extemporary the upcoming traffic and it adjust your route according to it which is pretty amazing right now coming to the secondapplication which is the social media if we talk about Facebook so one of the most common application is automatic friend tanks suggestion in Facebook andI'm sure you might have gotten this so it's present in all the other social media platform as well so Facebook uses face detection and image recognition toautomatically find the face of the person which matches its database and hence it suggests us to tag that person based on deep facenow if the face is Facebook's machine learning project which is responsible for recognition of faces and define which person is in the picture and italso provides alternative tags to the images already uploading on Facebook so for example if we have a look at this image and we introspect the followingimage on Facebook we get the alt tag which has a particular description so inour case what we get here is the image may contain sky grass outdoor and naturenow transportation and commuting is another industry where machine learning is used heavily so if you have used an app to book a cab recently then you arealready using machine learning to an extent and what happens is that it provides a personalized application which is unique to you it automaticallydetects your location and provides option to either go home or office or any other frequent basis based on your history and patterns it uses machinelearning algorithm layered on top of historic trip date had to make more accurate ETA predictions now uber with the implementation of machine learningon their app and their website saw a 26 percent accuracy in delivery and pick upthat's a huge a point now coming to the virtual person assistant as a namesuggests virtual person assistant assist in finding useful information when asked why a voice or text if you have the major applications of machine learninghere a speech recognition speech to text conversion natural language processing and text-to-speech conversion all you need to do is ask a simple question likewhat is my schedule for tomorrow or show my upcoming flights now for answeringyour personal assistant searches for information or recalls your related queries to collect the information recently personal assistants are beingused in chat pods which are being implemented in various food ordering apps online training web sites and also in commuting apps as well again productrecommendation now this is one of the area where machine learning is absolutely necessary and it was one of the few areas which emerged the need formachine learning now suppose you check an item on Amazon but you do not buy it then and there but the next day you are watching videos on YouTube and suddenlyyou see an ad for the same item you switch to Facebook there also you see the same ad and again you go back to any other side and you see the ad for thesame sort of items so how does this happen well this happens because Google tracks your search history and recommends asked based on your searchhistory this is one of the coolest application of machine learning and in fact 35% of Amazon's revenue is generated by the products recommendationnow coming to the cool and highly technological side of machine learning we have self-driving cars if we talk about self-driving car it's hereand people are already using it now machine learning plays a very important role in self-driving cars as I'm sure you guys might have heard about Teslathe leader in this business and the excurrent artificial intelligence is driven by the hardware manufacturer Nvidia which is based on unsupervisedlearning algorithm which is a type of machine learning algorithm now in media state that they did not train their model to detect people or any of theobjects as such the model works on deep learning and Traut sources it's datafrom the other vehicles and drivers it uses a lot of sensors which are a partof IOT and according to the data gathered by McKenzie the automotive data will hold a tremendous value of 750 billion dollars but that's a lot ofdollars we are talking about it now next again we have Google Translate now remember the time when you travel to the new place and you find it difficult tocommunicate with the locals or finding local spots where everything is written in a different languages well those days are goneGoogle's G and M T which is the Google neural machine translation is a neuralmachine learning that works on thousands of languages and dictionary it uses natural language processing to provide the most accurate translation of anysentence of words since the tone of the word also matters it uses other techniques like POS tagging named entity recognition and chunking and it is oneof the most used applications of machine learning now if we talk about dynamic pricing setting the rice price for a good or a service is an old problem ineconomic theory there are a vast amount of pricing strategies that depend on the objective sort be it a movie ticket a plane ticket or a cafe everything isdynamically priced now in recent year machine learning has enabled pricing solution to track buying trends and determine more competitive productprices now if we talk about uber how does Oberer determine the price of your right who was biggest use of machine learningcomes in the form of surge pricing a machine learning model named as geosearch if you are getting late for a meeting and you need to book an uber ina crowded area get ready to pay twice the normal fear even for flats if you're traveling in the festive season the chances are thatprices will be twice as much as the original price now coming to the final application of machine learning we have is the online video streaming we haveNetflix Hulu and Amazon Prime video now here I'm going to explain theapplication using the Netflix example so with over 100 million subscribers thereis no doubt that Netflix is the daddy of the online streaming world when NetflixPD dries has all the movie industrialists taken aback forcing themto us how on earth could one single website take on Hollywood now the answeris machine learning the Netflix algorithm constantly gathers massive amounts of data about user activities like when you pause rewind fast-forwardwhat do you want the content TV shows on weekdays movies on weekend the date youwatch the time you watch whenever you pause and leave a content so that if you ever come back they would such as the same video the rating events which areabout four million per day the searches which are about three million per day the browsing and the scrolling behavior and a lot more now they collect thisdata for each subscriber they have and use the recommender system and a lot ofmachine learning applications and that is why they have such a huge customer retention rate so I hope these applications are enough for you tounderstand how exactly machine learning is changing the way we are interacting with the society and how fast it is affecting the world in which we live inso if you have a look at the market trend of the machine learning here so as you can see initially it wasn't much in the market but if you have a look at the2016 side there was an enormous growth in machine learning and this happenedmostly because you know earlier we had the idea of machine learning but then again we did not had the amount of big data so as you can see the red line wehave here in the histogram and the power plot is that of the Big Data so Big Data also increased during the years and which led to the increase in the amountof data generated and recently we had that power or I should say theunderlying technology and the hardware to support that power that makes uscreate machine learning programs that will work on the spectator so that iswhy you see very high inclination during the 2016 period time as compared to 2012so because during 2016 we got new hardware and we were able to find insights using those hardware and program and create models which wouldwork on heavy data now let's have a look at the life cycle of machine learningso a typical machine learning life cycle has six steps so the first step iscollecting data second is video wrangling then we have the third stepper be analyzed the data fourth step where we train the algorithm the fifth step is when we test the algorithm and the sixth step is when we deploy thatparticular algorithm for industrial uses so when we talk about the fourth stepwhich is collecting data so here data is being collected from various sources and this stage involves the collection of all the relevant data from varioussources now if we talk about data wrangling so data wrangling is the process of cleaning and converting raw data into a format that allowsconvenient consumption now this is a very important part in the machine learning lifecycle as it's not every time that we receive a data which isclean and is in a proper format sometimes their value is missing sometimes there are wrong values sometimes data format is different so amajor part in a machinery lifecycle goes in data wrangling and data cleaning soif we talk about the next step which is data analysis so data is analyzed toselect and filter the data required to prepare the model so in this step wetake the data use machine learning algorithms to create a particular modelnow next again when we have a model what we do is strain the model now here weuse the data sets and the algorithm is trained on between data set through which algorithm understand the pattern and the rules which govern theparticular data once we have trained the algorithm next comes testing so the testing data set determines the accuracy of ourmodels so what we do is provide the test dataset to the model and which tells us the accuracy of the particular modelwhether it's 60% 70% 80% depending upon the requirement of the company andfinally we have the operation and optimization so if the speed andaccuracy of the model is acceptable then that moral should be deployed in the real system the model that is used in the production should be made with allthe available data models improve with the amount of available data used to create them all the result of the moral needs to be incorporated in the businessstrategy now after the model is deployed based upon its performance the model is updated and improved if there is a dip in the performance the moral isretrained so all of these happen in the operation and optimization stage nowbefore we move forward since machine learning is mostly done in Python and us so and if we have a look at the difference between Python and our I'mpretty sure most of the people would go for Python and the major reason whypeople go for python is because python has more number of libraries and python is being used in just more than data analysis and machine learning so some ofthe important Python libraries here which I want to discuss here so first of all I'll talk about matplotlib now what Matt brought lib does is that it enablesyou to make bar charts scatter plots the line charts histogram basically what itdoes is helps in the visualization aspect as data analyst and machine learning ingenious what one needs to represent the data in such a format thatit is used that it can be understood by non-technical people such as people frommarketing people from sales and other departments as well so another importantPython library here we have a seaborne which is focused on the visuals of statistical models which includes heat maps and depict the overalldistributions sometimes people work on data which are more geographically aligned and I would say in those cases he traps are verymuch required now next we come to scikit-learn and scikit-learn is the one of the most famous libraries of python i would sayit's simple and efficient or data mining and for data analysis it is built onnumpy and my rock lab and it is open-source next on our list we have pandas it is the perfect tool for data wrangling which is designed for quickand easy data manipulation aggregation and visualization and finally we havenumpy now numpy stands for a numerical Python provides an abundance of usefulfeatures for operation on n arrays which has an umpire's and matrices in spiteand mostly it is used for mathematical purposes so which gives a plus point to any machine learning algorithm so as these were the important part in larry'swhich one must know in order to do any price and programming for machinelearning or as such if you are doing Python programming you need to know about all of these libraries so guys next what we are going to discuss othertypes of machine learning so then again we have three types of machine learning which are supervised reinforcement and unsupervised machine learning so if wetalk about supervised machine learning so supervised learning is where you have the input variable X and the output variable Y and you use an algo I know tolearn the mapping function from the input to the output so if we take the case of object detection here so or face detection I rather say so first of allwhat we do is input the raw data in the form of labelled faces and again it'snot necessary that we just input faces to train the model what we do is input amixture of faces and non-faces images so as you can see here we have labeled faceand labeled on faces what we do is provide the data to the algorithm the algorithm creates a model it uses the training dataset to understand whatexactly is in a face what exactly is in a picture which is not a face and afterthe model is done with the training and processing so to test it what we do is provide particular input of a face or an on face what we know see the major partof supervised learning here is that we exactly know the output so when we areproviding a face we our selves know that it's a phase so to test that particular model and get the accuracy we use the labeled input rawdata so next when we talk about unsupervised learning unsupervised learning is the training of a model using information that is neitherclassified nor labeled now this model can be used to cluster the input data inclasses or the basis of the statistical properties for example for a basket fullof vegetables we can cluster different vegetables based upon their color or sizes so if I have a look at this particular example here we have what weare doing is we are inputting the raw data which can be either apple banana or mango what we don't have here which was previously there in supervised learningare the labels so what the algorithm does is that it visually gets the features of a particular set of data it makes clusters so what will happen isthat it will make a cluster of red looking fruits which are Apple yellow local fruits which are banana and based upon the shape also it determines whatexactly the fruit is and categorizes it as mango banana or apple so this isunsupervised learning now the third type of learning which we have here is reinforcement learning so reinforcement learning is the learning by interactingwith a space or an environment it selects the action on the basis of its past experience the exploration and also by new choices a reinforcement learningagent learns from the consequences of its action rather than from being taughtexplicitly so if we have a look at the example here the input data we have whatit does is goes to the training goes to the agent where the agent selects thealgorithm it takes the best action from the environment gets the reward and themodel is strange so if you provide a picture of a green apple although the Apple which it particularly nose is red what it will do is it willtry to get an answer and with the past experience what it has and it willrecreate the algorithm and then finally provide an output which is according to our requirements so now these were the major types of machine learningalgorithms next what we never do is dig deep into all of these types of machinelearning one by one so let's get started with supervised learning first and understand what exactly is supervised learning and what are the differentalgorithms inside it how it works the algorithms the working and we'll have alook at the various algorithm demos now which will make you understand it in a much better way so let's go ahead and understand what exactly is supervisedlearning so supervised learning is where you have the input variable X and theoutput variable Y and using algorithm to learn the mapping function from the input to the output as I mentioned earlier with the example of facedetection so it is cos subbu is learning because the process of an algorithm learning from the training data set can be thought of as a teacher supervisingthe learning process so if we have a look at the supervised learning steps orwhat will rather say the workflow so the model is used as you can see here wehave the historic data then we again we have the random sampling we split the data enter training error set and the testing data set using the training dataset we with the help of machine learning which is supervised machine learning we create statistical model now after we have a model which is being generatedwith the help of the training data set what we do is use the testing data set for prediction and testing what we do is get the output and finally if we havethe model validation outcome that was third training and testing so if we have a look at the prediction part of any particular supervised learning algorithmso the model is used for operating outcome of a new data set so whenever performance of the model degraded the model is retrained or if there are anyperformance issues the model is retrained with the help of the new data now when we talk about supervisor in there are not just one butquite a few algorithms here so we have linear regression logistic regression this is entry we have random forest we have made biased classifiers so linearregression is used to estimate real values for the cost of houses the number of cars the total sales based on the continuousvariable so that is what Rainier generation is now when we talk about logistic regression it is used to estimate discrete values for examplewhich are binary values like zero and one yes or no true and false based on the given set of independent way so for example when you are talking aboutsomething like the chance of winning or if we talk about winning which can be the true or false if will it rain today which it can be the yes or no so itcannot be like when the output of a particular algorithm or the particular question is either yes/no or binary then only we use a logic regression now nextwe have decision trees so so these are used for classification problems it works for both categorical and continuous dependent variables and if wetalk over random forest so random forest is an N symbol of a decision tree itgives better prediction and accuracy that decision tree so that is another type of supervised learning algorithm and finally we have the Nate Byarsclassifier so it is a classification technique based on the based theorem with an assumption of independence between predictors so we'll get moreinto the details of all of these algorithms one by one so let's get started with linear regression so first of all let us understand what exactlylinear regression is so linear regression analysis is a powerful technique you operating the unknown value of a variable which is thedependent variable from the known value of another variable which is the independent variable so a dependent variable is the variable to be predictedor explained in a regression model whereas an independent variable is a variable related to the dependent variable in a regression equation so ifyou have a look here as a simple linear regression so it's basically equivalent to a simple line which is with a slope which is y equals a plus B X where Y isthe dependent variable a is the y-intercept we have P which is the slopeof the line and X which is the independent variable so intercept is the value of the dependent variable Y when the value ofthe independent variable X is 0 it is the the line cuts the y-axis whereas slope is the change in the dependent variablefor a unit increase in the independent variable it is the tangent of the angle made by the line with the x-axis now when we talk about the relation betweenthe variables we have a particular term which is known as correlation so correlation is an important factor to check the dependencies when there aremultiple variables what it does is it gives us an insight of the mutual relationship among variables and it is used for creating a correlation plotwith the help of the Seabourn library which I mentioned earlier which is one of the most important libraries in Python so correlation is very importantterm to know about now if we talk about regression lines so linear regression analysis is a powerful technique used for predicting the unknown value of avariable which is the dependent variable from the regression line which is simply a single line that best fits the data in terms of having the smallest overalldistance from the line to the points so as you can see in the plot here we havethe different points or the data points so these are known as the fitted points then again we have the regression line which has the smallest overall distancefrom the line to the points so you have a look at the distance between the pointto the regression line so what this line shows is the deviation from theregression line so exactly how far the point is from the regression line solet's understand a simple use case of linear regression with the help of a demo so first of all there is a real state company use case which I'm goingto talk about so first of all here we have John he has some baseline for pricing the villa's and the independent houses he has in Boston so here we havethe data set description which we're going to use so this data set has different columns such as the crime rate per capita which is CRI M it hasproportional residential residential land zone for the Lots proportion of nonretail business the river the United Rock side concentration average number of rooms and the proportion of the owner occupying the built prior to 1940 thedistance of the five Boston employment centers in excess of accessibility to Riedl highways and much more so first of alllet's have a look at the data set we have here so one number I don't thing here guys is that I'm gonna be using Jupiter notebookto execute all my practicals you are free to use the spider notebook or theconsole either so it basically comes down to your preference so for mypreference I'm going to use the Jupiter notebook so for this use case we're gonna use the Boston housing data set so as you can see here we have the data setwhich has the CRI mzn in desc CAS NO x the different variables and we have thedata set of form almost I would say like 500 houses so what John needs to do isplan the pricing of the housing depending upon all of these differentvariables so that it's profitable for him to sell the house and it's easier for the customers also to buy the house so first of all let me open the codehere for you so first of all what we're gonna do is import the library is necessary for this project so we're going to use the numpy we're going toimport numpy as NP import pandas at PD then we're gonna also import thematplotlib and then we are going to do is read the Boston housing data set intothe BOS one variable so now what we are going to do is create two variables xand y so what we're gonna do is take 0 to 13 I'll say is from CR I am two LSdat in 1x because that's the independent variable and Y here is dependentvariable which is the MA TV which is the final price so first of all what we need to do is plot a correlation so what we're gonna do is import the Seabournlibrary as SN s we're going to use the correlations to plot the correlationsbetween the different 0 to 13 variables what we gonna do is also use ma DV herealso so what we're going to do is SN s dot heatmap correlations to be going to use the square to differentiate usually it comes up insquare only or circles so you don't know so we're gonna use square you want tosee you see map with the Y as GNP you this is the color so there's no rotationin the y axis and we're gonna rotate the excesses to the 90 degree and let's wegonna plot it now so this is what the plot looks like so as you can see here the more thicker or the more darker the color gets the more is the correlationbetween the variables so for example if you have a look at CRI M and M a DVright so as you can see here the color is very less where the correlation is very low so one thing important what we can see here is the tax and our ad whichis the full value of the property and RIT is the index of accessibility to theradial highways now these things are highly correlated and that is naturalbecause the more it is connected to the highway and more closer it is to the highway the more easier it is for people to travel and hence the tax on it ismore as it is closer to the highways now what we're going to do is from SQL and dot cross-validation we're going to import the Train test split and we'regonna split the data set now so what we are going to do is create four variables which are the extreme X test Y train white tests and we're going to use atrain test split function to split the x and y and here we're going to use the test size 0.3 tree which will split the data set into the test size will be 33%well as the training size will be 67% now this is dependent on you usually itis either 60/40 70/30 this depends on your use case your data you have thekind of output you are getting the model you are creating and much more then again from SQL learn dot linear model we're going to import linear regressionnow this is the major functions we're gonna use just linear regression function which is present in SQL which is a scikit-learn so we going to createour linear regression model into LM and the model which are going to create and we're going to fit the training videos which has the X train and the why trainthen we're gonna create a prediction underscore 5 which is the LM dot credit and I take the X test variables which will provide the predicted Y variablesso now finally if we plot the scatter plot of the Y test and the y predicted what we can see is that and we give the X label as white test and the Y labelhas y predicted we can see the regression line which we have plotted in at the scatter plot and if you want to draw a regression line it's usually itwill go through all of these points excluding the extremities which are here present at the endpoints so this is how a normal linear regression works inPython what you do is create a correlation you find out you split the dataset into training and testing variables then again you define what isgoing to be your test size import the reintegration moral use the training data set into the model fitted use the test data set to create the predictionsand then use the wireless code test and the predicted Y and plot the scatterplot and see how close your model is doing with the original data it had andcheck the accuracy of that model now typically you use these steps which wascollecting data what we did data wrangling analyze the data we trained the algorithm we use the test algorithm and then we deployed so fitting a modelmeans that you are making your algorithm learn the relationship between predictors and the outcomes so that you can predict the future values of theoutcome so the best fitted model has a specific set of parameters which bestdefines the problem at hand since this is a linear model with the equation y equals MX plus C so in this case the parameters of the model learns from thedata that are M and C so this is what more fitting now if it have a look atthe types of fitting which are available so first of all machine learning algorithm first attempt to solve the problem of underfittingthat is of taking a line that does not approximate the data well and making itapproximate to the data better so machine does not know where to stop in order to solve the problem and it can go ahead from appropriate to overfit moresometimes when we say a model overfits we mean that it may have a low errorrate for training data but it may not generalize well to the overall population of the data we are interested in so we have under fact appropriate andover fit these are the types of fitting now guys this was linear regression which is a type of supervised learning algorithm in machine learning so nextwhat we're going to do is understand the need for logistic regression so let'sconsider a use case as in political elections are being contested in our country and suppose that we are interested to know which candidate willprobably win now the outcome variables result in binary either win or lose thepredictor variables are the amount of money spent the age the popularity rank and etc etcetera now here the best fit line in the regression war is goingbelow 0 and above what and since the value of y will be discrete that is between 0 & 1 the linear rain has to be clipped at 0 & 1 now linear regressiongives us only a single line to classify the output with linear regression ourresulting curve cannot be formulated into a single formula as you obtain three different straight lines what we need is a new way to solve this problemso hence people came up with logistic regression so let's understand whatexactly is logic regression so logistic regression is a statistical method foranalyzing a data set in which there are 1 or more independent variables that determine an outcome and the outcome is a binary class type so example a patientgoes a followed a teen checkup in the hospital and his interest is to know whether the cancer is benign or malignant now a patient's data such assugar level blood pressure eight skin width and the previous medical history are recorded and a daughter checks the patient data and it reminds the outcomeof his illness and severity of illness the outcome will result in binary thatis zero if the cancer is malignant and one if it's been I know no strictregression is a statistical method used for analyzing a dataset there were say one or more dependent variables like we discuss like the sugar level bloodpressure skin with the previous medical history and the output is binary class type so now let's have a look at the lowest ikregression curve now the law disintegration code is also called a sigmoid curve or the S curve the sigmoid function converts any value from minusinfinity to infinity to the discrete value 0 or 1 now how to decide whether the value is 0 or 1 from this curve so let's take an example what we do isprovide a threshold value we set it we decide the output from that function so let's take an example with the threshold value of 0.4 so any value above 0.4 willbe rounded off to 1 and anyone below 0.4 we really reduce to 0 so similarly wehave polynomial regression also so when we have nonlinear data which cannot be predicted with a linear model we switch to the polynomial regression now such ascenario is shown in the below graph so as you can see here we have the equation y equals 3x cubed plus 4x squared minus 5x plus 2 now here we cannot performthis linearly so we need polynomial regression to solve these kind of problems now when we talk about logistic regression there is an important termwhich is decision tree and this is one of the most used algorithms insupervised learning now let's understand what exactly is a decision tree so our decision tree is a tree like structure in which internal load represent testson an attribute now each attribute represents outcome of test and each leafnode represents the class label which is a decision taken after computing allattributes apart from root to the leaf represents classification rules and a decision tree is made from our data by analyzing thevariables from the decision tree now from the tree we can easily find out whether there will be came tomorrow if the conditions are rainy and less windynow let's see how we can implement the same so suppose here we have a data set in which we have the outlook so what we can do is from each of the Outlawz wecan divide the data as sunny overcast and rainy so as you can see in the sunnyside we get two yeses and three noes because the outlook is sunny the humidity is now and oven is weak and strong so it's afully sunny day what we have is that it's not a pure subset so what we'regonna do is split it further so if you have a look at the overcast we have humidity high normal week so yes during overcast weekend play and if youhave a look at the Raney's area we have three SS and - no so again what we're going to do is split it further so when we talk of a sunny then we have humidityin humidity we have high and normal so when the humidity is normal we're goingto play which is the pure subset and if the humidity is high we are not going to play which is also a pure subset now so let's do the same for the rainy day soduring rainy day we have the vent classifier so if the wind is to be itbecomes a pure subset we're going to play and if the vent is strong it's a pure substance we not gonna play so the final decision tree looks like this sofirst of all we check if the outlook is sunny overcast or rain if it's overcastwe will play if it's sunny we then again check the humidity if the humidity ishigh we will not play if the humidity is normal real play then again in the case of rainy if we check the vent if the wind is weak the play will go on andsimilarly if the wind is strong the play must stop so this is how exactly adecision tree works so let's go ahead and see how we can implement logisitics relation in decision trees now for logistic regression we're going to usethe Casa data set so this is how the data set looks like so here we have theeye diagnosis radius mean - I mean parameter mean these are the stats of particular cancer cells or the cyst which are present in the body so we havelike total 33 columns all the way starting from IDE - unnamed 32 so ourmain goal here is to define whether or I'll say predict whether the cancer ispinang on mannequin so first of all what vinegar - is from scikit-learn dot smallselection we're gonna import cross-validation score and again we're going to use numpy for linear algebra we're gonna usepandas as PD because for data processing the CSV file input for data manipulationin sequel and most of the stuff then we're going to import the matplotlib itis used for plotting the graph we're going to import Seabourn which is used to plot interactive graph like in the last example we saw we plotted a heatmapcorrelation so from SK learn we're going to import the logistic regression whichis the major model or the algorithm behind the whole logic regression we'regonna import the train dressed split so as to split the raita into two paths training and testing data set we're going to import metrics to check theerror and the accuracy of the model and we're gonna import decision treeclassifier so first of all what we're gonna do is create a variable data anduse the pandas PD to read the data from the data set so here the header 0 meansthat the zeroth row is our column name and if we have a look at the data or the top six part of the data we're going to use the friend data dot head and get thedata dot info so as you can see here we have so many data columns such as highlydiagnosis radius being in text remain parameter main area means smoothness mean we have texture worst symmetry worst we have fractal dimension worseand lastly we have the unnamed so first of all we can see we have six rows and 33 columns and if you have a look at all of these columns here right we get thetotal number which is the 569 which is the total number of observation we have and we check whether it's non null and then again we check the type of theparticular column so it's integer it's object float mostly most of them are float some are integer so now again we're going to drop the unnamed columnwhich is the column 30 second 0 to 33 which is the 30 second column so in thisprocess we will change it in our data itself so if you want to save the old data you can also see if that but then again that's of no use so theta dotcolumns will give us all of these columns when we remove that so you can see here in the output we do not have the final one which was the unnamedthe last one we have is the type which is float so latex we also don't want theID column for our analysis so what we're gonna do is we're gonna drop the ID again so as I said above the data can be divided into three paths so let's dividethe features according to their category now as you know our diagnosis column is object type so we can map it to the integer value so we what we wanna do isuse the data diagnosis and we're gonna map it to M 1 and B 0 so that the outputis either M or B now if we use a rated or described so you can see here we have8 rows and 1 columns because we dropped two of the columns and in the diagonals we have the values here let's get the frequency of the cancer stages so herewe're going to use the Seabourn SNS not count plot data with diagnosis and Leewill come and if we use the PLT dot show so here you can see the diagnosis for 0 is more and for 1 is less if you plot the correlation among this data so we'regoing to use the PLT dot figure SNS start heat map we're gonna use a heat map we're going to plot the correlation c by true we're going to use square trueand we're gonna use the cold warm technique so as you can see here the correlation of the radius worst with the area worst and the parameter worst ismore whereas the radius worst has high correlation to the parameter mean andthe area mean because if the radius is more the parameter is more area is more so based on the core plot let's select some features from the model now thedecision is made in order to remove the : era t so we will have a prediction variable in which we have the texture mean the parameter mean the smoothnessmean the compactors mean and the symmetry mean but these are the variables which we'll use for the prediction now we'll gonna split thedata into the training and testing data set now in this our main data is splitted into training a test data set with the 0.3 test size that is 30 to 70ratio next what we're going to do is check the dimension of that training and the testing data says so what we're going to do is use the print command andpass the parameter train dot shape and test our shape so what we can see here is that we have almost like 400 398 observations were 31 columns in thetraining dataset whereas 171 rows and 31 columns in the testing dataset so then again what we're going to do is take the training datainput what we're going to do is create a Train underscore X with the prediction underscore rad and train is for y is for the diagnosis now this is the output ofour training data same as we did for the test so we're going to use test underscore X for the test prediction variable and test underscore Y for thetest diagnosis which is the output of the test data now we're going to create a logistic regression method and create a model logistic dot fit in which you'regoing to fit the training data set which is strain X entering Y and then we're going to use a TEM P which is a temporary variable in which you canoperate X and then what we're going to do is we're going to compare to EMP which is a test X with the test Y to check the accuracy so the accuracy herewe get is 0.9 1 then again what we need to do this was like location normalroads retribution are we going to use classifier so we're going to create a decision tree classifier with random state given as 0 now what next we'regoing to do is create the cross-validation school which is the CLF we take the moral we take the train X 3 and Y and C V equals 10 thecross-validation score now if we fit the training test and the sample weight wehave not defined here check the input of his true and XID x sorted is none so ifwe get the parameters true we predict using the test X and then predict thelong probability of test X and if we compare the score of the test X to testY with the sample weight none we get the same result as a decision tree so thisis how you implement a decision tree classifier and check the accuracy of the particular model so that was it so next on our list is random forest solet's understand what exactly is a random forest so random forest is ansymbol classifier made using many decision tree models so so what exactlyare in symbol malls so n symbol malls combines the results from different models the result from an N simple mall is usually better than the result of theone of the individual model because every tree votes for one class the final decision is based upon the majority of votes and it is better than decisiontree because compared to decision tree it can be much more accurate it rests if efficiently on the last data set it can handle thousands of input variableswithout variable deletion and what it does is it gives an estimate of what variables are important in the classification so let's take the exampleof weather data so let's understand I know for us with the help of thehurricanes and typhoons data set so we have the data about hurricanes and typhoons from 1851 to 2014 and the data comprises off location when the pressureof tropical cyclones in the Pacific Ocean the based on the data we have to classify the storms into hurricanes typhoons and the sub categories asfurther to predefined classes mentioned so the predefined classes are TD tropical cyclone of tropical depression intensity which is less than 34 knots ifit's between thirty four to six to 18 oz it's D s greater than 64 knots it's a cheer which is a hurricane intensity e^x is esta tropical cyclone s T is lessthan 34 it's a subtropical cyclone or subtropical depression s s is greaterthan 34 which is a subtropical cyclone of subtropical storm intensity and then again we have L o which is a low that is neither a tropical cyclone a tropicalsubtropical cyclone or non and extraterrestrial cyclone and then again finally we have DB which is disturbance of any intensity now these were thepredefined classes description so as you can see this is the data in which we have the ID name date event say this line it's your longitude maximum whenminimum when there are so many variables so let's start with impthe pandas then again we import the matplotlib then we gonna use the aggregate method in matplotlib we're going to use the matplotlib in linewhich is used for plotting interactive graph and I like it most for plots so next what we're going to do is import Seabourn as SNS now this is used to plotthe graph again and we're going to import the model selection which is the Train test split so we're gonna import it from a scaler and the scikit-learnwe have to import metrics watching the accuracy then we have to import sq learnand then again from SQL and we have to import tree from SQL or dot + symbol we're gonna import the random forest classifier from SQL and Road metricswe're going to import confusion matrix so as to check the accuracy and from SQLand on message we're gonna also import the accuracy score so let's import random and let's read the dataset and print the first six rowsof the data sets you can see here we have the ID we have the name date time it will stay this latitude longitude so in total we have 22 columns here so asyou can see here we have a column name status which is TS TS TS for the foursix so what we're gonna do is data at our state as visible P dot categoricaldata the state so what we can do is make it a categorical data with quotes sothat it's easier for the machine to understand it rather than having certain categories as means we're gonna use the categories as numbers so it's easier forthe computer to do the analysis so let's get the frequency of different typhoons so what we're going to do is random dot seed then again what are we gonna do isif we have to drop the status we have to drop the event because these are unnecessary we're gonna drop latitude longitude we're gonna drop ID then namethe date and the time it occurred so if we print the prediction list so ignorethe error here so that's not necessary so we have the maximum and minimum and pressure low went any low when deci low when s top blue and these are theparameters on which we're going to do the predictions so now we'll split that into training and testing data sets so then again we have the trained comettest and we're gonna use a trained test split especially in the 70s of 30 industrial standard ratio now important thing here to note is that you can splitit in any form you want can be either 60/40 70/30 80/20 it all depends uponthe model which you have our the industrial requirement which you have so then again if after printing let's check the dimensions so the training datasetcomprises of eighteen thousand two hundred and ninety five rows were twenty two columns whereas the testing dataset comprised of eight thousand rows withtwenty two columns we have the training data input train x we had a train y sostatus is the final output of the training data which will tell us the status whether it's a TS d d which it's an hu which kind of a hurricane ortyphoon or any kind of subcategories which are defined which were like subtropical cyclone the subtropical typhoon and much more so our predictionor the output variable will be status so so this is these are the list of the training columns which we have here now same we have to do for the test variableso we have the test x with the prediction underscore rat with a test y with the status so now what we're going to do is build a random foils classifierso in the model we have the random forest classifier with estimators as 100a simple random for small and then we fit the training data set which is a training X and train by then we again make the prediction which is the worldor predict that with the test underscore X then that and this will predict forthe test data and prediction will contain the rated value by our model predicted values of the diagnosis column for the test inputs so if you print themetrics of the accuracy score between the prediction and the test and a score why to check the accuracy we get 95% accuracy now the same if we're going todo with decision tree so again we're gonna use the model tree dot decisiontree classifier we're going to use the Train X and tree in Y which other training data sets new prediction is smaller for a task ortext we're going to create a data frame which is the Parador data frame and ifwe have a look at the prediction and the test underscore Y you can see the state has 10 10 3 3 10 10 11 and 5 5 3 11 and 3 3 so it goes on and on so it has 78402 rows and 1 column and if you print the accuracy we get a ninety-five point fiveseven percent of accuracy and if you have a look at the accuracy of the random for us we get 95 point six six percent which is more than 95 point fiveseven so as I mentioned earlier usually random forest gives a better output orcreates a better more than the decision tree classifier because as I mentionedearlier it combines the result from different models you know so the final decision is based upon the majority of votes and is usually higher than thedecision tree models so let's move ahead with our knee by selca rhythm and let'ssee what exactly is neat bias so nave bias is a simple but surprisingly powerful algorithm for predictive modeling now it is a classificationtechnique based on the base theorem with an assumption of independence amongpredictors it comprises of two parts which are the nave and the bias so in simple terms an a bias classifier assumes that the presence of aparticular feature in a class is unrelated to the presence of any other feature even of these features depend on each other or upon the existence of theother features all of these properties independently contribute to the probability that a fruit it's an apple or an orange and that is why it is knownas a noun a base model is easy to build and particularly useful for very large data sets in probability theory and statistics Bayes theorem which isalternatively known as the base law or the Bayes rule also emitted as Bayes theorem describes the probability of an event based on the prior knowledge ofconditions that might be related to the event so Bayes theorem is a way to figure out the conditional probability now conditional probability is theprobability of an event happening given that it has some to one or more other events for example your probability of getting a parkingspace is connected to the town today you park where you park and what conventionsare going on at the same time so base Hyrum is slightly more nuanced and anutshell it gives us the actual probability of an event given information about tests so let's talk about the base Hyrum now so now givenany I policies edge and evidence II Bayes theorem states that the relationship between the probability of the hypothesis before getting theevidence pH and the probability of the hypothesis after getting the evidence which is P H bar e is PE bar H into probability of H there are a probabilityof e which means it's the probability of even after in the hypothesis interpriority of the hypothesis divided by the probability of the evidence so let's understand it with a simple example here so now for example if a single card isdrawn from standard deck of playing cards the probability of that card being a king is 4 out of 52 now since there are 4 kings in a standard deck of 52cards the rewarding this if the king is the event this card is a king thepriority of the king that is the probability of king equals 4 by 52 whichin turn is 1 by 30 now if the event is is varieties or instance someone looksat the card that the single card is a face card then the posterior probability which is the P of King given it's a face can be calculated using the Bayestheorem given the probability of King given its face is equal to probability of the face given its a king there is a probability of face into the probabilityof King since every King is also a face card so the probability of face givenits a king is equal to 1 and since there are 3 face cards in each suit that are jacking and Queen the probability of face card is 3 out of 30combining these given likelihood ratios are we get the value using the pastetheorem of probability of King events of face is equal to 1 out of 3 so foreignjoint probability distribution with events a and B the probability of a intersection B which is the conditional probability of a given B is now definedas property of intersection B divided by the probability of B now this is how we get the base theorem now that we know the different basic proof of how we gotthe base theorem so let's have a look at the working of the base your answer with the help of an examples here so let's take the same example of the radius setof the these forecasts in which we had the sunny rainy overcast so first of all what we're gonna do is first we will create afrequency table using each attribute of the data set so as you can see here we have the frequency table here for the outlook humidity and the wind so forOutlook we have the frequency table here we have the frequency table for humidity and the wind so next what we're gonna do is create the probability of sunny givensay s that is three out of ten find the probability of sunny which is five out of 14 and this 14 comes from the total number of observations there and fromyes and no so similarly we're gonna find the probability of yes also which is 10out of 14 which is 0.7 one for each frequency table will generate these kindof likelihood tables so the likelihood of yes given it's a sunny is equal to 0.51 similarly the likelihood of no givensunny is equal to 0.40 so here you can look that using Bayes theorem we havefound out the likelihood of yes given it's a sunny and no given it's a sunny similarly we're gonna do the save all likelihood table for humidity and thesame for wind so for humidity we're gonna check the probability of yes given its high humidity is high probability of plane no given the humidity is high isyour going to calculate it using the same base theorem so suppose we have a day with the following values in which we have the outlook as rain humidity ashigh and wind as we since we discussed the same example earlier with the decision tree we know the answer so let's not get ahead of ourselves andlet's try to find out the answer using the Bayes theorem let's understand how neat bass works actually so first of all we gonna usethe likelihood of yes on that day so that equals to probability of Outlook of rain given it's a yes into probability of humidity high given SAS interpretiveNVQ NCS into probability of yes okay so that gives us zero point zero one ninesimilarly they're probably likelihood of no on that day is the outlook is rain in units and no humidity is high given its and no and win this week given so knowthat equals to zero point zero one six now what we're going to do is find the probability of V s and no and for that what we're going to do is take theprobability the likelihood and divide it with the sum of the likelihoods obvious and known so and that really gonna get the probability of yes overall so youthink that formula we get the probability of years as zero point five five and the probability of no as zero point four five and our model predictsthat there is a fifty five percent chance that there will be game tomorrow if it's rainy the humidity is high and the wind isweak now if you have a look at the industrial use cases of any bias we have new scatterings use categorization as what happens is that the news are comesin a lot of tags and it has to be categorized so that the user gets information he needs in a particular format then again we have spam filteringwhich is one of the major use cases of Nate Byars classifier as it classifies the email as spam or ham then finally we have with a prediction also as we sawjust with the example that we predict whether we're going to play or not that sort of prediction is always there so guys this was all about supervisedlearning we discussed linear regression logistic regression we discussed namedpies we've discussed random forests decision tree and we understood how therandom forest is better than decision tree in some cases it might be equal todecision tree but nonetheless it's always gonna provide us a better result so guys that was all about the supervised learning so but before thatlet's go ahead and see how exactly we're gonna implement nay bias so guys here we have another data set run or walk it's the kinematic data setsand it has been measured using the mobile sensor so let the target were able to be Y assign all the columns after it to X using scikit-learn a by asmall we're going to observe the accuracy generate a classification report using scikit-learn now we're going to repeat the model onceonly the acceleration values as predictors and then using only the gyro value aspirators and we're going to comment on the difference in accuracybetween the two moles so here we have a data set which is run or walk so let meopen that for you so here I was data sets run or walk so as you can see wehave the date time user name risk activity acceleration XY assertions see Cairo ex Cairo y Cairo Z so based on it let's see how we can implement the nameby is classifier and so first of all what we're gonna do is import pandas at speedy then we gonna import matplotlib for plotting we're gonna read the run orwalk data file with pandas period or tree and a CSV let's have a look at theinfo so first of all we see that we have 88 thousand five hundred eighty eightrows with 11 columns so we have the date/time username rest activityassertion XYZ Cairo XYZ and the memory uses is send point 4 MB data so this ishow you look at the columns D F dot columns now again we're gonna split the dataset into training and testing data sets so we're going to use the Traintest flight model so that's what we're gonna do is split it into X train X testy train by test and we're gonna split it into the size of 0.2 here so again I amsaying it depends on you what is the test size so let's print the shape ofthe training and see it's 70,000 observation has six columns now whatwe're going to do is from the scikit-learn dot knee pius we're going to import the caution NB which is the question a bias and we're going to putthe classifier as caution NB then we'll pass on the extreme and white rainvariables to the classifier and again we have the wireless co-credit which is theclassifier predict X text and we gonna compare the Y underscore predict with the y underscore test to see the accuracies for that so for that we'regoing to import sq learn dot matrix we're going to import the accuracy score now let's compare both of these so the accuracy what we get is ninety fivepoint five four percent now another way is to get a confusion matrix bill sofrom scikit-learn dot matrix we're going to import the confusion matrix and we're gonna plot the matrix of five predict and white test so as you can see here wehave 90 and 699 that's a very good number so now what we're gonna do iscreate a classification report so from metrics we're gonna import the classification because reports we're going to put the target names as walkcomma run and friends the report using white s and by predict within target means we have so for walking we get the precision of 0.92 and the recall of 0.99f1 score is zero point nine six the support is eight thousand six hundred seventy three and for runway appreciation of ninety ninety percentwith the recoil of 0.92 and f1 score of zero point 95 so guys this is how youexactly use the Gaussian in me or the new pie's classifier on it and all ofthese types of algorithms which are present in the supervisor or unsurprised or reinforcement learning are all present in the cyclotron library so onesecond assist SQL learn is a very important library when you are dealing with machine learning because you do not have to code any algorithm hard codingalgorithm every algorithm is present there all you have to do is just passed it either split the dataset into training and testing dataset and thenagain you have to find the predictions and then compare the predicted Y withthe test case Y so that is exactly what we do every time we work on a machinelearning algorithm now guys that was all about supervised learning let's go ahead and understand what exactly is unsupervised learning so sometimes thegiven data is unstructured and unlabeled so it becomes difficult to classify the data into different categories so answer learning helps to solve this problemthis learning is used to cluster the input data in classes on the basis of their statistical properties so example we can cluster different bikes basedupon the speed limit their acceleration or the average that they are giving soI'm supporting is a type of machine learning algorithm used to draw inferences from Veda sets consisting of input data without labeled responses soif you have a look at the workflow or the process flow of unsupervised learning so the training data is collection of information without anylabel we have the machine learning algorithm and then began the clustering models so what it does is that distributes the data into differentclusters and again if you provide any unlabeled new data it will make a prediction and find out to which cluster that particular data or the data setbelongs to or the particular data point belongs to so one of the most important algorithms in unsupervised learning is clustering so let's understand exactlywhat is clustering so a clustering basically is the process of dividing the datasets into groups consisting of similar data pointsit means grouping of objects based on the information found in the data describing the objects or their relationships so clustering modelsfocused on identifying groups of similar records and labeling records accordingto the group to which they belong now this is done without the benefit of prior knowledge about the groups and their characteristics so and in fact wemay not even know exactly how many groups are there to look for now these models are often referred to as unsupervised learning models since thereis no external standard by which to judge the models classification performance there are no right or wrong answers to these model and if we talkabout why clustering is used so the goal of clustering is to determine the intrinsic group in a set of unlabeled data sometime the partitioning is thegoal or the of clustering algorithm is to make sense of and exact value from the last set of structured and unstructured data so thatis why clustering is used in the industry and if you have a look at the various use cases of clustering in the industry so first of all it's being usedin marketing so discovering distinct groups in customer databases such as customers who make a lot of long-distance callscustomers who use internet more than cause they also using insurance companies for like identifying groups of cooperation insurance policyholders withhigh average game rate farmers crash cops which is profitable they are usingcease mix studies and define probable areas of oil or gas exploration based on Seesmic data and they're also used in the recommendation of movies if youwould say they are also used in Flickr photos they also use by Amazon forrecommending the product which category it lies in so basically if we talk about clustering there are three types of clustering so first of all we have theexclusive clustering which is the hard clustering so here an item belongs exclusively to one cluster not several clusters and the data point belongexclusively to one cluster so an example of this is the k-means clustering so claiming clustering does this exclusive kind of clustering so secondly we haveoverlapping clustering so it is also known as soft clusters in this an itemcan belong to multiple clusters as its degree of association with each clusteris shown and for example we have fuzzy or the C means clustering which meansbeing used for overlapping clustering and finally we have the hierarchical clustering so when two clusters have a painting change relationship or atree-like structure then it is known as hierarchical cluster so as you can see here from the example we have a pain child kind of relationship in thecluster given here so let's understand what exactly is k-means clustering so today means clustering is an inquiry um whose main goal is to group similarelements of data points into a cluster and it is the process by which objectsare classified into a predefined number of groups so that they are as much it is similar as possible from one group to anothergroup but as much as similar or possible within each group now if you have a lookat the algorithm working here you're right so first of all it starts with an defying the number of clusters which is key then again we find the centroid wefind the distance objects to the distance object to the centroid distanceof objects to the centroid then we find the grouping based on the minimum distance has the centroid converge if true then we make a cluster false wethen I can find the centroid repeat all of the steps again and again so let meshow you how exactly clustering was with an example here so first we need to decide the number of clusters to be made now another important task here is howto decide the important number of clusters or how to decide the number of clusters we'll get into that later so force let's assume that the number ofclusters we have decided is three so after that then we provide the centroidsfor all the creditors which is guessing the algorithm calculates the Euclideandistance of the point from each centroid and assigns the data point to the closest cluster now Euclidean distance all of you know is the square root ofthe distance the square root of the square of the distance so next when thecentroids are calculated again we have our new clusters for each data point then again the distance from the points to the new clusters are calculated andthen again the points are assigned to the closest cluster and then again we have the new centroid scatter it and now these steps are repeated until we have arepetition in the centroids or the new centers are very close to the very previous ones so until unless our output gets repeated or the outputs are veryvery close enough we do not stop this process we keep on calculating the Euclidean distance of all the points to the centroids then we calculate the newcentroids and that is how claiming is clustering works basically so an important part here is to understand how to decide then value of K or the numberof clusters it does not make any sense if you do not know how many class are you going to make so to decide the number of clusterswe have the elbow method so let's assume first of all compute the sum squarederror which is the SS e for some value of K for example let's take two four sixand eight now the SS e which is the sum squared error is defined as a sum of the squared distance between each number member of the cluster and its centroidmathematically and if you mathematically it is given by the equation which is provided here and if you brought the key against the SS II you will see that theerror decreases as K gets large now this is because the number of cluster increases they should be smaller so this distortion is also smaller now the ideaof the elbow method is to choose the key at which the SSE decreases abruptly sofor example here if we have a look at the figure given here we see that thebest number of cluster is at the elbow so as you can see here the graph here genius abruptly after number four so for this particular example we're going touse for as a number of cluster so first of all while working with k-meansclustering there are two key points to know first of all be careful about whereyou start so choosing the first Center at random choosing the second Center that is far away from the first Center some of it choosing the NH Center as faraway possible from the closest of the all the other centers and the second idea is to do as many runs of k-means each with different random standingpoints so that you get an idea where exactly and how many clusters you need to make and where exactly the centroid lies and how the data is gettingconverged now he means he's not exactly a very good method so let's understand the pros and cons of k-means clustering z' we know that k-means is simple andunderstandable everyone don't see that the first go the items automatically assigned to the clusters now if we have a look at thecorns so first of all one needs to define the number of clusters this is a very heavy task as us if we have 3/4 or if we have 10 categories and if you donot know but number of clusters are gonna be it's very difficult for anyone to you know to guess the number of clusters now all theitems are forced into clusters whether they are actually belong to any other cluster or any other category they are forced to to lie in that other categoryin which they are closest to and this against happens because of the number of clusters with not defining the correct number of clusters or not being able toguess the correct number of clusters so and most of all it's unable to handle the noisy data and the outliners because anyways and machine learning engineersand data scientists have to clean the data but then again it comes down to theanalysis what they are doing and the method that they are using so typicallypeople do not clean the data for k-means clustering or even if the clean there are sometimes are now see noisy and outliners data which affect the wholemodel so that was all for k-means clustering so what we're gonna do is now a use k-means clustering for the movie data sets so we have to find out thenumber of clusters and divide it accordingly so the use case is that first of all we have at the air set of five thousand movies and what we want todo is group them look the movies into clusters based on the facebook lights soguys let's have a look at the demo here so first of all what we're gonna do is import deep copy numpy pandas Seabourn the various libraries which we're goingto use now and from map rat levels when you use ply PI plot and we're gonna usethis GD plot and next what we're gonna do is import the data set and look at the shape of the data set so if you have a look at the shape of the data set wecan see that it has five thousand and forty three rows with 28 columns and if you have a look at the head of the data set we can see it has five thousandforty three data points so what we're gonna do is place the datapoints in the plot we take the director Facebook Likes and we have a look at thedata columns yeah face number in poster cast total Facebook Likes directorFacebook Likes so what we have done here now is taking the director FacebookLikes and the actor 3 Facebook Likes right so we have five thousand forty three rows and two columns now using the key means from s key alone what we'regoing to do is import it first when import key means from SQL or cluster remember guys sq done is a very important library in Python for machinelearning so and the number of cluster what we're gonna do is provide as five note this again the number of cluster depends upon the SSE which is the sumsquared errors or the we're going to use the elbow method so I'm not going to go into the details of that again so we're gonna fit the data into the k-means dotfit and if you find the cluster centers then for the k-means and print it sowhat we find is is an array of five clusters and if you print the label ofthe k-means cluster now next what we're gonna do is plot the data which we havewith the clusters with the new data clusters which we have found and for this we're going to use the Seabourn and as you can see here we have plotted thecard we have plotted the data into the grid and you can see here we have fiveclusters so probably what I would say is that the cluster three and the clusterzero are very very close so it might depend see that's exactly what I wasgoing to say is that initially the main challenge and k-means clustering is to define the number of centers which are the key so as you can see here that thethird center and the zeroth cluster the third cluster and is your cluster arevery very close to each other so guys it probably could have been in one another cluster and the another disadvantage was that we do not exactly know how thepoints are to be arranged so it's very difficult to force the data into any other cluster which makes our analysis a little differentworks fine but sometimes it might be difficult to code in the k-means clustering now let's understand what exactly issiemens clustering so the fuzzy c means is an extension of a key meansclustering and the popular simple clustering technique so fuzzy clusteringalso referred as soft clustering is a form of clustering in which each data point can belong to more than one cluster so he means tries to find thehard clusters where each point belongs to one cluster whereas the fuzzy c means discovers the soft clusters in a soft cluster any point can belong to morethan one cluster at a time with a certain affinity value towards each fuzzy c means assigns the degree of membership which ranges from 0 to 1 toan object to a given cluster so there is a stipulation that the sum of fuzzymembership of an object to all the cluster it belongs to must be equal to 1 so the degree of membership of this particular point to pool of theseclusters 0.6 and 0.4 and if you add a peak at 1 so that is one of the logicbehind the fuzzy c means so on and this affinity is proportional to the distance from the point to the center of the cluster now then again we have the prosand cons of fuzzy c means so first of all it allows a data point to be in multiple clusters that's a pro it's a more neutral representation of thebehavior of genes genes usually are involved in multiple functions so it isa very good type of clustering when we are talking about genes first of and again if we talk about the cons again we have to define C which is the number ofclusters same as K next we need to determine the membership cutoff value also so that takes a lot of time and it's time-consuming and the clusters aresensitive to initial assignment of centroid so a slight change or deviationfrom the center's is going to result in a very different kind of you know a funny kind of output we get from the fuzzy see means and one of the majordisadvantage of a C means clustering is that it's this are non-deterministic algorithm so it does not give you a particular output as in suchthat's that now let's have a look at the third type of clustering which is the hierarchical clustering so uh hierarchical clustering is analternative approach which builds a hierarchy from the bottom up or the topto bottom and does not require to specify the number of clusters beforehand another algorithm works as in first ofall we put each dita point in its own cluster and if I that closes to cluster and combine them into one more cluster repeat the above step till the datapoints are in a single cluster now there are two types of hierarchical clustering one is elaborated clustering and the other one is division clustering so acumulative clustering builds the dendogram from bottom level while the division clustering it starts all the data points in one cluster from clusternow again her archaic clustering also has some sort of pros and cons so in thepros though no assumption of a particular number of cluster is required and it may correspond to meaningful taxonomies whereas if we talk about thecourse once a decision is made to combine two clusters it cannot be undone and one of the major disadvantage of these hierarchical clustering is that itbecomes very slow if we talk about very very large datasets and nowadays I think every industry are using last year as its and collecting large amounts of dataso hierarchical clustering is not the app or the best method someone mightneed to go for so there's that now when we talk about unsupervised learning sowe have k-means clustering and again and there's another important term whichpeople usually miss while talking about us was learning and there's one very important concept of market basket analysis now it is one of the keytechniques used by large retailers to uncover association between items now itworks by looking for combination of items that occurred together frequently in the transactions to put it it another way it allows retailers to analyze therelationships between the items that the people buy for example people who buy bread also tend to buy butter the marketing team at the retail storeshould target customers who buy bread and butter and provide them an offer so that they buy a third eye like an egg so if a customer buys breadand butter and sees a discount or an offer on eggs he will be encouraged to spend more money and buy the eggs but this is what market basket analysis isall about now to find the association between the two items and make predictions about what the customers will buy there are two algorithms whichare the Association rule mining and the ebrary algorithms so let's discuss each of these algorithm with an example first of all if we have a look at theAssociation rule mining now it's a technique that shows how items are associated to each other for example customers who purchase bread have a 60%likelihood of also purchasing Jam and customers who purchase laptop are more likely to purchase laptop bags now if you take an example of an associationrule if you have a look at the example here a aro B it means that if a personbuys an Adam 8 then he will also buy an item P now there are three common ways to measure a particular Association because we have to find these rules onthe basis of some statistics right so what we do is use support confidence and lift now these three common ways and the measures to have a look at theAssociation rule mining and know exactly how good is that rule so first of all we have support so support gives the fraction of the transaction whichcontains an item a and B so it's basically the frequency of the item in the whole item set whereas confidence gives how often the item a and Boccurred together given the number of item given the number of times a occur so it's frequency a comma B divided by the frequency of a now lift whatindicates is the strength of the rule over the random co-occurrence of a and B if you have a close look at the denominator of the lift formula here wehave support a into support B now a major thing which can be noted from this is that the support of a and B are independent here so if the value of liftor the denominator value of the lift is more it means that the items areindependently selling more not together so that in turn will decrease the value of lift so what happens is that suppose the value of lift is more that impliesthat which we get it implies that the rule is strong and it can be used for later purposes because in that case thesupport in to support p-value which is the denominator of lift will be low which in turn means that there's a relationship between the items a and Bso let's take an example of Association rule mining and understand how exactly it works so let's suppose we have a set of items a B C D and E and we have theset of transactions which are t1 t2 t3 t4 and t5 and what we need to do iscreate some sort of rules for example you can see a D which means that if aperson buys a he buys D if a person buys C he buys a if it wasn't by his a he byC and for the fourth one is if a person buy a B and C he is in turn by a nowwhat we need to do is calculate the support confidence and left of these rules now head again we talk about a priori algorithm so a priori algorithmand the associated rule mining go hand-in-hand so what a predators is algorithm it uses the frequent itemsets to generate the Association rules and itis based on the concept that a subset of a frequent item set must also be a frequent Isum set so let's understand what is a frequent item set and how allof these work together so if we take the following transactions of items we have transaction T 1 T 2 T 5 and the items are 1 3 4 2 3 5 1 2 3 5 to 5 and 1 3 5now another more important thing about support which I forgot to mention wasthat when talking about Association rule mining there is a minimum support countwhat we need to do now the first step is to build a list of items set of size 1using this transaction data set and use the minimum support count 2 now let'ssee how we do that if we create the tables see when if you have a close look at the table C 1 we have the item set 1 which has a support 3 because it appearsin the transaction 1 3 & 5 similarly if you have a look at the item set thesingle item 3 so it has a supporter of 4 it appears in t 1 D 2 D 3 and T 5 but if we have a look at the items at 4 it only appears in thetransaction once so it's support value is 1 now the item set with the support rally which is less than the minimum support value that is to have to beeliminated so the final David which is a table F 1 has 1 2 3 and 5 it does notcontain the 4 now what we're going to do is create the item list of the size 2and all the combination of the item sets in f1 are used in this iteration so we've left four behind we just have 1 2 3 and 5 so the possible item sets of 1 21 3 1 5 2 3 2 5 & 3 5 then again we'll calculate these support so in this caseif we have a closer look at the table c2 we see that the items at 1 comma 2 is having a support value 1 which has to be eliminated so the final table F 2 doesnot contain 1 comma 2 similarly if we create the item sets of size 3 andcalculate these support values but before calculating the support let's perform the peirong on the data set now what Spearing so after all thecombinations are made we divide the table see three items to check if thereare another subset whose support is less than the minimum support value this is a priori algorithm so in the item sets 1 2 3 what we can see that we have 1 2 andin the 1 to 5 again we have 1 2 so we'll discard poor of these item sets andwe'll be left with 1 3 5 & 2 3 5 so with 135 we have three subsets 1 5 1 3 3 5which are present in table F 2 then again we have 2 3 2 5 & 3 5 which are also present in tea we'll have to so we have to remove 1 comma 2 from the tableC 3 and create the table F 3 now if we're using the items of C 3 tocreate the adults of c4 so what we find is that we have the item set 1 2 3 5 thesupport value is 1 which is less than the minimum support value of 2 so whatwe're going to do is stop and we're gonna return to the previous item set that is the table c3 so the final table f3 was one three five withthe support value of two and two three five with the support value of two now what waiting a Jew is generate all the subsets of each frequent itemsets solet's assume that our minimum confidence value is 60% so for every subset s of AI the output rule is that s gives I two s is that srecommends i ns if the support of I divided by the support of s is greaterthan or equal to the minimum confidence value then only we'll proceed further so keep in mind that we have not used lift till now we are only working withsupport and confidence so applying rules with Adam sets of f3 we get rule 1 whichis 1 comma 3 which gives 1 3 5 & 1 3 it means if you buy 1 & 3there's a 66% chance that you'll buy item 5 also similarly the rule 1 comma 5it means that if you buy 1 & 5 there's 100% chance that you will buy 3also similarly if we have a look at rule 5 & 6 here the confidence value is lessthan 60% which was the assumed confidence value so what we're going to do is we'll reject these files now an important thing to note here is thathave a closer look to the rule 5 and rule 3 you see it's it has 1 5 3 1 5 3 3 1 5 it's very confusing so one thing to keepin mind is that the order of the item sets is also very important that will help us allow create good rules and avoid any kind of confusion so that'sdone so now let's learn how Association rule I used in market basket analysisproblem so what we'll do is we will be using the online transactional data of aretail store for generating Association rules so first of all what you need to do is import pandas MLT ml X T and D libraries from the imported and read thedata so first of all what we're going to do is read the data what we're gonna do is from ml X T and e dot frequent patterns we're going toimprove the a priori and Association rules as you can see here we have the head of the data you can see we have inverse number of stock code thedescription quantity the inverse TTL unit price customer ID and the country so in the next step what we will do is we will do the data cleanup whichincludes reviewing spaces from some of the descriptions given and what we'regoing to do is drop the rules that do not have the inverse numbers and remove the Freight transaction so hey what what you're gonna do is remove which do nothave an invoice number if the string contains type seen was a number thenwe're going to remove that those are the credits remove any kind of spaces from the descriptions so as you can see here we have like five iron and 32,000 rowswith eight columns so next what we wanted to do is after the clean up we need to consolidate the items into one transaction per row with each productfor the sake of keeping the data assets small we gonna only look at the sales for France so we're gonna use the only France and group by invoice numberdescription with the quantity sum up and C so which leaves us with 392 rows and1563 columns now there are a lot of zeros in the data but we also need tomake sure any positive values are converted to a 1 and anything less than 0 is set to 0 so for that we're going to use this code defining end code units ifX is less than 0 it owns 0 if X is greater than 1 returns 1 so what we're going to do is map and apply it to the whole data set we have here so now thatwe have structured the data properly so the next step is to generate the frequent item set that has support of at least 7%now this lumber is chosen so that you can you get close enough now what we're gonna do is generate the ruse with the corresponding support confidence andlift so we had given the minimum support at 0.7 the metric is lift frequent itemset and threshold is one so these are the following rules now a few rules witha high lift value which means that it occurs more frequently than would be expected given the number of transaction the product combinations most of theplaces the confidence is high as well so these are few of the observations whatwe get here if we filter the data frame using the standard pandas code for largelift six and high confidence 0.8 this is what the output is going to look likethese are 1 2 3 4 5 6 7 8 so as you can see here we have the eh rules which arethe final rules which are given by the Association rule mining and that is how all the industries or any of these we've talked about large retailers they tendto know how their products are used and how exactly they should rearrange and provide the offers on the products so that people spend more and more moneyand time in the shop so that was all about Association rule mining so so guysthat's all for unsupervised learning I hope you got to know about the different formulas how unsupervised learning works because you know we did not provide anylabel to the data all we did was create some rules and not knowing what the datais and we did clusterings different types of clusterings k-means simi's hierarchical clustering so now coming to the third and last type oflearning is the reinforcement learning so what reinforcement learning is it's a type of machine learning where an agent is put in an environment and it learnsto behave in this environment by performing certain actions and observing the rewards which it gets from those actions so a reinforcement learning isall about taking an appropriate action in order to maximize a reward in the particular situation and in supervised learning the training theater comprisesof input and expected output so the model is strained with the expected output itself but when it comes to reinforcement learningthere is no expected output the reinforcement agent decides what actionsto take in order to perform a given task in the absence of a training dataset itis bound to learn from its expertise so let's understand reinforcement learning with an analogy so consider a scenario wherein a baby is learning how to walknow this scenario can go in two ways first the baby starts walking in and makes it to the candy now since the candy is the end goal thebaby is happy it's positive the baby is happy positive reward now coming to thesecond scenario the baby starts walking but falls due to some hurdle in between now the baby gets hurt and does not get to the candy it's negative the baby issad negative reward just like we humans learn from our mistakes by a trial and an earth reinforcement learning is also similar and we have an agent which isbaby a reward which is candy and many hurdles in between the agent is supposed to find the best possible path to reach the reward so guys if you have a look atsome of the important reinforcement learning definitions first of all we have the agent so the reinforcement learning algorithm that learns fromtrial in err that's the agent now if we talk about environment the world through which the agent moves or the obstacles which the agent has to conquer or theenvironment now actions a are all the possible steps that the agent can take the state s is the current conditions returned by theenvironment then again we have reward R and instant return for the environment to appraise the last action then again we have policy which is PI it is theapproach that the agent uses to remind the next action based on the current state we have value V which is the expected long-term return with discountas open to the short-term what are then again we have the action value Q this issimilar to value except it takes an extra parameter which is the current state action which is a now let's talk about reward maximization for a momentnow reinforcement learning agent works based on the theory of reward maximization this is exactly why the RL must be trained in such a way that hetakes the best action so that the reward is maximum now the collective rewards at a particular time and the respectiveaction is written as G T equals RT plus one RT plus two and so onnow the equation is an ideal representation of rewards generally things do not work out like this while summing up the cumulative rewards nowlet me explain this with a small gape in the figure you see a fox right some meat and a Tyler our reinforcement learning agent is the Fox and his end goal is toeat the massive Otto meat before being eaten by the tiger since this fox is clever fellow he eats the meat that is closer to him ratherthan the meat which is close to the tiger because the closer he goes to the Tiger the tiger the higher are his chances of getting killed as a resultthe reward near the tiger in if they are bigger meat chunks will be discounted this is done because of the uncertainty factor that the tiger might kill the Foxnow the next thing to understand is how discounting of reward works now to dothis we define a discount called the gamma the value of gamma is between 0 &1 the smaller the gamma the larger the discount and vice versa so our cumulative discounted reward is GTsummation of K 0 to infinity gamma to the power P as DKt plus k plus 1 where gamma belongs to 0 to 1 but if the Fox decides to explore abit it can find bigger rewards that is this big chunk of meats this is called exploration so the reinforcement learning basically works on the basis ofexploration and exploitation so exploitation is about using thealready known expert information to heighten the rewards whereas exploration is all about exploring and capturing more information about the environmentthere is another problem which is known as the K armed bandit problem the Karmed bandit it is a metaphor representing a casino slot machine with K pull levers or arms the users or the customer pulls any one of the levers towin a projected reward the objective is to select the leeward that will provide the user with the highest reward now here comes theepsilon greedy algorithm it tries to be fair to do opposite cause of explorationexploitation by using a mechanism of flipping a coin which is like if you flip a coin and comes up head you should explore for memory butter comes up daysyou should exploit it takes whatever action seems best at the present moment so with probability while epsilon the epsilon greedy algorithm exploits thebest known option with probability epsilon by 2 epsilon 0 it explores thebest known option and with the probability epsilon by 2 with probability epsilon by 2 the algorithm explores the best known option and withthe probability epsilon by 2 the epsilon greedy algorithm explores the worst known option now let's talk about Markov decision process the mathematicalapproach for mapping a solution in reinforcement learning is called Markov decision process which is MDP in a way the purpose of reinforcement learning isto solve a Markov decision process now the following parameters are used to attain a solution set of actions a set of states s we have the reward ourpolicy PI and the value V and we have translational function T probabilitythat our forum leads to s now to briefly sum it up the agent must take up anaction to transition from the start state to end state s while doing so theagent receives the reward R for each action he takes the series of actions taken by the agent define the policy PI and the rewards collected by collectedto find the value of V the main goal here is to maximize the rewards by choosing the optimum policy now let's take an example of choosing the shortestpath now consider the given example here so what we have is given the above representation our goal here is to find the shortest path between a and D eachedge has a number linked to it and this denotes the cost to traverse that edge now the task at hand is to traverse from point A to D with the minimum possiblecost in this problem the set of states are denoted by the nodes ABCD ad the action is to traverse from one node to another are given by a arrow Bor C our OD reward is the cost represented by each edge and the policyis the path taken to reach each destination a to C to D so you start offat node a and take baby steps to your destination initially only the nextpossible node is visible to you if you follow the greedy approach and take the most optimal step that is choosing a to see instead of a to B or C now you areat node C and want to traverse to node T you must again choose the path wisely choose the path with the lowest cost we can see that a CD has the lowest costand hence we take that path to conclude the policy is a to C to D and the valueis 120 so let's understand Q learning algorithmwhich is one of the most use reinforcement learning algorithm with the help of examples so we have five rooms in a buildingconnected by toast and each room is numbered from 0 through 4 the outside ofthe building can be thought of as one big room which is tea room number five now dose 1 & 4 lead into the building from the room 5outside now let's represent the rooms on a graph and each node each room has anode and each door as link so as you can see here we have represented it as agraph and our goal is to reach the node 5 which is the outer space so what we'regonna do is and the next step is to associate a reward value to each toe sothe dose that directed read to the you will have a reward of 100 whereas thedoors that do not directly connect to the target have a reward and because the dose had to weigh two arrows are assigned to each room and each rowcontains an instant about valley so after that the terminology in the q-learning includes the term states and action so the room 5 represents a stateagents movement from one room to another room represents in action and in thisfigure a state is depicted as a node while an action is represented by the arrows so for example let's say can eat in that Traverse from room to to theroof I so the initial state is gonna be the state to it then the next step is from stage 2 to stage 3 next is to moves from stage 3 to stage either 2 1 or 4 soif it goes to the 4 it reaches stage 5 so that's how you represent the hole traversing of any particular agent in all of these rooms a represents theiractions via notes so we can put this state diagram and instant reward valuesinto a reward table which is the matrix R so as you can see the minus 1 here inthe table represents the null values because you cannot go from 1 to 1 right and since there is no way from to go from 1 to 0 so that is also minus 1 sominus 1 represents the null values whereas the 0 represents zero reward and 100 represents the reward going to the roomfive so one more important thing to know here is that if you're enrolled fireman you could go to room five the reward is hundred so what we need to do is addanother matrix Q representing the memory of what the agent has learned to experience the rows of matrix Q represent the current state of the agentwhereas the columns represent the possible action leading to the next state now if the formula to calculate the Q matrix is if a particular Q at aparticular state and the given action is equal to the R of that state in action plus gamma which we discussed earlier the Kurama parameter which we discussedearlier which ranges from 0 to 1 into the maximum of the Q or the next statecomma all actions so let's understand this with an example so here are thenine steps which any Q learning algorithm particularly has so first ofall is to set the gamma parameter and the environment rewards in the matrix R then we need to do is initialize the matrix Q to 0 select the random initialstate set the initial state to current state select one among all the possible actions for the current state using this possible action consider going to thenext state when you get the next state get the maximum Q value for this next state based upon all the actions compute the Q value using the formula repeat theabove steps until the current state equals your code so the first step is to set the values of the learning parameters gamma which is 0.8 andinitial state as room number one so the next initialize the Q matrix a zeromatrix so on the left hand side as you can see here we have the Q matrix which has all the values as 0 now from room 1 you can either go to room 3 or room 5 solet's select room 5 because that's our end goal so from room 5 calculate the maximum cube value for this next state based on all possible actions so Q 1comma 5 equals R 1 comma 5 which is hundred plus zero point eight which isthe gamma into the maximum of Q 5 comma 1 5 comma 4 and 5 comma 5 somaximum or five comma one five comma four five comma five is hundred so the Qvalues from initially as you can see here the Q values are initialized to zero so it does not matter as of now so the maximum is zero so the final Q valuefor Q 1 comma 5 is 100 so so that's how we're gonna update our Q matrix so Qmatrix the position has 1 comma 5 in the second row gets updated to 100 so the first step we have turned right now that for the next episode we start with arandomly chosen initial state so let's assume that the stage is 3 so from rule number 3 you can either go to room number 1 2 or 4 so let's select theoption of room number 1 because from our previous experience what we've seen is that one has directly connected to room 5 so from room / 1 calculate the maximumQ value for this next state based on all possible action so 3 comma 1 if we takewe get our 3 4 1 plus 0 point 8 comma into maximum of T's we get the value as80 so the matrix Q gets updated now for the next episode the next state 1 nowbecomes the current state we repeat the inner loop of the Q learning algorithm because tip 1 is not the goal state from 1 you can either go to 3 of 5 so let'sselect 105 as that's our goal so from room row 5 again we can go from all of these so the Q matrix remains the same since Q 1 5 is already fed to the agentand that is how you select the random starting points and fill up the Q Qmatrix and see where which path will lead us there with the maximum providepoints now what we gonna do is do the same coding using the Python in machinelearning so what we're going to do is improve an umpire's NP we're gonna take the R matrix as we defined earlier so that the minus 1 are the nerve valueszeros are the values which provides a 0 and hundreds is the value so what we'regoing to do is initialize the Q matrix now to 0 we're going to put gamma as 0.8and set the initial state as 1 now here returns all the available actions in the state given as an argument so if we define the ofaction with the given state we get the available action in the current state sowe have the another function here which is known as a sample next action what this function does is that chooses at random which action to be performedwithin the range of all the available actions and finally we have action which is the sample next action with the available act now again we have anotherfunction which is update now what it does is that it updates the Q matrix according to the path selected and a Q learning algorithm so so initially our Qmatrix is all 0 so what we're gonna do is we're gonna train it over 10,000iterations and let's see what exactly gives the output of the Q value so ifthen the agent learns more through for the iterations it will finally breach converges value in Q matrix so the Q matrix can then be normalized at isconverted to percentage by dividing all the non-zeros entities by the highest number which is 500 in this case so once the matrix Q gets close enough to thestate of convergence agent has learned the most optimal path to the goal State so what we're gonna do next is divide it by 5 which is the maximum here so Q Rand P Q max in 200 so that we get a normalized now once the Q matrix gets close enough to the state of convergence the agenthas learned or the paths so the optimal path given by the Q learning employerThomas if it starts from 2 it will go to 3 then go to 1 and then go to 5 if itstarts at 2 it can go to 3 then 4 then 5 that will give us the same results so asyou can see here is the output given by the Q learning algorithm is the selected path is 2 3 1 and Feinstein from the Q State - so this is how exactly areinforcement learning algorithm works it finds the optimal solution using the path and given the action and rewards and the various other definitions or thevarious other challenges I would say actually the main goal is to get the master reward and get the maximum value through the environment and that's howan agent learns through its own path and going millions and millions ofiterations learning how each part will give us what reward so that's how the Qlearning algorithm works and that's how it works in Python as well as I showed you so now that you have a clear idea of the different machine learningalgorithms how it works the different phases of machine learning the different applications of machine learning how supervised learning works howunsupervised learning works our reinforcement learning works and what to choose in what scenario what are the different algorithms under all of thesetypes of machine learning next move forward to the next part our sessionRich's understanding about artificial intelligence deep learning and machine learning well data science is something that hasbeen there for ages nonetheless and data science is the extraction of knowledge from data by using scientific techniques and algorithms people usually have acertain level of dilemma or I would say a certain level of confusion when it comes to differentiating between the terms artificial intelligence machinelearning and deep learning so don't worry I'll clear all of these doubts for you artificial intelligence is a technique which enables machine to mimichuman behavior now the idea behind artificial intelligence is fairly simple yet fascinating which is to make intelligent machines that can takedecisions on their own now for years it was thought that computers would never match the power of the human brain well back then we did not have enough dataand computational power but now with big data coming into existence and with theadvent of GPUs artificial intelligence is possible now machine learning is asubset of artificial intelligence technique which uses statistical method to enable machines to improve with experience whereas deep learning is asubset of machine learning which makes the computation of multi-layer neural network feasible it uses the neural networks to stimulate human-likedecision-making so as you can see if we talk about the data science ecosystem we have artificial intelligence machine learning and deep learning deep learningbeing the innermost circle is very much required for machine learning as well as artificial in but why was deep learning required sofor that less understand the need for deep lolly so a step towards artificialintelligence was machine learning and machine learning was a subset of ei play it deals with the extraction of patterns from the last dataset haslam la datasetwas not a problem what was a problem was machine learning algorithms could not handle the hight dimensional data where we have a large number of inputs andoutputs which rounds thousands of dimensions handling and processing suchtype of data becomes very complex and resource exhaustion now this is also termed as the curse of dimensionality now another challenge faced by machinelearning was to specify the features to be extracted so as we saw earlier in allthe algorithms which are discussed now we had to specify the features to be extracted now this plays an important role in protecting the outcome as wellas in achieving better actress therefore without feature extraction the challenge for the programmer increases as the effectiveness of the algorithm very muchdepends on how insightful the programmer is now this is where deep learning comesinto picture and comes to the rescue but deep learning is capable of handling the high dimensional data and is also efficient in focusing on the rightfeatures on its own so what exactly is deeper so deep learning is a subset of machine learning as I mentioned earlier where similar machine learningalgorithms are used to Train deep neural networks so as to achieve better accuracy in those cases where the former was not performing up to the MAbasically deep learning mimics the way our brain functions and learns from experience so as you know our brain is made up of billions of neurons thatallows us to do amazing things when the brain of a small kid is capable of solving complex problems which are very difficult to solve even using thesupercomputers so how can we achieve the same functionality in programs now thisis where we understand artificial neuron and artificial neural networks so firstof all let's have a look at the different applications of deep learning we have automatic machine translation object classification beforeautomatic handwriting generation character text generation we have image caption generation colorization of black and white images we haveautomatic game playing and much more now google lens is a set of vision basedcomputing capabilities that allows your smartphone to understand what's going onin a photo video or any live feed for instance point your phone at a flower and google lens will tell you on the screen which type of flower it isyou can in that camera at any restaurant sign to see the reviews and other recommendations now if we talk word mushroom transition this is a task whereyou are given words in some language and you have to translate the words to a desired language see English but this kind of translation is classic exampleof image recognition and final application of deep learning which we have here is image polarization so automatic colorization of black andwhite images as you know earlier we did not had color photographs back there in40s and 50s we did not have any color photographs so through deep learning analyzing water shadows is present in the image how the light is bouncing offthe skin tone of the people automatic colorization is now possible and this isall possible because of deep learning now deep learning studies the basic unitof a brain cell called a neuron now let us understand the functionality of a biological neuron and how we mimic this functionality in the perceptron or whatwe call is an artificial neuron so as you can see here we have the image of abiological neuron so it has a cell body it has mitochondrion nucleus we havedendrites there we have the axon we have the node of the ran of ear you have thescavenge cell and the synapse so we need not know about all of these so what weneed to know mostly about is dendrite which receives signals from other neurons we have a cell body which sums up all the inputs and we have axon whichis used to transmit the signals to the other cells now an artificial neuron orperceptron is a linear model which is based upon the same principle and is used for binary classification it models a neuron which has a set ofinputs each of which is given a specific weight and the neuron computes somefunctions on these weighted inputs and gives the outputs it receives n inputscorresponding to each feature it then sums up those inputs applies the transformation and produces an output it has generally two functions which arethe summation and the transformation but the transformation is also known as activation functions so as you can see here we have certain inputs we havecertain weights we have the transfer function and then we have the activation function now the transfer function is nothing but the summation function hereand it is the schematic for a neuron in a neural network so this is how we mimica biological neuron in terms of programming now the way it shows theeffectiveness of a particular input move the weight of input more it will have animpact on the neural network on the other hand bias is an additional parameter in the perceptron which is used to address the output along withthe weighted sum of the inputs to the neuron which helps the model in a way that it can best fit for the given data activation functions translate theinputs into outputs and it uses a threshold to produce an output there aremany functions that are use has activation functions such as linear or identity we have unit or binary step we have sigmoid logistic tan edge ray Luand soft Max now if we talk about the linear transformation or the activation function so a linear transform is basically the identity function wherethe dependent variable has a direct proportional relationship with the independent variable now in practical terms it means that a function passesthe signal through unchanged now the question arises when to use linear transform function simple answer is when we want to solve a linear regressionproblem we apply a linear transformation function and next in our list of activated functions we have your next step the output of a unit step functionis either 1 or 0 now it depends on the threshold value we define astep function with the threshold value five is shown here so let's consider X is five so if the value is less than five the output will be zero whereas ifthe value is equal to or greater than five then the valuable one this equal tois very much important to consider here because sometimes people put up the equal two in the lower end of the side so that's not it how it is used butrather it's used on the upper hand side where if the value is greater than particular X greater than or equal to X then only the value will be one now asigmoid function is a machine that converts an independent variable of nearinfinite range into simple probabilities between 0 & 1 now most of its outputwill be very close to either 0 or 1 and if you have a look at the function herewe have 1 divided by n plus y raise to power minus beta X so I'm not going tothe details or the mathematical function of a particular sigmoid but it's very much used to convert the independent variables of very large infinite rangeto the values between 0 & 1 now the question arises when to use a sigmoid transformation function so when we want to map the input values to avalue in the range of 0 to 1 where we know the output should lie only between these two numbers we apply the sigmoid transformation function note an H is ahyperbolic trigonometric function now unlike the sigmoid function the normalized range of tan H is minus 1 to 1 it's very much similar to the sigmoidfunction but the advantage of tan H is that it can deal more easily with negative numbers now next on our list we have Ray Lu now rail you or the rectifylinear unit transform function only activates our node if the input is abovea certain quantity while the input is below 0 the output is 0 but when theinput Rises about a certain threshold or if we take in this case at 0 but if youhave a certain value X if it crosses that certain threshold it has a linear relationship with the dependent variable now this is very much different from anormal linear transformation so has certain threshold now the questionarises here again when to use a railroad transformation function so when we wantto map the input values to a value in the range so as input X to maximum 0comma X that is it Maps the negative inputs to 0 and the positive inputs are output without any change we apply a rectified linear unit or the railroadtransformation function now the final one which we have is sort max so when we have four or five classes of outputs the softmax function will give theprobability distribution of each it is useful for finding out the class whichhas the maximum probability so soft mass is a function you will often find at theoutput layer of a classifier now suppose we have an input of say the letters ofEnglish words and we want to classify which letter it is so for that case we're going to use the sort max function because in the output we have certainclasses but I would say in English if we take English we had 26 classes from A toZ so in that case softmax activation function is very much important nowartificial neuron can be used to implement logic gates now I'm sure you guys must be familiar with the working of all K that is the output is one ifany of the input is also one therefore a perceptron can be used as a separator ora decision line that divides the input set of or gate into two classes thefirst class being the inputs having output as 0 that lies below the decision line and the second class would be inputs having output as 1 that lie abovethe decision line or the separator so mathematically a perceptron can be thought of like an equation of weights inputs and bias as you can see here wehave f of X is equal to weight into the input vector plus the bias so let's goahead with our demo understand how we can implement this perceptron example which is of an or gate using neural networks using artificial neuron or theperceptron and here we're going to use tensor flow along with Python so let'sunderstand what exactly is tensor flow first before going it to the demo so basically tensor flow is a deep learning framework by Googleto understand it in a very easy way let's understand the two terms of tensorflow which are the tensors and the flow so starting with tensors tensorsare standard way of representing theater in deep learning and they are just multi-dimensional arrays it is an extension of two-dimensional tablematrices through the data with higher dimension so as you can see have first of all we have a tensor of dimension 6 then we have a tensor of dimension 6comma 4 which is 2d and again we have a tensor of dimension 6 4 and 2 which isreading now this dimension is not restricted to 3 we can have four dimensions five dimensions it depends upon the number of inputs or the numberof classes or the parameters which we provide to a particular neural net or aparticular perceptron so which brings us tensorflow intensive flow thecomputation is approached as a data flow graph so we have a tensor and then againwe have a flow in which we suppose for taking the example here we have the data we do addition then we do matrix multiplication then we check the resultif it's good then it's fine and if the result is not good then we again do some sort of matrix multiplication or addition it depends upon the functionwhat we are using and then finally we have the output so if you want to know about it as a flow we have an entire playlist on tensor flow and deeplearning which you should see i'll give the link to all of these videos in the description box so let's go ahead with our demo and understand how we canimplement the or gates using perceptron so first of all what we're going to do is import all the required libraries and Here I am going to import only onelibrary which is the tensor flow library so what we're going to do is import tensorflow a steal now the next step what we're going to dois define vector variables for input and output so for that we need to createvariables for storing the input output and the bias for the perceptron so asyou can see here we have the training input and again we have the training output now what we're going to do next is define the weight variable and herewe are we will define the tensor variable of the shape 3 comma 1and for our weights and we will assign some random values to it initially so we're going to use T AF dot variable and we're going to use TF run random normalto assign random variables to the 3 cross 1 tensor next what we do is defineplaceholders for input and output and so that they can accept external inputs onthe run so this will be T F dot float32 so for X we are going to use a dimensionfor 3 and for y it's dimension of 1 now as discussed earlier the input received by a positron is force multiplied by the respective weights and then all of theseweights input our sum together now this sum value is then fed to the activationfor obtaining the final result of the or gate perceptron so this is the outputhere what we are defining so it's TF dot neural networks dot relu using the reluactivation function here and we are doing the matrix multiplication of the weights and biases in this case I have used the rayleigh function but you arefree to use any of the activation functions according to your needs the next what we're going to do is calculate the cost or ere so we need to calculatethe cost which is the mean squared error which is nothing but the square of the differences or the perceptron output and the desired output so the equation willbe loss equals D F dot reduce some and we'll use the TF dot Square output minusy now the cool of a perceptron is to minimize the loss or the cost or the error so here we are going to use the gradient descent optimizer which willreduce the loss and it is a very important part of any neural network touse any sort of optimizer so here we are using the gradient descent optimizer you can know more about the gradient descent optimizer in other a Drake of videos ordeep learning and neural networks now the next step comes is to initialize the variables so variables are only defined with TF dot variables theinitially what weighted so we need to initialize this variable define so for that we're going to use the T F dot global variable initializer and we'regoing to create the F dot session and we will not run with the initialization variables so as all the variables are initialized notcoming to the last step what we're going to do is we need to train our perceptron that is update away our values of the weights and the biases in the successiveiteration to minimize the error or the Ross so here I will be training our perceptron in hundred epochsso as you can see here for I in range hundred we are going to run the session with training data in and why as a trainee at the output and we're going tocalculate the loss and feed it directly to the X train and why train and again and print the epoch so as you can see here for the first iteration the losswas two point zero seven and coming down if as soon as the iterations increasethe loss is decreasing because of the gradient optimizer it's learning how the data is and coming down to the hundredths or the final epoch here wehave the loss of zero point two seven start with two point zero seven here initially and we ended up with zero point two seven loss which is very goodthis was how perceptron works on a particular given data set it learnsabout it and as you saw earlier we gave a set of input the input variables weprovided weights we had a summation function and then we use the rail uactivation function in the code to get the final output and then we trained theparticular model for hundred iterations with the training data so as to minimizethe loss and the loss came down all the way from two point seven to zero point two seven well if you think perceptron solves all the problem of making a humanbrain then you were wrong there are two major problems first problem being that the single layer perceptron cannot classify non linearly separable datapoints and which other complex problems that involve a lot of parameters cannotbe solved by a single layer perceptron now consider the example here and thecomplexity with the parameters involved to take a decision by the marketing team so as you can see here for every email direct paid referral program or organicwe have certain number of social media subcategories Google Facebook LinkedIn we have twitter and then we have the type such as the search ad remarketingas interest as ad look like ads and again the parameters to be considered are the customer acquisition cost money span leads generated customers generatedtime taken to become a customer and all of these problems cannot be solved by a single layer of perceptron our one neuron cannot take in so manyinputs and that is why more than one neuron would be used to solve these kind of problems so neural network is really just acomposition of perceptrons connected in different ways and operating on activation functions so for that we have three different terminologies in aparticular neural network we have the input layers we have the hidden layers and we have the output layers so in hidden layer we have hidden nodes whichprovide information from the outside world to the network and heart together referred to as the input layer now the hidden nodes perform computations andtransfer information from the input nodes to the output nodes now a collection of hidden nodes forms idle layer in our image we have one two threefour hidden layers and finally the output nodes are collectively referred to as output layers and are responsible for computation and transferringinformation from the network to the outside world now that you have an idea of how a perceptron behaves the differentparameters involved and the different layers of neural networks let's continue this session and see how we can create our own neural network from scratch inthis image as you can see here we have given a list of faces first of all thepatterns of local contrast is being computed in the input layer then in the hidden layer 1 we get the face features and in the hidden layer 2 we get thedifferent features of the face and finally we have the output layer now if we talk about training networks and weights in a particular neural networkswe can estimate the weight values for our training data using stochastic gradient descent optimizer as I mentioned earlier now it requirestwo parameters which is the learning rate and as I mentioned earlier learning rate is used to limit the amount of each weight is corrected each time it isupdated and epoch is a number of times to run through the training data while updating the way so in the previous example we had 100 ebox so we trainedthe whole model hundred times and these along with the training data will be the arguments to the function as data scientists or data analystsor machine learning engineers working on the hyper parameters is the most important part because anyone can do the coding it's your experience and your wayof thinking about the learning rate and the epochs the model which you are working the input data you are taking how much time it will require to trainbecause time is limited and as you know these hyper parameters are the only things which are successful data centers will be guessing when creating aparticular model and these play a huge role in the model such as even a slight difference in learning create of the e box might result in the model trainingtime so as it will take longer time to Train having a large amount of data using the particular data set that these all things are what data scientist ormachine learning engineer keeps in mind while creating them all let's create our own new network and here we are going to use the MN is DDS a so the MN IC dataset consists of 60,000 training samples and 10,000 testing samples ofhandwritten digit images not the images are of the size 28 into 28 pixels andthe output can lie anywhere between 0 to 9 now the task here is to train a modelwhich can accurately identify the digit present on the image so let's see how wecan do this using tensor fro and Python so firstly we are going to use theimport function here to bring all the print function from Python 3 into python2.6 or the future statements let's continue with our coneso next what we are going to do is from pencil for examples tutorials we can take the mi nasty data which is already provided by tensorflow in their exampletutorials data but this is only for the learning part and later on you can use this particular data for more purposes for your learning now next what we aregoing to do is create MN ist and we're going to use the input data tour tree data set and one hot is given us through here so we're going to import tensorflowand whack plot lib next what we are going to do is define the hyper parameters here so as I mentioned earlier we have few hyper parameterslike learning rate equals batch size display step is not a very big hyper parameter to consider here but so the learning rate we have given here is0.001 training epochs is 15 that is up to you because more than number ofepochs the more time it will take for the model to Train and here you have to take a decision between the amount of time it takes for the model to train andgive the output versus the speed again we have the batch size of 100 now thisis one of the most important have a parameter to be considered because you cannot take all of the images at once and create the radius so you need to doit in a bath size manner and for that we define a bad size of 100 so out of 60,000 we're going to take 100 as a bath size 100 images which will go through 15iterations and the training set has 60,000 images so you do the math how many batch we will require and how many epochs for each batch we'll have 15 abox the next step is defining the hidden layers and the input and the classes sofor input layers have taken 256 numbers these are the number of perceptron Ineed or the number of features to be extracted in the first layer so this number is arbitrary you can use it according to your requirements and yourneeds so for simplicity I am using two bits X here and the same I'm going to use for the hidden layer 2 now for the number of inputs I'm going to use 784and that is why because as I discussed earlier the MST data has an image or theshape 28 cross 28 which is 784 so in short we have 784 pixels to beconsidered in a particular image and each pixel will provide immense amount of data so I am taking a 784 input and number of output classes Here I amdefining ten because the output can either range from zero one two three four five six seven eight and nine so the total number of classes or theoutput classes here I'm going to use are ten and again we are going to create x and y variables X for the input and Y for the output classes now as you cansee here we have the multi-layer perceptron in which we have defined all the hidden layers and the output layers so the layer one will do the additionand first I will do the matrix multiplication of the weights and the input with the biases and then it will provide a summation and then again theoutward for this one will be given to layer two by using the activation function of rail you here so as you can see here we have rail you activationfunction for layer 1 layer 2 will take the input of layer 1 with the weightsprovided in h2 hidden to layer with the biases of b2 layer it will do the multiplication of layer 1 into weights it will add the biases and then againwe'll have a rail lu activation function and the output of this layer 2 will be given to the output layer so as you can see here in the final output layer wehave matrix multiplication of layer 2 into weights of the output layer plus the biases of the output layer and what we're going to do is return the outputso let's mention the weights and the biases so here we are taking randompoints for that and next what we're going to do is use the prediction of the multi-layer perceptron using the input weights and biases and one thing moreimportant what we're going to do here is define the cost so we're going to use the TF naught reduce mean and we are using the short max cross entropy withlogits this is a function and here we are using the atom optimizer rather than the gradient descent optimizer with learningrate provided initially and what we're going to do is minimize the costso again we're going to initialize all the global variables and we have twoarrays for cos history and accuracy history so as to store all the values and train our model so we're going to create a session and the training cyclefor epoch in the range of 15 we first initialize the average cost at zero andthe total patch is the MN asset in number of examples divided by bass has which is 100 and we loop it over all the patches run the optimization or the backpropagation and the cost operation to get the loss value and then we have todisplay the logs per each Ipoh for that will show the epochs and the cost ateach step we're going to calculate the accuracy add the last to the correct prediction and will append the accuracy to the list after every epoch we willappend the cost after every epoch because that is what and we have created cos history and the accuracy history for that purpose and finally we will plotthe cost history using the matplotlib and we'll plot the accuracy history alsoand what we're going to do is we're going to see how accurate is our model so so let's train it now and as you can see at first epoch we have cost 188 andaddress is 0.85 so if you see just have the second epoch the cost has reducedfrom 188 to 42 now it's 26 as you can see the accuracy is increasing from 0.85to 0.909 one you have reached five epochs you see the cost is diminishingat a huge rate which is very good and you can use different types of optimizers or gradient descent or be it atom optimizer and not go to the detailsof the optimization because that is another half an hour or one hour to explain you guys what exactly it is and how exactly it works so as you can seetill the tenth epoch or 11th epoch we have cost 2.4 and the accuracy is 0.94let's wait a little further till the 50th epoch is turn so as you can see in the 15th eat walk we have cost 0.83 and actress is 0.94 westart with cost 188 and accuracy 0.85 have you ever east the accuracies of 0.94 so as you can see this is the graph of the costit started from 188 ending at 0.8 3 we have the crop of the accuracy whichstarted from 0 point 8 4 or 8 5 2 all the way to zero point nine four so asyou can see the 14th epoch reached an accuracy of 0.9 4/7 as you can see herein the graph again and in the 15th epoch we came to the accuracy of 0.9 for nowone might ask the question the accuracy was higher in that particular epoch whyhas the accuracy decreased another important aspect or have a parameter to consider here is the cost the more lower the cost the more accurate will be yourmod so the goal is to minimize the cost which will in turn increase the accuracyand finally accuracy here we have a 0.9 for tonight which is very good now thiswas all about deep learning neural networks and tensorflow how would create a perceptron or deep neural network what are the different hyper parametersinvolved how does a neuron work so let's have a look at the companies hiring these professionals these data professionals in the data scienceenvironment we have companies all the way from startups to big giants so themajor companies here we can see as our Dropbox Adobe IBM we have Walmart whowere chase LinkedIn Red Hat and there are so many companies and as I mentionedearlier the required for these professions are high but the people applying are too low because you need a certain level of experience tounderstand how things are working you need to understand machine learning to understand deep learning you need to understand all the statistics andproperty and that is not an easy task so you require at least 3 to 6 months ofrigorous training with minimum one to two years of practical implementationand project work I would say to go into data science career if you think that's the career you want to go so Yurika as you know provides data sciencemaster program we have a machine learning master program but as you can see in the data master program we have Python statistics we have our statisticswe have data size using our Python for data science we have Apache spark and Scylla we have PA and deep learning with tensorflow we have tableau so guys asyou can see here we have 12 courses in this master program with 250 hours of interactive learning via capstone projects and as you can see here we havea certain discount going on the hike in salary you get is much more if you gofor data science rather than any other program so you can see we have Python statistics a statistics data science using Python we have Python for datascience Apache spark and Scala which is a very important part in data science you need to know what the Hadoop ecosystem we have deep learning withtensorflow you have tableau and this is a 31 feet course as I mentioned earlier it's not an easy task and you do not become a Dassigned all in one month or in two months you cry a lot of training and alot of practice to become a data scientist or machine learning engineer or even a data analyst because you see a lot of topics on a vast list of areas iswhat you need to cover and once you cover all of these topics what you need to do is select an either which you wanna work the kind of datawhich you're going to be handling whether it be text data it would be medical records if it's video audio or images for processing it is not an easytask to become a data scientist so you need a very good and a very correct pathof learning to become a real scientist so so guys that's it for this session Ihope you enjoyed the session and got to know about data science the different aspects of data science how it works all the ways to either from statisticsprobability machine learning deep learning and finally coming to AI sothis was the path of data science and I hope you enjoyed this session and if youhave any queries regarding session or any other session please feel free to mention it in the comment section below and we'll happily answer all of yourqueries till then thank you and happy learning. I hope you haveenjoyed listening to this video, please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest.Do look out for more videos in our playlist and subscribe to edureka! channel to learn more. Happy learning!\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data FileA_7.txt'}, 'embedding': None, 'id': 'a8bbe415a553e50b1e14cc7451fa718f'}>, <Document: {'content': '\\nShort introduction of regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66a205739f9140dbf632322d675e348e'}>, <Document: {'content': 'Hello and welcome to our class that today on Regression. Today’s class is going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb4e014c648a5ef58170fc9a84f1012d'}>, <Document: {'content': 'be a very short introductory level treatment of the topic regression analysis and the idea ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87a64d6ba60074a85c9f8cebf1f28bc1'}>, <Document: {'content': 'here was to a kind of position just towards the end of your modules and descriptive in inferential statistics, so that you are at point by you know enough to appreciate, how ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8a0532bc57033483a361dbc21054c05'}>, <Document: {'content': 'regression uses these concepts. But, we will also be revisiting regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '93b0fcbf21a4f40c5ec0304d24050e90'}>, <Document: {'content': 'down the road, where we will actually talk about the mechanics about, how you implement it and that is something we will do, right after you get an introduction to machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90fd570fb7166d89c49f7de8784478c9'}>, <Document: {'content': 'and more specifically to supervised learning. So, jumping into, what a regression is. Essentially a regression analysis and I say regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '564f682d26f260465bbe92d5ce09f0f9'}>, <Document: {'content': 'analysis here, because the word regression itself in different fields it is used differently ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '750e08206942e0402680a5adbb1745c'}>, <Document: {'content': 'and in fact, even in statistics there is a concept called regression towards a mean, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4cb971a5958aca473f725abe6da8c29c'}>, <Document: {'content': 'which has a lot to do with regression analysis, but it is also quite a different from, how we understand, how we use regression analysis today. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee609f6fcb7127e24d43b6b06c465cc9'}>, <Document: {'content': 'So, again the idea is to you know it use the word regression analysis. So, regression analysis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '72b821ac3bb1b29690d3240ab44c8be6'}>, <Document: {'content': 'essentially is the study of relationships between variables, more specifically it looks ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c754fec4615b23f4d0d1d417f902ba41'}>, <Document: {'content': 'at understanding the relationship between 1 or more input variables and their relationship ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'afb0b5f77ce96ec4cfacaa90c827dfb3'}>, <Document: {'content': 'to an output variable. And the exact nomenclature is sometimes different, we use the words I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '843a1e045bb73314fafd4fb3185f6bf'}>, <Document: {'content': 'am using the words input and output, you might also come across the term response to describe ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8c553eded55b64c9ead7a957f1d6bad'}>, <Document: {'content': 'the output or dependent variable to describe the output and for the input variable you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b14dd77f482a7216591dff98382d1dc8'}>, <Document: {'content': 'might come across the terms explanatory variable or independent variable for the input. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ae7bb5b38e7958271e432b8de8c578d4'}>, <Document: {'content': 'So, the idea is to study this relationship between some set of inputs or one input variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd44ae8801e6f06e940c6919bb87b27c'}>, <Document: {'content': 'to this output variable that you have in mind. And, so right from the minimum you are looking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3db3c48dd0186704e74d8ab6e9e08349'}>, <Document: {'content': 'at two variables and the whole idea is, have you come across this kind of studying relationships ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b9e777ec286e325cba7321e23835a61'}>, <Document: {'content': 'between variables, so what is regression doing newly. And the idea is yes, you have, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d3b49995534183eafc22cbecfb9e1c0'}>, <Document: {'content': 'studied, when we started looking for instance two sample tests, two sample t tests for instance. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6178b4641dd6c2f196013344416e15f8'}>, <Document: {'content': 'Right there, you for the first time you encountered scenarios, where there were two variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5153bae258c7748cf712d3e0258c881'}>, <Document: {'content': 'involved to jog your memory. An example of the two sample t test that we looked at was, for instance blood pressure and the idea of the average blood pressure was lower when ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '507815130fbe468a9c067c3f3dbbd3b4'}>, <Document: {'content': 'given calcium supplements versus when given placebos. Placebos are just sugar pills that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87ca6f39851e61e97d1ae7701f0d0ab1'}>, <Document: {'content': 'kind of it look like, the original medicine. So, the idea was there were two variables involved; there was your notion over output variable or response variable, which was your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2aa5bdab7189b051fcf0b1b32ed3ec43'}>, <Document: {'content': 'blood pressure. It is a quantitative variable and you had another variable and this other ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ffeb11999a197541da0f9ce6d53f75a'}>, <Document: {'content': 'variable was, you can call it calcium. And this other variable calcium, it is a variable, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20d052c4ef84870fc84a4694b5bbf2e2'}>, <Document: {'content': 'because it can take on two states. It can take on the state yes, meaning you have given the actual calcium supplement or you can take on the state no, which I am using as proxy ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd70cdbc4c6e92a134bdda2d051acd74d'}>, <Document: {'content': 'to mean that you were given instead the placebo. So, you now have two variables and you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22493f226c23d492f49092bddd30fe01'}>, <Document: {'content': 'think of your, this variable calcium is often called the treatment. But, you can think of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e508eec6ef4d15ed17ee9ad7e4de9402'}>, <Document: {'content': 'it as the input variable, which takes on two possible states and you are looking at essentially that to see, if there is any difference in blood pleasure for people, who given calcium ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d2d4ee4a881cf09605dc00ac243a0f3'}>, <Document: {'content': 'versus people, who have given placebo. Right there, you are looking at a relationship between ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6bd50e93a040ca2cdfc8ba2a8d85784b'}>, <Document: {'content': 'two variables, you are looking at the relationship between the variable calcium and the variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59cd2fc252093544bcb4013ae2c6843b'}>, <Document: {'content': 'blood pressure. Now, another example that we saw on the two sample case was boys and girls in 10th standard in public schools to see, if the average height ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77fdc44cae048b2346cf0a937f5af61e'}>, <Document: {'content': 'of boys was equal to the average height of girls. Again, here your output variable is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4568b95fd44eacab3c15cc1161077f3'}>, <Document: {'content': 'the height and your input variable is gender, whether you are a boy or a girl. You are looking to see, if there is a relationship between gender and height, yes the answer is; obviously, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f0f95da6b32a71fa19cda4376b86697'}>, <Document: {'content': 'there is, but may be in 10th standard is one higher than the other in public schools. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ae6d15ad29c0ea456ddc564ac289373'}>, <Document: {'content': 'So, you went even beyond that, for instance with something like an ANOVA, Analysis of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '799a329a062b59a1b9f87795b201b384'}>, <Document: {'content': 'Variances that we discussed in the last class. You go beyond just having two categories of your input variable, you can have multiple categories. We saw the example, where we looked ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84c929c0e61ef181ba3fa0e2722b2d06'}>, <Document: {'content': 'at states as the input variable, we looked at height of 10th standards, to make it simple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ad51d34aea4ca6b8829d1e0c6f4b79'}>, <Document: {'content': 'I just say boys. But, you looked at the heights of Tamil Nadu, Karnataka, Maharashtra, there you looking at the relationship between state as your input variable and height as the output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1de4905dccb38ee58bb55b031c2b625e'}>, <Document: {'content': 'variable. Is there any relationship? Is there any difference between boys of different states and their heights? And finally, we also looked at something like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'adbd6fca3fbec9d50d8b8a6412b2551f'}>, <Document: {'content': 'test of independents, where here you were looking at two categorical variables and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b6bc3d5cb2b29d741eb0182f0d3cbea'}>, <Document: {'content': 'were looking to see looking to see there was a relationship between them. But, so the reason ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3bf95877c83b0859c9fada8a505c4382'}>, <Document: {'content': 'for going into all of these and kind of revising some of the topics is to say that, you have studied the relationship between variables. But, for the first time you are going to go ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ebaf96e660be4309463dfd5de7e736c'}>, <Document: {'content': 'beyond dealing only with categorical variables. Regression analysis is especially powerful, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a8c61e18bb860b0cf951eac3a740ec3'}>, <Document: {'content': 'because it creates a relationship between two variables, but this relationship can, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a01fdec5af4409d7f692fb89150117ac'}>, <Document: {'content': 'both these variables can be continuous and quantitative and therefore, the scope of application ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa18b9296a9309aa3a46e04051e79d61'}>, <Document: {'content': 'for regression really becomes much greater. So, to give you an extension for instance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6e7ab2f4f570c7b211bf8e6a55671a58'}>, <Document: {'content': 'of some of the previous examples we have looked at from the top of my head, one thing I can think of is we will looking at the relationship between blood pressure and you would given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6bf4ef6bd37e2879b4a03483f5d08510'}>, <Document: {'content': 'calcium plus or placebo, so there is just two possible states. Now, what would happen ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4730847718058c5b4add864a5a3c0aa'}>, <Document: {'content': 'if I gave, if I had 20 different types of terms, which started on one end from the placebo, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ac2c21f0dbc1e697393a39d790b3a35'}>, <Document: {'content': 'which had 0 calcium and went all the way to the other end, which has 100 percent calcium. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '63e7fc9418f464bb80a9103b5a781109'}>, <Document: {'content': 'And in between, the 20 different types of pills, I am not saying there are 20 pills there are 20 different types of pills. The different types of pulse had varying degrees ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3aa8419faf767acb0bfcdde4123c4a8'}>, <Document: {'content': 'of, varying percentages of calcium. So, now, you have at least 20 states and those number ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3bc2655330522bdec9794cffc41c5f81'}>, <Document: {'content': 'of states can, if especially if we are randomly sampling, that can be infinite number of states, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e26c2144cc701305661af5bb6e46a5b3'}>, <Document: {'content': 'but you just have a range. So, it is a continuous quantitative variable between 0 to 100 percent of calcium and your output variables still remains your blood pleasure. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca27ab85604a5ea1295594b51f4d3f90'}>, <Document: {'content': 'So, you can actually create a relationship between these two variables, it can be a mathematical ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abc2b6f431f36d97c2658cad606a1373'}>, <Document: {'content': 'relationship, it can be a graphical relationship, but it goes beyond just 2 states or 3 states ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a1cf053a71d86288ca0b56ef75c5388d'}>, <Document: {'content': 'or n states. It has the potential to go to infinite number of states of the input variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e7660377c192e47811b72633dbf13e17'}>, <Document: {'content': 'or variables and, so it can describe you know, richer things. The idea again behind regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5890d839cf77e39977d64c77264766d'}>, <Document: {'content': 'analysis is to not just unlike most of what we studied with inferential statistics. It ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16a855373d90b123d9b5512ae0e07985'}>, <Document: {'content': 'is, the goal is not just to establish that there is a relationship, but it is to quantify ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af9fe315de672e7eaa9825cdc2349ab7'}>, <Document: {'content': 'that relationship as well. So, a lot of emphasis in a regression is given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a40c62076e9700470eb7a4c7b5589da3'}>, <Document: {'content': 'to the model itself, that mathematically equation that describes this relationship between the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4f5fef9051aee4b8cf48c7232d6ac7a'}>, <Document: {'content': 'input and the output variable and that can be either linear on non-linear relationship. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ae5e5bf7859728748c69bcd6adaa1c'}>, <Document: {'content': 'That, the relationship between amount the calcium that is given and the drop in blood ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '24799d6c7aa050ed3bb834acfead30d1'}>, <Document: {'content': 'pressure if there is 1 can be a linear or non-linear relationship and you can extend this to all the other examples. Now, as with most supervised learning techniques and that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7aba2b15249bd050b2e1499a81571e3f'}>, <Document: {'content': 'is something that we briefly discussed in the course overview. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58fb30626c6e3ae371eeacd00ea18827'}>, <Document: {'content': 'But, it is also something that you will hear a lot more often the upcoming lecture. The ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22b3893c32cd431caff2b92b6e4633f6'}>, <Document: {'content': 'goal of a regression can be two fold, it can be towards prediction, the idea that you now ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1de5a849e379ce6f11d94a2b8debd06b'}>, <Document: {'content': 'create this linear relationship. And, so I can now use it to predict, what the drop on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '810613ef91487a5e3b95ed0103a594c9'}>, <Document: {'content': 'blood pressure would be for a given amount of calcium supplement and that particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a63b2ae776b68ab15c57fadb762af3'}>, <Document: {'content': 'amount of calcium supplement might not have even been something that was given in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '626441ba0dac50d9481e92885ab0b0d9'}>, <Document: {'content': 'original data set from which I built the line. But, the fact that I used this data to create ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14aba095e86cb1bc6df802acf3b389b'}>, <Document: {'content': 'this relationship in the form of a mathematical equation, which represents a line, which represents ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '129b60edb9a2b8d74764463379c131ca'}>, <Document: {'content': 'the relationship between calcium supplements, the amount of calcium given to the drop and blood pressure allows me. This, the fact that I have created this allows me to use this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1198fd2d4fdd67b4dc7407c7233ba1d5'}>, <Document: {'content': 'tomorrow to predict, how much the drop and blood pressure would be. If someone came and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e099584ab14643550ddcae4d4f183c3e'}>, <Document: {'content': 'told me, I am thinking of giving this amount of calcium supplement, it allows me to make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1b15ceb0a07b2d48fac865b203a1005'}>, <Document: {'content': 'predictions. The other thing that it also does is, given that you are now able to create a mathematical formulation, it gives you an understanding ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcfc27b6824b3859dd1db4210ecd233a'}>, <Document: {'content': 'of the word. It gives you an understanding of, how far how much extra calcium that I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d46b72885c8e445efebc11f8c8771ba'}>, <Document: {'content': 'gave, how much of a blood pressure drop will I see. So, it is an understanding of how these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb0334234c585c8bcf7156321c4ac9e7'}>, <Document: {'content': 'two variables are actually related and that is, what we essentially call as one prediction, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef341e04ee919b66d82d2be56d669b81'}>, <Document: {'content': 'which is one goal, which is the kind of predict and the other is interpretations, which is the kind of interpret the world that we level to through the lens of this linear relationship. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '458db3250a14417b1617599940a6224a'}>, <Document: {'content': 'So, let us just talk about just to give you some feel for where a regression analysis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35287a720f267f5577504880c20a9fcf'}>, <Document: {'content': 'is useful of what kinds of contexts. Here are some examples that we have listed. For ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '89993126f2b43fae6fa909f6d11e1e6d'}>, <Document: {'content': 'instance, so how do wages, how do salaries of employees get affected with variables, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '382ba19a0723f70c9833fdcb885ec2ed'}>, <Document: {'content': 'such as experience, education, promotions, etc. So, the idea here is that salary is the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb1e660a9d94ba1e7472dee8a58138b0'}>, <Document: {'content': 'dependent variable, it is the output variable, it is the response variable and variable such ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9328dce7cdfff537881564e156449ffc'}>, <Document: {'content': 'as experience, which might be measured in number of years, education again measured in terms of number of years after high schools or something and promotions, again a numerical ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d66c6a5db36bf28c152661355e2e9e2'}>, <Document: {'content': 'value, how do these variables affect the output variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77fc9131b3da461692d4844c780b21a2'}>, <Document: {'content': 'Another example is how does the current price of a stock or a share depend on it is past ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b2b6f4efd827baa16330d5ed9973374'}>, <Document: {'content': 'values and perhaps, also on values of the market indices or other stocks. A subtle difference ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e9be485cf184c8d7963e688e7f4cfd01'}>, <Document: {'content': 'here that you might notice and we will circle back on this is, in the case of the stock ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3300bd814d209d835645d781fc1889ea'}>, <Document: {'content': 'for instance you will notice that the output variable is the price of the stock, but the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd43e31d64aa0025c2db9f08645a46b6f'}>, <Document: {'content': 'input variable is also the price of the same stock on previous days. So, how do it is current ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a03695c369f3dea0f42993ac15547a55'}>, <Document: {'content': 'price depend on it is past values? So, here the past values become the input variable and the current value becomes the output variable. So, we call that time series and we briefly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ee4798e23c50e466b313705ae5ba4fc'}>, <Document: {'content': 'revisit that concept as well, but regressions are also used in that context. So, the next ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94cf5e3fe91cb790c7f5a4e004f8600d'}>, <Document: {'content': 'example is, how does sales revenue get affected as functions of advertising expenses and comparative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '141dbe442c057c798c13f7c4a3afdfac'}>, <Document: {'content': 'advertisements. So, the more money through into advertising you might have belief that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90f4dcb5516b91c1a89f7c3fb9e48b0d'}>, <Document: {'content': 'the sales goes up, but what is their exact relationship. Can I look at past data on different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f14abd9c9eff12810b345a1c454c05b5'}>, <Document: {'content': 'advertisements that have different campaigns, advertising campaigns that are made for different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'badaa8d50e039c7641cc4f0dcbee8067'}>, <Document: {'content': 'products and what my competitors did and how that affected my sales revenue and create ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fde62b6e69125c152ca0adb674725a25'}>, <Document: {'content': 'this relationship? And also something else from the mechanical engineering, how would be relationship between speed of a car fuel efficiency within a certain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed21b5b29fc8d51a3ebcdbf4cbe46805'}>, <Document: {'content': 'range, when you are driving on a certain gear, what is a relationship between the speed, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6db5d13c0f151f997a4d092838334bb'}>, <Document: {'content': 'which you drive in a fuel efficiency and you could take a sample of 10 cars, make them drive at different speed or you know, take the same car, make it drive it at a certain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e1fde3d020e7e3ef2d5cf26d179c183'}>, <Document: {'content': 'speed and get data and try to fit a line in this data to understand the relationship. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce0899e92af0b159c11f9650c691f263'}>, <Document: {'content': 'And finally, another example could be how does the price of a house get affected by the number of bedrooms, the square footage of the house, you know it distance from the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a639b43e9a40c867de6f891e224768fa'}>, <Document: {'content': 'center of the city and so on and so forth. Again some of these input variables need not neatly fit into being continuous quantitative variables, but the idea is that a regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8debdd1f4ecf5d8ece5b5da3c1654626'}>, <Document: {'content': 'is not constrained by that. A typical application of regression would use continuous variables, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd06ab649d1e204bc53fbfa0ddcb3c6ef'}>, <Document: {'content': 'but the whole selling point associated with a regression is that, it is a technique that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa45bc80e1527aaa3360d5e7d0fe1bb0'}>, <Document: {'content': 'also capable of dealing with continuous quantitative variables, whereas a typical ANOVA or a t ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5a2fd42ca2d271ec585d0c0d8f83f2e'}>, <Document: {'content': 'test or any of the tests that you seen so far are built around the idea of using a categorical ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1521137e398e1b61c635a638505691c5'}>, <Document: {'content': 'input variable and that is, what they meant to do. So, let us talk about some more concepts associated with regression. Just for reference, I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb53ad4c197547384705dc2f8d24a5'}>, <Document: {'content': 'shown you a typical graph, graphical representation of regression, where the x axis is your explanatory ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff3b1eaf1fd54b860a8450cf3dd49d04'}>, <Document: {'content': 'variable, your y axis is your response or output variable. And each square is a data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52d5f0fbcfdb2e3690d0b5a710d13e1f'}>, <Document: {'content': 'point, by square I mean each of these small points, it is a data point and the idea is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f28237fef87b9236989e553f18a83b36'}>, <Document: {'content': 'loosely the idea is to kind of fit a line through this data point and that is what we would call linear regression. If you are trying to fit a line through the data point, we would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31bc57f9aac0dd4b9ba3d3d07fec2f1c'}>, <Document: {'content': 'call it linear regression. The word actually linear regression could also be used, when you are doing a linear combination of variables. But, some of these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ad3ecb6d07a5418183d7e62c52d1fc'}>, <Document: {'content': 'variables themselves represent a non-linear transformation. I think, the simple way to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cb79616b1c0cfbe4e176958b6f316ff'}>, <Document: {'content': 'put this is whether you are looking to have a linear relationship between the input and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e37b7e4c335fe2b63d49796d7b7b07'}>, <Document: {'content': 'output variables or a non-linear relationship. You could still use the core concept of regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4385eeaf0e790235ab2c0a1f310e081'}>, <Document: {'content': 'but you should be a little careful in terms of what gets called linear and what gets called non-linear. Because, often the camp of linear regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8445ba294c91b342e61c89fc6674fdf'}>, <Document: {'content': 'could include variables, the input variables themselves which are non-linear transformations ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58146d26f6153d0d3691f9b40ea49256'}>, <Document: {'content': 'and, so it would still be called a linear regression, but it will not exactly look like the graph that you are looking at. You also have the categorization of simple versus multiple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2bc1d2191767369ec60736481d397b4'}>, <Document: {'content': 'regression. The idea behind simple regression just means that there is one input variable, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58b0e2e993fb52eaa9fa10700cb2e2bf'}>, <Document: {'content': 'whereas in multiple regression you have more than one input variables. So; obviously, in the graph that you see in the slide, you looking at a simple regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'acd061d8712abeb1fa4f35bcabb85ae3'}>, <Document: {'content': 'because there is one explanatory variable, which is the x axis, if you have more than one that becomes hard to represent on a two dimensional slide, you would have to use a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80d628ab1791563b0b34f48c754fa733'}>, <Document: {'content': '3D model for two variables and then, after that it becomes much harder. But, the idea ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5748ea277d2a98e648375a3f5d44f966'}>, <Document: {'content': 'is that if you have one input variable it is simple regression, if you have multiple input variables more than one input variable it is multiple regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6a266819a0db60b7bc10eb1c1b37104d'}>, <Document: {'content': 'We then come to the next concept that we briefly discuss which is cross sectional versus time series. The idea behind cross sectional is that, it is not a function of time. Your data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a456a15b709b3b04081d2c9f01397398'}>, <Document: {'content': 'is collected across the board and that temporal, the time component of basically means that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5981396302061da935d3099666b97a8f'}>, <Document: {'content': 'it is not either function of a time or it is not a function of it is previous values. So, you can put all the variables in the basket and think that they were all created at the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ab04f8b81e473c35aaeb5d4f54db049'}>, <Document: {'content': 'same time or at least that you do not care, whether they were created a different points of time. You are still going to treat them it is just different data points and look ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '690fd597676c9b4d298c5d2fb93e8239'}>, <Document: {'content': 'at your relationship and that would be cross section, whereas time series is the idea that previous values in time affects subsequent values. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64c6294f361f0402162cba0ab2dfef70'}>, <Document: {'content': 'So, your modeling techniques themselves become a little different and classic example of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2be8205dbb6105cb9e2c77c66458682'}>, <Document: {'content': 'the time series was the stock example that we spoke in the previous slide, where previous ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '796686a9bf250f486ca722f79e703c99'}>, <Document: {'content': 'price of the share influence tomorrows price, where as the equivalent of the cross section ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50ea9bef260ad0a90d8318a5bb22001e'}>, <Document: {'content': 'analysis of that would be to completely ignore the stock price of this to completely ignore ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e62760e3f673ed652988df07a16582da'}>, <Document: {'content': 'the previous days or previous time periods stock price and just say can I predict what this stock is this stock is value is going to based on other stocks or other indices. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90c6046d4bcbe660c9f214d9c7ff651c'}>, <Document: {'content': 'So, can I use the, NIFTY index can I use another stock to kind of say based on the stock this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8696602987d82692f2ec7b6ece8c9c5'}>, <Document: {'content': 'is what the stock should be this is what the output the response variable stock value should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8d49032970f410b37909b3fbfc3a927'}>, <Document: {'content': 'be. Next we just come to the idea that this is notion of response variable or it is called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4506a68647568b8b8037f61daa234d98'}>, <Document: {'content': 'dependent variable and the input variable is called explanatory variable or independent this is terminology that you will keep hearing and, so it is kind of important. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9ddf5c9f50737b7fa892b2bd66e41b0'}>, <Document: {'content': 'So, the next important point is that we learnt of about something called scatter plots during ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8e4b312221cb2c657d7f45238f8bcc8'}>, <Document: {'content': 'the descriptive statistics phase and regression analysis in many ways essentially captures ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22c81a843b64e4bd7b07a388721e8757'}>, <Document: {'content': 'mathematically, what does scatter plot tries do graphically. So, scatter plot tries to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70fe5c24630a868d4ec2d5b2e4b5a045'}>, <Document: {'content': 'graphically show you that relationship between two quantitative variables, where is regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c235d8cdc992c536842761d0cada5b3c'}>, <Document: {'content': 'analysis tries to go beyond that and tried to actually fit a line to this or fit a model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ca91c4f9d1b2887c5b25ba3c05a4f5b'}>, <Document: {'content': 'to be more general fit a functional from to this data and therefor that is the improvement ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9a3f0e25715cf39efbf7e7213744e0ba'}>, <Document: {'content': 'on scatter plot that regression analysis does. But, often even before getting into regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42ae59b2ae260a62d7f33a1fe01d522e'}>, <Document: {'content': 'analysis scatter plot could be very useful graphical window into what you should expect ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62317b71d3248f2780e57e09a26455ea'}>, <Document: {'content': 'to even see if you should try to fit a linear equation or non-linear equation, because just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2fb21ce6ac3acaa70ee95fe51c2e0aef'}>, <Document: {'content': 'visually you might be able to say you know this does not make sense to fit a linear equation, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75550ba97f47e4a83b7951fbc2c48809'}>, <Document: {'content': 'because that is not the co relationship. Another big advantages scatter plot is that if there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6e1fe0092e00aa6fb6238db6e1a27'}>, <Document: {'content': 'these outliers in by outliers we mean essentially there are data points, which look like they ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc01c433f3d0dab9683c47c8a15caf23'}>, <Document: {'content': 'are essentially just errors or something. So, you let us say you had this data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2ac0739eb5e13b9c87cc1589b3bb892'}>, <Document: {'content': 'that was in some where completely unrelated that could come from an error in that it completely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54b47b9301a1e5cf3e58cb022384f05a'}>, <Document: {'content': 'ruin in a regression analysis along, but if you see it in a scatter plot you might choose this say I want to ignore this data point that is idea a behind scatter plots and outliers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dfbf8d530f5e2bb45307f18ea5eb50e5'}>, <Document: {'content': 'The next concept is an unequal variance we going to talk about this little bit, but the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e187e2c088eff1f702a7dbe3f70d4e4'}>, <Document: {'content': 'idea could be that they could be a very strong linear relationship or non-linear relationship. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '331f569e4d58d5476705a7823454dd35'}>, <Document: {'content': 'But, the variability at different points of x or your input could be different you could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '465d95e3a3e631f6ad60e7d47794715'}>, <Document: {'content': 'have something that looked like this in terms of the data. So, this is the line that your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14a5371bbfcdd0ccbe82151268d654ee'}>, <Document: {'content': 'fitting, so there is this central line is still the line that you are fitting and it is a linear line. But, the data points in the lower end, so in this side of x the data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a27050a322bf6fcc1278efa450a3019f'}>, <Document: {'content': 'point a closer to the line, where as once you go to this side of x once you go to the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b11198d2ec64be4d0448ad02d0121247'}>, <Document: {'content': 'right side of data points of far more spread out. And that influences, how the line gets created in there is in techniques that available to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e62d062a58170b43e44869185646592'}>, <Document: {'content': 'kind of counter this problem it is a problems, because again visually you my just see this speaker phone kind of shape and say still I am just going of fit a line in the center, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '47d6cfa06d2adc0a62a9d50e1e66c145'}>, <Document: {'content': 'but this higher variability on one side and lower variability kind of makes does not always ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3dafc99d18ae0303350c2002ddc66511'}>, <Document: {'content': 'result in the correct line being fit and you need some kind of transformation to understand ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f697432583039174a5f2be8cd673dbc'}>, <Document: {'content': 'the relationship better. So, the last concept associated with the slide ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61c2be83150dc7aa0769de061e3f459c'}>, <Document: {'content': 'in what I want to talk you about is with co relationship. So, co relationship you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '573265ddafaa01868972509cbe7fcbe7'}>, <Document: {'content': 'think of is a more advanced form of summary of descriptive statistics we didn’t speak about in detail or in the descriptive statistics part. But, the idea is that when you have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1cda5234014aceeaae825ed5f63a12'}>, <Document: {'content': 'two variables just like a scatter plot can be used to graphically describe these two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30c47a028484d489178357bc7491b480'}>, <Document: {'content': 'variables, correlations is a single number that describes that a linear relationship. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4791bacb40dbc653428d6aeff9e839f8'}>, <Document: {'content': 'And its essentially a quantitative indicator of that linear relationship, what you will discover is that in a regression analysis you will also be coming up with certain numbers ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f714668e7ba61670196686e8dd64ccf9'}>, <Document: {'content': 'that quantify this linear relationship and if you have not already heard if this you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bc93c518d2862e0f6a8f77820b6125c2'}>, <Document: {'content': 'will come across this something called R square and that has a direct quantitative mapping ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a7985b6ec58c7c746fb9735ff8e5befd'}>, <Document: {'content': 'correlations. So, it is not a new concept essentially R square is nothing but, correlation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e6d316ed2f8fc86c94917c354def295'}>, <Document: {'content': 'square, but you will come across it under the term R square, where as you might of heard ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4d2226b487ba284bec5b885bd72f7b1'}>, <Document: {'content': 'is of the word correlations more colloquially used or used more in the concept context of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ce94cc279ed8b278f58f9da151462c5'}>, <Document: {'content': 'descriptive statistics. Now, let us just briefly talk about the exact roll of descriptive and inferential statistics. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '950a4453e822514089fb3489dd6bbc35'}>, <Document: {'content': 'If we start first with descriptive statistics the whole idea there was either graphically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e57a1cd80eaf057905ebae04d71d9c49'}>, <Document: {'content': 'or to quantitatively summarize the data that you see. And one way of summarizing that one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'efff5cdc709a7d9b2ccc72fbbc2be978'}>, <Document: {'content': 'essentially summarizing that you do of the data in regression is to capture the linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2af2654bd1c239dd50e46ada26ad4a1b'}>, <Document: {'content': 'relationship, which is there in the form of the actual data into line. And, so you essentially summarize this relationship in the form of an equation and many of you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '541ebaa9f9ee6513597511f039fbea2b'}>, <Document: {'content': 'might know that if there is if the simple line in two dimensions you have this formula ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f230312ddd28fd5c8e41a073412f9911'}>, <Document: {'content': 'y is equal to m x plus c you might of come across that in high school in some form or the other, where c essentially represent the intercept of the line on the y axis and m ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '220e9d49d412aa03295b54d6bd700e53'}>, <Document: {'content': 'represents the slope. So, you can think of ms that angel m represents ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b950bfe8970723e99b2f3cf158ace0'}>, <Document: {'content': 'that slope and you can in regression often we use the terms beta and here I am just using ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b006eb4fc57354d774ddaefd2b3e1c1'}>, <Document: {'content': 'b not and b 1, because I am representing the sample based of the sample that we have we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb0a703aed8986e42858f2edb2f6d9fc'}>, <Document: {'content': 'have some estimate of b not and b one. But, the idea is that this is some form of summarizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f0dcf3efc5952de4aff9519d49de47f'}>, <Document: {'content': 'the data, because you taking the data and summarizing it to the single equation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b501a44cba7cf06e95308764abfb557'}>, <Document: {'content': 'So, what is the concept between behind, how you do it and the idea is there is some form ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5046b64ced66b927befa389511bd8719'}>, <Document: {'content': 'of optimization it is the form of optimization, because you have a set of y s and x s. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3939d66a9727aa58f167f47fe9278d0'}>, <Document: {'content': 'you are given a series y s and x s and that is your data and for a each y and x you essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57bd87157b318766e3b718be48ebbdf9'}>, <Document: {'content': 'plot it on this graph you than go had an you take line and this is that line that we are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1316ec6560066c726d6384fddb78f91'}>, <Document: {'content': 'going to play with and you choose some criteria to fit the line through the data points. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7ae45006f4f2abf9a2aa8766e6ca79e'}>, <Document: {'content': 'So, you might say that my goal is to fit a line through the data points and you know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe9cb123060f81067995dfb66acb8f8a'}>, <Document: {'content': 'I do not want to draw a line out here that has nothing to do this data. So, what kind of a line as something to do the data I am its say well I want the line to split the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1aeaa8ef30ea5d6d7b2dd332b4e073d'}>, <Document: {'content': 'number of data points equally above and the below it. So, this will do be overlap the, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '12751b41001126f2500d0541f353f016'}>, <Document: {'content': 'but we get the point, which is that you might want you might want that line to split the data points another idea could be its say I want the line to go through the two extreme ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6687bf4ea6ccde5f5071ed1ab26361fe'}>, <Document: {'content': 'points. So, there are two extreme points I want two extreme points to be connected by a line and you can have various other criteria and another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b8027a4dfe2756298d6613a37518eea'}>, <Document: {'content': 'very common one could be to say I more advance one could be to say I want to minimize the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84c54807411c0d2ef9a8520a7cdaf427'}>, <Document: {'content': 'distance between each point to the line and that incidentally happens to be what we call ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5770db358ea5e4c49bb8dd9d3047b92'}>, <Document: {'content': 'is the ordinary least squares. But, we will get into that later, but that could be one criteria, another criteria could very well either I want to minimize the perpendicular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '15b6cfa9b541acfd8a9bac96419564fb'}>, <Document: {'content': 'distance to each points to the line. So, you could have various criteria and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5029faba7e3ef468b577b265e8fa6c6'}>, <Document: {'content': 'know you can choose some criteria and you can say I want to achieve that criteria and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '592421178869897218570fd61bf1a773'}>, <Document: {'content': 'that is how a want to fit a line. And, so at the root of doing that is the series of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '514466e320c42bc9edc93540006665fc'}>, <Document: {'content': 'techniques and optimization where you say I want to do some form of optimizations. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7005979393db8066d809d4ecedf3e90'}>, <Document: {'content': 'you might ask the question, what are you relay optimizing and the answer is your really optimizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'edf5dff1713ff4979f78bdf566b00869'}>, <Document: {'content': 'this objective of minimizing some metric and let us stick to the metric of minimizing this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af7823040d43e9e98e121e5e2cdeb1c0'}>, <Document: {'content': 'distance this sum of this distance. So, how are you optimizing that, what it can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '167468ab3bdb9cfd811dc49deb994574'}>, <Document: {'content': 'you change what you can change is this line and what I am going do is I am going to change ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb0c565ddec977a122224db7d3b325f1'}>, <Document: {'content': 'the two parameters associated with this line and the two parameters are m and c. So, c ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4574f1bd5deca2a7cdbf019c36299015'}>, <Document: {'content': 'is, where this line intersects on the y axis, so I am going to try and move this line above ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e2a38cc32dcaec3808e949cadf71505'}>, <Document: {'content': 'and below keeping that is slope this same. And try to see, where should this line b such ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '716200bad1d82ea6fe7d1b3f325f253e'}>, <Document: {'content': 'that I minimize the sum of these distances those red lines such you see my goal is to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e835531d69658edbd81b3134ee520c3e'}>, <Document: {'content': 'minimize the sum of these distances. And that is an optimization to see that is my objective function my objectives is the minimize the some of these distances. And ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b832c6f0f9e069c78a807fa23ca818b'}>, <Document: {'content': 'I am going do that by changing two variables I am going to change the variable c by moving ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '976c1656568f52c6a5b799428986f8ce'}>, <Document: {'content': 'this line above and below and I am going to change the variable m by rotating this line, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55448a7bf3261d401baae3906c437ad2'}>, <Document: {'content': 'so by rotating this line, so this way and this way. So, simultaneously I am trying to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1810409161d1fe2380acf0d8ef2bbf6'}>, <Document: {'content': 'change two variables and thereby find that line by changing my m and c I find that line, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4eb06b839b10a7c555683a3f37e0958'}>, <Document: {'content': 'which minimizes this deviation the deviation of each data point to that line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1789b9ce5a30550829b6f5c0e15fa8e4'}>, <Document: {'content': 'And there are different ways of doing there I am just giving you one objective that am ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7a27dbd54a82cbbd7e06544cf46c2b1c'}>, <Document: {'content': 'I choose to optimize. But, at the root of it you need to realize that ultimately the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bc305e653df92a853a02b817459f13'}>, <Document: {'content': 'process of regression fitting a regression line is process of optimizing and you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68c770a418d0df10aee3ac834b3aa40c'}>, <Document: {'content': 'different criteria ordinary least squares, which is one type of regression uses the criteria ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d3c9f620edf8f1033c158c5212bfab'}>, <Document: {'content': 'of taking this distance of each point to the line and squaring it and trying to minimize ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6986ab3e5f6bcdd5d2ae135d4a9fd4e1'}>, <Document: {'content': 'the some of those squares. Because, each point will have some distance to the line and then, you can take that distance square it and then, you can take go to the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9dadfafd324294d980c1429fbe88ba84'}>, <Document: {'content': 'next point in do this same and then, you can sum all those squares and least squares tries to minimize that objective function. But, that need not be the only objective function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcaec12c6e7a74727107d19d5c8776e3'}>, <Document: {'content': 'there can be many other objective functions, but at the root of it the idea behind putting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fadc41b1e01cf78fe5442c42f6942591'}>, <Document: {'content': 'line through many data points is to say I am interested in maximizing or minimizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e112dab6f641a2291f2ab992f8140e0'}>, <Document: {'content': 'some goal associated with the process of fitting the line. And I am going to do that by changing the two variables set I can, which is m and c ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc925049027414117dbbca94469ba10e'}>, <Document: {'content': 'or in other words you are b not and b 1. So, that is essentially the summary statistics ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ca4907fa7d74aed4be43e6ee9e1fd6'}>, <Document: {'content': 'the process of describing the data through line and that is how that line gets created. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '49bc5ca5acff0589d3eab68b193cfa17'}>, <Document: {'content': 'Now, where is the concept of inference the concept of inference again comes from the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '706b3fd6b8d4b9d154dc553e0953d059'}>, <Document: {'content': 'score idea there ultimately these data points are samples they do not represent the population. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e7c5c2c4a83f7e886f9daee061ae771'}>, <Document: {'content': 'So, any b naught and b 1 that you calculate are coming from this samples, so there essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5afa8dc7e2080a6996034b4b1ecf4c1'}>, <Document: {'content': 'the sample statistics. So, but just like x bar in some sense is the sample mean, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4925d96b7d49dc3b9db73473d17ddbd3'}>, <Document: {'content': 'is a point estimate of mu, which is the population mean these this b naught and b 1 essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '36eb0b2cc6fd94a251c08ab8d01ddcca'}>, <Document: {'content': 'represent beta naught and beta 1. So, you can you essentially have the population parameters ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e0359ccf60ed7d8beb49487de55365f8'}>, <Document: {'content': 'beta naught and beta 1 yes these are the population parameters let us just beta naught in beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b9e9c45080f7e4f47b5a48ff47026579'}>, <Document: {'content': '1. Now, the idea is that, which statistically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5fdb3ab2695daa1341b172abde0d21ae'}>, <Document: {'content': 'inference what you doing is your trying to see if either of these terms beta naught or ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '916947394f9c95490a26e4605376e80e'}>, <Document: {'content': 'beta 1 are actually equal to 0. Because, if they are then, they do not have any business ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e281b95c8a07dea1b079eecbce9318d8'}>, <Document: {'content': 'in being a part of this equation this equation that you have out here now the this equation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62760e9c48cb7bd314a01a9fcb1eda14'}>, <Document: {'content': 'that you have out here that is an equation that you started with before you even fit anything. Now, you a just in the process of finding out to b naught and b 1 through some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '159120dd7735f9670ac71b91178cf849'}>, <Document: {'content': 'optimization procedure, now imagine a situation, where x has nothing to do with y, x has nothing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3467204d0ee0de69777aafe37652de0f'}>, <Document: {'content': 'to do with y. Now, you calculate the sample of x s in the sample of in y s if you had infinite number of sample you might arrive at the conclusion ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b42e331e7db739eaead901a99e63f4ff'}>, <Document: {'content': 'that b 1 is thoroughly equal to 0 therefore, this terms itself becomes 0 on should not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'be0eaf444b1f0974d20bbda0ef140135'}>, <Document: {'content': 'be there in this equation. But, you still only have a sample and the sample is 5 data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1dbc4067e4430141d5fe992ba353cfd7'}>, <Document: {'content': 'points 10 data points 20 data points. So, it is perfectly possible that even though true beta 1 is equal to 0 that is the null hypothesis even if that is true you it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8a20a6a129d2d57c9a9f784c2cde2936'}>, <Document: {'content': 'possible that you just take a sample of y comma x s and you get some beta b 1, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '892a59db4f670a2dc526fdc4f211bf47'}>, <Document: {'content': 'is not equal to 0. So, b 1 winds up not being equal to 0 even ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3acd96daac774a8d50114a664b0b5724'}>, <Document: {'content': 'though beta 1 is equal to 0. So, you might be erroneously thinking that x has this relationship ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7a7fa0470e81143cdb2036ef9707bde5'}>, <Document: {'content': 'to y, where is in reality beta 1 is 0. So, the whole idea is to do a statistical test ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e5266a00ac71455a279afac8a3db5c6'}>, <Document: {'content': 'to see to test the hypothesis the beta 1 is 0. So, the null hypothesis would be that beta not is 0 and you can think of it as also beta 1 is 0 and it turns out that under certain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33642b667ce94d692555856a976bfcdb'}>, <Document: {'content': 'assumptions of normality and even when those assumptions are violated to some extents central ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e044d0ad1bfd50f9c116dff6c0f95578'}>, <Document: {'content': 'limit theorem it terms out that the distributions of the betas of these coefficients winds up ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '603112ced346d4f0dd2555a2c52d96e8'}>, <Document: {'content': 'being t distribution, so like t distribution. And, so you can use essentially t test to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa57de62c109ad7f6d068c4501f36f07'}>, <Document: {'content': 'test the hypothesis that each of these coefficients are actually could the 0 not and the idea ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '427c412fd40e95214c2930311ee2ace8'}>, <Document: {'content': 'is that if you wind up you need to reject the null hypothesis and reject the hypothesis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b312445e3a95e9de4503ac785a77611'}>, <Document: {'content': 'that beta 1 is equal to 0 and only then can you actually have the term in the model and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fbdfe1fc61fa681d8086b445b8a30ba6'}>, <Document: {'content': 'this become really useful when you have you know multiple input variables. So, you might have something that is beta naught plus beta 1 x 1 plus beta 2 x 2 and so on, and it can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8120640298afafce4094e699a4e4c5b'}>, <Document: {'content': 'go on you can have many variables. So, which of these terms actually get to stay ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6b62a09b76e73ad0cf9837476c2090a'}>, <Document: {'content': 'in the modeling, which do not is not just the function of this magnitude of beta 1 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88b25e0e9efee86082f5ceb8ca01e808'}>, <Document: {'content': 'beta 2, because that is the magnitude becomes really function of it becomes also a function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42912b3ea1ab11634e52e895a7381f58'}>, <Document: {'content': 'as magnitude x 1 x 2 in the units and so on. But, what really determine whether these things ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '479ffc05649dc2782a61d88f0fa85a22'}>, <Document: {'content': 'stay in the equation or not is the inferential statistic test that you do for each of their ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd60cbeb1a4783d0bd45480d10f5ad55d'}>, <Document: {'content': 'population parameters, which is for beta 1 for beta 2 and so on. And in, so for is you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b94345b737d754e8bf67734e358564b'}>, <Document: {'content': 'can reject the null hypothesis that these are equal to 0, then you can leave them in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d999e9302120e7ec3cde35c81b5d9c7'}>, <Document: {'content': 'the model. But, if you can if you if you cannot the reject the null hypothesis than these do not these terms do not have any business being in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba9fccac6b36e81ba2f7f87fc1e1232c'}>, <Document: {'content': 'model. So, that is the idea behind doing statistical inference on the individual parameters you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e6e846fc8a66829d37f4950bc362b7f3'}>, <Document: {'content': 'will also notice there in a typical regression analysis or regression analysis output independent ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3587789d01ac34518b4a1ccf3a48e9d9'}>, <Document: {'content': 'of the software that you use there is you will find an inference for the overall model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '617eda449222b5fdc3c85e2d11056b80'}>, <Document: {'content': 'So, in a side from the inference such you see for beta 1 beta 2 separately. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee7cc8c9290df1fccdcba803ffded65b'}>, <Document: {'content': 'For given model you will have an inference and that inference again uses the concept ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb83c65329c0c5d3ad682b6fb548762c'}>, <Document: {'content': 'of an ANOVA and the way it does that is by looking at if you remember the ANOVA in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f65b776f91c438f99ac765296d14702'}>, <Document: {'content': 'past you have the concept of mean squares between and mean squares error, where the mean square between was trying to quantify the effect of each treatment or each state ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7454da60a902f92c9092d14541f62dc7'}>, <Document: {'content': 'of the input variable and mean square error was trying to quantify the inherent noise that was there even with in each state. Here; however, you do not have finite number ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '886310e8417730c5c01962c38f320d81'}>, <Document: {'content': 'of means square between, because they could be infinite states of the input variable x. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fd618a073bf114a5351616ded09b3cbd'}>, <Document: {'content': 'So, you have a single term that tries to capture how much of the variability is being captured ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '801fccf7e62a513f2f93f54c01927a1'}>, <Document: {'content': 'by this linear or non-linear model that you built, how much of variation in x is being ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cbd7ca562f379496948278fc4371e43d'}>, <Document: {'content': 'captured by this equation, how much of the variation in, how much of the variation in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7812908cce5a4e34e2ecb28195bf411'}>, <Document: {'content': 'y is being captured by this model versus how much of the variation in the data points is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cacedca4dbcb9a213736d712a6c25e8'}>, <Document: {'content': 'not captured by the model. So, 1 becomes the mean square of the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d20605dba3f003238fb8c05c8b93b4'}>, <Document: {'content': 'essentially and the others becomes the mean square error, which is just the general, which is some notion of general noise that is there in the system and essentially the ANOVA out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '537cb66f2584132c9e04476ae421a3cc'}>, <Document: {'content': 'here. Again will be using the F test the same ways use the F test when we were describing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8584338aef0f74b835bc6ec616bb98fb'}>, <Document: {'content': 'the ANOVA in the previous class for categorical variables. And there by rejecting that null ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f8dd19e944f74c8f2e3e39e130022d4'}>, <Document: {'content': 'hypothesis would mean that this model explains variability in y, where as if you fail to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '425693130c31a0d1d86cd94fa31aba5d'}>, <Document: {'content': 'reject it the idea is that you cannot say that this model essentially you cannot say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac6be47c20df8168c481402d73050694'}>, <Document: {'content': 'that this model is any different than a few word to just randomly choose y s completely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f29f7a701f080e1a93a2c246349ba6f1'}>, <Document: {'content': 'unaware of what the model was. So, no variability in y is being explained ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd4a9d2356abbdc2ad55fef2a673cf7b'}>, <Document: {'content': 'by different points in x. So, an essentially when you think of that nothing but, the concept ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f682e45efa50ccae22b940d595eb9a'}>, <Document: {'content': 'of straight line it’s essentially concept of something were not describing the variability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ef9ddb637405c36a0d522e722859654'}>, <Document: {'content': 'in y at different points of x. So, various points of x you looking at the same position ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8a25c29c7a50595bacdccd93f51b1d5b'}>, <Document: {'content': 'same position of y I hope that made that gave you an introduction to the concept of regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f13a5530b9f8605c23a183dc8ab5d5ab'}>, <Document: {'content': 'and like I mentioned after we introduce the idea machine learning and supervised learning we will be revisiting regression to look at more end up treatment of how you actually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5da257252320fd117cdac33ca14f4bb3'}>, <Document: {'content': 'do it and give you an understanding for how the co derivation are made. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85a0e155d37796fdb28b5c8b7a45cba9'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Introduction to machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b100a0988325b61d9bb447cb4534285f'}>, <Document: {'content': 'Welcome to this module on Machine Learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52394380f3e831270e47d581ffca469'}>, <Document: {'content': 'So, till now we have mostly looked at data analysis. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'edc4b21fe99c3c70aeafae320de69128'}>, <Document: {'content': 'So, all the tools and techniques that we have looked at have to do with analyzing data and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f9f065048d931b0dea4af5e26f77637'}>, <Document: {'content': 'trying to understand the data better with possibly the exception of regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b34e55dac971be0fd0e90671e666fd25'}>, <Document: {'content': 'From now on for the rest of the course, we will be looking at how you can infer models ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b3622d29574b2df491bc9c99ddc9718'}>, <Document: {'content': 'about the process that generated the data by looking at the data alone. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35afea8d683bddd8e80d83ec74dedae'}>, <Document: {'content': 'This is essentially the idea behind machine learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26cddb61e8fcd3f35c979aa453d9aa50'}>, <Document: {'content': 'So, what we call, even though it is a kind of a fancy name when you have a visions of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f812d787b19d8b27978a2988dcf2311'}>, <Document: {'content': 'Robots and terminated suiting around in your head, but machine learning is essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d641be9e45c0f1071ea0fbe4c972f11'}>, <Document: {'content': 'trying to learn models about the process that generated the data from the data itself. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc8b0a9feb4fa69604a9209a3e911642'}>, <Document: {'content': 'So, you could look at examples where you learn about, how the rainfall pattern varies over ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33726c79eb230841da4dd44a86fa0313'}>, <Document: {'content': 'a season. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a098a2c760b5ae9805f80b9dd9d256f0'}>, <Document: {'content': 'So, you could say that rainfall pattern is very intermittent over the season or you could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac2466697e5e7abf15f8639b5a6f612c'}>, <Document: {'content': 'look at, how the temperature of certain equipment is varying with time of operation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '960c1c5f2c82b6db33bdb50d097a259b'}>, <Document: {'content': 'You could say that the temperature showing a linear growth, it is possible to do this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74dadb2c5dcca3c921f030fd580f9608'}>, <Document: {'content': 'kind of learning from the data by a machine and this kind of machine learning algorithms ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff4ae62129c2b90acd0021efa3dd2536'}>, <Document: {'content': 'becomes essential, when we move away from data analysis into either predictive, descriptive ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84f5f64431eadf7ed8c1be87063a42b1'}>, <Document: {'content': 'or prescriptive analytics. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90c155c228c9ae7f6e49335cb9c3c39b'}>, <Document: {'content': 'For example, I can ask what will be the temperature in 15 minutes of a particular equipment know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '839324da949d6fccf129a7838aa6bcaf'}>, <Document: {'content': 'and if I know that the pattern is that of a linear growth, I should be able to tell ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2645dbacdf633eb65eae803143b28fd'}>, <Document: {'content': 'you what the temperature would be in 15 minutes and likewise, I can ask you what are the areas ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '454e6bd36e13d8bbacbf0d1041b4ffc9'}>, <Document: {'content': 'that are likely to receive rain in the next season. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f62f2f86ebf4887eee8e8cb87005fe59'}>, <Document: {'content': 'Then, if I know what the variation has been seasonally with the data, then I should be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6af0479a6b8007cb94fd0fe54100372c'}>, <Document: {'content': 'able to tell you, what are the areas that will likely to receive rain in the next year ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '49ae28a289f4b9e7b8d58b967da234a9'}>, <Document: {'content': 'and so, when I talk about prescriptive analytics, so the first two questions I was asking you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '96d7954f1b3c83e3d65fdecbd2af1e03'}>, <Document: {'content': 'or more about questions on the system and prescriptive analytics, you will be asking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '728cb8bb6c8672808d155489b281fcc6'}>, <Document: {'content': 'questions about what I should do in response to the patterns that you are describing. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87cfe5a49a21f100a75b3a05a188cbe3'}>, <Document: {'content': 'For example, I can find out patterns in stocked data and I could ask a question, should I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b31895839a464ec232379f1b4fdfe2c7'}>, <Document: {'content': 'invest in this stock or not. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9cc683179d5a4a6c4e41d21a2932f756'}>, <Document: {'content': 'For all of these kinds of analytics, so we need to have a technique tools and techniques ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff5fbe9163b82cff77314ffc43d082f3'}>, <Document: {'content': 'from machine learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e67307b1ae40d8d023053356dbbdc0b'}>, <Document: {'content': 'So, what exactly is machine learning? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cdeed9303c62329a3a92c401314a4b8e'}>, <Document: {'content': 'So, I will fall back on this old definition from Tom Mitchell. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '135664579000e9d14337ac2e1e8bcfb5'}>, <Document: {'content': 'So, Tom Mitchell said an agent is said to learn from experience with respect to some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '818d90b6d5a363310c66f93f0110f54c'}>, <Document: {'content': 'class of tasks and the performance measure P, if the learner’s performance at tasks ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a82e349dd34f88587a2d36d91bc20ce'}>, <Document: {'content': 'as measured by P improves with experience. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b0449eaebabcb5fff7b8bf1d48d113a5'}>, <Document: {'content': 'There is lot of qualifications here, in the first case he did not say a machine he just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '833f48dbe78ac22dfdb5e4e6453a8db7'}>, <Document: {'content': 'said an agent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '952f211a3fa6edaa9f29bb2a88eac02e'}>, <Document: {'content': 'So, in Tom Mitchells opinion, this applies to all learning agents, it could be humans, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6cdcd5c399f6ad9ebbe675bc5552ce52'}>, <Document: {'content': 'animals or machines. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82cbeb12ea25ac548acedf179d634a59'}>, <Document: {'content': 'So, an agent is said to learn from experience, so you have to have experience in trying to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '548a21c843735e045875a834e90bdc4d'}>, <Document: {'content': 'solve something and now, you measure this experience with respect to some class of tasks. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84fedbb91fd234d377a678656a5b334b'}>, <Document: {'content': 'I mean, it is not that you can learn every things you have to be very specific about, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c176a274f186be270cfc2e6ed856d3e'}>, <Document: {'content': 'what is said that you are trying to learn and the performance measure P. So, you need ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d8706a4b21f1999dcd192aec795f8d3'}>, <Document: {'content': 'to know how well you are doing in that particular task that you are learning about it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2cbfdf5d2750a0b7bb2a10ff14c86b9d'}>, <Document: {'content': 'And then, if you are said to learn if your performance as measured by P keeps improving ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf3444f34167f5c5b3ca9ad519de2c5a'}>, <Document: {'content': 'with experience the very, very inclusive definition of learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '982dfcbacccd74af543375f286c644ba'}>, <Document: {'content': 'So, you have to be very careful when you use this, because you could even apply it to desired ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6637127c299c5a163e678f6a254c713'}>, <Document: {'content': 'scenarios. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cbd725b2fb76a295f2cd9b3088d31441'}>, <Document: {'content': 'For example, you could think of a slipper, a new slipper that becomes more comfortable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ca7c2264c6a6a865708633c72fabf67'}>, <Document: {'content': 'as you keep wearing it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '355cd9f2abe651211e241259e9b2c4f3'}>, <Document: {'content': 'So, you cannot really say that the slipper is learning to fit your feet with experience. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa21444fb23679cffa67f118a2bdeb78'}>, <Document: {'content': 'So, you have to be careful about, how you apply this definition, I mean it is a fairly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dfc4e18ac5054aaf2b45d258ce05b3e4'}>, <Document: {'content': 'serviceable definition as we will see as we go long. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a38cf31951f00b17dfee316875288f6a'}>, <Document: {'content': 'So, the rest of the course we will be looking at three different machine learning paradigms. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74a0f1c9485f99e3ca50868a7225c59d'}>, <Document: {'content': 'So, the first one is called supervised learning, where you expected to learn a mapping from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6991e694ad69ecacf63bef89f334cad3'}>, <Document: {'content': 'certain input variables to output variables. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd722ecd49710b1dfd31ba7a56d2ec421'}>, <Document: {'content': 'So, we already looked at one example of such a supervised task, when you looked at regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b55c39d19d2c022e73218bb33f30187'}>, <Document: {'content': 'so where the output variables was a continuous valued variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bc149828375db1a672a4a6775285999'}>, <Document: {'content': 'So, and then the input was described by a set of attributes and if the output is a categorical ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b9e724de53722fe75fa02ed5ed826bf'}>, <Document: {'content': 'output, where it could be one of many classes. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f937a9b6b27482ae3dfbd8b4bcb47eee'}>, <Document: {'content': 'So, you see, is the patient sick or is it, see he is not sick, will the customer default ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe9148d2871b954e19d07282bed39b14'}>, <Document: {'content': 'on the payment or will they not default on the payment. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0454adf1a6e7579fc0c8e5fa2434f16'}>, <Document: {'content': 'So, these are like categorical attributes, both these examples, where the output could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c894685ea970cdce6db383bd85a173a'}>, <Document: {'content': 'be either 0 or 1, you could think of outputs with multiple such levels. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abf1dc1dc3bd085171e40df0f5dd25b8'}>, <Document: {'content': 'In such cases, the learning problem is called the classification problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb2b5a33d0cc67597c01bc29499016bb'}>, <Document: {'content': 'So, but what distinguishes supervised learning from the other forms of learning is that, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c688c015b0772811c0d72f1312016a'}>, <Document: {'content': 'in the form of experience that you will get. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f7bbd341e2b942c5ab8bb6a050ceb5b'}>, <Document: {'content': 'So, whenever I give you an input, the sample input I will always have an expected output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '49d410e07acca64995889d45bbd1eede'}>, <Document: {'content': 'for this input. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e837d45b497ab8fe0783b0e9e0acad4'}>, <Document: {'content': 'So, I am going to, I will give you a set of samples which consists of an input vector ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '144993682470740471e9891b95ffd4f2'}>, <Document: {'content': 'and an expected output for that and that is what makes you supervised learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a0dc69b58f1152ef0150509c073fb8f'}>, <Document: {'content': 'In unsupervised learning, the goal here is to discover patterns in the data that need ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc0d58e654913d6ddb8885e681e2e09f'}>, <Document: {'content': 'not necessarily be any output that I am trying to produce. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf3f3e26e9a8dede9d8bc9702164b2a4'}>, <Document: {'content': 'So, there are many different unsupervised learning problems, we will be particularly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ead9c16e606283083851a065ec217f83'}>, <Document: {'content': 'looking at two in this course. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '720c60a4f561268ea15b290606e7a8e0'}>, <Document: {'content': 'One of them is called clustering, but the idea is to find cohesive grouping among the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26298f8a302af05076f8d66310aecd04'}>, <Document: {'content': 'data points that are given to you, so in order to find any patterns that are occurring. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b14ef1dc279ae0386b4d020658980236'}>, <Document: {'content': 'So, will see how this works in a little while, so but you can readily think of the rainfall ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2dea6f353973260d6a56de52a367af8'}>, <Document: {'content': 'task that I was talking about earlier. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69144b04c3324f389947a8f2814476bd'}>, <Document: {'content': 'So, you can group regions that somehow are similar in their rainfall behavior. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c281c2d05b9a4d59b8fb8457cb3acd7f'}>, <Document: {'content': 'And the second unsupervised learning task is known as the association mining or association ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '53929354bdfc655358d1c5ff6563c7c9'}>, <Document: {'content': 'rule mining sometimes and the goal here is primarily to find the data points that occur ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4000f897a3b4b3ceee7bd7745e8a45c'}>, <Document: {'content': 'together or co occur frequently. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd76ac7fd5f61506ea8491dfae650763'}>, <Document: {'content': 'So, in association rule mining you essentially you are trying to figure out, which data is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '800a0cfddbb90df1451ce44d8cca1e3e'}>, <Document: {'content': 'associated with which other data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6940483baa0236b51afa18d849e62ef1'}>, <Document: {'content': 'So, how it is which co occur frequently. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c8fff71a2c70f429f565781551343a'}>, <Document: {'content': 'So, in both of these cases, as you can see there is no real output that you are expected ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e183a96f8f8bc8a278c9e9480d2bbc1'}>, <Document: {'content': 'to produce except to find the patterns on the data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e92a6f6b8204b8eb6e1ea33e858918b'}>, <Document: {'content': 'And the third class of machine learning problems which are called the reinforcement learning, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '95e3995b3b457a766b76275f0e8855bd'}>, <Document: {'content': 'essentially has to do with learning how you would control a system. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54f1a8420b4a3d5c3c07619d511158ab'}>, <Document: {'content': 'We will talk more about it towards the end of the course, but roughly you can think of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6d199a8e42b1f57faa04d58094e44a7'}>, <Document: {'content': 'the following problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd0f0a1b70bc53d7011e85c893678e778'}>, <Document: {'content': 'So, how did you learn to cycle? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd9412c9e6af0346e0608f99bf6beece'}>, <Document: {'content': 'So, it is not just discovering patterns, there is nobody giving you an input output pairs, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '971e353f628cd3862f07a0eb06e722ac'}>, <Document: {'content': 'that tell you how to cycle and somebody actually does give you an input output pair, then probably ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c8bfd340da7a3a2a84c1c7aa13bdc0c'}>, <Document: {'content': 'not learn to cycle. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c11dfd75acea339966d4935c1d3ed29'}>, <Document: {'content': 'If your cycle is tilting by 30 degrees to the horizontal, then you should push down ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3606a85a131112197c5c5408b26e3805'}>, <Document: {'content': 'with your right foot with so many Newton’s of pressure. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa2cfccc70f664238115690278e0c67d'}>, <Document: {'content': 'If somebody gives you directions like that, you will never going to cycle. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb43ae197594c47eaf64a4e061831e1b'}>, <Document: {'content': 'So, you have to do some kind of style and error learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69fe43c8c3be19099a7f5f4f728c75b3'}>, <Document: {'content': 'So, that is essentially what reinforcement learning talks about. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'db9d4374275c6fd5cb46a138236222d8'}>, <Document: {'content': 'So, you will do a little bit of this towards the end of the course. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '434e6a6c36eed43c93542e29bd0cbab'}>, <Document: {'content': 'So, the different machine learning tasks that we are talking about, so classification, regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef63e3994f46b8ae5dfd83ec0174344f'}>, <Document: {'content': 'which are essentially supervised learning problems and you remember, I told you that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9f02979a39c64dbcf0e6da0275bb77a'}>, <Document: {'content': 'you really need a measure by which you are going to decide whether the algorithm is performing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5037126e8148b463e965f7ee56e1ea5'}>, <Document: {'content': 'well or not and the measure in the case of classification and regression is just going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74125a39e39775ac0c7a2fc2bf727e75'}>, <Document: {'content': 'to be error. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6de9f39a984e89c7547a8475d69b9a24'}>, <Document: {'content': 'In the case of classification, it will be the classification error that is the how many ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6b078abbd20bfa796baeee7499932cd'}>, <Document: {'content': 'mistakes you make in predicting the categorical outputs and in the case of regression, it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '93bca0d194308154f43b9ac7af0feb5b'}>, <Document: {'content': 'is going to be the prediction error which is, how far away you are from the actual value ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5467f2ab59ef3827cdffdf5e336d7601'}>, <Document: {'content': 'that you need to predict. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ffd981234b5d6385d442ae8c273a5cd'}>, <Document: {'content': 'In the case of unsupervised learning problems, it is a little tricky as to what the measures ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2489a2baebb1ef1d75a05d1e628615ce'}>, <Document: {'content': 'should be and there are, let say when we look at clustering and association rules, we will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4cdfae05f6140709f5fe741b46d72e4e'}>, <Document: {'content': 'talk about many such measures in detail. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e76df5eb6f01c58d88180b23d4a653e'}>, <Document: {'content': 'But, roughly, in clustering one of the measures is how tight your clusters are or how scattered ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0aa902c9d6858eb4302538b531b229b'}>, <Document: {'content': 'they are and I am talking very roughly here, but we will formulize as we go long. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ccb6ec25a7bbdf546760795abe2ccfd'}>, <Document: {'content': 'And in the case of associations, it is more on how confident you are that these two items ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fce0e210b77ce98aa1cf36837b158932'}>, <Document: {'content': 'are associated and what is the fraction of the population in which these associations ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dbc3e681ed0f660ffb8e2cc9ad02f4c'}>, <Document: {'content': 'appear. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8de725c5b4a1e81988c1e42253969f6'}>, <Document: {'content': 'So, that is what we mean by support and confidence. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1703601b60eb40fa7e059ed20a657bd'}>, <Document: {'content': 'And again as I said, this is just to give you an idea that for every task you are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fd0b0adf8a9152524cb3fb8f4290ff7c'}>, <Document: {'content': 'to have an associated measure and we will elaborate on the actual measures as we go ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '72c7baa4ee385aedf19963234dd8849f'}>, <Document: {'content': 'along. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa2d2d32b145db50141b3f96f49bb680'}>, <Document: {'content': 'So, having said this, there are many challenges that we need to address. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '83e214103ab57445378a83cb8b712e0b'}>, <Document: {'content': 'So, one, the first challenge in any machine learning problem is to figuring out, how good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e1c3df53c4a57b14303b4a664419169'}>, <Document: {'content': 'is your model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf9f58206420d1517c84db558be8034a'}>, <Document: {'content': 'If somebody gives you a machine learning algorithm and say, there, so here is a model that has ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb7a0bd40cc27aa7758a01a7ed05798a'}>, <Document: {'content': 'been learnt by the algorithm, how do you decide how good that model is. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16b9ab1bf22fd8df76fcdc1c8579fe93'}>, <Document: {'content': 'So, you could use the measures that I showed you on the previous slide, but that could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '745d047a57a56efd58736dd75e13d6a4'}>, <Document: {'content': 'be other ways of deciding, how would the model is. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '118c0d18bcf09ecfa91853c918067e70'}>, <Document: {'content': 'So, I will be elaborate on this as we go long. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9a8bd7425d222e154dbdef0e0577fd8'}>, <Document: {'content': 'For example, you can build a very, very, very, very detailed model that gives you almost ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c12c99467068666047038fc6f9429f6a'}>, <Document: {'content': 'zero error on all the data that is given to you at the beginning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd57a71d64a4b9bc4a9a523dbb6e7fe57'}>, <Document: {'content': 'But, then this model might essentially be useless, when it starts looking at unseen ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '11f1d6aca2c7ad8ef8e23d5787e4d26c'}>, <Document: {'content': 'data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '86094b893a710bfcc5e1d991be421774'}>, <Document: {'content': 'When I actually wanted to make predictions on data that I have not seen before, this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '423576b1f3e94c8df7b62326f47283ac'}>, <Document: {'content': 'model might not perform well at all. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2fa051327ee90f648e057749aa310d2'}>, <Document: {'content': 'So, how do I look at that kind of a trade off? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '160598b70917c056189e1ad7483f3980'}>, <Document: {'content': 'So, there are different ways of measuring, how good a model is and then, given the data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18723fa89a6f78952f5fed81fd1413a7'}>, <Document: {'content': 'and given the class of models, here we are going to look at how do I choose the right ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a649d27af6fea6b0146e27d04c21e36a'}>, <Document: {'content': 'model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd23a7106180f96cab59992430951c0'}>, <Document: {'content': 'So, that is the second challenge and many different machine learning algorithms are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16e2acd1eacc80142797c31945cfe94c'}>, <Document: {'content': 'all about answering this, the choice of the model question, but then, it is not all about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '426544140a49b5f396ddee31b163d5f'}>, <Document: {'content': 'modeling. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca126bd5e04cfc4a263deb327cd719ca'}>, <Document: {'content': 'So, you have to be very, very cognizant of the data that you are operating with. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e0e58b8236118965c3f8513d2194c41a'}>, <Document: {'content': 'So, the first question that plagues all of us is, do I have enough data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5282008e27d786eb34f425cbc00187f5'}>, <Document: {'content': 'That might surprised some of you, who have heard of terms like big data and when having ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd082444cdd4e9b168cb235bb192bc836'}>, <Document: {'content': 'excess of data, data delusions, so on and so forth. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2f3bae42c3497b0aa5023af5701f308'}>, <Document: {'content': 'But, then getting in a supervised learning problem, getting label data is incredibly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '243c95ffcff2c7a1924bbb3cfc4dddea'}>, <Document: {'content': 'hard and so, you have to have an expert that this looking at all the data and then labeling ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e20b2ae9f53546b409d4e6abf20e1d30'}>, <Document: {'content': 'them for you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c5163f16c8f94d8ac12a38340b2d4e8'}>, <Document: {'content': 'So, getting such labeled data is incredibly hard and so, do you have sufficient labeled ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e409783b38836341d871ae0d81f811'}>, <Document: {'content': 'data or do I, can I make use of unlabeled data in a clever way. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9c3ae1b8c91134a3bd6c11ad5aaba35'}>, <Document: {'content': 'So, these are all kinds of question that you will have to think about. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e77d330df93243a0a3b421ca9e86b06'}>, <Document: {'content': 'Is the data of sufficient quality that could be errors in the data? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '28885b4943c64fd90bea8aea889d3dd2'}>, <Document: {'content': 'For example, age could be recorded as 225 or there could be noise in very low resolution ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da188eae94e443247856626e2b7b8974'}>, <Document: {'content': 'images, that you are feeding your algorithm and so, the algorithm is not able to make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '180e5f0841020e5e68e95457f92c1067'}>, <Document: {'content': 'out, what is that in the image or it could be that some values are missing or not been ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c276f375260b54b0aa31e98c5030b62d'}>, <Document: {'content': 'recorded in your data and then, your algorithm has to deal with that. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82c023c1a799033b1277c07f9f8955b2'}>, <Document: {'content': 'So, it is a very important question that the data has sufficient quality and in any almost, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dea29c1968974857bf547f4263401618'}>, <Document: {'content': 'in every large scale, real life machine learning insulation, a lot of effort has to go into ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '713c580c933a1b956e8ce1eccb32f475'}>, <Document: {'content': 'cleaning the data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5eedf13e43eb1b9a36f1758241de6c8'}>, <Document: {'content': 'So, making sure the data is of a sufficient quality to feed into your system and, so how ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c708f496fbd605376bf0c6003e54f5f4'}>, <Document: {'content': 'confident can you be of the results at the end of it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92fb6bf21633ce7ef2abffd1fad2bfad'}>, <Document: {'content': 'It is both the factor of the data quality, the data volume and as well as the machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38341aac3103d87eaa00cd3d74aca80'}>, <Document: {'content': 'learning, exact machine learning algorithm that you end up using. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '877a3bc4b2dd29c41e08bab413b55f8e'}>, <Document: {'content': 'So, all of these factors are together influence, how confident you can be of the results and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ee67ff17f9546da4a6f289930f6e61b'}>, <Document: {'content': 'at the end of it, there is one very important question, which typically you know it not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4d9795ac79aba2cb00a2e2edd7bb2b3'}>, <Document: {'content': 'address that carefully is, am I describing the data correctly. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c86906b4af5f895d0d7154b48f616237'}>, <Document: {'content': 'So, for example there are two classes of questions I could ask you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6d6efa38c94dd645c5acfa463b5a1f6'}>, <Document: {'content': 'So, the first thing is like, do I have enough information about what I am trying to classify ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '255ed052f49a993c31218d1d6bc6e6f3'}>, <Document: {'content': 'or age and income enough to describe all my customers or should I look at gender also, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f47280f05e55c42e3f5dbf05e5a0ed86'}>, <Document: {'content': 'how would I answer such a question. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34ba404f1f5534f5c8aa9da503678733'}>, <Document: {'content': 'A lot of it actually comes from the data analysis that we have been doing so far or age and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25824015e27f0188148bc1e67434b952'}>, <Document: {'content': 'income alone sufficient to explain all the variance that I see in the data or should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '616e4c559dc183947eed38b93ddf5cf0'}>, <Document: {'content': 'I include another variable, in order to make my model more accurate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22f718352d75430cd816cf443c0b4aa6'}>, <Document: {'content': 'So, such questions will tie back into all of the analysis that we have seen so far. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e42843322cab7b76bd212ad6b06638f2'}>, <Document: {'content': 'And the next question which is more often, it is partly an engineering issue and partly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c45e41b62746b974202d50d31a7a64f3'}>, <Document: {'content': 'a theoretical issue is, how I should represent my variables you know, how do I represent ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a166b424a8bcae56b571d79ec7830f6'}>, <Document: {'content': 'age, that age can be a number. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2d75639331892835935e310fd179b45'}>, <Document: {'content': 'Well, age can sometimes be merely a number, but sometimes you can classify age into different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f781e00ed1f6313a4edce02cea2b1f8'}>, <Document: {'content': 'levels as young, middle aged and old. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c57caccd1b0ccac3a74fd14838105051'}>, <Document: {'content': 'So, what kind of an encoding would you choose for representing age and again, it goes back ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c9d29a4138b462e2d11d519ad2dbb6a'}>, <Document: {'content': 'to the data analysis if you are talked about and… ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33726ab82e7a23cda09affff51bc9304'}>, <Document: {'content': 'So, these are things that you have to pay a lot of attention too and the more often, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2cf5b969be9d4d302bcf24178275e09'}>, <Document: {'content': 'than not when you are looking at a simple exercises that you could do on the web, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25175158e8e552ff570ff5dc26457f52'}>, <Document: {'content': 'end up looking at the data that is already where this questions have already been answered ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cbef843ed59bff0aaa41b769593d618'}>, <Document: {'content': 'and they are given to you and you are able to do it very easily. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad5a1e90f1cb261e634af279497b0b86'}>, <Document: {'content': 'But, when you go out and you are trying this, all your first real problem using machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a932550cdbda1eab85f16a62896956e2'}>, <Document: {'content': 'learning you will find that these are very important questions that we have to answer. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3c439eb5c5a3c86397e6a61e6ffec26'}>, <Document: {'content': 'So, in the next module we will look at more detail at the different learning paradigms ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '309e5aaca2543c8dd50fcb1be63d6ee8'}>, <Document: {'content': 'that we talked about today. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62a69747bf71896221e3d9724e7c7f56'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Supervised learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1fc6b1b47128c3b0469874f10f4b6c4c'}>, <Document: {'content': 'Hi and welcome to this module, where will be I am introducing you to various machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a8b96845454031d553e6daef00d47bb'}>, <Document: {'content': 'learning tasks. So, we already saw in the previous module and that, machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cb223e979ff1d9c1354e88b19083bf3'}>, <Document: {'content': 'is essentially improving the performance of artificial agent with experience. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '753784ae6512b7e80765e25f2bd076a0'}>, <Document: {'content': 'So, today we are going to look at the first supervised learning, where the experience ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c696c67fe5dd1dba1d5bf3d5e6b074e'}>, <Document: {'content': 'we are going to call the experience as training data here. So, in this case I am showing you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de6ebb3a7d0f5a57a13a82873b90f608'}>, <Document: {'content': 'data points that this distributed in a 2D plane, so it could be let us say age and income ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92f2c2ac9882b38e939b7b1f90955f61'}>, <Document: {'content': 'that describe customers that come to a particular store. So, in the case of supervised learning, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '53482fa12e0483d0f3221e009f0a5345'}>, <Document: {'content': 'this training data is going to carry labels. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c98bec255354072894daee9670e42a6d'}>, <Document: {'content': 'So, it could be that whether the customer is going to buy a computer or not going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '539116ceaa79dbde0252c7343b6a94d2'}>, <Document: {'content': 'buy a computer. So, the customers marked in green are going to buy a computer and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c221fb8d02db191ecd938d31f6f45d2e'}>, <Document: {'content': 'customers marked in red are not going to buy a computer. So, this is the kind of experience ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4dbf4ea5d1a622101163e6b1bc5b27f'}>, <Document: {'content': 'that is going to come to you and in the case of classification task, which is this, so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ade8b2f475ad94e2f409fdd79d7807a'}>, <Document: {'content': 'we are going to call it label training data, where the labels are drawn from small discrete ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4225300f127cd72ffa344350e2e63b03'}>, <Document: {'content': 'set, in this case it is just yes or no. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7956476d72268cd6e2fdfb6e0f29de89'}>, <Document: {'content': 'So, what is your goal here? So, your goal in this problem is to figure out, how are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cbc79cb0221f6d5d0e8669e708ca99f8'}>, <Document: {'content': 'the yes’s and no’s distributed in this two dimensional play. So, the simplest model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a1fc503946d591ea276ac340487f0a68'}>, <Document: {'content': 'for this is going to be to draw a straight line. So, what does this straight line mean ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20aaa4b0fbc463ed8c9f90c873f19067'}>, <Document: {'content': 'here? So, it’s essentially saying that people, who have an income below a certain level are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9bac2829192de5d2bd95d6fb15b2a6d1'}>, <Document: {'content': 'not going to buy the computer; people, who have income above a certain level are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f058b9f7ee877fe403c601d563ddb7d6'}>, <Document: {'content': 'to buy the computer. So, if you think about it, we are really coordinate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '372ed0bb11af32a445975914b1ab98cd'}>, <Document: {'content': 'more or less correct, there are a few points like that and that, which are incorrectly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61711158b48cb524d2715a17b588dd35'}>, <Document: {'content': 'classified and here is another one, who is actually going to buy a computer, but we are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42528d7ca739050edbe6c754645201ab'}>, <Document: {'content': 'classifier is going to say, it is not going to buy a computer, because it is on the wrong ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7532ccbcf7350a8a46d7717e6e4c6ac'}>, <Document: {'content': 'side of the line. So, you can do better, so that is a slightly better classifier. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2dff27b93c00660d907052e5cfdd65e'}>, <Document: {'content': 'the two, the red points that we had incorrectly classified previously that one and that one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee7164b69e426049966fc4adf2e2106a'}>, <Document: {'content': 'are now correctly classified, but we still are making a few errors. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66bd56e04c3819aa573934c93c477760'}>, <Document: {'content': 'So, what has happened from the previous classifier to this one is that, you have made it slightly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc9eb4c32888aba45782393168f45a65'}>, <Document: {'content': 'more complex. So, if you think about it, the previously classifier was essentially say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '51ddaa2bd45b5259d0306725c793e5c8'}>, <Document: {'content': 'x equal to some constant, so it is just saying income is equal to some number, if it is less ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6f1865c5523567f3ddfeb1719c99729'}>, <Document: {'content': 'than that, it is essentially no, if it is greater than that it is yes. So, now, what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'acb2f7f1df629dca9430610b6e3408c2'}>, <Document: {'content': 'has happened is we have added a slope to the line, it is no longer; just based on the income, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5fc5808179685f4da85e30b3692f24d'}>, <Document: {'content': 'but the age also has to play a role here. So, there are more parameters that are needed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d309b9b5f9cd09c3125c4613f3a8225'}>, <Document: {'content': 'for describing this classifier, earlier we just needed to have one parameter, now we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c89cc65b12bcfad007a514cbba3fbbf4'}>, <Document: {'content': 'need at least two parameter to describe this classifier. So, can we do better? Obviously, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46139f7099bfb85902b5937e8cbd8fb7'}>, <Document: {'content': 'so we can do better we can draw a parabola like this, so in this case you need more parameters. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '355050a60a0180f7e26abb6adeba67bf'}>, <Document: {'content': 'So, you have a x square plus b x, so you are now going to get additional parameters that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd5f4296de2217dabb11fa33d3b2b688'}>, <Document: {'content': 'you need to describe this classifier. So, are you doing good here or should can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54eb386dc4443309510c9c568052da14'}>, <Document: {'content': 'we do better, it looks like we could do better because we still have an error. But, it starts ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f17ddf06f83fca893111016c14e2a5cf'}>, <Document: {'content': 'looking a little weird, so we are essentially now, this is going to acquire a lot of parameters ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd155e35eaa9b28633d7fa213d944e8b9'}>, <Document: {'content': 'to describe what this classifier does, you know that little wiggle there that goes out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35adb590a8188259bbf719ce5a9722fb'}>, <Document: {'content': 'and gets that additional red that you missed and say now, becomes increasing complicated. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94decb3634a27ea39936deea23326fad'}>, <Document: {'content': 'So, what really was happening is probably that data point is a noisy point, it could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1263834f9c77236b6cd90a4bf1d57e1'}>, <Document: {'content': 'be noisy due to variety of factors, it could be that person came to the shop to buy a computer, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '156ef4477d6d8bdcb8399f8ff0037e9b'}>, <Document: {'content': 'but for some reason he received a call and then, had to leave immediately without buying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8a8531bd8f94ad699dcdcd6fa16d2be2'}>, <Document: {'content': 'one or may be the data has been erroneously recorded you know person actually bought a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25ce2f2d824c13cb0b9b8b095e9cd713'}>, <Document: {'content': 'computer, but it has not been recorded. So, in there are several situations, where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1863d2de9f5a73e003c34a835e355aa5'}>, <Document: {'content': 'which says noise could enter into the system. And we have to be very careful that we do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c064e5421959733dbffb39d1a3cbeac8'}>, <Document: {'content': 'not end up modeling such noise in the data, which will lead us to make really wrong predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b64c3d91942998a7c6c002b7f034fed1'}>, <Document: {'content': 'in the feature data. So, probably the best solution to this problem is this parabolic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b760cbde0a9456f40c1d46d05fa35e66'}>, <Document: {'content': 'curve, because it gives us a good balance between the complexity of the classifier verses ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c559d480ac159a0eb06671864f53c6d'}>, <Document: {'content': 'the accuracy that we have on the training data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23a4bbd945a7a89b63311affd519a92c'}>, <Document: {'content': 'So, remember that this is all evaluated on the training data and what you really are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d7fa556ad5bae5737bc97ed9bedc218'}>, <Document: {'content': 'looking for it is, how well is this classifier going to work on the test data on the deployment. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92047d432217ebf9f4b8676c5ef0355'}>, <Document: {'content': 'So, I am going to train this classifier and then, I am going to use this to predict whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee51ee44d1896e239cdd8896159a0e3f'}>, <Document: {'content': 'the new customers who come to my store are going to by a computer or not. So, when I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22391b450ebcba5d74f884fd76444083'}>, <Document: {'content': 'am actually operating it with this new data I want this classifier to do well and therefore, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30d98eca8f47fd724bffa995527d4fc9'}>, <Document: {'content': 'over fitting the classifier to the training data might be a bad idea. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2b70db1b1bdf7b840db4e3def36450e9'}>, <Document: {'content': 'So, one thing that you should remember here is that my goal is not just to do well on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75b27bf3eeb1bd9556cf1b5df08904aa'}>, <Document: {'content': 'the training data if I am only interested in doing well on the training data, what is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9a8fdb33fda29cea1b1295fa21a67274'}>, <Document: {'content': 'the best way to do it yes I just have to remember all the data points it was given to me. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77aed329e260fe5530166078ccb229e1'}>, <Document: {'content': 'I can never make a mistake ever again and I can be perfectly accurate in making predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ce05aae01127f9b6666d83420491dcb'}>, <Document: {'content': 'on my training data, what I am interested in is to be able to perform well on data that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '314fee621379e40a01af18222285f4f2'}>, <Document: {'content': 'I have not seen before and therefore, I need to be able to generalize to unseen data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b69afe8ea6ee5813d55dca501cffccba'}>, <Document: {'content': 'And the only way I can generalize to unseen data is by making assumptions about the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13d0bd58b589b3aa84adbc88134b088c'}>, <Document: {'content': 'I have to make some kind of assumptions about, what kind of lines in this case, what kind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10535b486c8df101a89a9de2d2dca2aa'}>, <Document: {'content': 'of lines should be separating the buyers from the non-buyers. So, whenever I have this kind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7eed678806af78c32c9e14cbcc14755'}>, <Document: {'content': 'of need to generalize it translates to assumptions on these lines. So, such assumptions on the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb5290af9e45516813a06f0dcda3e41d'}>, <Document: {'content': 'models that you are generating are called inductive bias. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a9a443f8e985b09e5b3dfc7ae207c7f'}>, <Document: {'content': 'So, the whole paradigm of learning that we are talking about is called inductive learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e680fb01d770dbbf2aff99b232d29fa8'}>, <Document: {'content': 'and the assumptions that allow us to get this kind of generalization are known as inductive ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7fff4ac9fae194a1e8b4dbb4a9a8719'}>, <Document: {'content': 'bias and there are two forms of inductive bias generally. So, one is called the language ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7a55680fad1c71b04470f41398fb044'}>, <Document: {'content': 'bias, which essentially is the restriction that we had on the kind of lines if you say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '19e1debc0e915aef9adf21cdc358d419'}>, <Document: {'content': 'I am going to only consider straight lines that are parallel to the x or y axis that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd127ba621df7234b7544386954e9d0e9'}>, <Document: {'content': 'is one kind of a language bias or if you say that am going to consider only parabolas that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '76d29dab02f9b9bda91e0277c962f3a4'}>, <Document: {'content': 'is another kind of language bias and the other kind of bias is search bias. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5eb38d182fbb5a08091e4b4d4ec0ac0'}>, <Document: {'content': 'So, given that you have picked a language in which, you are going to represent your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e496135cbf22519e865c85c8704636d'}>, <Document: {'content': 'classifier, how do you search among the possible classifiers in that language in order to find ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '206a9293c27fc24842c394e5f50440c7'}>, <Document: {'content': 'the right one. And that again influences what kind of classifier that you are going to find, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31ba4eb6958a09ab89a44131e17506df'}>, <Document: {'content': 'because you typically will end up with the first classifier that you find that has an ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22880eff411f341fd5c0e7af700943ca'}>, <Document: {'content': 'acceptable performance. And therefore, that will be influenced by your search bias and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a749bb597a1c1a05efd4760c036b440c'}>, <Document: {'content': 'we will elaborate on this as we go along and this just to give you a feeling of what is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5a98f6ef17a170a0920aa1c82f98be8'}>, <Document: {'content': 'involved in trying to do this kind of learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b30cf6d6005cafb0298ddc3627eb5fa'}>, <Document: {'content': 'So, I like to elaborate a little bit more on the whole process of this supervised learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a38e6fcc353ca21ec5963294a4ed1e84'}>, <Document: {'content': 'and this is kind of common to the other algorithms you look at as well, but am just going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9313a3c6c5b00e9d63904ae474b6030e'}>, <Document: {'content': 'talk about supervised learning little detail in this module. So, you start off with training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2911c391c18314fdd60156358b760cfd'}>, <Document: {'content': 'data that is given to you. So, training data consists of vector x and an output y, so if ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0d418623e0066671bcea091e127cfc5'}>, <Document: {'content': 'you think about it. So, in this example that we just saw on the previous slide, so x would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d1590dde9d765f7d1bdbc0cdf75ab34'}>, <Document: {'content': 'be looking like a the tuple of income and age. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1d85b41061e60864dd098f712fbd1e2'}>, <Document: {'content': 'So, the first data point could be that person has a 30000 rupees per month income and he ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e25001ba07642f1574c1ee90b2df6759'}>, <Document: {'content': 'is 25 years old and he did not buy a computer and the second person had a 80000 income he ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6a60718f6574754b5766298398421ef8'}>, <Document: {'content': 'is 45 years old and he buys a computer. And, so he is essentially going to have a series ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c814d2ab4f5d317a45d326c9d60df7e3'}>, <Document: {'content': 'of data points like these that are given to you and you have to use your now, your training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '477fa3f4cd0b8587c991267cc4d430a7'}>, <Document: {'content': 'algorithm in order to learn a classifier. But, if you stop and think about it, so for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b0b245c9b9880352ab0aaac0a6eaad8'}>, <Document: {'content': 'us to be able to implement this in a numeric fashion, so you are going to have to encode ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3057fde382874b10a3602b2a3cb9db9a'}>, <Document: {'content': 'your data. So, you probably take your income levels and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9af16510070ab7507d2f33fe9f4a5b15'}>, <Document: {'content': 'then, do some kind of a normalization, so I have normalized it between 2 lakhs and 0 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '73082ab54ec9cae3cfab28e600477dfd'}>, <Document: {'content': 'income. So, and that gives me like the first person has a income of 0.15 the second person ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c51fb6a9261dfbd81105990608129a3'}>, <Document: {'content': 'has a income of 0.4 and so on, so forth. And likewise I have normalized the age between ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b918fb10d43b7c3a2d4c36dec97e5945'}>, <Document: {'content': '0 and 100 and I get numeric value for the age. And the labels are encoded again, so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f715bcfe9e430a2616597e5fc653bb62'}>, <Document: {'content': 'not buying a computer is now represented as minus 1 and buying a computer is represented ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3dda6e87629b909bc3ef167035df642b'}>, <Document: {'content': 'as plus 1. So, whether you use a minus 1 or a plus 1 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d44ef37a63ba22dd5f61255171d91f3'}>, <Document: {'content': 'or whether you use 0 or 1 of whether you can use y and n depends on the kind of algorithm ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5abb91c23ae4c0a18c724ea504bb648'}>, <Document: {'content': 'that you are working with and as we go long as we different machine learning algorithms ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e52fb0fd411f0fd695673677fe641af'}>, <Document: {'content': 'you will find you will learn about the importance of the selection. So, once a have this encoded ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '97a436b4b3efb1951cfe5a5589b3bde1'}>, <Document: {'content': 'data, so the training algorithm is going to work on it and it is going to produce the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '91523713d531712997bbf56d42ec1289'}>, <Document: {'content': 'classifier, but I need to know how good the classifier is and I need to know if I can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ebccc958f413d0fbcd34e3053fb77e71'}>, <Document: {'content': 'stop. So, if I go back on the training data and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ed20aa72d55bbd5c44accf8e0f5a4b6'}>, <Document: {'content': 'then, ask how good the classifier is typically we might end up over fitting the data like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '158d0d4ffd67fb3097c8b0baf689ecc3'}>, <Document: {'content': 'I was showing you in the couple of slides ago. So, what we do is set aside a testing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bc0e18ebf6c25b8e231798941a40476f'}>, <Document: {'content': 'set, so which is not something, which was used to build the classifier right and then, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c353a5c9014366a4051847c9d0771f1'}>, <Document: {'content': 'we validated the classifier on this testing set. And then, if the validated if the validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b56609e67aba79e09f92dac792dea78a'}>, <Document: {'content': 'tells us the classifier is good enough, then we can go ahead and output it or if the validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b4b41b17e34492aa98582877eccced1'}>, <Document: {'content': 'process tell us known the classifier is not yet good not yet good enough. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b0d1b42672ddfb3bdf2d9f889143bdf'}>, <Document: {'content': 'And then, I can go back and modify the parameters of my training algorithm or iterate over the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c16e5e294558fec1a52fb86d0cb9ac4'}>, <Document: {'content': 'data again until I converge to something that is acceptable. There are many different ways ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '404b64ef7386d925990849ee6542bc43'}>, <Document: {'content': 'from this validation can be done and again you look at some of this later. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25b9586e4a066905822738f1704e19d6'}>, <Document: {'content': 'So, what exactly goes on in the training module, so the input, which is denote by x comes to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '803dfc250f102417abeca959e444741b'}>, <Document: {'content': 'the learning agent the learning agents uses the current setting of the parameters and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '156a87d195deb86125159d2ef511c411'}>, <Document: {'content': 'it the outputs a value, which is y hat, which is its guess of what the output should be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e5ba2e3ac167a35a89f777a8433bed6'}>, <Document: {'content': 'for this input x. And if you remember in the training data we already have a target y, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74adeff4e94d9731faeaf56e5414dad'}>, <Document: {'content': 'which is the actual output that you expect from the agent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '130e12ddc2e77ef5b74d7d9318832e6f'}>, <Document: {'content': 'So, I am going to compare the target y with the guess the agent output, which is y hat ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b295a09a03cc3cb6e587aa95368392b'}>, <Document: {'content': 'that allows me to form an error, which is the error in the prediction and it gets fed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e71297a898fb259b79fa07430efb111'}>, <Document: {'content': 'back in to the agent and then, the agent uses the error in order to modify its parameters. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6c4d26b48f08595b9d24c0494a9a4c'}>, <Document: {'content': 'So, this is something, which you have already seen to some extent in the regression setting, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3bb64268a1db0a0525edcd32927ba36'}>, <Document: {'content': 'but we will look at a more of a learning approach regression in the next few classes, but you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5746687fc3184cdc88a31489872011ec'}>, <Document: {'content': 'can also see that this is exactly, what the classifier needs to do. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59f45838bc751492be8547b40240bdc6'}>, <Document: {'content': 'So, there are many, many applications for classifications, so that a credit card fraud ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d62fcc4249e48afae7c77bcda5b01a3'}>, <Document: {'content': 'detection, which is a very I know that a few years back this was talked as a marquee application. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ba92bc539d9e6f40438bdcc727fef5'}>, <Document: {'content': 'So, when a person swipes a card you can say whether that is a valid transaction or not. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df2ce40e66b416b401065ed48c2111be'}>, <Document: {'content': 'And the other application which there a lot of buzzer around it nowadays a lot of startups ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f1ea8ac8aca03b3c409c4ac95af20dd'}>, <Document: {'content': 'and many companies are focusing on it is sentiment analysis or some time called opinion mining ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21a3be23ad513a9291bc2ef463352415'}>, <Document: {'content': 'or buzz analysis etcetera this is looking at social media data whether the tweets or ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fdfd1eb883f689865fdee0412c5aee29'}>, <Document: {'content': 'whether they are blog posts, so on, so forth. And then, trying to figure out whether they ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d20e175ad3301562e16be17f7cb0b70'}>, <Document: {'content': 'are saying something positive about you or something negative about you, so by negative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cf05ebb4d7edd0d2a66d05461bca866'}>, <Document: {'content': 'about by you I mean whatever is of interest to you it could be a movie, it could be a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a359d35e7442e687d94afba8c08b537'}>, <Document: {'content': 'new album, it could be a product for those release. So, instead of marketers going door ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fd79191564c80235f9ba003591bbede7'}>, <Document: {'content': 'to door asking people what they feel about the products we are now able to analyze the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '908fdf6af1b8c8d26c91a644ac5e37fa'}>, <Document: {'content': 'post that people put in the public forum and automatically figure out whether the opinion ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d27ca3490ffdcd3f3f34321bd99bdf2'}>, <Document: {'content': 'is positive or negative. Another application which has lot of commercial impact is one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a0dccef92d6c0e0d4dba780c64490f'}>, <Document: {'content': 'churn prediction. So, a churner is somebody is your user from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9fd423011ba38dfd36a82785cdae0ac'}>, <Document: {'content': 'your service who is likely to leave it is like am going to switch from Vodafone to Airtel. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ecedd6e36531de3b56192ff2604a0fc'}>, <Document: {'content': 'So, all I am going to switch from a windows machine to a Mac. So, these are these people ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8913ee4158d757ca775f261e3648c382'}>, <Document: {'content': 'are churners you know they are habitual users of a of a particular service who are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cfff558e55bdda4e1b29f19626aa62e1'}>, <Document: {'content': 'to leave it and move to another service. So, many companies are very interested in identifying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d6f26d517fc338433c163037d62bcf0'}>, <Document: {'content': 'such churners and then, taking measures to retain them. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '630bbfc47e680c7b1ab699558146bcd8'}>, <Document: {'content': 'Because, attracting newer customers to your services is little harder than keeping the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67c526724aac17d5273546ac7290b42c'}>, <Document: {'content': 'customers that you already have and people are willing to spend money to do that, so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2a24fda9c81d0d6ff74e1843b5e9746'}>, <Document: {'content': 'that is another interesting application. And of course, increasingly medical diagnosis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e5fba4672e887d292fa8128729f33eb'}>, <Document: {'content': 'is proving to be a very fertile ground for classification algorithms though it is the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe145111d02670f3a7b5ad98099f66d2'}>, <Document: {'content': 'medical communities really not in a position to completely accept, what machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a37272e434273fdecc4c8a2a1e191a26'}>, <Document: {'content': 'people tell them. But, the machinery people any way keep working ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb9b48ce2bc84693984e1fa4c259f5fa'}>, <Document: {'content': 'on lot of medical domains and there are many competitions that are run that ask people ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a3b543c92de1d1784afc1b4bf695e00'}>, <Document: {'content': 'to build classifiers that can predict whether a patient is sick or whether a particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e53831457df2d4534c032dd74ee54152'}>, <Document: {'content': 'blotch that they see on a x-ray or a scan whether that is a cancerous thing or whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64790a075b371673edd6fa4453d4e6c9'}>, <Document: {'content': 'it is benign. So, all this kind of diagnosis questions are asked of machine learning algorithms. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bfe09b64abd1e52dd7ff81edc98cc399'}>, <Document: {'content': 'Then, we fair amount of success, but then still quite a bit of way to go and another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '680c921bb38a1e8d279c04723f6ca193'}>, <Document: {'content': 'place, where machine learning is used in medical diagnosis is in risk analysis. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75999d593426b63305d1561f0a139b24'}>, <Document: {'content': 'So, I will elaborate on a little bit more when I talk about regression and there are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d990a1e6c4dfa36670a8adefb6a62d8'}>, <Document: {'content': 'in supervised learning. In fact, the classification is one of the most widely studied machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '349fd2faddf0bf4f01fa6881440ff1ab'}>, <Document: {'content': 'learning paradigm. So, that many, many approaches for classification and some of them are very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e39d5fb469cdc1e60a4e636b26c82662'}>, <Document: {'content': 'famous a few or few might recognize the names here they can produce learning supervised ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e255973632e87d8df04babd6d20809f'}>, <Document: {'content': 'learning can produce different architectures. So, it could be artificial neutral networks ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31e28a47667468245b8a5a5d9b3b7a1c'}>, <Document: {'content': 'which we will talk about later support vector machines, decision trees or just sets of rules ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e9dbbb1ad6fd091b8449e70cda0ae28f'}>, <Document: {'content': 'you know and other popular methods such a nearest neighbor methods, where you remember ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb52e697aacada9a7cdd1288a7bf2166'}>, <Document: {'content': 'all your training data and then when a new data point comes in then, you try to make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84c596647ddd2ac0e1ba93f0b6f4123c'}>, <Document: {'content': 'a prediction based on which, of the training data looks like the new data point right and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a75b63748fe42ddcf70d4098d5a0a237'}>, <Document: {'content': 'also problestic methods are typically based on Bayesian approach to learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3d1cc610df953ae49ca45809def8b3e'}>, <Document: {'content': 'So, we will be covering at least the first three in this in this course in detail the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '917ce7a5a8781a988c4d5160f897af7d'}>, <Document: {'content': 'other supervise learning problem that we are going to talk about is regression or prediction. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c06e21b0768eb69af0099524d8038d3'}>, <Document: {'content': 'So, here the data is going to have continuous outputs, so the input in this case this is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c36379aa900b5108a6d82bacfeec898'}>, <Document: {'content': 'simple example I have is temperature. So, the x axis is the time of day at which, the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '304481b3c25976cdfd6504648f5a6d30'}>, <Document: {'content': 'which temperature is measured and the y axis is the temperature, which is the output. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5068d666c11574cd5f3f23ed434b8a21'}>, <Document: {'content': 'So, we could see that typically day time temperatures are higher and night time temperatures are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a859720479952c4b268d2c8dca01865'}>, <Document: {'content': 'lower and now I can try and fit a curve to this, so ideally what you would like to fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5073e28ec6c60d185a40cf1b6e6a0ae4'}>, <Document: {'content': 'well all of us know about linear regression. So, what we are going to do first is try to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '97dbe15c21ee00189c7953ff77735881'}>, <Document: {'content': 'fit a straight line, but then you can try to be little cleverer and you can try to fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '177d4b5b303adcee0266d8b3880f9fd'}>, <Document: {'content': 'a more complicated curve like that that sounds its looks a little reasonable or it could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c2799c6bce5320c32d9deeeb60bb347'}>, <Document: {'content': 'over fit the data like we talked about earlier. And then, try to make the curve really complex ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1547e5ea657332c7afb05269cbfb974c'}>, <Document: {'content': 'and then, try to follow all the micro variations in the temperature. So; obviously, in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1fb2e139471dd9f38a07d16fc60d2070'}>, <Document: {'content': 'night time you see a data points that is temperature is almost as high as the day time that is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd9f78d8575c361ef9b052e96eb5bbc8'}>, <Document: {'content': 'noise, but if you try to over fit it you are going to get incorrect results. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a61edcf206ebc4cedcca40fde51c6c9'}>, <Document: {'content': 'So, we already looked at the regression little bit of detail, so that we know that we are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4932353eca44f94a827e2e60fff6b16c'}>, <Document: {'content': 'essentially trying to minimize the sum of square errors when you are trying to make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ff130eef67889c992aa4e9e2ccea278'}>, <Document: {'content': 'a prediction. So, how do I fit this line I just figure out the difference between the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ea37e8a1f79a5ff30b18beb07ee123a'}>, <Document: {'content': 'line the prediction made by the line that I fit and the actual data point that was recorded ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54f98f64438ecd5aa11f62004e1b94f'}>, <Document: {'content': 'take the square of the error and try to minimize that, so that is the typical approach that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ec89b369bf7fde4709e3d6d462e931a'}>, <Document: {'content': 'we use for fitting lines. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '107d45c8484ddfb794ca0f257c06549e'}>, <Document: {'content': 'So, as we have already seen with a sufficient data doing linear regression is simple enough ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e93544e6c5157dd06ee6040789d6be15'}>, <Document: {'content': 'just a set of matrix operation. But, then if we have many, many dimensions in the last ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61a17c0256f2573c2c3c4b22dabf0879'}>, <Document: {'content': 'slide our data had only one dimension is essentially, what was the time of day that was my x vector. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '521d3c266525aede2a5b99175c9ad9b5'}>, <Document: {'content': 'But, then if you have many, many, many dimensions and then, you really need a lot of data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9431550a4fba46ef28cdc9abf19386f8'}>, <Document: {'content': 'in order to avoid over fitting, because if I have like a 1000 dimension data and I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc63b4636d635d66195aa6ee267f0832'}>, <Document: {'content': 'only 100 data points it is very easy to over fit to those 100 data points, because I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5007b3d50b380a003b060b8c534c4386'}>, <Document: {'content': 'a 1000 parameters. So, how do I avoid that kind of over fitting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2917062ee9a03c7c0b290366f2d86ca9'}>, <Document: {'content': 'is by adapting something called regularization. So, we will ensure that of all the models ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6273a145bf3523970a3884119229a6de'}>, <Document: {'content': 'that can fit the data to a certain extent we will try to try fix find something that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22ad86524eb744f8e6fe5ceba039f151'}>, <Document: {'content': 'is simple enough, so that we do not over fit the data. So, here we are talking about linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '396bb787d34266f42adfbf0208bed2f'}>, <Document: {'content': 'regression, but suppose I want to do a higher order regression. So, if you remember in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '206d14f89aa0271959090db1639dd569'}>, <Document: {'content': 'previous slide, so we looked at an example where, so again it its looked like a quadratic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ea8b874e2745df414576e0782e39e99'}>, <Document: {'content': 'curve was slightly better fit to the data than a linear fit. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a41b79cab752bae7e4f54114d4167e1f'}>, <Document: {'content': 'So, how do we handle higher order functions one simple way of doing this is to look at, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7bcd0f02c150c0064baf2e20d7b28e73'}>, <Document: {'content': 'what are known as basis transformations. So, where you take the input, so input could be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2811bef6c01f9461ccb817155bdbbfa5'}>, <Document: {'content': 'say x 1 and x 2 there are two variables at that describe the data and then, I translate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c685f99a2fb3504549dcbf3c54eccab9'}>, <Document: {'content': 'that in to a much larger description by adding second order terms. So, I take x 1 x 2 I create ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b4d0bb567b3bbf4f9e43e1e99bcd013'}>, <Document: {'content': 'x 1 squared x 2 squared the product x 1 x 2 and then, the original variables x 1 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42eb1e26b08ca4d9b6b197aca2014121'}>, <Document: {'content': 'x 2, now that gives me a five dimensional data, so I took the one two dimensional data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8ce99437d51a8258842086b4d707c62'}>, <Document: {'content': 'and I converted it in to a five dimensional data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf6d0462a82cc26e8096e0dc8a45ae2b'}>, <Document: {'content': 'And now, I can do linear regression on this five dimensional data and this is going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee19412766cbe59e66bc0d568e3bd937'}>, <Document: {'content': 'end up giving me quadratics. So, I can use the same technique of linear regression and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a311618260e83e3b71172846bc613a6'}>, <Document: {'content': 'I can get a more complex fits by doing this kinds of basis transformation. So, linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d0e2fe55416a5f762299f742fb2a617'}>, <Document: {'content': 'regression is not really that weak of a method because I can do more complex fix using linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c783dbfbe19434da9264d2dbb1125b8'}>, <Document: {'content': 'regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c0ffabcbc6865cda8c118633f0b3db6'}>, <Document: {'content': 'There are many. many different applications of regression, so the one thing, which I already ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3596d3822fb750e21bdff5c98f8f393'}>, <Document: {'content': 'mentioned earlier was an time series predictions. So, you could try to build a model that predicts ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '48a1844bbac4653dd708e6459910e85'}>, <Document: {'content': 'rain fall in a certain region or you can try to build a model that predicts, how much money ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a256a51e04547cb1d69ca375c55942bb'}>, <Document: {'content': 'a user is likely to spend on a particular service let us say calling right sometime ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f00125ac445ac977467367eceb450fe1'}>, <Document: {'content': 'you can even use linear regression to do classification. So, instead of saying whether the person will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2439d91abe0cc8471be55ca6699dc3e'}>, <Document: {'content': 'buy the computer or will not buy the computer you can try to predict what is the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f305ecac12bcc2886fa9973d96cdfb68'}>, <Document: {'content': 'the person will buy a computer and you can try to solve the cerebration problem and is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '696b35919b5d3d2cb9a28c79cfef477e'}>, <Document: {'content': 'roughly. So, I have more I have more no answers to that right and you can use regression as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d9959a47ff3a897f6d37bed3f8ead56'}>, <Document: {'content': 'a data reduction tool. So, instead of giving you like a 100000 data points I can just fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a15d701180379b8860302100a8cd32d6'}>, <Document: {'content': 'a low dimensional line low dimensional curve to that it could be even a straight line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b77f8e5673f492bfa240fdc223e7c9ac'}>, <Document: {'content': 'And then, I can just tell you that these are the parameters of the line instead of giving ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bc5a4d11e825be63bef50371f35e841c'}>, <Document: {'content': 'you the hundred thousand data points and that allows me to have a very, very large data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9fd9485846c74e5c0dcb1dd58647189a'}>, <Document: {'content': 'reduction and the other thing is to look at trend analysis. So, it is slightly different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b057fd186a055302359bf505aa45cb2'}>, <Document: {'content': 'from the time series prediction problem that I was mentioning earlier, because I am not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b44fb5e60fb9bd0fc459a2d80d8d4403'}>, <Document: {'content': 'really interested in making predictions here. But, I am more interested in the data analysis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba2c0c4b7591ad431dba956bd48950af'}>, <Document: {'content': 'part you know not necessarily in the predictive analytics part here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4c073bd5c36bb07336cec0f39538e8'}>, <Document: {'content': 'So, I would like to know whether the growth rate is linear or exponential or somewhere ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '63f5696bc90b5501d63cf791bb1bb77b'}>, <Document: {'content': 'in between. So, those kinds of questions can be answered by suitably solving a regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94f46869a4a244a9fee3cc4eac36ac9e'}>, <Document: {'content': 'problem and then, going back to what I mentioned earlier about risk analysis or in the classification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ef37b1235148630463a9efa5c6b8fa'}>, <Document: {'content': 'case. So, you could think of risk analysis here, so if you remember in linear regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5f5f47426d41dbf24b8f5a8298578a91'}>, <Document: {'content': 'you learn coefficients for each of the input variables. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1ae93d0b3843145318e0095c39479dc'}>, <Document: {'content': 'So, the input variables that has a larger coefficient is the one that is going to influence ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe89d26027d9cefd69962ed59e4dfcb4'}>, <Document: {'content': 'the output the most. So, that way you can look at risk analysis by looking at the factors ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98b7c359803bd7b5a7c8c90a29cd333'}>, <Document: {'content': 'that contribute most to the output. So, this is essentially on supervised learning method, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52f7b6458551f22328ffa3d49ca3ee1f'}>, <Document: {'content': 'so we looked at two of these regression and classification. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4a9256da790d5febf6ada09f5bc53d1'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Unsupervised learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c94c28490a74517a246f02cde6d965f1'}>, <Document: {'content': 'So, in unsupervised learning, so what is our experience going to look like. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2a2a25df1278879108c61e5ec6db383'}>, <Document: {'content': 'So, we have looked at label training data during classification. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c59011fb6ee006c9a44732630058129'}>, <Document: {'content': 'In unsupervised learning, there is not going to be any labels, it is going to be completely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2d7af75d9233fd15dada9a164355f56'}>, <Document: {'content': 'unlabelled training data and, so the points of this going to look… ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3b7c17ad3d3c26cb0805f5e0367cd4b6'}>, <Document: {'content': 'The input data is going to look like points in n dimensional space. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6588c6a62edf4127d322f57562f1c440'}>, <Document: {'content': 'So, in this case it is a two dimensional space. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c2628111dac9111b553025a3c123fe7'}>, <Document: {'content': 'So, now, whatever I interested in doing here, so in the task of clustering what I am interested ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16554bb5c90d981e9a16a9a727ee1096'}>, <Document: {'content': 'in doing is to find groupings of similar data points in my input space. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee008bc86adf391daebec624e555f68c'}>, <Document: {'content': 'So, for example, this could be possible clusters, so that could be one cluster. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad72be669c5d4081a8bd516e13e1ff5c'}>, <Document: {'content': 'So, you can see that, there is some kind of a gap between the data points in that cluster ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a941fc1d9b3b8b30095e6a4220b5c98'}>, <Document: {'content': 'and the others and not a huge amount, but some amount and then this could be a another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '60e4204a466447401c180a57cdbf25b4'}>, <Document: {'content': 'cluster and that could be another cluster and that could be another cluster. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c42f3d26efd78cd7f880cf00bd7ed0e'}>, <Document: {'content': 'So, you could see that, a couple of things that we wanted to notice here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f546d14ad33cb1dc40694a2ba316b011'}>, <Document: {'content': 'So, one is that, the clusters all seen to be ellipses, there are some kind of ovals ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b28e6c9fe4f1bf0b3b9a2ad52c141a9'}>, <Document: {'content': 'in the input space and that is a choice that we have to make. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b78f666dee5fa98aa0368cac940c578'}>, <Document: {'content': 'So, that again gives you one kind of an inductive bias, you know. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2aac9f1a46c56bee49d48d846be23cab'}>, <Document: {'content': 'So, so you have to make this kind of a language choice, even when you are doing clustering. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4fb00c05dcfe41998178d781f30bc189'}>, <Document: {'content': 'And the second thing I want you to notice, there are the few data points that do not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ac5a8f6be3d58810a4f17372f7d6906'}>, <Document: {'content': 'belong to any of these clusters. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ecacb9d6356064e96ca9e272f21037b4'}>, <Document: {'content': 'It could be because, they lie equidistance from both clusters or they lie far away from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8264956a59d417aab4ad0f448001ec13'}>, <Document: {'content': 'all of the clusters, but then, they do not belong to any of the clusters. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b55c86e6b7613235a358452f697feaa'}>, <Document: {'content': 'For example, look at this point, which seems to be far away from all of the clusters that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c98cf966d3d275737a9d5666bba202d'}>, <Document: {'content': 'we have identified so far. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3480826b2714211ab9e03d1cd967f1a2'}>, <Document: {'content': 'So, such data points which do not fall into any of the clusters are called outlier. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8ec335c5f06366a8f9874dc5ab98870'}>, <Document: {'content': 'So, what I would like you to note here is that, this particular outlier that I am pointing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbe0310fa4d69651b2210a897afb1b2c'}>, <Document: {'content': 'to actually lies in the middle of the input space. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5418d1e2f63b3d6d3fc3ecfbff31f89'}>, <Document: {'content': 'I mean, there are data points left of it, right of it, above it, below it, everywhere. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1234400777330ab754ddc56c6ee5b91f'}>, <Document: {'content': 'So, usually there is a conception that an outlier is something that is far away from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c55778d0de5cd9c1b5f321ea5506e7'}>, <Document: {'content': 'the other inputs, it need not necessarily be the case. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '699b1beb27cf4b577bf7e7272ce4a0cb'}>, <Document: {'content': 'An outlier is something that does not fit into the current patterns that we have discovered ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd70ec75becf32efa57e8e892ce782020'}>, <Document: {'content': 'in the input. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd62a2d2aa8446e57e1093f8338becb1b'}>, <Document: {'content': 'It need not necessarily be something that is very far away from the input, so that something ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32fb70cb405cac87743174b6f3cb4ced'}>, <Document: {'content': 'which you have to keep in mind. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85060c9a5c386e9be9b786a22372c274'}>, <Document: {'content': 'There are lot of different applications, so you could look at, you know again customer ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '488190bc539ac906632d2c9ce25f21dd'}>, <Document: {'content': 'data you could discover classes of customers or in image processing, people try to discover ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f249e3226e0176ed1bf3cc7740651dd1'}>, <Document: {'content': 'regions in the image like shown in the figure that, which by doing clustering on the image ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '466535d1f8b2530eea76daecf3e2ebd6'}>, <Document: {'content': 'pixels and it could take words, look at the occurrence of words and the context in which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ddb8622aed720ac08129ebb1af89213e'}>, <Document: {'content': 'the words occur, try to cluster those context together and find synonyms or you could do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5428982cf8de5458810560b20d837c6'}>, <Document: {'content': 'even better cluster documents together and find topics or you could do the flip of it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '276b6865febaca49f723cb203f85aa85'}>, <Document: {'content': 'You know, you could cluster data together and try to find outliers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c95e6e3d4712133b9cf98c1bdca8090'}>, <Document: {'content': 'So, outlier mining, which is the very important task in several situations, where we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '78146bd5f038bb556705f4ebf4db8b7a'}>, <Document: {'content': 'to find, say anomaly in a data, you know. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ed53a9c41a37d83cf04d79cd3cd1bba'}>, <Document: {'content': 'So, you would like to build a secure system and you want to find a figure out if somebody ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '664b1340a2a450cbacfba7c324bf10d5'}>, <Document: {'content': 'is trying to track the system. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '491366be538c81a47eaeae3dcd549272'}>, <Document: {'content': 'So, any kind of anomalous behavior should be flagged. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd315c940d31c1f26946afe20c73b6695'}>, <Document: {'content': 'So, then you would not want data points that lie in clusters, but you want to find data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0aa817198746da77b62499af8f645db'}>, <Document: {'content': 'points that lie outside clusters. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd95ebd47a343400f5d044dbf4a79a18'}>, <Document: {'content': 'So, this is called outlier mining. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '228708d656b89fb5c7d41b13b861e0c7'}>, <Document: {'content': 'So, there are many different applications are for clustering and when we look at clustering ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b3bd212744cbd13ae58d7e130dd13d2'}>, <Document: {'content': 'in detail, we will see some more of these. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f6217504159578b1585b603f7de3075'}>, <Document: {'content': 'So, the other unsupervised learning task I want to talk about today is association rule ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd506e34904d8d90fc17d3f9f674d83db'}>, <Document: {'content': 'mining. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4e4a748d4e491bb7e118ea9c6bc2eb9'}>, <Document: {'content': 'So, so the idea behind the association rule mining is that, I want to figure out what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e253ac91789818ba1f18efda6f3bd4a'}>, <Document: {'content': 'kind of entities are frequently, you know co occurring in my input and so, then I can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6c29725ab6758ea13298c5408596c08'}>, <Document: {'content': 'say that there are some association between these. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d472d247ffab080a3d6814203e0b3f5'}>, <Document: {'content': 'So, this typically goes in two stages, so the first stage I find frequent patterns. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd09bac4396b6723de7c67c2de1152073'}>, <Document: {'content': 'So, this is typically the stage, where you analyze the data closely and this is where, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb5e5fbe521a236f31bddfd1835728'}>, <Document: {'content': 'this is essentially the quote unquote analytics part of it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4cda650d314a727655fa6ea40c01fff'}>, <Document: {'content': 'And in the second stage I want to derive associations of the form that, if A occurs, then B is likely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb1566266bbd62c0ede7494c4e3a8011'}>, <Document: {'content': 'to occur. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ec410869a8a6f06d451d9f2cf4304e7d'}>, <Document: {'content': 'So, A implies B, from these frequent patterns. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1869bf85c5e23f81350b9a2193be848e'}>, <Document: {'content': 'So, this is essentially a two stage operation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '860804e4c647145e91af8db8dfba578c'}>, <Document: {'content': 'First find the frequent patterns, once I have found the patterns, then try to find these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '783a3a5b1abd8a861fbd06a798f39698'}>, <Document: {'content': 'kind of associations. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '278493d2815c778889872b1eedee9672'}>, <Document: {'content': 'So, more often than not, the challenging part here is finding the patterns, finding the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17f250e837c12a3d48d811f8e48016f7'}>, <Document: {'content': 'frequent patterns. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17fbf8d8e58ff31a2443da496fb2b80f'}>, <Document: {'content': 'So, once you find the patterns, then deriving association is not too hard and as we will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '372c16b37459534063ae128914dc26bc'}>, <Document: {'content': 'the, see when we look at association rule mining in detail, but the performance measures ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef5218d46888b42fc0bf96d973d727be'}>, <Document: {'content': 'that you are looking at, typically are associated with the association rules. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90fd1a2579f168371e0416e37b817694'}>, <Document: {'content': 'I mean, how useful is this particular association that I have discovered is and more than, another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd49ae60fbd9702ffcdbd3512b0aebbd7'}>, <Document: {'content': 'frequent pattern part of it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '165438d93729775e15184a62f37a6ff0'}>, <Document: {'content': 'So, you could find patterns in sequences, you could find like time series data or a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3915e01b5963858cad8606c127126a2'}>, <Document: {'content': 'fault analysis, you could find patterns in graphs you know, where people typically use ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c99188b3dd0de91c63916eea044ee8e'}>, <Document: {'content': 'this in computational biology or social network analysis and other domains or you could find ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cbe810b32647cc5dd8987abf6e55224'}>, <Document: {'content': 'patterns in transactions, which is essentially the first domain in which people introduce ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb87b2957af823c8fdbc342dfdbe557'}>, <Document: {'content': 'this association rule mining problem, you know. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd54d7f2b0d77f254a97aa948524d4b4a'}>, <Document: {'content': 'So, association rule mining in some sense is an interesting problem, because that was ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16d6beb22bd68cbe7e9a56fb794f59a'}>, <Document: {'content': 'the first problem to which the word data mining was properly applied to. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '41b4b02966579fa77c8ecdae29e64220'}>, <Document: {'content': 'You know, in some sense you could say that association rule mining kick started the whole ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f83139015a82aab736f9a4515b436952'}>, <Document: {'content': 'world of data mining. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee1665d46ef5e5d7c1c2e71915470454'}>, <Document: {'content': 'So, what is, what I am mean by transactions here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd7ae455b1582c74d4885ae2cfd0dc03'}>, <Document: {'content': 'So, so transaction let say is a collection of items that are bought together. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8a395bf0dafc4029f470d6228e376ad'}>, <Document: {'content': 'I do not know, you just go to super market and you could buy a set of items and then, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5f2b9caef794edeefb618321841f40d'}>, <Document: {'content': 'so all of the items are go together in the basket that you bring to the check out would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4687149264ee6c167d8231fafd3a4bc'}>, <Document: {'content': 'form a transaction. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '209ed37c4c179356453d1623c7b5b60a'}>, <Document: {'content': 'So, so this, the original form of this problem as post as a market basket analysis, so it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ae87eff1153ca45e930e8360adbb5f1'}>, <Document: {'content': 'is essentially what goes in to your basket when you do your shopping. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ca6f45f42f945f0d65f7e447998aaee'}>, <Document: {'content': 'So, you look at the items that are there in the basket and then, try to figure out which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d7f8456903862df1863631c93d8f35a'}>, <Document: {'content': 'all items that people buy often together. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4da4af0ec25a209346287c444e1f722'}>, <Document: {'content': 'So, it could be, there need not be individual items it could be sets of items and then this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '556367e08cc786df1bb15829ce4e6ddb'}>, <Document: {'content': 'community, these sets of item are called item sets, just as a terminology. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '374fd7e35a45611a4e4ac13687730b7a'}>, <Document: {'content': 'So, I will be using item sets frequently when I talk about the association rules. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75c580800a6c58fd238624af2974fa7b'}>, <Document: {'content': 'So, the goal here is to find item sets that are frequent, once you have the set of frequent ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd45acc1dc1469105c8f458eccc89b192'}>, <Document: {'content': 'item sets, then I can go ahead and form rules of the form that, item set A implies item ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5423bacf196d378323757365c35306fd'}>, <Document: {'content': 'set B, if both A and A union B are frequent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2b9de593166f4f5bb034f1505ae2969'}>, <Document: {'content': 'So, A is frequent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4db4a530f19e0e5c1992656dc4a01875'}>, <Document: {'content': 'So, I say I buy milk, I also buy bread, so if A is frequent and milk and bread, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f9992a027cb1b2aeb23d9aa17b2db1a'}>, <Document: {'content': 'is essentially if milk is frequent, so lot of people buy milk and lot of people buy milk ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd0c37ef8d32c4664061eb518c916918'}>, <Document: {'content': 'and bread together. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '95bc7f5315873cef51795ece2370c1d1'}>, <Document: {'content': 'So, I could say that milk implies bread, so if you buy milk, you are likely to buy bread. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac89d4d9d84bcc44c20e5dd1d6b86cc7'}>, <Document: {'content': 'So, this is the whole idea behind mining transaction data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f728cc79034143117266b544aa78ab03'}>, <Document: {'content': 'So, lot of applications here again, but just to highlight a few. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4bd22c573e30df6a00a3931962e9b7ad'}>, <Document: {'content': 'It is, the first one is market basket analysis. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c9aef0125d8776087d06250f706e527'}>, <Document: {'content': 'People have actually tried to extend this to a point, where they try to look at the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff0df18ad402dc4dbc95207b54c7f8a2'}>, <Document: {'content': 'arrangement of items in a store based on what is frequently bought together and people could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '49f76aabf985cf25d32d5f97b3554510'}>, <Document: {'content': 'try to design promotions that take advantage of this kind of market basket analysis. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b82f52552f38711a1d4a471c2a4221e7'}>, <Document: {'content': 'And the second thing is looking at predicting co occurrence and where the things tend to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f9d83a7f4b02886e64089f2e8e5f8c92'}>, <Document: {'content': 'occur together, this is the very generic application. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4c449044a495b69fd49ba517f400f8eb'}>, <Document: {'content': 'And in the context of graphs, so mining of frequently occurring sub graphs in the graph, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e91b904d6ef76046eae3fe2a41863e2'}>, <Document: {'content': 'allows us to have a lot of insight into a kind of, you know behavior you would seen ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '313aef521399d492cdd30a3d27cc26b7'}>, <Document: {'content': 'biological data and social networks and so on and so forth. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd86b7ca6414c8c9bfae9ee1f24aa4f7'}>, <Document: {'content': 'And you could also look at this in time series looking at frequent co occurrence of events ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd33ba7be75c3dc7a9185e1ea4429addb'}>, <Document: {'content': 'in a time series can be looked at as identifying trigger events. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1956fc90153fa74d9df1d38485dc0dc4'}>, <Document: {'content': 'So, if this event has happened when quite likely another event is going to happen very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bba1066be59f8cc37189eb4c36c294fd'}>, <Document: {'content': 'soon. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c80c927fbab230941c0e950ef957a95'}>, <Document: {'content': 'So, those kinds of a trigger event modeling, all of these can be done using frequent item ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2baff664ec1ecee94157898a240124f'}>, <Document: {'content': 'sets or association rule mining. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a4df99466e54bb9c02119c8cf36821b'}>, <Document: {'content': 'Why we have been looking at machine learning in such detail. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7dad4b139f248f77acde593811aaf312'}>, <Document: {'content': 'So, data analytics in particular data mining in my opinion is machine learning really, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ef0acde5d709c39c04f5f2832afeaf3'}>, <Document: {'content': 'but applied to very, very large data sets, very noisy data sets and very real data sets. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba747b60012a2c4741f3e27d3130a7ec'}>, <Document: {'content': 'So, what do I mean? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbdf5e7819f4f6c9da822a3d340b0134'}>, <Document: {'content': 'So, classically machine learning has been more concerned with getting accurate parameter ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '719e1e7e4a3413814dd592318f65b1a4'}>, <Document: {'content': 'estimation from small volumes of data and trying to do the best that you could do with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58af4077af80bab0d01c3bea0b4062b0'}>, <Document: {'content': 'little data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16f7d97b4ed30a81a59802ebdae38a0b'}>, <Document: {'content': 'But, now with very large volumes of data available, some of the focus is moving away from handling ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd096541a714947f6964f2870c9634e5c'}>, <Document: {'content': 'small data to things like, you know how do I make sure that my algorithm will finish ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4566e1541fa6fb6c9321d4d5b2dca72'}>, <Document: {'content': 'running in, you know in a few weeks. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e5438360fe5ca361d01d19b863a5c51'}>, <Document: {'content': 'And, so looking at scaling issues, looking at things like data selection, do any to run ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f01e3d0c33a5026eeccbad206aca2b77'}>, <Document: {'content': 'my algorithm on the entire data, looking at feature selection is spoke about that a little ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6d31e33399f4bb170c8732556077ac8'}>, <Document: {'content': 'bit earlier and looking at the actual design of your data base, so how are you going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cad4820c781db5262b30ad9008091784'}>, <Document: {'content': 'represent the data and so on and so forth. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8e90b16c3f6d87a21b7add2dd6b9942'}>, <Document: {'content': 'So, all of these issues now have come to the forefront. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '633fa0b4b8de1ce092e0e9c2a7b33391'}>, <Document: {'content': 'So, when you are working in these kinds of domains, you call it data mining and you then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '712c6932545dfab854b3744abe2a4c9e'}>, <Document: {'content': 'you go back and look at specific algorithms and error measures and so on and so forth. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f0040b71f7bf36c0d297927bc0261e9'}>, <Document: {'content': 'Then, you call it machine learning, but then the lines are the kind of a, you know getting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b31809fc25d528e4c6db418cc7241b55'}>, <Document: {'content': 'a very fuzzy between the two. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'be2f34bc4be6816d79a25648ebde2376'}>, <Document: {'content': 'And then, the second difference error’s mentioning is on noisy data sets. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9016c09c951056636c9a5543d0f88f81'}>, <Document: {'content': 'So, again I mention a little bit of this earlier, so you have to worry about cleaning the data, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb74e8f7ec96fb9ceefc2b105b3af03b'}>, <Document: {'content': 'you have to worry about missing values and you have to make sure that your algorithm ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '497aea55537651af659c4603bbc17d85'}>, <Document: {'content': 'is robust, when something goes wrong that you did not expect earlier, you know. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e65d8cfd9d4aa1d7bc5b061caf9a1be7'}>, <Document: {'content': 'Something goes missing or some data gets corrupted, you should have some kind of a failsafe mechanism. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'db609af3b0b43cb5dd157a8af744f168'}>, <Document: {'content': 'And the last thing is a little more technical which is that, real data typically does not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff4cd76b57e864235614d85308a7224b'}>, <Document: {'content': 'satisfy some of the nice theoretical assumptions, many of the machine learning algorithms were ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b168e873a9242b0645f574ec2ab58c7'}>, <Document: {'content': 'operating under for several decades. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3efa97ce8bc3471fbdf2b612a09eb8d2'}>, <Document: {'content': 'So, typically the assumption is that the data is independent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd3cbd201a84b64b97e091abb684ee9f5'}>, <Document: {'content': 'So, one… ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c59afbda26d82f086cc7f1fafdd1edb'}>, <Document: {'content': 'So, we looked at the training data earlier, so their assumption was that each of those ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '321c92e277d12fc525b786c536b99b1'}>, <Document: {'content': 'points in that age incomes space was sample independent of the other and then, there were ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '47c610e4e5e771935c5f2652deaca318'}>, <Document: {'content': 'sample from an identical distribution. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fccf6647b40d1edfca60058ebe589243'}>, <Document: {'content': 'So, it is not like each data point came from a different population, they are all sample ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7c53158417ab7a6200426d23b42b35a'}>, <Document: {'content': 'from the same population, the same kind of age income distribution and then, they were ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c8a2daf60549f13088af22650258da8'}>, <Document: {'content': 'sample independent of one another. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f799735b0bad3b8c279cbdf7391451f'}>, <Document: {'content': 'It is not going to be true, more often than not, because your friend goes to a shop and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f4605ccc98eb44442e2c86d293af1ee'}>, <Document: {'content': 'he likes a computer and buys something there, you are likely to go there as well, but it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16be513539897179da53cc39b9808543'}>, <Document: {'content': 'is not like the fact that the first guy bought a computer is not influencing, whether you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45d4c23d2fd7932444c69e5101db3224'}>, <Document: {'content': 'are going to visit the shop or not. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e12621eb4f14ca108586ee588f1125e'}>, <Document: {'content': 'So, these kind of independent assumptions are typically not valid in real data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d803d32bbf7115b82e524bd9b3b3a82'}>, <Document: {'content': 'So, you will have to think about that and then, so we looked at the classification problem, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c32e34f6ad50375c5d3e01f002471813'}>, <Document: {'content': 'where we saw that we had, you know positive and negative classes like buys a computer, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7292fc19bc859a8daed633a11c899839'}>, <Document: {'content': 'does not buy a computer and more or less equal, I mean that is about 60 40 split between buying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '559bf0d09558a68df8905e98acd09c1b'}>, <Document: {'content': 'and not buying computers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b630af0dfff4bd29e84f89f425413367'}>, <Document: {'content': 'But, in reality it is not so. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9596ede9622ffe1601930b239cd2345'}>, <Document: {'content': 'For example, you take any medical domain, then fraction of people who are sick is very, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8748638392fe64b222a9698e11cf85b6'}>, <Document: {'content': 'very small thankfully, but still it is very, very small and therefore, it makes a machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3929ea33dda7bef90787eab5ad00b0ed'}>, <Document: {'content': 'learning problem very hard. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33fd7f68ec44695d717eb3a619c8929b'}>, <Document: {'content': 'So, if I tell that everyone who comes to the hospital, I mean or everyone I see in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f25b21f448024b1b32866e109a2888c'}>, <Document: {'content': 'population at large is healthy, I will probably be correct with, say some 97 percent accuracy, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b695681903b2f25d3a5415958bd8488e'}>, <Document: {'content': 'which case it is not a bad predictor you know, but that is useless for us. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '946dd0405ff1c9920017fbf20589dd64'}>, <Document: {'content': 'So, we have to worry about handling this kinds of class imbalance to what actually make sense, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4fc07187d8956cdb31fff116ff5ec582'}>, <Document: {'content': 'when the data is so imbalanced and the another thing is, in many real worlds scenarios like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ccfa49933f9ca31e8a8395ce3753a62'}>, <Document: {'content': 'in medical domains and also in security domains, like you cannot be happy with the acceptable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fbe39ef1431f51883b360f9bc496947f'}>, <Document: {'content': 'levels of performance, you know. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d586698b5548ddcabb252ce62a2a966'}>, <Document: {'content': 'They have to be near perfect for you to be able to deploy these things in practice. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b9262f18b80fe2712c0d5291d02efcc'}>, <Document: {'content': 'So, this causes a lot of trouble, you says no more the case being able to do the best ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abddea9d3f6d33105b65c2a779f92ead'}>, <Document: {'content': 'effort and get away with it, you really have to be the best. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1575b73249da0cfb2409fbb5abe346d7'}>, <Document: {'content': 'So, so such issues also are slightly different from what people worried about in a machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b85c04294e6e72f6da1c4c4b93c50eb2'}>, <Document: {'content': 'learning community. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8290b3d45d9b6338235697dda0da0d5'}>, <Document: {'content': 'The last thing is, you have to operate under the resource constraints. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e63f4fab9175f5b85d337d13e07319a'}>, <Document: {'content': 'You know, maybe want them things wrong on your hand, your handheld device not that there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b383d26a36b7c60164920115e60c0d62'}>, <Document: {'content': 'is a much of resource constraint any more, but it could still be and I have to work with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad35d3bddeaba0f5e456add9abc6ea76'}>, <Document: {'content': 'limited computational power and you might not have the luxury of all the time in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3b4ee1e7fdfa61102eabcfcf263148e'}>, <Document: {'content': 'world to produce an answer, you might have to produce answers in real time. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c6f356663542006587f60b6650dbdac'}>, <Document: {'content': 'So, how do you go about doing that? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29b4d9ea35f3b16e2c38b05a2f305052'}>, <Document: {'content': 'So, there are lots of issues, so but at the core of data mining is machine learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9053660e5e51129e6d7343377aad3a22'}>, <Document: {'content': 'So, machine learning gives you the kind of algorithms and the data mining addresses all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3eaf62acf94765c4d84cac40f81a7cd1'}>, <Document: {'content': 'these issues on top of those algorithms. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '872fef90d97107f550ee78243ccad335'}>, <Document: {'content': 'So, that is one of the reasons while we look at machine learning and we will continue to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5f64083e4a8952c7f0d7d2401802c2e'}>, <Document: {'content': 'do so in future modules in greater details. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e964401559efe1f3be54c94f33181e5'}>, <Document: {'content': 'Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '170f7be97195bd40737de367c3c69a78'}>, <Document: {'content': 'Ordinary Least Squares Regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6f5a5caadcf22379a8315f79093941f'}>, <Document: {'content': 'Hello and welcome to our second lecture on Regression. In the previous lecture we provided ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e0c850af6dfe1f62eae649a276098ab'}>, <Document: {'content': 'you with motivation for why linear regression could be a very useful data analytic tool. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '760be47f83f8833609967f639b95309b'}>, <Document: {'content': 'And today we are going to take the ordinary least squares regression, which is one type ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de2a7fd04201b2642a73c98d9ce751ec'}>, <Document: {'content': 'of regression and actually step through the process and in some sense, derive the formulas ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3937bef4fb06cf2c6afa2706b99cfd61'}>, <Document: {'content': 'or the math that enables you to convert the data to performing a regression analysis and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5aa9326cf7c0697bb28228562f681dec'}>, <Document: {'content': 'the context in which, we are going to do that is, we are going to do a simple regression, which just means that this single input variable only involved and we are going to step through ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25648649d5da85f449ac5357a88e2bbd'}>, <Document: {'content': 'a mechanics doing that. So, what is the broader context of this exercise, so we introduced, we gave a motivation for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb59b4dc752c013e1da4b90b23e52de5'}>, <Document: {'content': 'linear regression in the previous class. And since then, you should have had a few classes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b131f25380977571332e63142d97e12a'}>, <Document: {'content': 'by Professor Ravindran talking about machine learning in general and also a module on supervised ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c358734613145a5e5bd65f37cdf11c3'}>, <Document: {'content': 'learning and we purposefully choose to straddle regression before and after supervised learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ebd8047d4add29f0c48f8b24d15eab53'}>, <Document: {'content': 'Partly because, it is important to realize that you know regression, linear regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c6ce36415d039b3007d4c7b9195b609'}>, <Document: {'content': 'the whole process or any other form of regression is a supervised learning tool. You know supervised ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1111d955b171e05eb2c4c3c0c1f996d'}>, <Document: {'content': 'learning being a more an umbrella term, definitely encompasses a regression and regression based ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '91ba8ba26eafd9762fc8f5833903bc13'}>, <Document: {'content': 'approaches. And this is despite the fact that for instance regression is something that has existed many, many, many years before you know even the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61d9a59d09b9f34b171bf4d195d336ae'}>, <Document: {'content': 'terms machine learning or supervised learning or artificial intelligence was even thought off. So, you know the context that you often learn regression could be quite different, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1c05d8f5309d3806055a578903c01b'}>, <Document: {'content': 'where you learn it from statistics course, whereas in a machine learning course the emphasis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a33e3232a32cecca7b54df9310a0d6d6'}>, <Document: {'content': 'sometimes might be on other tools or not I mean depends on where, what the focus is. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf9e819d99e87e7da44f640ff0d2116c'}>, <Document: {'content': 'But, the important thing is to acknowledge us, while regression sometimes stand alone in your statistics text books, not sharing pages with the other machine learning techniques. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dbd5ad3be89147993f39957fbfe07a0d'}>, <Document: {'content': 'The regression, linear regression is just as much as supervised learning tool or anything else or any other supervised learning tool. Another source of confusion that I just wanted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88054bb853e81c743f78b0cc829fcf9a'}>, <Document: {'content': 'to clarify before proceeding is, supervised learning techniques tend to get broadly classified ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84cbd3979890ebcc0d85d406c4423d92'}>, <Document: {'content': 'as regressions type problem versus classification problems and there, what people I meaning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bde29a2f41635de77af9c1e0d293f02'}>, <Document: {'content': 'is quite different from what we are learning as regression at this stage. Out there, what people I am talking about and it is just a definition is, when they ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1c8ee33c0e2ceed8a94350e86364dc2'}>, <Document: {'content': 'say it is a regression problem in supervised learning, all they are saying is that the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ac4c334358db5ebf26e59e8a7814735'}>, <Document: {'content': 'output variable is a continuous quantitative variable, whereas when they say they dealing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8aa1c76d4333c28ca1f6b472c9a86ac'}>, <Document: {'content': 'with a classification problem, they are saying that your output variable is a discrete or categorical variable. And you know within those two broad classes you have many techniques ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ecc11ef062dc5aef1b5fe3e5adb452b'}>, <Document: {'content': 'and some techniques comfortably handle both types of data. But, that is sometime gets confusing people saying, it is a regression problem does not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9079d058bff6ed489faccca90f0b0d4f'}>, <Document: {'content': 'mean I am doing regression no or there people, what people are meaning is that the output variable is continuous quantitative. Having said that let us proceed with, what we wanted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9fac396a277bfb4de350636bf67fad25'}>, <Document: {'content': 'to do today, which is deriving the ordinary least squares regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '542b8cf0f2b8529c5b2aae148985b2ce'}>, <Document: {'content': 'So, the goal out here is to fit a line essentially of the form y is equal to m x plus c. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5dc456f0c2a224a2dc80660080886c3b'}>, <Document: {'content': 'that is the form you might be heard of more frequently, what we are going to use in this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '854fb32d2835c4a5b2b0d75617391879'}>, <Document: {'content': 'class and in most classes is y is equal to beta naught plus beta 1 x and that is you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31d1a5fb4d5d90fbf1d684c43555b914'}>, <Document: {'content': 'can readily see those are both the same, I just replace m and the c with two other terms. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '884d62a90a78d3e2bfa15c43ed3cac18'}>, <Document: {'content': 'So, the coefficient is beta 1, the intercept is beta naught, so a line once someone comes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '81750dd38135f07129548f06f07e619e'}>, <Document: {'content': 'and tell you the values of beta naught and beta 1 or m or m and c whatever you prefer, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb6053375e19d6c4ef4a206ce3a9aa94'}>, <Document: {'content': 'but someone comes and gives you those two values, then you can define a line. If someone ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4c112cfc945a112fa5ef2c17c0e65e01'}>, <Document: {'content': 'says draw a line, you can draw different lines you can draw line like this, you can draw ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f23c5bc9a07af38e00bab2da33c87b61'}>, <Document: {'content': 'line like this, you can draw line like this these are all lines, now these are all as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad4775e86bd02a5eb99a894d52140a84'}>, <Document: {'content': 'straight as you see them lines. But, once someone comes and gives you the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f54f24888c69c332bbb36ba45ba53d58'}>, <Document: {'content': 'exact beta naught, there is the intercept and the slope that is a very specific line, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c8322a38ca8677baa8c0747ca50a4599'}>, <Document: {'content': 'only one line will have that exact beta naught and beta 1. So, that those two terms are what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '359b560bf73f0ada856d19bac30a1f81'}>, <Document: {'content': 'define the line and to give you some intuition beta naught is nothing but, where the line ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68982db1bc7b233965d6a600fc80b21c'}>, <Document: {'content': 'intersects the y axis. So, if you wanted different lines with the same beta naught, but different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a4e33992c034bbd7a109505dbb2242f'}>, <Document: {'content': 'beta 1s, then you can think of many lines that go like this, go like this, that go like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb267ca9214b84129f6b6afc769c46db'}>, <Document: {'content': 'this, these are all lines and just keep in mind I am trying to draw as straight as possible. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '72ebbd4a2fa325e6db19ff6cda314ea5'}>, <Document: {'content': 'So, these are all lines that essentially have the same intercept beta naught and different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a32d7bd7821a3092877732055020f17a'}>, <Document: {'content': 'slopes beta 1s. Similarly, you could have different lines that have the same slope, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '15c8d7c74fcf354af9fa345b6f08584a'}>, <Document: {'content': 'but different intercepts, so that would look a little bit like this. So, these lines all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f32643c57e8606eff90ff52b6926e787'}>, <Document: {'content': 'at least in, what at least in terms, in theory have the same slope, but different intercepts. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ee053ac9bb55c100a13a713bb00c830'}>, <Document: {'content': 'Now, if but once you defined a slope and intercept there is only one line that has that, so that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94efe2b9f2492235b5d9398e8651d333'}>, <Document: {'content': 'is the idea and what we are trying to do out here is saying, what should that slope and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a1f4d7d16e36db9b576112c42d952c8'}>, <Document: {'content': 'intercept be; such that you feel like that is going through lot of your data points. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '203021ebdc065d2c4b96461876275a03'}>, <Document: {'content': 'Now, I have said that in a fairly vague way, but I am going to define that more formally. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '233335af8b158ed75b097474c4af9998'}>, <Document: {'content': 'To define that more formally you want to have a concept of the actual data point, this is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5354bfd13457699f7082bd35c2dc65a7'}>, <Document: {'content': 'the actual data points, all those squares are the actual data point and what the estimated ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '829e4ec642eff0ee52f0602b9f15e307'}>, <Document: {'content': 'value of those data points are. So, this data points has a value a particular value, so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '95867797994504554f260963f60afd7c'}>, <Document: {'content': 'we will call that y 1 and this data point has another y 2 and so on. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f64e4da93166e823f4383064a2e5b321'}>, <Document: {'content': 'And, so we call those the actual values as y i and for each of these, now if I chose ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd4838cf587fe5546fb3c8caaaec0126'}>, <Document: {'content': 'to fit a particular line that I feel is like going through this data, I am going to have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8189c13cbd178538aa3959e5380facbc'}>, <Document: {'content': 'some predicted values. So, what I will do is I will fit this line that is I will put this line here and I will say, my prediction of this y 2 is y 1 is nothing but, for that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'efc73eb466e77c87ef92f0b6b71078fa'}>, <Document: {'content': 'value x 1, where is my line. So, I push this value x 1 up to the line, what value am I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31e404fa9dd9dda6c689fd9bd0803539'}>, <Document: {'content': 'getting a y and this is my predicted y 1 and it is represented usually with the small hat ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9af2026799192e2460e075b7863fd8c6'}>, <Document: {'content': 'that you put on top. And this same process for y 2 I will I will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30c53b6ddcf23d1ca3b48f5dbfb37062'}>, <Document: {'content': 'try to write a this actually this line is not perfectly correct, so let me just erase ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ee948c496b16be6f111460068c139af'}>, <Document: {'content': 'that. Essentially, what I will do is this is my y 2 I will draw a line there this is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a005ea5dbcd1ba24677ea4703170d158'}>, <Document: {'content': 'y 2, but my prediction for y 2 is here. So, I am going to put a dash line here and this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52f8da40e5aaf44e489e687080bf2f96'}>, <Document: {'content': 'is x 2 and my prediction is y hat of two out here and you guys can see what I have done ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '299edbd2acd0725aca87fef4449e5792'}>, <Document: {'content': 'here. So, I have basically said look this is value of y 2 and it corresponds to some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4e32f32675cb8ac8de5639a16c888f0'}>, <Document: {'content': 'x 2 and I am going to take x 2 and see, where my line goes through in terms y values. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc4257f2cb0494de11bda9b7f76b8d9d'}>, <Document: {'content': 'So, this is in some sense my actual value and this is in some sense what I would wind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc6d7ebf4bcbe7380c3fa2502f1a732b'}>, <Document: {'content': 'up predicting for y 2, because I have tried to kind a fit some line through a data and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2829c47f57cdb877c9f6bde74c1a252e'}>, <Document: {'content': 'you might ask a question. So, if this is x 2, then why do not we then just predict y ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9173959c73c5cc4fb502179c6105933'}>, <Document: {'content': '2 in the sense y is not y hat of two but equal to y 2 and the answer is fairly simple you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e009329d654b5e740ebfc7816a7ea22e'}>, <Document: {'content': 'do not want to predict the exact data point because you are getting a sample we discussed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5fd1e2586c7681f44ed96d4a71db4c81'}>, <Document: {'content': 'how this is not a population. So, the population here for y 2 would be a for the same x 2 I had entire universe of possible y is and we know that for this same ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '236917473cbed6c38791ff9adb0ea5f1'}>, <Document: {'content': 'x value if you what take a other sample that might not fall exactly on this data point. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13f06642a7511f298bdf2f64e1ea9a2e'}>, <Document: {'content': 'The next one could wind up falling somewhere here, the next one could wind up falling somewhere ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2ffb6611da0859817132b7eaac0b799'}>, <Document: {'content': 'here, the next one could wind up. So, we do not have the entire population of possible y is at the x values at the input variables x 2 and, so what we wind up doing instead ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ed39b83abb60004a6d3ff526d485ba9'}>, <Document: {'content': 'is not predicting exactly on top of that value that you got. But, instead trying to fit this line acknowledging that there is going to be some noise above ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '212a3329a684e0d73c511a84aff4dbcb'}>, <Document: {'content': 'and below and you might do better of predicting at this point, where x intersects with the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa40de923d139533fc502ee035b79e59'}>, <Document: {'content': 'line and that is your predicted y 2. This is, so that you do not wind up getting fooled ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45431a57b00affe98a8f6303e7711134'}>, <Document: {'content': 'in some sense by just some amount of noise or uncertainty there is above and beyond the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14ee65c9f00061733f2634940843c317'}>, <Document: {'content': 'exact that the trend that you are setting the line in some sense indicative of the trend that is, which is in general when x seems to go up y seems to go up and that is, what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6441de8060a32c876d733d1eaca2dc01'}>, <Document: {'content': 'the line is showing at least this particular line with the positive slope. And you want to capture that, so that tomorrow someone say, what will, what do you expect ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7794d4ce0ef4673ba51516c44b87e4c2'}>, <Document: {'content': 'when x is equal to this value you go to the line rather than you going actual in individual ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b77770886d61077140693ad07b7cf2fb'}>, <Document: {'content': 'data point. So, let us see an idea you now have the concept of actual y i and y i hat ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4051afc6c33952a3caa27d182d61ceb9'}>, <Document: {'content': 'and the goal in terms of what we are trying to do is that we are trying to minimize the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7554f9b631a568b677f6200b35eb97f'}>, <Document: {'content': 'squared deviation between the actual and the estimate. So, we are actually saying this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc9c3edc9bd4db54a1e23ba545449ed8'}>, <Document: {'content': 'is measure which is y i minus y i hat and sometimes y i minus y i hat is going to be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd30488cab5986cd36ea297a409954c8a'}>, <Document: {'content': 'positive this case this case is positive, because actual is greater than the estimate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7826b029d459427b28ca351c1766bc6b'}>, <Document: {'content': 'In some case it is going to be negative, but you take all these positive thing and negative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '403d4f10af148c4f82f28e5d69ac95a5'}>, <Document: {'content': 'numbers for each number and square it, then they all become positive. And then, you sum ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ee8bf767a4a5eb0c2928f48b7fcb7b0'}>, <Document: {'content': 'it. This is the sum of the square deviation between the actual and the estimate and it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '712d9697e4c97cdc14387a11ff89550b'}>, <Document: {'content': 'is a measure of how close the line is to its data points and what we are going to try and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3dbcb44ac774236973400b0041883779'}>, <Document: {'content': 'do with an ordinary least squares regression is figure out that line. And, how do you define ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1541435a590993e28c4155774321a7c4'}>, <Document: {'content': 'a line you define it with beta naught and beta 1 once you fix with the beta naught and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd20b03ba60d0294aeb14d7c79b4d21f'}>, <Document: {'content': 'beta 1 the line gets fixed. So, we are going to try and figure out the goal of this exercise to figure out that beta naught and that beta 1, which defines the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d9394986218332c857bd3983d50bdcc'}>, <Document: {'content': 'line, which results in minimizing the squared deviation between actual and estimate. Because, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3b70085514385a550fe6c3e9f0bdf75'}>, <Document: {'content': 'may be this line with another beta naught and beta 1 is not is very far away from your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '856396d401fe04867c28408394391d7e'}>, <Document: {'content': 'points. So, the square deviation between actual and estimate is going to be huge or take another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c89779393b62e0f417eef11bbf07bba7'}>, <Document: {'content': 'example this line, which is like this is also not going to work very well. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aceb8a0900ee475c6da8901986a94e62'}>, <Document: {'content': 'Because, look at the kind of deviation that you have between actual values and estimated ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca436b4ef2fc7b2f0af892c47e7e932b'}>, <Document: {'content': 'values. So, this line with it is beta naught and beta 1 this line in red with its beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '682a20f460ab7ee17ee86984ac0a28fe'}>, <Document: {'content': 'naught and beta 1 might again not do to well. So, what whatever we trying to do is we are trying to figure out that line when I am saying we are trying to figure out I am saying we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30ce487495e0a70464f3898c159f49ed'}>, <Document: {'content': 'are going to figure out that beta naught and that beta 1, which is what represents the line. So, we are trying to figure out that line, which minimizes deviation between actual ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9a71954483ad31a1cc0cd45be98106d'}>, <Document: {'content': 'and estimate, so does that kind of make sense? good. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10eda84834755320c28acb5504fb9cda'}>, <Document: {'content': 'So, how do we go about doing this the way we go about doing this is the process of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23cc2d6d0fb54e65361c4d9802f092b9'}>, <Document: {'content': 'derivation. We start by saying this is the functional model we have we have the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb32ef037ad4116f9d06db331c62dc1f'}>, <Document: {'content': 'which says that y i is nothing but, beta naught plus beta 1 x i, which is the line that we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9807d23c2138501ce02e57b580afa33'}>, <Document: {'content': 'are creating we do not know beta naught and beta 1 is yet. But, if you had beta naught and beta 1 your line would nothing but, beta naught plus beta 1 plus some amount of error ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b903885b0153333fb6264b9e93fba2f'}>, <Document: {'content': 'just going back. For instance to this, what we are saying is each y i, which is nothing, but this value this is y 1 is nothing but, is equal to, where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '532504442df3a23b14ca96b7a14060c5'}>, <Document: {'content': 'you can get to in the line this is equal to this distance and this distance can be defined ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9dc6e9679649fa2086741398e83d0e5f'}>, <Document: {'content': 'as beta naught plus beta 1 x 1, because this value is x 1. So, this distance y 1 is nothing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '19af63d44546cd7686c0a4346f5480f0'}>, <Document: {'content': 'but, this distance beta naught plus beta 1 x1 plus some amount of deviation, which I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64f64268742b1a4856661f58f977e4d1'}>, <Document: {'content': 'am going to call as error this is the deviation between actual and estimate that y i minus ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45e29c01054711d229cf6b7f61270001'}>, <Document: {'content': 'y i hat that distance I am calling as the error. So, ultimately y i is nothing but, beta naught plus beta 1 x 1 plus e i and I shall really ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e50f76c091bce6ae0a279a438e8b361'}>, <Document: {'content': 'say that beta naught plus beta 1 x i plus e i, which is what I have done out here, as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a226264c63f4c3f3805f15fefbf3c6cb'}>, <Document: {'content': 'said y i is nothing but, the model plus the noise. So, we will call that model or you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c5062bba73d1cad6c1b99b0bea26ee'}>, <Document: {'content': 'can call that y i hat and the noise, now all I do is just rearrange the terms such that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9656d1206b5515f34617f9891c3fb183'}>, <Document: {'content': 'e i is on one side and we have said that our goal is to minimize. So, this is the deviation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40361637929f4d535f7087fc2f0e0194'}>, <Document: {'content': 'between y i and y i hat and our goal is to minimize the square of the deviations for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '554a5994a006429e57038e0c8de895bb'}>, <Document: {'content': 'each data point. So, I am go through i equals 1 through n I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94802a418bd4dc69f42f05989c10704e'}>, <Document: {'content': 'am going through each data points 1 through n all the way and for each data point I am ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ee320c959513ffdd5da815f2b5faca6'}>, <Document: {'content': 'trying to look at the deviation between actual and the model and this is your estimate or ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f0ddee40596c65f6e9629faf09bc345'}>, <Document: {'content': 'you can think it as y i hat. So, this is what you are estimating and this is what is actual ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5de79ecf42992e2dd64f454f4c6dda3'}>, <Document: {'content': 'value you are taking the difference between them and squaring it and it is the you get ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d22d877c229cfd04051193ebc4bd54'}>, <Document: {'content': 'the two minus signs because you can think of it is minus and put the beta naught plus beta 1 x i into the brackets, then you open the brackets minus comes in front of both ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84d150fc51ac048ff4510568a1fb1858'}>, <Document: {'content': 'terms. So, you are ultimately just taking the summation of the square the square term is here the square of the deviation between actual and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c25fc3f6088654ab7a0c540938d03c65'}>, <Document: {'content': 'estimate. And; that is, what we are going to call as a sum of squares error and that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b180320febdcd8df6a66821962391d'}>, <Document: {'content': 'is what we are going to try and minimize you essentially want to minimize the squared deviation between actual and estimate. And you can also kind of think of it this is one way of getting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ada904b9c6fc665a2535c72c1f1913a7'}>, <Document: {'content': 'into sum of squares you can also think of this definition, which is I started by saying I want to minimize the actual minus estimate square and we know that the estimate is nothing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2b927273ed342c30b1c08a1f2ad118f3'}>, <Document: {'content': 'but, so y i hat is beta naught plus beta 1 x i. See notice the difference y i is beta naught plus beta 1 x i plus the error term, where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c17d011adf4bd370c2ae1a9064d11306'}>, <Document: {'content': 'as y i hat, which is the estimate of y i is just beta naught plus beta 1 x i this basically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '866a760d3ba8a6cc98a3d08848e7b8ac'}>, <Document: {'content': 'defines the actual and this defines the estimate. So, you can just plug in this beta naught ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69a0df371da1d5815b0665ebb27b82a3'}>, <Document: {'content': 'plus beta 1 x i and I am using the word beta, but really these terms are still b. So, b ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cf503498eab54da49d6964a9f845717'}>, <Document: {'content': 'naught plus b 1 x i will be more accurate. So, b naught plus b 1 x i and when you plug ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90359da2b92bcd9049ebda826f8e07a2'}>, <Document: {'content': 'and expand that this is exactly what you get the same notion of sum of squared error, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35e91154121bbbed756644a3a4643da0'}>, <Document: {'content': 'is cycle through each data point and look at difference between estimate and actual. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1af6e9343cbb719ecaf2d5ee8077c28e'}>, <Document: {'content': 'So, our goal in determining the beta naught and beta 1 see the y i and x i are data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98e5db9f7a382d5e6d57a39979813f24'}>, <Document: {'content': 'that you collected from the field x i represents the input variable y i represents the output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '309abfb46448363790f1eeeb9d5b853a'}>, <Document: {'content': 'variable. So, you have 10, 20 or a 100 or 1000 of x and y pairs, so for a particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61c934d23aeb161938bc2b024bc3ff51'}>, <Document: {'content': 'x there was a particular y and there are i such there are n such x and y pairs and i ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca7da58520805d27b322277f80c56ef0'}>, <Document: {'content': 'is just the index that represents a particular combination. So, x and y are actual data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7da274835bb05e1189ba008d4636c33b'}>, <Document: {'content': 'b naught and b 1 is what we are going to determine and the way we are going to determine that is by finding, what values of b naught and b 1 minimize this function. So, that the exercise ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3c55c8e689df55f6b2c3d197840d840'}>, <Document: {'content': 'that we are embarking upon. So, how do we do that, like I said a goal is to minimize this term and the way we going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2b2f10a06b9bdb3bb2d189f4e5e97c0a'}>, <Document: {'content': 'to do that is to take a very basic idea from calculus, which is that you take the first ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ebf2759487921946ab6bec5cdb25d03'}>, <Document: {'content': 'derivatives of this term and equate the that first derivative to 0, why do we do that it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5dfe2a6706ae37a8a3fd43ab5810b66'}>, <Document: {'content': 'is a very basic idea from calculus, which is when you if you take on different values ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '36d1c8cc71e4d4bdf5c795bff2063153'}>, <Document: {'content': 'of beta naught and beta 1. And right now, these are the two variables of interest y i and x i are actual data the idea that if you fix one let us say you fix beta 1 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb52c94c672c5834c3cb35662681d28e'}>, <Document: {'content': 'you keep on changing beta naught. For a given beta 1 there exist a beta naught, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aca8cdc584e60d566cf85fc8a6a39dbc'}>, <Document: {'content': 'where this error will be the lowest and, so for a fixed beta 1 if this was the variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da129d3916a78f2cd7dc4ab8c2cb8b9b'}>, <Document: {'content': 'beta naught I am plotting the beta naught here and I am plotting the sum of squared ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c93305651ad9e41b8d91aecb2ca862d'}>, <Document: {'content': 'error this is SSE out here. The core idea is that you are going to get as you keep on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ceda68d9ba57fad394fb030741c74609'}>, <Document: {'content': 'changing beta naught for a fixed beta 1 you might get a function that looks, let me make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a09a93125020a0272e2101682ee9c660'}>, <Document: {'content': 'more smooth I will just try again am I get beta naught you might get function smooth ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3f8e49fb344fcb8ecbe727f5b0dc536'}>, <Document: {'content': 'function like this, which basically says like this there exist a particular beta naught ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e44125a1119491c5a269d8e0d941f2fb'}>, <Document: {'content': 'value, where the sum of squares is minimum. Now, how do you go about finding that, it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7d301956ac5820d7bca05a889a96462'}>, <Document: {'content': 'is a simple idea if you take slope of this function the slope of this function at different point at the point at, which the sum of squares error is lowest that is slope is equal to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f85744e5d41467828865e5c50575addc'}>, <Document: {'content': '0. So, the idea is that the slope is nothing but, the tangent to this function just like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a89b98ed0ddf664751c3e3f51ed2645c'}>, <Document: {'content': 'the slopes always are and this is, what is considered a positive slope a flat line is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2cdb8c1198ccfa7302c5e410c900982f'}>, <Document: {'content': 'considered as 0 slope and this is negative slope on the left hand side. So, the idea is that if I take the first derivative, which is nothing but, the derivative of a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c6840346fd564492ff4c9e13f95f1d0'}>, <Document: {'content': 'of a particular function is nothing but, the slope of the function and if I take the derivative in equate it to 0 and I should be able to find out that value of beta naught, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40033723de02cf2ff21330e859f8b05'}>, <Document: {'content': 'gives me the lowest value, now remember I of course, said beta naught for a given beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2baa5dae7e17cdcf3223d087a95249b2'}>, <Document: {'content': '1. So, I might get that in the form of in the form of beta 1, but then what I can do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88601bf112b421c8fbbf9e0abafcd5a1'}>, <Document: {'content': 'is then I can do this same exercise that I just did for beta 1 I can say for a given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4cd147aa0dc33898a2fbbc20a83f0810'}>, <Document: {'content': 'beta naught as I keep changing beta 1, what is that value of beta naught that minimize it. So, essentially you wind up having a concept ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58867d9b17908933b9387597aaf9d540'}>, <Document: {'content': 'two equations with two unknowns the two unknowns are beta naught and beta 1 the two equations ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '802b5836fcdc63fba619fa74431b9cc4'}>, <Document: {'content': 'are, what we get are what you get when you derive with respect to beta naught and what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1625cd7edf268d33385e6592f3a53594'}>, <Document: {'content': 'you get when you derive with respect to beta 1, so b naught and b 1 again. So, what you get when you derive with respect to b naught and what you get when you derive with respect ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '266c62d63b3634b091402057d32dc21f'}>, <Document: {'content': 'to b 1, then you equate that to 0. And then, you have two equations with two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5771f6797e32da561168bfd489dc608'}>, <Document: {'content': 'unknowns; that is just a simple form of simultaneous equation for you to solve it. And as you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3097f259a99ebc4971455c1e02d7b8cd'}>, <Document: {'content': 'see what you have done in each of these two is to take the first derivative and these are partial derivatives, because you are clearly deriving with respect to beta naught but, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c62a30a3a4e65796b6406dc1a8932fc'}>, <Document: {'content': 'you also have an another variable in this equation beta 1 same here these are partial derivatives. Because you deriving with respect to b 1 and you have another variable which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f90c77cf16ad2eddf7be7920837cc66'}>, <Document: {'content': 'is b naught in the second equation. So, but the core idea is this, which is you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd41583aeec9f726b50c7558809e421f4'}>, <Document: {'content': 'take two partial derivatives of sum of squares error with respect to b 1 b naught and b 1 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'efdf08ec5cd6e085ced29151a74f87ba'}>, <Document: {'content': 'and solve for the values of b naught and b 1; such that you will be able to get that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c60cdaab01e1191b2a32d2b70e66e7b7'}>, <Document: {'content': 'b naught and b1, which minimize this sum of squared derivation in a sense. So, we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87d9e451ba6a67bdaec6968d96da5e2e'}>, <Document: {'content': 'explained a principle lets actually go to the steps that how we will do it. Now, first we are going to take care of first equation that we saw, which is this equation, so let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2379fd32027b2bc5d42de31405306abd'}>, <Document: {'content': 'just call this one and this is two, now we are going just to do the process for 1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f0f7a6c12b7874dc2aa2c6faee58627'}>, <Document: {'content': 'You take the first derivative and all, what you might notice, now is that I have essentially brought this differentiation in because it is a sum of that we would looking at the first ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ce6d7d65a7cfa8877202bb19f432a8'}>, <Document: {'content': 'derivative of a sum of terms, which should logically be the same thing as sum of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f9a044799d464ead8cd06b6e750a507a'}>, <Document: {'content': 'first derivatives of those terms and what I do here is I am differentiating with respect ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66ce5ffa010185fb2620114eb1e89ad7'}>, <Document: {'content': 'to the beta naught. So, you can do this in many ways you can just basically take this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '382bbb4dc446eed10c40521ac9b6bef'}>, <Document: {'content': 'square and just expand it and say what is y i minus beta naught minus b 1 x i times. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2b7685eff3d0b2f1328ba483a251393c'}>, <Document: {'content': 'Because, it is a square times the same term again and then, you get many terms and then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '882f791ba0081b680c8ba7c6bfb3cbb1'}>, <Document: {'content': 'break them up or you can use something fairly simple called the chain rule, which is just that this is the function of b naught. So, I will first take this square of the function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c6ac62d6adf1eab0c85f6c238ce5306b'}>, <Document: {'content': 'and do you know the usual idea that I difference the first difference of x square the derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbd1e4098c6665a58b852eb1875f0d10'}>, <Document: {'content': 'of x square is 2 x. So, the 2 moves out and the derivative of minus beta naught is minus ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2b11da364408cc7056222c2ef48bf4'}>, <Document: {'content': '1, so the minus also comes out. And, so essentially use can use whichever the approach in differentiation that you like, but this is the answer to this step, now all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca5277128fe135860be935868d3381b2'}>, <Document: {'content': 'I am going to do is equate to 0 and then, I am going start solving it. So, what I do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ebe54ea608f4adeefafbf2cf8d1b78a5'}>, <Document: {'content': 'is there are many separate term here again, so summation of a minus b minus c you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '305e961f679eb361e47556d975e30685'}>, <Document: {'content': 'basically say it is summation of a minus summation of b minus summation of c I have done that and I have reshuffled the terms in this equation such that beta naught comes to one side, because ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef9e23705dab83a3078ade43ee148ce4'}>, <Document: {'content': 'we are interested in beta naught. Now, beta naught is essentially a constant ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '27091bd960e344a6d4349ecad4a72ad7'}>, <Document: {'content': 'it mean it is a variable in this equation, now but it is take on one value it is not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d5449a5574f294de567ca3095bd7105'}>, <Document: {'content': 'like x i, which takes on different value depending the value, what is the value of i is . So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb64cb015526142fe613a82e24581c9c'}>, <Document: {'content': 'if I am doing the summation from i equals 1 to n each x i will be a different value, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8be7c9ff9dbace1b711e39d36c775869'}>, <Document: {'content': 'but beta naught is not a function of i it is a same beta naught for whatever value of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50cd20101407f512787716704fa09a93'}>, <Document: {'content': 'i you pick. So, it is essentially out here is all you are doing is here adding n such ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '394e8e0ad3f781248ba3e5787bcb37be'}>, <Document: {'content': 'b naughts and that is nothing but, n times b naught. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50ff27bd84578474f11222fa55bffcb9'}>, <Document: {'content': 'So, what will do in the next step is to just isolate b naught and therefore, if you notice ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c7f81776a3c6bb20dbcd166ac425d37'}>, <Document: {'content': 'we took that n that was coming up on this side and we moved it as the denominator. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '598c93a41775bd5792ee14d8c0ab95c8'}>, <Document: {'content': 'that is, what you are seeing here in terms of moving from previous step to the step. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c86db5f930c421921aed38cd59af260d'}>, <Document: {'content': 'And finally, we realize the sum of y i divided by N, which is the number of times y different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f9a1461d31540b608f7a84093f91593'}>, <Document: {'content': 'y is nothing but, y bar is nothing but, the sample mean the sample mean is nothing but, the sum of your data points divided by the number of data points, so this is the easiest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5188a269096e8e1c2f7f5b30193a548f'}>, <Document: {'content': 'representation. Again we just simplified equation 1 of a 2 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '877215b1f3c0729325f63a74ab6f7ea5'}>, <Document: {'content': 'equation combination with two unknown variables hence, b naught is described as a function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f14041428fdd655080182272fd04a6a'}>, <Document: {'content': 'of b 1. Now, we are going to solve for b 1 by substituting values of b naught with the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '15a1381dcf7881eff35abeb929c8df5c'}>, <Document: {'content': 'term on the hand side, so let us do that. Here is the derivation of b 1 again a same idea, which is you are doing partial derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2ab01680a00a8c4e9afc50eb6e5de7f'}>, <Document: {'content': 'over the summation and again you can take this term inside, which should be fine. And ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c432baf9e2c9238f3ce9b6c8d020157d'}>, <Document: {'content': 'again you can use the chain rule for deriving it or just basically expand this whole set ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f40e9139c170c8d7281450b17bdd99d7'}>, <Document: {'content': 'of terms expanded by the square and you can do it that is your convenience. But, one additional ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '345778462ebd6e5b796a710d1d03556a'}>, <Document: {'content': 'thing is it, which won’t look exactly at b naught, because the coefficient for b naught was just this minus 1, where is a coefficient for b 1 is you have the minus, but you also ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6739dc693b8903de89af39f4ad736ffb'}>, <Document: {'content': 'have x i, so minus x i. So, the answer is also going to look it different, the result of this derivation is this value and essentially you have an x i in brackets ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c91aa52e0da740f21566575d7882a9c6'}>, <Document: {'content': 'y i, but this derivation again should be fairly straight forward once you do this derivation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d8b1ceffbc5e57092de625c7830dba9'}>, <Document: {'content': 'you get this and you again go through the process of breaking this down or simplifying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e17261d45b577b890a22c3dbd8f651ff'}>, <Document: {'content': 'it. Again you had a summation over the entire set you can break up into many summations, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4b9e916419662f0694f4e88ef687afe'}>, <Document: {'content': 'so that what we have done here in the next step. Now, reshuffle things such that the b i the b 1 comes to one side, so that is what we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd43a48705ff9f73a7209f2109511cd50'}>, <Document: {'content': 'have done here b 1 come to this side and on this side you would have had only these two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1b263a476edb9a732aaa6157727f678'}>, <Document: {'content': 'terms. But, the problem is, so this b 1 I just shifted to this side, which is what you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13146a487019145001f9cd0342bac078'}>, <Document: {'content': 'are seeing here, but the look the right hand side is looking different and the reason for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '39a4b87723e413126947c67bc1419d21'}>, <Document: {'content': 'that, because in the right hand side only these two terms should have been there. But, again look it is the function of b naught and we know, now from the previous exercise ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a758f22f19097812ef0838bf4c0b221e'}>, <Document: {'content': 'we know that b naught is equal to y bar minus b 1 times x bar that is was the conclusion ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1b10df7192232d3cbeabaad9e11a509'}>, <Document: {'content': 'and actually show you that value that is just erase we conclude that b naught is equal to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1715564ca4e5547d31322c5439ba221'}>, <Document: {'content': 'y bar minus b 1 time x bar and let me also erase this and that is exactly what we are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d342dc0071e80a33ddcaeb7403c21d7'}>, <Document: {'content': 'substituting, what we are doing is we are going ahead substituting this b naught with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '39d680a8b5fbd63ee59fae64e2fd429d'}>, <Document: {'content': 'that term. So, yes we have shifted this to this side ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3e97f503aa8d6b90391cb08d3e3ed17'}>, <Document: {'content': 'and that is how you get the left hand of the equation when the right hand side in addition ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e7d7278d933642923d71e91f1d23eb8'}>, <Document: {'content': 'to this y i x i we substituted the value of b naught with another term. Again note this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa1da8342e7f1f7e59f61c1148e41b85'}>, <Document: {'content': 'summation y i divided by N is nothing but, y bar summation x i divided by n that the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '197ebdbc7761c4bbb6cb5f41fe15bea9'}>, <Document: {'content': 'two term are here and here are nothing but, y bar and x bar, so we said y bar minus b ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18ac734e6236766b1d243fc81b53659e'}>, <Document: {'content': '1 x bar and of course, this x i out here just stays out here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '664a435af0a5c20dde65aba70ae7c663'}>, <Document: {'content': 'So, that should give an idea, where the expansion is and, now again what we are trying to do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5494d7e531e55b78ff1a08c6439ce44'}>, <Document: {'content': 'is we trying to keep all b 1s to one side. So, this guy out here also gets shifted here ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd23818695af288c9706e070117e3d13e'}>, <Document: {'content': 'and that is what you are having on the left hand side of the equation and the right hand ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80b214d80aa0bb2e2dc5fde3c2969981'}>, <Document: {'content': 'side this step gets unaffected. And finally, you can just simplify this is just basic algebraic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a2e33abb5eee0d5572c68f3f4927d63'}>, <Document: {'content': 'simplification to get to the final form of b 1. So, what would you do when you given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '794d867f96a99c28d2c2119342585c58'}>, <Document: {'content': 'a whole bunch of x and y and you need to fit a line through it you use this formula essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da8bace394480e1983843afa58909fea'}>, <Document: {'content': 'to get slope b 1. And if you look there is nothing in this that has b naught in it is just a function of y’s x’s and N and x i squares, but again that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a51409f7f81eb7f3ae1f794328ebb866'}>, <Document: {'content': 'is from x i and N is nothing but, total number of data points and you can use this and get ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2e4235efad01277d3b62fe79f330489'}>, <Document: {'content': 'b 1, which is the slope of the equation. And then, you can go and substitute b 1 out here ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'afb7270f98003cb92cbd90e7849f1576'}>, <Document: {'content': 'and you know x bar and y bar from the data and get b naught and as you know once you have b naught and b 1 you have a line on your hands. And we essentially use this is the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc304ef10a463727ab4d0c3775278ea6'}>, <Document: {'content': 'process of ordinary least squares regression where you take a bunch of data x’s, y’s, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4086310b43086c88634e4096a89b4d3f'}>, <Document: {'content': 'x and y pair of inputs and outputs and fit a line through that such that you minimizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c974c3a0ec77afeb17f1f4814b81111'}>, <Document: {'content': 'the square deviation between the line and the line represents your estimated values ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '71db79fe7ae0c9745460363410fde648'}>, <Document: {'content': 'y for a given x and the actual data point y. I hope that was clear and that is your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba3e48d2a0640e5d3058143e0dc58485'}>, <Document: {'content': 'ordinary least square derivation. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c357aa8753f5b778c8e58286832b0177'}>, <Document: {'content': 'Simple and Multiple Regression in Excel and Matlab ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcdcea969b418fae44b73808761492bd'}>, <Document: {'content': 'Hello and welcome to our series of lectures on the topic of Regression. Over the past ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ad1969942b74d378eaa0274f683405f'}>, <Document: {'content': 'few lectures we discussed the idea; we motivated the use of this approach called regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4610f5d5f95cfbee46d861f1b96af528'}>, <Document: {'content': 'And in the last lecture we even looked at the idea of deriving the ordinary least squares ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '735af9806b8770b59c3761bdd448840e'}>, <Document: {'content': 'regression and we did that in the simple linear regression case. Today, we are going to look at how you can implement a regression through software and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '228daf20583a29d6c839ec1275bef429'}>, <Document: {'content': 'we will show to you in both excel as well as MATLAB. But, the important thing is to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd64d25f65d281b3282b6d264ec9dc0a'}>, <Document: {'content': 'just understand the core concept and the terms, so tomorrow you would be able to implement it with any software of you are choosing. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a7aa6037dee582335ee0bc4b835e4068'}>, <Document: {'content': 'Now, with respect to the main topic that we discussed last time, which was simple regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84fa5bc6d0e6d9b5ac176cfdc946f3f6'}>, <Document: {'content': 'So, when we did this whole analysis on the derivation for the ordinary least squares, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ded194d3f07518966bfbc9a73fb40bdf'}>, <Document: {'content': 'we took a case of simple regression which just means that there is one input variable; ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb0877da11756d3de69eba1845bec451'}>, <Document: {'content': 'that is involved. Today, we are going to extend that, we are going to look at the case, where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3bd3f1b404a1fb31e30300e18f0b084'}>, <Document: {'content': 'there are multiple input variables. So, let us start by setting up an example ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '615932e39db5aa19b5aef982c7fec650'}>, <Document: {'content': 'and I will show you a demonstration in excel. For performing the multiple regression, we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b21ab6db896a677aef0cfbde2f8a97fb'}>, <Document: {'content': 'can talk about some terms that you might encounter when using this software. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '575ae3a05ebb5f3d1c53e5aeb60bf610'}>, <Document: {'content': 'So, this is the sample data set and what you have in column A, there is serial numbers ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21fb0bde798dfc064d5d078a557e9834'}>, <Document: {'content': 'to give you an idea of how many data points there are. I have chosen an example, where there are 88 data points. The variables B, C, D and E as you can see selected on the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '602f5402c983ed41509eb5d8f582b11f'}>, <Document: {'content': 'monitor are actually the input variables and column F, which is shown with, which is highlighted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17512838aab621790698102e38a0d7df'}>, <Document: {'content': 'and also labeled as y is your output variable. And, so for the first time you are interested ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16a2a7c241254f8b0ecb71c99139dca3'}>, <Document: {'content': 'in creating an equation, regression equation that maps these input variables to y, the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '73ad97918d83435705ac897cc0acd051'}>, <Document: {'content': 'output variable. So, the way you would do that and I am using Microsoft excel out here ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8bbbb6e5af6a7f95f80c6f4b22b80a56'}>, <Document: {'content': 'would be to go to, you see the topics on top that say file home insert page layout where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dfb1e271f744c265e7b3e18dd3d27397'}>, <Document: {'content': 'my mouse is and in that, you select data. So, usually your screen might be at home, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '717384095ed3e03024e7fa8d7f00aceb'}>, <Document: {'content': 'you would go click on data, then you would go to data analysis, click on that. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5840e00c4270ddd6702e6499fe0872b'}>, <Document: {'content': 'And that will pop up a set of possible tools that you could use analysis tools. And for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '609c231ab065ed89488de44805851de6'}>, <Document: {'content': 'some versions of excel you might not have this readily; that is you will not have this data analysis button already and there you usually need to go and add it in and it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1fda9e5a9859ff8296cb0755e106910'}>, <Document: {'content': 'called an add in and it should be there, it is there in the software, but it is just not installed for you. So, here you would choose the topic regression and you would click on, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4c6d41ab4c180afcfc7c5c08ade5f13'}>, <Document: {'content': 'now it is asking you to tell it, where the data is, so that is what I am going to do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb583bac713927e85335ccd1a156cf97'}>, <Document: {'content': 'I am going to show where the data is and the way I did that was to kind of select that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5534f4e024f86f2f69809b731aa536a8'}>, <Document: {'content': 'whole column. And if you are bigger familiar with excel, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c01b73601b469a6b4b7718fb18597ab'}>, <Document: {'content': 'you can just use some hot keys to do that as well. Just note that I am also selecting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '97652525e58e27d43cef4ee240060471'}>, <Document: {'content': 'the titles of the variables when I am in putting the variables. And, because I do that I need ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13c3d83d63b8e16a05438d561a2dc81a'}>, <Document: {'content': 'to check this box versus labels, meaning that I have also provided you with labels. The ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f702679920382cf64e72818b310f9f66'}>, <Document: {'content': 'idea here is, the constant is 0 basically means should the regression line be force ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1f393c3df0f62e76aa3bb36d975b9f8c'}>, <Document: {'content': 'to go through the origin and we do not want that, so we will not check that. That basically means in your equation y is equal to beta naught plus beta 1 x 1 plus beta 2 x 2 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd242a3387b27aa342d679de5708f9288'}>, <Document: {'content': 'so on, your beta naught is going to be forcefully said to 0 and that is not something you want. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '643cd0cac2f62745beafc6074a931e6e'}>, <Document: {'content': 'The second also gives you out here you have something called confidence level and that gives you the option of choosing a confidence level or essentially an alpha; that is different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87ce64208cdc2b788ac559429735eaf0'}>, <Document: {'content': 'from 0.05 or 95 percent, but we are quite happy with 95 percent, so we will leave that. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f8e431ffbdf21f56614e9a6ea6e993a5'}>, <Document: {'content': 'There is a couple of other options, the thing is I said that I want my results in a new ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f8da2cd6ef1ef69c20c3530d3c2d2fb'}>, <Document: {'content': 'worksheets, so that is what I am going to select. So, I say and this is the output that I get you might notice put in to a new sheet. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e42cafdd4c76607ef714f9614ae3d3b'}>, <Document: {'content': 'let us just look at this output and try to process this. So, what it says out here is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '918fccb247b15475280efc2a2952038'}>, <Document: {'content': 'that the important things we have to note is the R square, the R square is in some sense ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b71b0127b7bae8a0d227a0353ef85de5'}>, <Document: {'content': 'a measure of how could your over all fit is. Essentially, it tries to say, how much of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c56923d32a0c566919524a1e86a725b5'}>, <Document: {'content': 'your variation in y is actually being explained by the model that you created versus how much ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '631fc314e46324b0569076c85e0e7ccb'}>, <Document: {'content': 'of the variation in y, it is just beyond noise. So, the R square is essentially a number that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '658c07fe2e687e958e5dfe4432803de'}>, <Document: {'content': 'goes from 0 to 1 and it is essentially the square of the multi, but it is called the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7620b49b9d409bb7e3b0cc9816262fa7'}>, <Document: {'content': 'multiple R. There is another term called adjusted R square is a very important term and we will come to that in a minute. And you finally, have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd4ba00c95f969dd7c3535dc33b73b11'}>, <Document: {'content': 'standard error out here, which is been highlighted and standard error is nothing but, for a given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '327d0c48ef991a85e295b76e63863fc0'}>, <Document: {'content': 'predicted point, how much is the general deviation of the actual point from the fitted model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b8d724356a6c16143fe9b6f17abb55d'}>, <Document: {'content': 'for on average, so that is, what is being captured in standard error. So, an observation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '508a45a8187828c00e2b821feb134b74'}>, <Document: {'content': 'is just the number of observations there are, below this top table that we have just discussed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3cabfaa1a3bdba7b1b7669f77eb30d1b'}>, <Document: {'content': 'you see two tables. Let me first talk to you about the bottom most table and then I will come to the table above, the idea here is that here you have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e96a70688dbac1a1544d6562d8dcd21'}>, <Document: {'content': 'the intercept and the four input variables. Now, what is called as the coefficients correspond ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8adc0ddbaf688e7c4decb5d7a1f6055f'}>, <Document: {'content': 'to the betas? So, when it says intercept the coefficient 9.29 essentially talks about beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb6b1f2edb949c90af978dc7cef92a6f'}>, <Document: {'content': 'naught and corresponding for A, B, C and D are their respective betas. Because, just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67074c9f9e8ad15cace53f2526b86739'}>, <Document: {'content': 'remember that we have a functional form of the nature, I am just going to type it out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a475e15afdfd8db3eb5309707b85b76'}>, <Document: {'content': 'here y is equal to some constant, let us called beta naught plus beta, let us call it 1 times ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74b67d2ddb02c99a967fef7ab6a47a48'}>, <Document: {'content': 'A plus beta 2 times B and so on; that is the functional form that we have and it goes all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1fb0c3e3104b0256921dac3da7867f82'}>, <Document: {'content': 'the way from C and D. The big question is, what is beta naught, beta 1, beta 2 and that is what these coefficients represents. You can call them beta 8 times ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c4f98680e8e7a00016f1a23c15391af'}>, <Document: {'content': 'A if you want, but essentially the coefficient corresponding to A is what we are talking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8984b586d8575e01da159fbb0668fa2e'}>, <Document: {'content': 'about here and these are the coefficients. These actual values that you see here, excel ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e632f444739d72fb0c44ad05da5b3de2'}>, <Document: {'content': 'has done this regression for you and it gives you these results. The standard error that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57047d56f0e258b11118cfb55738de3b'}>, <Document: {'content': 'is being discussed here is the standard error around this coefficient, what is… So, this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b5aeac52da5f9d7eb2202d56b4f63d5'}>, <Document: {'content': 'coefficient is nothing but, an estimate based of sample going back to a topics in inferential statistics. And the standard error pertains to the uncertainty ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c10d0bc031aa63adbcb57757be3146d2'}>, <Document: {'content': 'or the standard deviation using quantifying it with standard deviations, uncertainty around ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35cd13f86b65fbf30054eb602cf18600'}>, <Document: {'content': 'that coefficient, which is being quantified by the standard deviation associated with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a1802c6cbf2db43bb3a8c500047fec0c'}>, <Document: {'content': 'this estimate. Because, this is an estimate and there is some uncertainty around an estimate just like you had a sample mean and the sample mean, which you got from the data, which is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '73356e27f73ac752affd6bd960b2d055'}>, <Document: {'content': 'supposed to represent the population the sample mean essentially is a random sample from a distribution. Because, each time you take a sample and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98478b5405dbdc0024c968092d04ff25'}>, <Document: {'content': 'take a mean you are not going to get the exact same value. So, you are getting a value from a distribution and that distribution has some standard deviation we for instance earlier ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98d921ade26cd974ea34a5512b3503d6'}>, <Document: {'content': 'discussed about this standard deviation of the sample mean it is something that you can get from the distributions. So, similarly the standard error quantifies the uncertainty ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df3813b520cf1d35e8a4212f309d2bf5'}>, <Document: {'content': 'around this coefficient around each of their respective coefficients and it does through ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1e437b4bcb287ed01f822965fe15fe6'}>, <Document: {'content': 'the measure of standard deviation great. So, now finally, from standard error we go ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9cd34ead372fe58efe1ba30ac0cf2c9'}>, <Document: {'content': 'to the t statistic and the reason for that is that this estimate, which is this coefficient. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a0c9db64b2e25f38c5afe1d0643296c'}>, <Document: {'content': 'The distribution associated with this estimate just like the distribution associated with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4b5cfd95853f4f1615d4bdb8c81b8ef'}>, <Document: {'content': 'the sample mean was t distributed if you did not know the standard deviation of the population ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '550fc05f60c705f42a8086ecc0f14b5a'}>, <Document: {'content': 'that is the same core idea here, which is that it is t distributed and excel calculates ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b2caefaf7a2d51df455b6992f3e075c'}>, <Document: {'content': 't statistic for you and gives you the p value associated with the t statistic. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a50f1c4a7843bdcb3b767d39dba42e9e'}>, <Document: {'content': 'And what you have out here are a setup p values and what they represent is that they represent ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38741fbe87e3b62f7445f0224b8a3569'}>, <Document: {'content': 'the p values in the t statistics correspond to the hypothesis that this coefficient. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56a07ac64aebec8271d32a1f2dd41c4e'}>, <Document: {'content': 'let us take one example, so I am just on a highlight this example, so we got some coefficient ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '12e9878f73c12026ea2e4056d7bb6c02'}>, <Document: {'content': 'for A and that is 0.2420, what we than chose to do is test the null hypothesis that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a7ac5f7e3b1b6ecde1a7752c8ed4fd56'}>, <Document: {'content': 'coefficient is statistically different from 0 and this p value is the probability that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cc8338766da2bba479caf344d598a44'}>, <Document: {'content': 'comes about as a result of performing this hypothesis test and as you might as you already ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3bcf1e7aff85f4de22b2154b8e74512'}>, <Document: {'content': 'know the p value is nothing but, the probability of seeing this data if the null hypothesis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c15ad73b2ad21d1cb69bb09d50d8560'}>, <Document: {'content': 'is true. So, if the null hypothesis that that the sample coefficient is equal to 0 is true, then the probability of seeing the data that we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d6bfadc255d23aa7ad5c4ff8e7ff53a'}>, <Document: {'content': 'seen is this value, which is 5.21 times 10 to the power 17, now that is the very low ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '19d68f1552afe86846b5a825ac698df9'}>, <Document: {'content': 'number that is a very, very, very low probability. And, so the idea would be there out here we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6ab7a0f04c74ecf0ffc50b8aa35056b'}>, <Document: {'content': 'would reject the null hypothesis that this coefficient is statistically no different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3594d099ada7003c634f222c3f19b3d'}>, <Document: {'content': 'from 0. So, if you take a look at this all of these p values at least are fairly low the sign 5.1 e power minus 17 just means times 10 to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1f558792e31d6ccde0d22f3dce13da60'}>, <Document: {'content': 'the power of minus 17, which means it is a very low number and the only value that is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4aed1e3678bac7fe2f105b8a9900965'}>, <Document: {'content': 'high is out here that is 0.33. And; that is the kind of value on which, you might not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eff676cce72c812ab14f865302385dc5'}>, <Document: {'content': 'be able to reject the null hypothesis or reject the idea that the coefficient is actually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f5685c60cb088f65746cb43a16ba35'}>, <Document: {'content': 'indistinguishable from 0 the null hypothesis that is that this estimate this parameter ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ccc7fae7deba145d3b6a89a1a7e9c99'}>, <Document: {'content': 'could actually be equal to 0. Going in line with the idea of confidence ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '457ea2dc993ee409554554774b6fc74d'}>, <Document: {'content': 'interval, so you also get confidence bounds, so this is the lower 95th percent confidence ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f783e4b50b86cb508ff5da5b1ee34f2'}>, <Document: {'content': 'bound the upper 95th percent. And if you had mention values different in the original pop ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a5944174109076776100ee8e2c7e14f'}>, <Document: {'content': 'up screen saying I want something more than 95th percent you would have gotten the 95th percent bounds as well as those new numbers, but if you have not then these numbers just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e4c8b9f169e9217927a13a53abf2f7a'}>, <Document: {'content': 'repetition. Obviously, noteworthy thing is that given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e57c6601683b909c6b056d2a6ad45e76'}>, <Document: {'content': 'that these are 95 percent confidence bounds you can reason from inferential statistics ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df6bdebf292f310d1554c0aa5b25223a'}>, <Document: {'content': 'that, whenever the p value is less than 0.05 or even actually technically less than 0.1, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc8a94bb479efd167761268b27ab71fa'}>, <Document: {'content': 'because it is the lower and upper confidence bound if it is anyway less than 0.1 you should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd62446f02f55d9b263162602bdf44f1d'}>, <Document: {'content': 'have the lower and upper on either side of 0. So, here the lower bound is 3.6 and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd741493db7b6959ccc997fd18c9fa5ce'}>, <Document: {'content': 'upper bound is 14, so it does not intersect 0. So, whatever sign your coefficient is your lower and upper bound are is going to be on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45f70346e0a7e6a4f15297a154f2ca1a'}>, <Document: {'content': 'that side of 0. Obviously, when a p values as high as something like 0.3, which means ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e536e2e8564c165334e5262250594f'}>, <Document: {'content': 'you potentially could not reject the null hypothesis, then you would have a lower bound ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1616655cd486b3ec63e808f9e1138d7'}>, <Document: {'content': 'that could be less than 0 and you will have an upper bound greater than 0 and that should make sense in terms of how we understand confidence bounds and p values. So, this gives you the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6dd8a2c4f194ea6498f676988149f7e9'}>, <Document: {'content': 'inferential statistics or the background associated with having each term in the model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c00d665d5e697eb19279c6ba9a1320e3'}>, <Document: {'content': 'So, in this model, which I am just going to highlight with some colors, so it is clear to you in this model that you have out here you now, know, which terms could potentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7fa326df9aab2840a8a9771095b554e'}>, <Document: {'content': 'go and which terms could not or which terms you can justify putting in there in which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a2e52ac66cabfda0e62cda579b1f829'}>, <Document: {'content': 'terms might just be a result of random noise. Now, in addition to these individual statistic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef67118514aa89f070b722d4155920a6'}>, <Document: {'content': 'associated with each individual term you also have an overall statistics that is being captured ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6681f5461e911bff830673b230a95458'}>, <Document: {'content': 'through this ANOVA, so I just give that a different color. So, the idea with the ANOVA is that you are trying to say how much of the variation comes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '891f78059c2dd25c618e496dc6b8ceaa'}>, <Document: {'content': 'from the regression model and this is the this cell is equivalent of the mean square ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '65af53b4a8c534fc854b495664cff7eb'}>, <Document: {'content': 'between I am talking about d 12 and you have cell d 13, which is the idea of how much of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8fb8af6c745a68b6f8da9f114166fcc4'}>, <Document: {'content': 'the variation still exits even after you fitted the regression model and you use the same ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'afdca16e876eeb60119acaf50e2d7be2'}>, <Document: {'content': 'concepts. So, this these the these two terms are kind of like a mean square between and mean square error they are the equivalent from the ANOVA the traditional ANOVA that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1688cb9c9819962c23bb7df030131130'}>, <Document: {'content': 'we study and you can calculate and f statistics and get a p value for the f statistic and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '820b079211307f55cacb3e4f88675130'}>, <Document: {'content': 'what that p value says is over all as a model. Never mind that you have the statistics associated ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2e3e1ff3281f8809e8f791a45795cdd'}>, <Document: {'content': 'with individual terms, but overall as a model can I test the null hypothesis that the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2190f80bcd412e2f5e1ebdf93300678'}>, <Document: {'content': 'explains some variation or is the variation explain by the model equal to 0. So, it should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad637ccc23dc0f703f4a6b82e0505db4'}>, <Document: {'content': 'give you an idea of that and in many instances you are really looking to make sure that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4f367f53ce63116ec1b57fdaa0860e5'}>, <Document: {'content': 'value is as low as possible and you want here R square and adjusted R square values to be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6891b0ad3fd2e5004b8eacabcf040e64'}>, <Document: {'content': 'as high as possible, so great. So, we discuss how you can do a multiple regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c889f043877e16cdd877b3dbb1b55c94'}>, <Document: {'content': 'in excel, but at this point we are going to also go a little bit beyond a standard multiple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a06fb29738c43ea16cc4f082d0e89116'}>, <Document: {'content': 'regression the software kind of allows you to do it conveniently, but you could also ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd85707b56a88c24b884af0616e91c662'}>, <Document: {'content': 'for instance perform a multiple regression by hand. And the way you would do that and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b51d43e9eba0ebc2761d8cab0cb1ea3'}>, <Document: {'content': 'this is really useful is, because you might not want to do an ordinary least squares and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2b2774648971a9e88d80e4479f041f9'}>, <Document: {'content': 'that is what we are going to show you how you do not have to do ordinary least squares. Just to jog your memory the idea of ordinary least squares was to say that I want to minimize ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29992002229c7cdda9a935e3babe6000'}>, <Document: {'content': 'the sum of the square deviation of each point from the line as measured along the y axis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70d7240108216132a0e1a57b6475e186'}>, <Document: {'content': 'line. So, I want to measure the square of the distance of each point from the fitted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6dc7b0f52fa127fca4619d71e2d3a2c6'}>, <Document: {'content': 'line and the rational for using the square was of course, to say that, that you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd741337920db5ef81cb7b1a986f3f025'}>, <Document: {'content': 'sometimes have data point that is above of the lines sometimes have data points that is below the lines. So, you have positive and negative values ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '241e511233877dba91bb7d33d4d0a1ac'}>, <Document: {'content': 'and you want to represent all deviations in the this same light you do not want the positive ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79545aeab03f8a73c8a6dae6c730ae33'}>, <Document: {'content': 'and negative values to cancel each other you just want to minimize deviation. So, you said if I squared all those deviations and just minimize the sum of those squares your it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '230fba73e7e796af9314afdf592cf525'}>, <Document: {'content': 'is sum of the squares, because each data point deviates from the line by some amount. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f1c7fb9459158644bf3b50554fca7b2'}>, <Document: {'content': 'you want to minimize the sum of the squared deviation of each data point from the line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40cffc7c1743a235154dae3b04414708'}>, <Document: {'content': 'So, that is the core idea and, so now, what you might want to do is and you know doing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45a119b90c0dc90476c208d794223820'}>, <Document: {'content': 'that conception allowed for this very neat derivation; such that you had of close forms solutions for beta naught and beta 1 and so on and it is very easy to implement. But, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '81d4f77be79da8521973de83cabea6'}>, <Document: {'content': 'you know now, we have computers that are reasonably fast and you might not want to have these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7659336f1fcdf09db3db49c06e7452c'}>, <Document: {'content': 'neat closed form solution, but you might just be willing to take an excel sheet and do the regression through some other metric that you want to minimize. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5facc2878df34412aadd74e0cb3511b9'}>, <Document: {'content': 'So, instead of minimizing the square deviation of the data points from the line you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4559954d4dad09a8cdd012268417eb62'}>, <Document: {'content': 'just want to take the absolute deviation, what we mean by absolute deviation is if the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5615d016a025fbc30f8ae9a318be0458'}>, <Document: {'content': 'data point is 7 and if the fitted line at that value of x predict say 5. This difference ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa3e3fc97562c1741bdeea4e202a2cf6'}>, <Document: {'content': 'between 7 and 5 I am just going to take it as 2 I am not going to think of it is positive or negative. So, whether the data point itself is above the line or below the line I am just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a962d7ed6638203e1f2c90f2652b880b'}>, <Document: {'content': 'going to measure the magnitude of the deviation and put them all as positive values and minimize ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62c4e0d09965b3e359a3eb1918ecebc4'}>, <Document: {'content': 'the sum of those. So, whether you want to do that or whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a36a5535a98dfb5b8a81d15a6dbf95b8'}>, <Document: {'content': 'you want to say my particular problem I want to penalize it by using cubes instead of squares ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'baee09ef92d320bfd80add06c90a74fd'}>, <Document: {'content': 'or cubes would not really work, but I want to use the 4th power or something like that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '961763c0b5e40d0687785b6c31b911df'}>, <Document: {'content': 'you can do it yourself. And, so this is what I call as they do it yourself regression and the idea here is that I have copy pasted the same data out here in the sheet. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a990b5549c1664fbaf0e1e99f557de08'}>, <Document: {'content': 'And, what I am saying out here is that these are the coefficients, so this value at b 3 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23ed82d398da8e114080bfec482ba50f'}>, <Document: {'content': 'is beta naught this value its c 3 is beta 1 and we can just see those with some initial ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd7d5cfcfe34bfabaa7f7635d7de4842'}>, <Document: {'content': 'values I am just going to see that with 0, 0.5 you want to start with some reasonable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f84ae29b471a14e4b97e88d226708265'}>, <Document: {'content': 'values. And let us just put some dummy numbers out here and 0 here, so these just some dummy ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '76c1af82ac2c9ae00819be58e9bd768b'}>, <Document: {'content': 'numbers with reasonable range and the ideas when you do that I ask the excel sheet, what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2fb5492e7613910c218a88b36c5660c7'}>, <Document: {'content': 'is my model predicting. So, it is essentially like saying that if ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18a630a62e746071246eb1c4078ebf67'}>, <Document: {'content': 'these were the betas then what would my model predict for these inputs. So, if these were ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74ff6ddbbfe5dde1b86ff255d948a025'}>, <Document: {'content': 'my betas I am just going to repeat that if everything in row 3 from b through f my betas, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3448e165d2390458f645742f9953000f'}>, <Document: {'content': 'then given the input, which is starts at row 6 and goes on. But we will take one sample ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dab5c9e9085fe45465b4fbabba2e8f90'}>, <Document: {'content': 'input, which is shown in row 6 from c through f given these inputs, what would my module ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f207d4062a3e019f1928f98da75ffb1'}>, <Document: {'content': 'predict and the math of that is fairly simple it is just of the it is just the math of beta naught plus beta 1 x 1 plus beta 2 x 2. So, that is, what is I do say b 3, which is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '639ffa984d34dcde5070305d6927f612'}>, <Document: {'content': 'nothing but, beta naught plus the some product of these arrays in 3 and 6 and sense the word ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35553e8213236142f533bb765140e350'}>, <Document: {'content': 'you know some product is nothing but, saying you give me to arise and I am going pair vise ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca480293f3e523ea48f3bd79b8b5cd61'}>, <Document: {'content': 'multiply the terms and add them all together. So, what is doing is its doing b 3 plus c ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '47f2f62a36d0211bdf78f3d1b1037dc5'}>, <Document: {'content': '3 times c 6, d 3 plus d 3 times d 6 and so on all the way till the end and that is was ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '41b69b268293bb0674e7a166df79dea0'}>, <Document: {'content': 'the model is predicting. So, I have the actual value, which happens ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2579eb42c8f881301ee440b839c63dcf'}>, <Document: {'content': 'to be 21 and I have something that the module predicted and from that I can very easily calculate the residuals. So, what shown in i 6 is nothing but, the difference between ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd501b0969bade22689be15cef678cf5b'}>, <Document: {'content': 'the actual y and the predicted y. So, we do that and we than we could square it or we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ece7ac8563c3ad663a350e4a31e2135'}>, <Document: {'content': 'can take the absolute value, which is what I am interested in and, so I have a set of square deviation and I have a set of absolute deviations it looks like I am in general chronically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8294a3aeba9f8f52740216fc26f29841'}>, <Document: {'content': 'over predicting, but that is just because I have given dummy betas for now. And out here, what I have in cell o 3 is the sum of the absolutes the residuals sum of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cc0d4e1605d0f12e10c4dd1b446b886'}>, <Document: {'content': 'the absolutes and in o 2 I have the residual sum of squares. Now, remember in an ordinary ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '86e7fd69cbbed2ff5d5da0ad95876c02'}>, <Document: {'content': 'least squares regression what you have in o 2 is what you are trying to minimize by ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29cac3a84d86421ff662a4f00db5d911'}>, <Document: {'content': 'changing the values in b 3 through f 3. So, you want to change the values in b 3 through ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38549401178c962ae4f61af730c3ca54'}>, <Document: {'content': 'f 3 and minimize what you getting in o 2. But, in our particular example, what we want ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5460d730d0dd6bd46262a0f1a614205e'}>, <Document: {'content': 'I want to illustrate you today I want to try and minimize, what I am getting in o 3. So, the way I would that is to again go to data and click on solver again if you do not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d4c0ea6ec9128811e88df91bd64f04a'}>, <Document: {'content': 'have it in variable you need to add it from add ins and I tell solver saying I want to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4e421bbeca7121dce83e5570a89b575'}>, <Document: {'content': 'set this objective, which is o 3 I want to minimize it. So, I can either maximize it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2b63dcfbbee82f4852d030673c7220a'}>, <Document: {'content': 'or set try to set it to value, but I want to try and minimized and I want to do that by changing this cells in b 3 through f 3 and a given the kind of optimization approach ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7759c3995672c2a855337c1be386d724'}>, <Document: {'content': 'you taking you might have to either give some constraints, which just means giving it some lower bounds and upper bounds. So, give it something fairly reasonable, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4b02814a078a9079c7244f6e2171451'}>, <Document: {'content': 'is what I have already done here and the way you do that just go click on add and then mention this cell in give it to lower bound then give it to upper bound and that is what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '577fbbe09b2fb74d30bd77c1a42aa09c'}>, <Document: {'content': 'I have done here. And you haves a couple of different methods of solving this optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b94dc788d57ea2d706d21990efc3bb2f'}>, <Document: {'content': 'problem I choose the evolutionary one you can play around with the others, because the I just feel like it takes time, but its I know for a fact the there is safe at its not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '440d28a933dcbd735dc75b08ccfe05ce'}>, <Document: {'content': 'it is in terms of computational time it is; obviously, not going to be the most effective. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6f8ce55abb366b158f3e0abc6bde49f'}>, <Document: {'content': 'But, I am more than happy to pay that price this is not this particular example is not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5555451f718c7fc2b6eb8cdd1c78cce6'}>, <Document: {'content': 'too hard problem to solve and I just click on solve. And then, as you can see right now, excel is doing the computation, so you know it takes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e07783610a9b27d6624bc8dc4f18721'}>, <Document: {'content': 'couple of minutes and what we would do is we will come back to this excel sheet and look at the results in a minute. So, going back to the slide, so that, so in today’s ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c12cb3579ae56aa48e4f0fd70df6d7aa'}>, <Document: {'content': 'class we have seen two demos one is using excel to perform the multiple regression. And the second one is just going beyond the ordinary least squares, where you might have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8fc1de56d36c8e3d4abae510c4eb0aa6'}>, <Document: {'content': 'some other objective function that you want to minimize and you should be able to you know fairly is re do that in excel by just doing at the way have demonstrate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a42dee502aabee851ea4e5bd099847dc'}>, <Document: {'content': 'Now, we come to our next topic, which is the whole idea of subset selections and it goes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a377c11bc3c53d8acacd821b5f24e87'}>, <Document: {'content': 'back on I think we have the excel results may be we will just take look at that and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eab922f2b3187574b6985c7be60fc969'}>, <Document: {'content': 'it says you want a keep the solve a solution and I say yes and this is it. So, and if you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a10c5d52ab216ddbef66ec6bf861aa3e'}>, <Document: {'content': 'notice this is little different from the output that you got it is not the exact same output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e7bf89c0b4ea5edb31127cc7b1abad9'}>, <Document: {'content': 'you know perhaps if you let the solve go on longer it mate up come up to something may ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f240ee69b179f261a74852a9f231f97'}>, <Document: {'content': 'be more similar, may be more superior in either case. Because, these objective functions different you should expect different answers and this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd3276004c5b943509c0f25d3fa1ba286'}>, <Document: {'content': 'is the linear equation that solver gave you one if it wanted to minimize absolute deviation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45c272c6c1ae23cb8741226a47f234b'}>, <Document: {'content': 'So, going back to our slide we spoke about, how if you in the regression analysis you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bfb00964fa20d1ad6d7a23ed761bace'}>, <Document: {'content': 'saw, so; obviously, the do it yourself regression does not give you any inferential statistics it is just is way of getting coefficients. Now, there is the other important part, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6cbba47e88195806e172a98e2fbf0d31'}>, <Document: {'content': 'terms do you leave in the model, which you take out and when looking at that we look at this fairly simple idea of measuring each individual term through the p value of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c8afff6ff1afb940d84bcefeb0b11acd'}>, <Document: {'content': 't statistic. Just you jog your memory I am talking about this output and I am talking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64acabf191a0c9d86af0ba83d041e670'}>, <Document: {'content': 'about these p values that you see in column e from 17 through 21 and you could have an ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b74b6bcc78261187495d03956ff5f94'}>, <Document: {'content': 'idea saying anything below certain values acceptable anything above is certain value is acceptable and that that would be this most simplistic way of doing it. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6118d1d518aca3a2af1822ebdc4fef02'}>, <Document: {'content': 'But, the problem becomes that I mean there your using only inferential statistic to decide ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '674bfaa7c9ab896356bbda5ce2d4c6e'}>, <Document: {'content': 'which gets included and which does not. And the idea is that sometimes having certain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6aac1f9c6d7b0d217b9f9c9f223daf3d'}>, <Document: {'content': 'term inside, which might be statistically significant could affect other terms, so the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a9671e75f013a1f9608d59863788cf5'}>, <Document: {'content': 'other variables could also get affected. So, the problem is not I mean you could use inferential ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32f6ce87519dd6e405d8a099480b84ec'}>, <Document: {'content': 'statistics and make one time decision, but what happens if you choose to add a variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd964d09b1c7a78f6aa7d90c50f3d680f'}>, <Document: {'content': 'and that changes the p value of another variable. And also you might you might be of the opinion ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f5f809be66966bcbcba92d54c135d02'}>, <Document: {'content': 'that what you care most about is your prediction accuracy whether terms its significant or ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '580f71d7512efd61ceda6160ef7a98b9'}>, <Document: {'content': 'not that you care about getting the best prediction accuracy in which, case it is still possible ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e25676282c8c55ad9eac6e6f491457a0'}>, <Document: {'content': 'that adding certain terms is detrimental and removing them is might be the best. So, often ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '89b9730afc42c2f0bc11ecc7bde4040f'}>, <Document: {'content': 'times, what we wind up doing is we wind up having some metric that measures, how could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '960ff264d8b848a3a7ac472aaf46a9e1'}>, <Document: {'content': 'be performing as a model as a whole not as the individual term. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ed887bc60ddb7bb87c75d4446a2cf14'}>, <Document: {'content': 'And we already saw that we for instance solve this solve that in the ANOVA that we have, so this is measuring the model as a whole. And we might say all I care about is this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac442e6251b4c404b147a19392d83ed3'}>, <Document: {'content': 'p value that you see in f 12 that is all I care about. So, can I make a decision on which, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '561a8913cd8707a050af9de8d234e871'}>, <Document: {'content': 'terms should be in the model and which terms should not based on this one metric you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef8c7e426c3deef003016ecaac0ed91'}>, <Document: {'content': 'have another metric, which says that I want to look at adjusted R square we have not talked about that at, but will we will come back to that, but you could have this one metric ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '750e870098276b3599393c35400023bf'}>, <Document: {'content': 'that you want to minimize. And as we get more advance you look you will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f9fcaa7dbdc4d34a8465d4284defa53'}>, <Document: {'content': 'we will realize how there are many other criteria, which we could use to say, what is a good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '435a6a974af0d0bcb9ca4631bdf42624'}>, <Document: {'content': 'regression and what is a better and have a standard frame work to compare two different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '36a458e60efdbc2d3708bbb4c51c4340'}>, <Document: {'content': 'models. Now, assume that you have the standard frame work or for instance assume that you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8113cdaea82993bb068fa6f787b49e42'}>, <Document: {'content': 'care about f 12 you care about minimizing f 12, which is a p value associated with the f statistic associated with the whole model. Now, it is perfectly possible that if you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1de7aeb2c7512f4e5e6771b0df13e6e'}>, <Document: {'content': 'choose to add some terms that your overall model becomes better. And if you choose to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b56b1ef6c7c73e5963d7d59bd137aed0'}>, <Document: {'content': 'remove some terms of, so may be if I removed d as an input variable as a whole I might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd01477ce6e71f8e7ca51d5ce3993322e'}>, <Document: {'content': 'get a better p value or it is also possible that if I added another variable e it could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cdb633de9b5657a9118f2731f232c1ff'}>, <Document: {'content': 'be a lower p value. But, how do you go about searching through the entire possible set ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba27364ddd85ebbdc2da56be9f764d06'}>, <Document: {'content': 'of input variables to figure out, which ones should be in there in which, one should not. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6278bef41e02f3e18d4627f956bf7070'}>, <Document: {'content': 'And this becomes very important question especially in light of one what we have already discussed, which is prediction accuracy I mean. So, imagine a problem that you faced with, which has about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25460f84d64ca6b22bfc194a04dfbb2f'}>, <Document: {'content': '20 input variables and one output variable, which subset of the 20 and I am including ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c71ba601caf9de90e9e624147816e15f'}>, <Document: {'content': 'that set, which is all 20 and I am including that set, which is none of the 20 the null ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92c631f94256cc6b8de85292f9230d3f'}>, <Document: {'content': 'set, but which subset of that of those 20 variables should be there in the model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c8d1eb8755d334bc778ae7b25e9bb6b1'}>, <Document: {'content': 'So, as to give you the best performance, where performance is measured for instance to the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '924a4ecf09472bfe474d4f1f32aaaf49'}>, <Document: {'content': 'p value of the f statistics that is one side of a this the other side also, which is that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b241aa5dda42c1725d747a3a40dc8bbb'}>, <Document: {'content': 'if you had a huge set of variables sometimes the interpretation of the model might not be, so clear. Because, these variables might be related to each other and the coefficient ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35fccbfdf77792b2879c6e23d129b274'}>, <Document: {'content': 'that one of them takes up would compromise the coefficient that the other takes up and so on. And it would be much easier you know if you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f40fcef2249cc23f12cdccb80541ab4e'}>, <Document: {'content': 'could interpret the model with just the smaller set of variables at least the ones that are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b031859eb9235426aadd03fb99d90624'}>, <Document: {'content': 'the strongest and in some sense that you know enables you to get the big picture of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '920f550f585ec35c9c8a7d0f54847cf5'}>, <Document: {'content': 'entire process. So, what we are going to talk about now, is that process of choosing that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6dc896794ca0a937fc3ffa4167e0e29'}>, <Document: {'content': 'subset of variables, which is the best in terms of creating a model. The goal standard ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc732354abd797a6f40f3b41c787aeb0'}>, <Document: {'content': 'in doing that is this process called, let me just make that full screen the goal standard ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf40f9b98b63e25ff2e1e7593c353008'}>, <Document: {'content': 'of doing that is this process called best subsets regression best subsets regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c9bc1e57698587c3634288f04802bbf'}>, <Document: {'content': 'basically says. So, you have 20 variables or let us say I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '559ad9b0e8354877eddfc3aebfb49710'}>, <Document: {'content': 'had ten input variables let us try every subset of those 10 variables. So, you will start, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7039db4c068a698ac356a870c71cb948'}>, <Document: {'content': 'where having none of the variables or and the end you will move you will have 10 c 1 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eeafa9d0fd36cb8bd6a86b5073a20406'}>, <Document: {'content': 'combinations of having one variable, which essentially means having variable 1 variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '802ab163c91710f02317059530550cb4'}>, <Document: {'content': 'and that is one module having variable 2 that is one model. And then, we will move to the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9853673ba164befe4b2d55c8f7a37ecd'}>, <Document: {'content': 'twos the 10 c 2 combination of two input variables are in the model I have 1 and 2 in the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee9fb034f697d13a93d638247bb442ae'}>, <Document: {'content': 'I have 1 and 3 in the model I have 2 and 3 in the model I have 4 and 5. So, all the combination of two variables and then, you will have all the combinations of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa55d044453c591e8fd4ac98f1b7ef64'}>, <Document: {'content': 'threes and you will go all the way to having all the 10 variables in the model that is a lot. So, that is 10 c 0 plus 10 c 1 plus 10 c 2 plus 10 c 3 combinations, but you know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5de740f2e2465ca17a0926e24136436e'}>, <Document: {'content': 'what with good computational speed and reasonably handle able problem you might just be able ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc78c6c61e4c8f0bc256d6c3113235fe'}>, <Document: {'content': 'to use of brood for approach and look at every subset that is possible and your done with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cb0d1acc9d860a441bfd67c4f782008'}>, <Document: {'content': 'it. So, best subset regression of in its kind of seen as the goal standard of doing it and it allows you to you know evaluate every subset. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40c984f7639629b0057d56d5506b9b5d'}>, <Document: {'content': 'So, it is essentially like creating that many different regression you possibly cant manually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3bd1d4b5b844df938c62950c7288e02d'}>, <Document: {'content': 'do it in excel, but what you can do is you might be able to do it in another language ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dafdf21bae556f44a58adffc9793a40d'}>, <Document: {'content': 'like you might need a little bit of programming document we are able to do it in other software. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '63b4506e73fa50337a88430e974c746e'}>, <Document: {'content': 'But, essentially that whole process of doing tools data analysis choosing a regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d4f048461091c6cd9fc8ce426a55b96'}>, <Document: {'content': 'and getting that new output sheet you would do that for various possible combinations ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2da0f4836bf37d3ff104d3b447ff82f0'}>, <Document: {'content': 'of the input variables for every possible subset of the input variable and that is called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84b4997113eac545d8fb122be925d52e'}>, <Document: {'content': 'best subsets selection. Now, when that is not possible we tend to go towards the sequential ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20624a495c8caaf9e3f13c3e1345aa4c'}>, <Document: {'content': 'methods and the problem with that is that there is a certain impact of adding and removing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4350ee7e797b7208f1cd3b776db871d7'}>, <Document: {'content': 'variables on each other. So, I can never say that I am going to add this variable and its ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31d08278c0ecfab0ed086792ebc1fffb'}>, <Document: {'content': 'going to cause this effect. And then, choose to remove another variable at a certain different point of time, but the effect of adding or removing a variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eebac2d8b4db46acb4f21ff7022cc74b'}>, <Document: {'content': 'impacts other variables and for that reason no form of a sequential approach is mathematically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9264c8227df7eabae2b65e3b3e57eab3'}>, <Document: {'content': 'going to be exactly the same we able to always recreate a best subset selection. But, for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abc75b84a44760db33d6bf32d48eb99e'}>, <Document: {'content': 'the greater part these methods that are sequential just work are just find and in that light ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6436b64ea6f5145145b7db0d9606a4de'}>, <Document: {'content': 'I would say the more popular approaches are often called step wise regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f66724b97f15a9e96382033a44c97c1'}>, <Document: {'content': 'You have forward step wise selection, which means you kind of sequentially start adding individual variables to your model and you keep adding till a point, where the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d62c09e48eb344a36aa4a9b81a934c4'}>, <Document: {'content': 'does not improve any further in fact, the gets worst and any take a step back in say this was the best stay. You have back word step wise selection where you start with all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd472d92af79de1d84f8aaa9088b62b18'}>, <Document: {'content': '10 input variables in your model in a sequentially removing them. So, for instance one way to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3629f8f50b0894f1f6a68c082f7655f'}>, <Document: {'content': 'kind of show you how you could that in excel is to say. So, I had all A, B, C, D in the model now D is a worst I am going to remove that and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd02c39f04e0b2353a006e854811996e'}>, <Document: {'content': 'then, say does that improve my p value of the f statistic in f 12. So, it would literally be like repeating the data analysis going back to the data going back to data analysis ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '76c1d10513891bd96459b29f4eb1e399'}>, <Document: {'content': 'saying I want to do a regression, but this time please do not include D in my range, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd52ba4d6cd399b94dd6b909db75fad1'}>, <Document: {'content': 'so that is what I am going to do I am going to include the label. So, it is now, I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eb26dc214de2978d61e50ed2e12d01c3'}>, <Document: {'content': 'done that again and now I have a different analysis and low be whole this p value is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9dc708370e3915b7d9524c60c679148c'}>, <Document: {'content': 'much lower than the p value of the previous analysis. So, may be d should have been remove and to kind of do this kind of a process sequentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9fba93300be5f538794cf1487d4f7e'}>, <Document: {'content': 'removing variables and getting to the point, where removing another variable is not good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '833cfc6497d24054dd310873c4e1cfa'}>, <Document: {'content': 'thing and you could use for instance the p value of the individual variables to tell you in which order to remove the variables and so on. Now, again this is fine as long ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7a1b6d457e2810fd813eb0eb5917caa9'}>, <Document: {'content': 'as you have a reasonable number of variables if you have 50 an odd variables it becomes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '745e605e12e30f5da847581c8b6b41b2'}>, <Document: {'content': 'a very cumbersome. So, you could just essentially use software package to do the same thing and we will briefly demo that. But, before we do that you finally, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '86755073e6368d771dd77af44f45c224'}>, <Document: {'content': 'have something called high bridge step wise which essentially is a combination of forwards ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6a245f5f707cce0c254fc89c3a6071d'}>, <Document: {'content': 'and backwards step wise you start with nothing in your model in a sequentially keep adding terms. But, it each state you see whether which term to add next or which term to delete ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a13dd4236b5d6adc5552179bd91e1f89'}>, <Document: {'content': 'next and you put them on a common platter and make a decision on which to try. So, that is hybrid step wise and what we are going to do now, is demonstrate how you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe78e20ddfd5be3b06ba2fa8e3d8c293'}>, <Document: {'content': 'actually do hybrid step wise selection in MATLAB this is the MATLAB screen if you have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37793181a1318691c2c172b1ca2547cf'}>, <Document: {'content': 'not seen it this is the version R 2013b the path that I am selecting essentially is called a command window and that is a place, where you can just type instructions and get outputs ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '300f17b954018ec1b343925a72d415ef'}>, <Document: {'content': 'and we are primarily going to be dealing only with that space. So, what I have done is I already done in this, but I just show you what I did, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '482e0250c6fd83ce26ba907329184b2a'}>, <Document: {'content': 'is I just selected the data and I just copy pasted it in MATLAB. So, I just to copy this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2dff30d829ac4b22491798901210089'}>, <Document: {'content': 'I went to MATLAB and I said oh by the way this variable y is equal to open square bracket ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe3b143f6ad861ea14e9e9d1d9c97a59'}>, <Document: {'content': 'paste the data close square bracket enter and MATLAB is created a variable for me called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '48d4ea007abe9208d2ec403643d9bf60'}>, <Document: {'content': 'y with the data and it the same process for x and all I am going to do now is present ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c25239b4d7cc7f7b5303e9e1260171e8'}>, <Document: {'content': 'MATLAB the with the simple command call step wise and I will given x comma y as. If you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ef874e29652b155acbe694477835678'}>, <Document: {'content': 'noticed prompted me in terms of what do already put in there. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '631286c7d6feabf88a437c51658ea3cd'}>, <Document: {'content': 'But, you can also provide MATLAB saying I want some terms mandatory they need to be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee6b8fb78e9a706f6732ed438348a38d'}>, <Document: {'content': 'in the model and you can tell that and you can also give a criteria for entering and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '964785bae68b285ec5467cae728bee26'}>, <Document: {'content': 'removing terms. Just like for instance you could have used some criteria out here to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '887372ef2a94cd944b81ba98d47fc797'}>, <Document: {'content': 'look at these p value is to decide, which order to go with MATLAB you can essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87724fa00deb506c1905776018da9540'}>, <Document: {'content': 'say if its below certain range I wanted to go on and if its above a certain range I wanted to come out the default I believe this 0.05 and 0.1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98a2afc876ec1bb9fe69a1c599020a3e'}>, <Document: {'content': 'But, we are not going to give any of those terms we are going to keep it simple and just hit n and what happens is MATLAB look at this data and it essentially comes up with an interface ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f5d10a5cad9068a5f34ae61301e1325'}>, <Document: {'content': 'like this and anything that is red basically means is not in the model. So, now this is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e1943cbf14ccb073ca4d8281d7a029a'}>, <Document: {'content': 'nothing in the model and this is the intercept and there is no f statistic, because is nothing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e1130305520b371a356e8d5042dc858'}>, <Document: {'content': 'in the model and so on. But, MATLAB gives you a suggestion what to do a next let’s just move x 3 in. So, all you will do is you will go to x 3 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b9d466048b9d46776ab033dff765ea6b'}>, <Document: {'content': 'and you will click on it and its moved in and immediately you notice that you have a f statistic you have a p value in, then says please move x 1 in see you go to the red dot ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '49f395cf98eda558fe1ecb95265a734'}>, <Document: {'content': 'and click on x 1 and that is moved in it looks like the p value even going to lower then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9741c136cd64ed8cdbce771f9f4ee974'}>, <Document: {'content': 'it as x 2 in and you still doing better and now, it says do not move any more terms in. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf5937e5f6fc30a6a6c92eaeb9877879'}>, <Document: {'content': 'So, we went through a case, where the sequentially said add a add b add c and you know smartly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba3b3b1fd1116665315f384d61ae8eb'}>, <Document: {'content': 'it said do not add anything more and do not do anything more. But, you could see situation, where it will ask you to then add something in then, go ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbebc508ee808287937461a68b390735'}>, <Document: {'content': 'back and delete something, but it kind of not only does it give you the interface do it. If, so you could for instance add d and c was the performance how the performance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '111dd049c4737a3c1d9eb5e99dfc6f17'}>, <Document: {'content': 'changes and you can see performance becomes worse after adding d. So, it is 1.76 if I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a58fe663cffa1e68ab949361d60f46dd'}>, <Document: {'content': 'take the d out again it is the p value is even more. So, essentially MATLAB not just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'acb70230c64145ca693588ae34a65f91'}>, <Document: {'content': 'I mean it allows you to do whatever you want, but it also gives you a recommendation by telling you what the next steps are. And at some points if you say export it will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cd2a81ea907cba9a01e29e83e3fe6d2'}>, <Document: {'content': 'export the model for you, but you can also visually see it its basically saying this is the coefficient of x 1 a this is the coefficient of b this is the coefficient of c that that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '264b68afc6cb0a54106c33a016aeba73'}>, <Document: {'content': 'you can see under the column co f and it is also tell you what the intercept is it will tell you what the R square is and it will tell you the route means squared errors and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '853ec9771c1dda91e03c89037d7f89ca'}>, <Document: {'content': 'so on. So, it is the very convenient tool to clear around with if you really had like lost of variables and you just wanted to play around more than do like brute force approach ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ef97b3fe65607cb2596037ca3156b4a'}>, <Document: {'content': 'to visually see, what happens if you add one variable and remove a variable this could be very convenient great. So, with that we conclude our lecture on kind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '937c9c293b14d2df7220348706fb7a31'}>, <Document: {'content': 'of showing you how to perform a regression and interpreting results through software as well as tackling the problem of variable selection or subset selection. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85325407b4f3710a93869a1935399376'}>, <Document: {'content': 'Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '170f7be97195bd40737de367c3c69a78'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Regularization/ Coefficients Shrinkage ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '51529ecff76dc40659bf66e8b9594e5d'}>, <Document: {'content': 'Hello today we will continue our lecture series in the topic of regression and in our last ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3931dd0cb36ad28313574b9302863ae'}>, <Document: {'content': 'class, we primarily focused on, mainly ended, fine with the ideas behind subset selection. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '850119617646b9a9ddf173f0a9155fe9'}>, <Document: {'content': 'So, essentially we were talking about this problem, where you have a lot of input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c7ad904d49efc2135be15a457f954df'}>, <Document: {'content': 'in, that you could use in your multiple regression model, how do you go about picking, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e9c93d533a2fd45bf98dd3547061a52f'}>, <Document: {'content': 'one should stay in the model and which one should go out. And in discussing that we would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ca54952eb13c1a8fd1177bd29190e48'}>, <Document: {'content': 'discussing a more broader topic which is, how do we measure how good a model is and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33878e07c6e3bbc2e659caf47ce59d33'}>, <Document: {'content': 'how do we do things in our regression practice, so that we get good predictive accuracy and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a86a8b65ae981939b5a02c02ff092fe2'}>, <Document: {'content': 'it is also useful for interpretation. In that regard, we will continue our lecture ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de266dcf9a7d3bdeb35f7805102814f0'}>, <Document: {'content': 'today and talk about two other small measures, which is the R square and the adjusted R square. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7edfebc5c25336c502ccbd234851c119'}>, <Document: {'content': 'The R square has at least been introduced to you, you know you have an idea of where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '435f97b7b4fe54d56ade6a515bfbbbd6'}>, <Document: {'content': 'you can find that in your analysis. We will talk a little bit about that and we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e14ea62e3ae4afeaeec85e43690989f'}>, <Document: {'content': 'will talk about them more as other matrix of measuring, how good our regression is and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e7939dce4a6e8d94490043b3a391eec8'}>, <Document: {'content': 'then, we will move on to this topic called regularization, which has to do with how do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1df2bc00a90683209a56d9825e4cb147'}>, <Document: {'content': 'you fine tune the datas, the coefficients of your regression model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea79313ee3ea2a2d3b83586bdfb828fa'}>, <Document: {'content': 'When we were discussing the topic of subset selection, which is, how do you go about choosing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '121c8eeab7faeb56ddccffdd127f6227'}>, <Document: {'content': 'which variable should stay in the model. We said, we would go about making those choices ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cbb8e8a934780c9318829c182913dcf8'}>, <Document: {'content': 'based off of some metric, not of the individual variables, but of the model as a whole, correct ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3605bdc695b9e86586433918df9e564e'}>, <Document: {'content': 'and in that context, we primarily discuss the use of the p value from the f test that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bc95e4aceb4bab75bcbd504346bb58fe'}>, <Document: {'content': 'you do with your ANOVA. So, for instance I have copy pasted the same results from our ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '264b21a5adeae5f0827b48c976c74b39'}>, <Document: {'content': 'examples that we took in our last class and so, this stable out here kind of… ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6dc7e8becb99080fd810da20502732c1'}>, <Document: {'content': 'It is just a copy paste from that previous excel sheet and we were focusing on this p ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75c842919646e2f5d109e7e56da78d0f'}>, <Document: {'content': 'value primarily saying, this ANOVA is essentially a measure of how good the overall regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcca5cf62a77b8451b53e25590235bc4'}>, <Document: {'content': 'is, not individual input variables, but can we use that, can we use this p value as a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9e0214809dd2e2fe938dffa609edcb7'}>, <Document: {'content': 'guide to see which variable stay in and which variable stay out and that is the context ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2a1cc919019d9a7cdafcb17043ab6f3'}>, <Document: {'content': 'in which we discuss the topic of best subset regression, we spoke about backwards, forwards ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa0f61135d5d28adace0f88506276e9a'}>, <Document: {'content': 'and hybrid stepwise regression. But, today what we are going to talk about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8964784ef32c84705daa1d29dbacbf3'}>, <Document: {'content': 'is, we talk about two other measures which are also available in which is, something ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3b1b44fe20c59fc511dc2dcc3c49e326'}>, <Document: {'content': 'that you could use and they primarily the R square and adjusted R square. So, the first ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50c9c37f019071138918a79deb94801d'}>, <Document: {'content': 'thing that we are going to start by saying is the R square is not a very good measure, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '303002fa51b27c1a55b4b44c97bfa305'}>, <Document: {'content': 'in terms of how the overall regression model is, especially in a multiple regression context. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '11aace14f7bde6d33dd1a0539ca42529'}>, <Document: {'content': 'I will explain that in a second, what we mean by… ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bfc219c67599ed3f0ea46ffb644f42d4'}>, <Document: {'content': 'Again just as a reminder, you know simple regression just means there is one input variable, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f436a2ff9b84d60cf80db0f6fa7cc943'}>, <Document: {'content': 'multiple regression means there are multiple input variable. So, the R square essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b330dc11b875da33daeaf4065ab7d562'}>, <Document: {'content': 'measures, is a measure of how much of your variation and here, you are talking about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1ea9ab957e19f342d99ce90988993f57'}>, <Document: {'content': 'variations in terms of y, your output variable. So, how much if your output variables variation, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67481206fe438dafd6c46eebd2aa4437'}>, <Document: {'content': 'the sum amount of variation and how much of that can I explain using my model and how ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8af51ac62bd594867b921be407f861e1'}>, <Document: {'content': 'much of that can I not explain using my model. So, if I take the overall variation of y and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ec50d1178152d9720e647329dc0bddbf'}>, <Document: {'content': 'I break it up into two chunks, the amount that can be explained by my model, which is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f3f75e80647cf52f40baff9a28bfd6f8'}>, <Document: {'content': 'my regression equation and the amount that cannot be explained by that. If I put those ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b39ed33ca5cf7c575164c474b9deb50'}>, <Document: {'content': 'two together I should get the overall variation in y irrespective of x. So, R square is nothing, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31d7022bbfa8e3d3c8111418e7e36bcb'}>, <Document: {'content': 'but this ratio how much variation is explained by the model divided by the total variation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6441405cb21bc67854599ccdcfffa98f'}>, <Document: {'content': 'So, depending on the software that you use and the text book that you use, people usually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '798fc52163c13c3e7b45c3e3c4d9d54c'}>, <Document: {'content': 'have sum of squares total and that is the total variation and sum of squares model that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10c27538ae8e1057378ce130764ffae8'}>, <Document: {'content': 'is the model variation. But, I have also seen sum of squares regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f87e3233e92d57c6219fb26530942b4'}>, <Document: {'content': 'as a proxy for sum of squares model and the sum of squares… If you take, if you say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2795e52336f02619684ee275ea17ca'}>, <Document: {'content': 'the total variation is sum of squares total and the model variation is either sum of squares ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c5bed990065e47e04a67205929fe48f'}>, <Document: {'content': 'model or the sum of squares regression, then the only the other quantity that is left is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ac47e94b3d008d822404a4c00fbbcd7'}>, <Document: {'content': 'the sum of squares error or you know, as some people again call it the residual sum of squares. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2b9a20c9a9af2082b21045cbe45bb40'}>, <Document: {'content': 'You might have seen it RSS or sum of squares error. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64aa323ea30f1023bb6065c9bd26c634'}>, <Document: {'content': 'So, this, your R square value therefore, is nothing… You can just basically look at ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a0fa623c9c3394c9be9f9275fbf5e009'}>, <Document: {'content': 'your ANOVA output, even if your R square for some reason disappears. It is nothing but, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a56a2fc2f203505106d29c9219ecde85'}>, <Document: {'content': 'the ratio of this number to this number, in which case I think that looks like it is about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd04b48f78f2b563800d3712153802b50'}>, <Document: {'content': '0.8, 0.9 invalid. To give you a little more mathematical intuition and also kind of giving ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3dc56908510be45f18cd33b177d708d2'}>, <Document: {'content': 'you some kind of formulas that are used for calculating these three values. As I mentioned, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52d0fa68778509fe8f7bd4bfac347fd9'}>, <Document: {'content': 'the sum of squares total is just essentially the variability, in some sense the total squared ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b521f6541f51fb827a589b54b877593'}>, <Document: {'content': 'distance of each data point from the grand mean, whereas sum of squares model is where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '851a5ad1519d29a0e5ba2e786b5ef113'}>, <Document: {'content': 'you go to each, not each data point, but here y of i is each data point. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b29c0c7899a5cfe7b35217e1a6ce94c8'}>, <Document: {'content': 'We look at the deviation of each data point, actual data point from the grand average. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2142e7c22b748ca05ea51f621cd62607'}>, <Document: {'content': 'With sum of squares regression or sum of squares model, you look at each predicted value y ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe43461bad17d5f36d3c50141b34a4a1'}>, <Document: {'content': 'i hat. It is nothing but, what value of y are you predicting for each x and look at ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67f7528632bcbba36e3174a558f3dbeb'}>, <Document: {'content': 'the deviation of that from the grand mean and finally, y i minus y i hat is how much ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '25632b0cc0903548c3dbddeac5525b9f'}>, <Document: {'content': 'is each data point deviating from each predicted value. So, that should hopefully give you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a86d561d7c283e0aeaec93e49b68b6a5'}>, <Document: {'content': 'some intuition us to how these values are calculated. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '47252ce9c886c72a54656c7bbd5bb078'}>, <Document: {'content': 'So, now, going back to a bigger topic, as a metric to see how good a regression model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46c77a8be3880bd8ccadf08f7af61c2c'}>, <Document: {'content': 'is especially in a multiple regression, where there many variables, many input variables. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5d01075e1f4d4b0224879cdde9d0d04'}>, <Document: {'content': 'This R square is not great, because R square by definition will only increase as you keep ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f5609f9d78b788b01029076958215d0'}>, <Document: {'content': 'on adding more and more variables. So, we will be discussing this in great detail in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e027d88d142d5eb436beaf1dad15af8b'}>, <Document: {'content': 'our lectures on the bias variance dichotomy, which is going to come up, but the core idea ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5f0d9f85256ce5b6b5ea3e04327d378'}>, <Document: {'content': 'just to kind of give you some feel for what we are talking about is that, if you keep ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9b6c1d44fd41282cfb2a70e7c2a88105'}>, <Document: {'content': 'on adding variables and if you keep on adding complexity, at some point you should be able ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6aa664fad7ba044ff05e0b50643adfe'}>, <Document: {'content': 'to explain away all the data that you have or in another words, take the example where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bdb0dfa718b5aeb69934ae384fce5021'}>, <Document: {'content': 'you have 10 data points and you had only 10 data points. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd611d08dd50944590b55f0fa320d1f92'}>, <Document: {'content': 'You should technically be able to fit a line or essentially fit a function that will go ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '879f801cce728567a813f3c0fd993fb7'}>, <Document: {'content': 'through all these 10 data points. If you just choose, you know if you say I am willing to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7e38ae00c07364e30b079c345b31cad'}>, <Document: {'content': 'fit a 9th order polynomial, essentially if the fitted model can get more and more and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2cbc5d2a745fc08729d2e605559bdcf3'}>, <Document: {'content': 'more complex, then for a finite set of data point you should be able to explain the way ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1920cab45b591beba645e94672adfb5'}>, <Document: {'content': 'everything, but that is not necessarily great. Because, when you go and try and predict with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '742f202edd12b48b11466ecb754465e3'}>, <Document: {'content': 'that model, you are not going to do too well, you just do kind of over fitted to the data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21bcd55b19b56f829bece95824b213ea'}>, <Document: {'content': 'by just constantly increasing the complexity of the model that you are using. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7001eae7536487f5bd1a394a2105efa'}>, <Document: {'content': 'Now, given that is the base and given that we were talking about this idea loosely when ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d6ff64cdcb031c2366fd50d14134aff'}>, <Document: {'content': 'we in our previous lectures spoke about, how it is really important to try and figure out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '494c25ef0ee87fac2ef2a4f3e691eac'}>, <Document: {'content': 'which of the subset of the variables you want in the model and you want to throw the others ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b2550df3153486d0afc049432dfb6df8'}>, <Document: {'content': 'away. This R square does not help, because R square will never decrease as you keep on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '76e998a9cde66e9aecc29ec6c0bd5118'}>, <Document: {'content': 'adding more input variable. So, let say I had a model with 5 input variables, I suddenly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9cb7406606634febf43c188da921b43'}>, <Document: {'content': 'come up and say, maybe I should have the 6th variable and add it, it is almost it is definite ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f1c9a690f6e4c74e353aee84f68fe18'}>, <Document: {'content': 'that R square will increase and so, R square itself is not a great measure of how good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fafde55df056d5f4f067be0b73480368'}>, <Document: {'content': 'a model is. Now, as we discussed you could definitely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9adf8e5448e7c28b153e7656f6366b10'}>, <Document: {'content': 'use the p value from the f test of the NOVA, but another metric that is also popular is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b0e39c2861ebb793b930870021b87e88'}>, <Document: {'content': 'called the adjusted R square and that is essentially nothing but, a modified version of R square ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd21c7140be5102e85c199007a99f612e'}>, <Document: {'content': 'that is shown in the formula here. So, it uses essentially to compute it, you can use ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '758572e0cb6d144ab33868debcbdbfba'}>, <Document: {'content': 'your R square values and the idea is that n is nothing but, the number of data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '876acd98015342752585532408c963d7'}>, <Document: {'content': 'and k is the number of independent variables that are there. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '917a938822fe60ceee0844ba760ddf84'}>, <Document: {'content': 'So, for instance in our example, I believe we had 88 data points in the excel example ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4b0a169bb265265c63cedfd43aea4f39'}>, <Document: {'content': 'that we were discussing yesterday. It is 88, I can infer that also from the total degrees ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e45af26b7f636bdb51819e9e228bd3a8'}>, <Document: {'content': 'of freedom and k essentially is 4, because we had four input variables. See, you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe60de6576767706101afecc878d9301'}>, <Document: {'content': 'use k, the k will be equal to 4, so 88 and 4. So, the idea out here is that, when you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e008284fa4bcc77c05af3b306124c9c6'}>, <Document: {'content': 'use the adjusted R square, it kind of penalizes modules, where the number of variables you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85f240d2a6aa2483832b46b0c1a3f075'}>, <Document: {'content': 'are using to explain it is too high. So, you can compare the adjusted R square ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c9c89c0b85d52ce65638463f9777975'}>, <Document: {'content': 'of one model versus the other, where you used different numbers of input variables and yes, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6fc9f0a3d95641be222b6233889f6a62'}>, <Document: {'content': 'the higher your R square the better it is. So, if you can get a very higher R square, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac2d653f26e951d32baf2937dcb44cfe'}>, <Document: {'content': 'you are going to get a, you know that is one way of getting a higher adjusted R square, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd0e0f31cb3dfdbcca7970f0ca6125582'}>, <Document: {'content': 'but not at the cost of, you know having too many variables. You want to keep k as small ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc334862cab3d4644f14f14230679e70'}>, <Document: {'content': 'as possible and have a very high R square. Notice that, there is a minus in front of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d50cc4c1f921f2526bf05393e33765e'}>, <Document: {'content': 'the k, but this whole thing also has a 1 minus, so just be careful when you are trying to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4957cf4e1d0aae9974e58b98bea21fda'}>, <Document: {'content': 'get an intuition of, how increasing or decreasing these values is going to effect the R square, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c7f8cd39bb5e94cff1e129202cd97cc'}>, <Document: {'content': 'adjusted R square. So, that is more just, again introducing you to the concept that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e2e6f7c4a8b24a40ecf65790f04cf75'}>, <Document: {'content': 'there is, it is not the p value of the f statistic as we discussed in the best subset selection ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4ea6e7aedcd76e8aba559845d080e53'}>, <Document: {'content': 'and stepwise regression, but this is the concept of adjusted R square as well and that is just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d763d6a1869597059cfb931998b91e1'}>, <Document: {'content': 'one of the them, there are other metrics also that can be used to measure regressions, a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42a4034da581f15c5dc4e0cce854575d'}>, <Document: {'content': 'regression model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9fc409ef4ad6a307118b16caedf96e09'}>, <Document: {'content': 'We now go on to the topic of regularization and you can also use the term coefficient ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3834864d5b031fe0319b3a292dcf3440'}>, <Document: {'content': 'shrinkage to express the same concept. What regularization does is also, you know as a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58685754f3c544633444c257e88eb9f6'}>, <Document: {'content': 'huge over lap what you try to do with sub set selection, which is the core idea being ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d233a761cb2a9a30729291d7fe64a1d'}>, <Document: {'content': 'how can I simplify my model in some sense. Because, I care about predictive accuracy, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c76903d7c6ed3710e8522d8a0e6b8c08'}>, <Document: {'content': 'I care about interpretation, I do not care about just trying to get this function to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ec841cd4a2ea69cda63fdac3674cae6f'}>, <Document: {'content': 'go though all my data points. So, how can I simplify the model in some way ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '631970a3a7999a315c2b0d802b0d5e20'}>, <Document: {'content': 'and one way of simplifying the model is to use fewer variables, that is what we saw in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '424c61c4574118ff54ed7510fae17736'}>, <Document: {'content': 'subset selection. But, once you fix the numbers of variables, once you say I have fixed these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79cef0b1a31109516764bcb0ac0d6dae'}>, <Document: {'content': 'are the variables that need to go in the module, is there some way to more smoothly determine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94a88d21fd007ae30b8b50f12f34c45d'}>, <Document: {'content': 'the coefficients. We saw how the coefficients were being determined through an optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a92fbcb2f111e465d88ab67e9b2e1995'}>, <Document: {'content': 'process, but somewhere can I go directly in there and put in my constraint of saying, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16baa3f62cee8cf3e3aa9db264e629d4'}>, <Document: {'content': 'I do want you to optimize something, but at the same time I want you to trying keep it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7dcd73dab4f8dd707cfe27e72d076b14'}>, <Document: {'content': 'as simple as possible without kind of over fitting the data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf2466dc22167159d6b3f619a051ce6d'}>, <Document: {'content': 'So, in some sense the problem of regularization is a problem of saying I have an algorithm ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '81457637dae92dc6648a52ce16cd32f2'}>, <Document: {'content': 'and I have a fixed number of input variables. So, I do not, I am no longer bargaining to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9b6c74e5bee2f1568d8dad8e41bce93'}>, <Document: {'content': 'throw variables out and keep them in, but is there some way of in my fitting methodology ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9df6c7a98691e1f34c97377a281094ce'}>, <Document: {'content': 'itself preventing this problem of over fitting or preventing this problem of trying to get ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2ebaf17e72a002c0d9905410b4ff5a0'}>, <Document: {'content': 'the functional form to be so hardwired to the data, that it does not do such a good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c070cdd5df6f297948d0966fbdbb165'}>, <Document: {'content': 'job of predicting and kind of, can I impose penalties upon myself to kind of over fitting. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f84249a77cd7b05fdfc52c14a0fae9'}>, <Document: {'content': 'One way in which over fitting happens is, in problems where the problems of what we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a2855c7bab126225bd25b0e3ccf40373'}>, <Document: {'content': 'call as multicollinearity. Multicollinearity is the idea that many of my input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2b6c38ea0563e39705af77422505f43b'}>, <Document: {'content': 'are correlated with each other and when you have that problem, the coefficients themselves ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '44c6c07e9b971f568bcbc48b645a2172'}>, <Document: {'content': 'through a regular regression process can get, the determining their betas can be kind of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '736701026d2feec79f1e4542ed99645e'}>, <Document: {'content': 'poor. Meaning that, there can be a lot of variance between one samples to the next. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b0a78b6bb01f62fcb0de00af24e71869'}>, <Document: {'content': 'So, it is like, ideally if I am doing a certain regression process and I take a sample of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4c718e5d28ba0b5c9db4874976e1b73'}>, <Document: {'content': 'data and I then do this regression fit. If I take other sample of data and I get completely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18613fe91e0dfb6ba9e42ca70e1a2f8e'}>, <Document: {'content': 'different set of betas, then that is not a very stable process and that tends to happen ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5aa2d031246edf7e92f752869293366c'}>, <Document: {'content': 'with least squares regression, when you have multicollinearity, meaning you have lots of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f538abcb6d4564083a642804338d6ee'}>, <Document: {'content': 'highly correlated input variables. I mean take this example that we have on the slide, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '737ade864912f8c844687310690cf605'}>, <Document: {'content': 'you have this idea, where you get this equation which says y is equal to 4 A plus 2 B. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a41524fecc5ebd9d929f7052c7ac2d82'}>, <Document: {'content': 'Now, imagine a word in which A and B was so highly correlated to the point, where they ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c5a9d1f06ea03ba6943935b3fc2d01f'}>, <Document: {'content': 'were practically the same variable. Then, a fitted model that says y is equal to 10 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c83161a8cea8baedeb4efbaa490182ea'}>, <Document: {'content': 'A minus 8 B should also a kind of give you the same results, in the sense that A and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '974e55182d183a2a592cb05d36f4b3c4'}>, <Document: {'content': 'B are practically the same data points. They correlated to 1 and now assume that they are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6420c4bed8b17cd22f0233c5b309291d'}>, <Document: {'content': 'also equal in magnitude, the 4 A and 2 B, you can substitute B with A and you net getting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fce04dece4231f3aa5cb0e43cdc61d1'}>, <Document: {'content': '2 A, which is the same net you are getting in the second equations. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f462862138a393b5b131c467e421bd7'}>, <Document: {'content': 'So, our both equations the same, now the idea with something like ridge regression is, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3446656dd0aa56283bf3341e6731ba3f'}>, <Document: {'content': 'in cases like this, you want to have the simplest possible equation and you want to have the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92cd8e40f4342db920605789137a8f65'}>, <Document: {'content': 'lowest possible magnitudes for your variables. So, you would be very happy with the y is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a9efe84c0d47b6d17204a2e69a5d8f3a'}>, <Document: {'content': 'equal to 2 A, I am just keeping it as simple as that or you can take it as 2 A minus 0 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '700ae8c0e934507d8ad1c6d1c94a63b6'}>, <Document: {'content': 'B. So, in addition to choosing which variables, you want to keep their magnitudes, the magnitude ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a387cf7c8b5775db7dafab39ea092f88'}>, <Document: {'content': 'of the coefficients, you want to keep them as low as possible. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '870aae8a76c931a831fb24970bd6382'}>, <Document: {'content': 'So, if you have off setting coefficients, you are not making a regression equation that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e1a1c21b4fe94081488d0f583eb213d0'}>, <Document: {'content': 'says y is equal to 10,000 and 2 A and you know, you know really large say 1000 and 2 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed1e67f37928bbd041fc0e8bc16ab8a5'}>, <Document: {'content': 'A minus 1000 and B. Your kind of blowing things out of proposition, it will not generalize ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7e5a6c417ea557805854f0b14f5a6f3'}>, <Document: {'content': 'very well and so on and so forth. So, how can we achieve this? How can we achieve this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a89add81e1ac4cdd6046a81ee8a09720'}>, <Document: {'content': 'goal of and really simple terms, not just allowing highly correlated variables to take ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8fb5bb3053b3253a21e9b2c5e3cf68a'}>, <Document: {'content': 'up opposing sides and you know, have really large coefficients. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9ff14a11af634c5dd2eb0fbe3aa3c48'}>, <Document: {'content': 'And the way we will do that is, by taking a standard least squares minimization problem ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1ec80f76ee55d2147b1b5aeecc0a59f'}>, <Document: {'content': 'and that is essentially what have written as the objective function here. So, the objective ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45da17e59732afefa4d70275de3ac380'}>, <Document: {'content': 'function here is no different for the ridge regression. At least the way have written ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f1319e484c5c7a0a79cf5855cee8173a'}>, <Document: {'content': 'it out here is no different than for least squares minimization, because all I am saying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df3b1aaf0db178af5281276369d665b4'}>, <Document: {'content': 'is let us minimize for each data point, the actual y minus the predicted y and the predicted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c755b3d79061069a56943d4fa16abf4e'}>, <Document: {'content': 'y is nothing but, beta not plus beta 1 x 1 plus beta 2 x 2 plus beta 3 x 3, goes on. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4bb0177a239a239dbf15b7bdd863b53'}>, <Document: {'content': 'So, essentially j goes from 1 to p, meaning p is the total number of independent variables, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '349b3a83e265909fb1044fefd1d801b3'}>, <Document: {'content': 'n is the total number of data points, so you do it for each data point. You look at the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '357bca0080af93cf3ed8c16418d43fb8'}>, <Document: {'content': 'deviation of the fitted value, the actual value to the estimate or the predicted value ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb6d9597e15695d639b53eaeb7f20162'}>, <Document: {'content': 'and the goal is, this deviation is what needs to be minimized. The square of this deviation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77852909af14dd80018c5b56d3c73abf'}>, <Document: {'content': 'is to be minimized in an ordinary least square regression, but in addition to that and what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ab7f8510761d433aa93eb05b9f5dc58'}>, <Document: {'content': 'we do with the ridge regression is, we do this optimization with a constraint and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68e736c7d686f2eb4c689bea36ca1947'}>, <Document: {'content': 'constraint can be written like this. The constraint said it is the subject to some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37909e7da7884a60548ece9d2f62cbbe'}>, <Document: {'content': 'beta, you take each beta which is each coefficients, square it and the sum of those square should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b98c03857c70da030657cf9a90662ce3'}>, <Document: {'content': 'be less than sum value s and that value s is essentially, it is not like you have particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10b0f4f3a6bb453ffaf1386e62a92d49'}>, <Document: {'content': 'number in mind. What winds up happening is that you can rewrite this optimization by ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62f782fc0dbdaed0b222619631463c5'}>, <Document: {'content': 'just not having this constraint, but essentially out here, just adding minus lambda and summation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75bdc447e20c915824523370eaaaff5a'}>, <Document: {'content': 'of beta j square. So, this is kind of like, you might have come ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7dca3cafd8c20e4c6a10afeb985ad631'}>, <Document: {'content': 'across this with like Lagrange multipliers, when you have a single constraint on the coefficients. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '433fbb7dd19d30721e198e89119cc14d'}>, <Document: {'content': 'You can just rewrite the objective function to have that constraint integrated in to the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1157f347457af5d0964e6ab3f1fdd13'}>, <Document: {'content': 'thing and essentially, it is like solving just the minimization without the constraint. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba83ff995661435822421d23aad55528'}>, <Document: {'content': 'But, essentially the solution to that is what will ensure that your data’s themselves ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5de0cf9b5080759c85f72b50edd1bd36'}>, <Document: {'content': 'are stable and you know, not taking really large values. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1003df129fd1dec6e5d4039f10a20f7d'}>, <Document: {'content': 'Another way to achieve the same thing is, again even with the lasso regression, this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e98b4326817d8e2ea94d3c206b4cd88b'}>, <Document: {'content': 'should not be ridge, this should be lasso. So, even with the lasso regression what you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d7661cf5c5b3ce82c4ef63fa0160926'}>, <Document: {'content': 'see is the same thing, you have the same objective function, but now it subjective to the constraint ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f3e340ab6b66e2b5b14bfc8854b5828'}>, <Document: {'content': 'that the sum of the betas is less than some value s and again, out here you can just wind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9840953b9bdf9269cc399ffd59c84aba'}>, <Document: {'content': 'up rewriting your objective function. But, out here rewriting it does not do you too ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b89f9ef774adbcfa3075b5334d26b616'}>, <Document: {'content': 'much good, because you cannot do the same trick as with standard calculus with Lagrange ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c197da4fe87b4fa9adf661e424c185'}>, <Document: {'content': 'multipliers, where you have a beta square. When you have a mod beta, it just becomes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8dc45be59174d9f9f3880aed2a6912af'}>, <Document: {'content': 'computationally harder, but you know just like we saw in the simple regression case. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '222dd4766647dc09e952a69f604a44f2'}>, <Document: {'content': 'If you got an excel sheet or a MATLAB, you can just do the optimization and put the sign ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '121c2285231c3f802ecfed09ab292ff7'}>, <Document: {'content': 'as a constraint and as long as your optimization technique is good enough, you should be fine, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18660c7ddf931d32fe412c70da2ca03f'}>, <Document: {'content': 'you should be able to redo it. I hope that gives you some idea of regularization techniques ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4e0fdce070ae229118d20d4a03e21fb'}>, <Document: {'content': 'and look forward to see you in the next class. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8b0e68019e552133fc6509c39f9bd17'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Data Modelling and Algorithmic Modelling Approaches ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b93e1cd003106c661774249f3da5a4df'}>, <Document: {'content': 'Hello and welcome to this lecture on K-Nearest Neighbors Techniques. So, in this lecture ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5cfc251ebec62767b48650d744e07a80'}>, <Document: {'content': 'we will introduce a new supervised learning approach called K Nearest Neighbors and a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a174ed2b9ec94f1232c9405eff74446b'}>, <Document: {'content': 'broader thing that we are trying do to with this lecture is by introducing this technique ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c6c385e6a32935b5f5099e6125aff85c'}>, <Document: {'content': 'and comparing it to something that you are already familiar with, which is regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2307cea609e11a0ad9695a3432160a47'}>, <Document: {'content': 'analysis. By doing that comparison, we are hoping to kind of illustrateÊand help you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '81dc8b3d5c61b68388cfc18ddaf75277'}>, <Document: {'content': 'appreciate two very different styles of performing the supervised learning analysis and you know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3a8ec2bcfebd181999cf433cd013f2e'}>, <Document: {'content': 'and therefore, the process of prediction itself. Two very different approaches, two predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4bf7cc41bf1f56e3bb713f66298b1a0f'}>, <Document: {'content': 'and it is a kind of important to know that, because up until now you heard of what supervised ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6dfb121ba0af0a2b58d5231b2eeca609'}>, <Document: {'content': 'learning is in theory and the first technique that we gone into and explained is regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5fea5c6780b98ec8844c42bd015be8b'}>, <Document: {'content': 'which is the certain way of for instance performing a prediction. The hope is by introducing k ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67e8105379d4e2c7eb185645aa57c5f8'}>, <Document: {'content': 'nearest neighbors, you see a very different way of achieving the same goal and it is important, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '379cd28ec7f42d5cd03cbd0c6f96c11a'}>, <Document: {'content': 'because you will realize that a lot of other machine learning techniques, take inspiration ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6fb80a4f9cd5b1ee78df8a0308a52df3'}>, <Document: {'content': 'from this approach. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8622c770099ac51565da5e3d2f2bb3c9'}>, <Document: {'content': 'So, this dichotomy if you will was something that was first pointed out by Leo Breiman, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5009e2a14a8a900345b6b4e0d0f72799'}>, <Document: {'content': 'a very famous statistician, where he said there are two very prominent cultures to statistical ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa9a98002acb5026e0ff08d9e5bd3b42'}>, <Document: {'content': 'modeling and out here, he primarily talking about supervised learning approaches and he ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c219e0d6f0d7c3e7846d3b7d30c72e5b'}>, <Document: {'content': 'highlights and you know, there is some amount of overlaps and so on. And I would not really ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7dd86c213549597d786eaa7401ac214c'}>, <Document: {'content': 'say these two cultures and two different, it is not like a classification as much as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '391b9833cf690763c8c5612afce748e9'}>, <Document: {'content': 'two very different styles. And the idea is that, he says look there is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34451c81dd28e928f72b2696d2620ff'}>, <Document: {'content': 'the data modeling approach, which is what you for instance in the standard regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ab1ca63d0c4ee31d44347db7d36bd856'}>, <Document: {'content': 'where you have some you know output variable y and you kind of envision this output variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '71d47580b8d5aa5c103b276263ecaef'}>, <Document: {'content': 'has some function of your input variable or variables x. So, for instance you would say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '323bddff9dfdb1972c55bb42eac4ffe7'}>, <Document: {'content': 'y is equal to some, you know f of x and plus there is of course, some noise. In the case ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '351bda2106c449b6c1413c15660832ef'}>, <Document: {'content': 'of linear regression and here a kind of shown you an examples of multiple regression, this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '842a52b4b721b09e3dc73d631fe1b61a'}>, <Document: {'content': 'f of x becomes fairly straight forward, it some intercept plus beta 1 x 1, beta 2 x 2 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f33a4dbe11c71da57301c5cc544d3754'}>, <Document: {'content': 'for how many ever input variables you have. Suppose, you just have one input variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f25342dee258c0ac8dfe0032aac386b2'}>, <Document: {'content': 'that would be beta naught plus beta 1 x 1, it is fairly straight forward. So, the point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '321adca2a5eae227a577616ef75b13c4'}>, <Document: {'content': 'he makes and it is generalization, because he says us about many other statistical methods ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10f5a3853b9905e66436ef384b1d56b9'}>, <Document: {'content': 'which he says, you have these whole breed of approaches to supervised learning which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a0d59ca945c242e48da183f7b8089c1'}>, <Document: {'content': 'capture a functional relationship between y and x and that function is in some sense ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '281bc19e9bbc8d0b1d83c61e9dbfac5'}>, <Document: {'content': 'cast in stone and the only job you are left with is just go, get the data and figure out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69303136e7c733c6c4bc8af707cbfa68'}>, <Document: {'content': 'what the parameters are of the function. These betas, the job is to take some data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac3e2aaf8811aa15ea0b7bf5e8fd5261'}>, <Document: {'content': 'and figure out the betas, this functional form itself which is said in this case of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f3a4c347542eb1c0f275c8e375428565'}>, <Document: {'content': 'the linear regression, that it is a linear combination of the different inputs plus some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5157b6c4d2e4cf405a71c414f784c678'}>, <Document: {'content': 'Gaussian noise that is for instance cast and stone. And he says that you have those approaches ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a4b262052957e72e3fc81fa77a7ba7a'}>, <Document: {'content': 'and then, you have an alternative breed of approaches and he calls them the algorithmic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7369392371f328f5264bdbffd4cce15'}>, <Document: {'content': 'modeling culture. So, the first one he calls is the data modeling culture, which is your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5871a025c3a251563f07a98078b72b8'}>, <Document: {'content': 'and the best examples of the data modeling culture could be is the multiple regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40e9836da56535927a363d892f4bbd75'}>, <Document: {'content': 'So, we covered that in good detail in the previous lectures. So, he says now look there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '399e45f23b403102cc7e237dddfa5868'}>, <Document: {'content': 'is another approach, another approach is what he calls the algorithmic modeling and there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a02e852a144398906f567d6594df116'}>, <Document: {'content': 'he says essentially, the focus is not as much on a rigid mathematical model that you have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8ff2ee3dc06bb6436b4b743d8e66435'}>, <Document: {'content': 'presupposed that have you apriori and that creates it is relationship between y and x, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2dbb612f10954405e0b8e8b382a93a85'}>, <Document: {'content': 'which is already cast and stone. But, instead you really have a set of algorithmic instructions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e9ddceaed23f71f004dfa5957bdb540'}>, <Document: {'content': 'or algorithmic ideas that relate the independent and dependent variables. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a0847769f6b123e400674ca2fe0a3a9a'}>, <Document: {'content': 'So, y is in the loose sense of the word of function of x, but that is really captured ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b37b1b666c46633bf7d0d3cd0523424'}>, <Document: {'content': 'by algorithms rather than rigid mathematical models. Now, these two, there is a reason ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa722a5adb3b7b10348d577727392c0b'}>, <Document: {'content': 'why in the start I said these are not like two classifications, but there are more two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69e3a9c9de524eb7ff8ab54741dae5a9'}>, <Document: {'content': 'styles only because you can describe any algorithm in a mathematical form and perhaps you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd202b1ad1997bf529f12de1c617f96a'}>, <Document: {'content': 'describe the math in an algorithmic form. But, what I am going do is I am going to explain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '91da0e2efdde790f4e0f9569f52d7ee6'}>, <Document: {'content': 'to you the k nearest neighbors approach and we are going to talk about the k nearest neighbors ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b16e6e3a0e16a3da446b94a4b675370'}>, <Document: {'content': 'approach as an example of the algorithmic modeling and hopefully at that point, it becomes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '867383cab48ee1488197a730faf68145'}>, <Document: {'content': 'really clear us to what the stylistic difference is between these two approaches. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e05cb0403d0b7a584754f66e75f032f'}>, <Document: {'content': 'So, let us look at how a prediction task happens with your linear regression approach. The ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd407e417367e46bf5cdd5eb216f03965'}>, <Document: {'content': 'way it happens is you have some data and right now I am focusing on the graph on the left ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8d45edeae2ae2f94efe8635f3777ffb'}>, <Document: {'content': 'hand side of the screen, you have some data, you do a regression analysis and what is the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c4e7f3238eaf5d0e9b913ca34ff8247'}>, <Document: {'content': 'regression analysis do, it fix the line through the data, it fix a line and if you are using ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c03094997d73756b5be97ff60ae13dc7'}>, <Document: {'content': 'ordinary least square regression, if it is a line that minimizes the square deviation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f3502f44a57e846a42c562caf79b9e'}>, <Document: {'content': 'between the data points to the line and so it chooses the line that achieves this target. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9b018c82669da353dff2ae32bae88d73'}>, <Document: {'content': 'So, you have you fit this line. Now, what you do when you need to make a prediction? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c9cf6b9e62f81f2da26e94b4c96dd54'}>, <Document: {'content': 'And someone comes along and says, oh great, so you done a regression analysis. Could you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e570807fc8200ef2c90ca6df8e6cbdb'}>, <Document: {'content': 'tell me what, y I should expect for a given x? So, they come and give you an x, so they ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f67a5745167845267fbd6aa4dd380aa6'}>, <Document: {'content': 'give you this x and let us call it x, let us just call it x 1. So, they give you an ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b90972c277ba5c204face5f36eb9c7bb'}>, <Document: {'content': 'x 1 and say can you tell me what why I should expect and what you do is you draw this straight ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8240350db8894d53a40d89ee60a24b5e'}>, <Document: {'content': 'line up like it is already there and you say, let me see where my what value of y I would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5bee1848b34285e8272706091e9927fc'}>, <Document: {'content': 'get based on my fitted model. So, I basically take this x value, draw a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd0533c1fea0fd948d39220dc2b97ce1e'}>, <Document: {'content': 'line up to my regression fitted line and then see what the height of that point is. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af71138a1c2597d42cf109b6205ce1f0'}>, <Document: {'content': 'this is my predicted y you know, so may be y hat of 1. So, if somebody comes and gives ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '386d8fa2c3c2a81a5df9775eb92010d8'}>, <Document: {'content': 'me this x all I do, I do not really I have used all of this data to create a line, but ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2a7f71dea2d1ec2692306a7e82b3b54'}>, <Document: {'content': 'then after that I can lose these data points, this data points can be erased from my memory. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98c8aecbafe80021b16c038195d3ef80'}>, <Document: {'content': 'All I need to do to make a prediction at this point is I need to know this line and this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a66721fa98bf57fac224581e1b483c1'}>, <Document: {'content': 'line I have already created with the regression. So, I have created this line I have it my ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3aa21b28acfb0c8dc855b980eab8a9e'}>, <Document: {'content': 'regression and someone comes and ask me a question is to what y would you see with particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2494110a4263af7aacb880f1d4d781e8'}>, <Document: {'content': 'x, as you know this line has a form y is equal to b naught plus b 1 x. So, someone comes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ab5e1efc95f95b4f14ff295e7c2d308'}>, <Document: {'content': 'and gives me a particular x. So, call it x 1 I will just substitute this x 1 out here, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66c25cf1d481575d909cdd0aeeef9532'}>, <Document: {'content': 'I know the values of b 1 and beta naught from the regression, that is how I was able to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd89ed4ff33fa1f04a032ae4ec5bc1d82'}>, <Document: {'content': 'plot the line. I will then just get a y, because I know all the three terms on the right hand ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ba2116f6af2dc6f83764e6112abb8d2'}>, <Document: {'content': 'side. So, I will get a predicted y and that is my ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4dbc741888095bc8cd0afbe52c5d9e6a'}>, <Document: {'content': 'prediction of what why I will see for a given x. So, that is how a prediction task takes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e451df45acf0f0d52740697c21197baa'}>, <Document: {'content': 'place of the regression approach. Now, let us take a look how the k nearest neighbors ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf41e943a83ba9388c03865eb1ce8284'}>, <Document: {'content': 'approach works, the way the k nearest neighbors approach works is that I basically I do not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '194384f5ac6b89d2363c97ef9580b65a'}>, <Document: {'content': 'fit a line. So, I do not have line or I do not have mathematical form, the idea behind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '99f40708e0117b5a64bca1e7c29fbd0f'}>, <Document: {'content': 'k nearest neighbors is that if you come to me with a question as to hey can you predict ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a3b39d7a79d177621c7ea0c82912a69'}>, <Document: {'content': 'for me what y I will see for a given x. I will take the given x and I will ask myself ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3583241f408ce4f0a0e35fa9dc4be782'}>, <Document: {'content': 'who it is nearest neighbors are. So, somebody walks up me and says can you give me a prediction ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c5911cce41cc22667e843df30712fe0'}>, <Document: {'content': 'of y for this x 1 let us call it x 1, let us call it x star maybe. So, x star for this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e24596534b83581e99a96cec16a8b56'}>, <Document: {'content': 'x star, because x 1 tends to have the connotation that it is the first data points. So, I going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf75e76aee0d3bc1dd5fe39d1e48fe3e'}>, <Document: {'content': 'to call it x star, so for this x star can you tell me what my predicted output would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d6d7e9d6ade45908eb254fe1d03a063'}>, <Document: {'content': 'be. Now, remember I do not have a fitted line, this is a completely different approach to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '980947eaeba809eefc453bdf728b02ec'}>, <Document: {'content': 'making predictions. But, what I do is I go to this line and on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b13857a1f40d5927184c09d951f2647c'}>, <Document: {'content': 'my x axis, on my input variable axis I try to find the nearest neighbors of neighboring ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5f36dd05511fb84c1be4b9e70a2d5111'}>, <Document: {'content': 'data points to the x under question. So, clearly this data point is kind of close to this line ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9b80121b4aa958a7bed5dbd3f1c76800'}>, <Document: {'content': 'and this data point is may be close to this line and so on. And the idea is that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4452d6360f7d8eeb86f5bb086d33365'}>, <Document: {'content': 'K-NN approaches has a parameter which is k. So, let us say I have chosen five as a parameter ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b3689dcd825e5ee1a61c00b0ad8ddcc'}>, <Document: {'content': 'and we will understand what the five means, it is means I am looking for the five nearest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf5bf73d88146a0a2f33aa3ee142238'}>, <Document: {'content': 'neighbors defined by the distance from my point under question, so this distance. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd180a61c63b667e2e39221203da05eee'}>, <Document: {'content': 'So, I am going to see the five closest data points and if I do that for instance, I see ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b49219ac6855c1a5cd8a20b9fa03c9bf'}>, <Document: {'content': 'that these five data points are the ones that are closest. So, what do I do once I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '890b57200fbeff39c2673b9ca938224b'}>, <Document: {'content': 'identified the data points I take their average wise to make a predictions. So, I take the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e7536e7aeb40d2d7ed5d62660b0b3916'}>, <Document: {'content': 'average of this y. So, let us call that y a this y, y b and so on. So, I do the same ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd30427186b622a6a5a0ae0c74116dead'}>, <Document: {'content': 'thing for this data point, this data point and I take the average of all those five specific ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea37c3f9158ea101169c07d050120955'}>, <Document: {'content': 'wise and that is my predicted y, the predicted wise the average and there are many modifications ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6827d31f8dc763d1284591d4bb599df5'}>, <Document: {'content': 'to this, sometimes you do not take the average, you might do like the localized regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cdc327cd82d1e08beb7c4544bf0ab067'}>, <Document: {'content': 'there, there are other ways, where you do not just take the arithmetic mean you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '73480b3e8b9bb0535ab63b88964e4737'}>, <Document: {'content': 'take the median you might do other things. But, the core approach with k nearest neighbors ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '978e262bc19b18793182a0537e34d2d8'}>, <Document: {'content': 'is that you have not fit a line, you not created any mathematical abstraction of the data, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '825ad5f1c80054d791319227f7065a3e'}>, <Document: {'content': 'you not abstracted away from the data. For instance, in the regression remember I told ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '477195b26423c031ab60d98b04db2f22'}>, <Document: {'content': 'you if I need to do a prediction task, I do not even need to remember the data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '36d4f9fde6fcc6ffbcbe09bdd003ad34'}>, <Document: {'content': 'I can throw all my data points wave. Once I used the data points to create this line, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c893887b6f6949381813f22e1166dc0'}>, <Document: {'content': 'I now only need the line to make a prediction I now have a y is equal to f of x. So, I have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e205057b88e99a90cb1758034401fc83'}>, <Document: {'content': 'this functional form and I just need to plug in my values of x that I want to use to predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1f66500f511f7b2259a5c6a71ae0966'}>, <Document: {'content': 'and now I will automatically get a predicted y. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50622ecf0ffbd4d68e7ee4a637dcd40d'}>, <Document: {'content': 'Here we are not doing that abstraction, we are retaining the entire data set of points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af96f5559bed63f14342c311171cf43b'}>, <Document: {'content': 'in the k nearest neighbors approach, we need all our data points. Now, you come and ask ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29c6d344870364dc9922ef5e1f577c10'}>, <Document: {'content': 'me a question about a particular x, I am going to go and look at the nearest neighbors of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '815bcbf05acd28f4b2febb44e8d9ad71'}>, <Document: {'content': 'that x and if it is five nearest neighbors I am going to look at the five nearest neighbors, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ab7d0c6c3ccc146d4c5b63bbfe80eb0'}>, <Document: {'content': 'if it is ten nearest neighbors I am going to look at the ten nearest neighbors in all ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84c5ebed4dfa50b0b9ef850435eec501'}>, <Document: {'content': 'these cases I am going to look at nearest neighbors and you know either take a vote ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '81672d372a0ee4db27bb2568efd5781d'}>, <Document: {'content': 'if it is a classification problem or if it is a regression problem am I choose to do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4275fad7b9c853ed9abe0279d5ff4507'}>, <Document: {'content': 'choose to take something like an average. So, that is fairly clear in this line that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe82bfc14d5562f0eae91f144181437'}>, <Document: {'content': 'I show is kind of what is approximately the arithmetic mean and we use that and finally, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f71924994599e64ed36f6c0c4cdda51'}>, <Document: {'content': 'that is the output that we going to predict. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32e9ee9daf13fd46205be6742bdbc1fd'}>, <Document: {'content': 'Now, that should kind of illustrate k nearest neighbors for you it is a fairly simple technique ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd56754e22335828964c8370793b73c98'}>, <Document: {'content': 'a lot of the focus in using this approach to solve problems is based on how, because ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3b24d51210fc2368110f4dd458af41c1'}>, <Document: {'content': 'it is computationally very hard. Because, it is storing all the data and you need shift ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b079dea25c932432803a8c2fcd1609'}>, <Document: {'content': 'through all the data to find the nearest neighbors good amount of the focus is on the data mining ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d04201637c512a7a6d5304d00408258'}>, <Document: {'content': 'aspect or the computational aspects using something like that, but it is also very convenient ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3bd33c2b34f441d23962d5a1a38ba995'}>, <Document: {'content': 'when you have no ideas to what the functional form is. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e3b5817ceb602aa2fd3243416f27534d'}>, <Document: {'content': 'So, suppose the linear regression approach can be very useful if you believe the relationship ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21ea7e52b3c4082c52f90c647e4456f7'}>, <Document: {'content': 'between y and x is the straight line. But, if you do not want to make that assumptions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8ad2aa5a77ecaaf8eb599e0ba43437f'}>, <Document: {'content': 'at all k nearest neighbors approach is fairly flexible to any function of form that y and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b30900afdcd8ea61ecd4a392b0632d91'}>, <Document: {'content': 'x could have. And there are two points to note here just an addition to what we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd32680e7d9fc18f0a67bbf9e8b82c673'}>, <Document: {'content': 'discussed which is this k nearest neighbors approach; obviously, works when you have multiple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e71f5dca55cd84a90599f1d619bcc7ae'}>, <Document: {'content': 'input variables. So, the examples that we took in the previous case there was one input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c697b031e9eab2cb89a04e08c103d2cb'}>, <Document: {'content': 'variable and then there was this output variable, this is the output variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af0f69e68046695097b534f2fc750637'}>, <Document: {'content': 'Now, what if had multiple input variables, the same idea. So, here we have one input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cabef32cbec1ca5158265ba526a3e0f7'}>, <Document: {'content': 'variable in the x axis, one input variables in the y axis. So, where is the y, where is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '443bdd430a637a0cca461e3c52c00a62'}>, <Document: {'content': 'your output variables, one way to think of it is that it is coming out the screen essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c98625fa4641bbfa8f7c7af65bdaa38e'}>, <Document: {'content': 'we are not able to in a two dimensional screen show you the three variables. But, if you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '342b59e5322569d7dcf2398299c4a30e'}>, <Document: {'content': 'assume that y is kind of coming out of the screen, you could graphically represented ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '60c8c3db53ab873feba69cf6cb117db3'}>, <Document: {'content': 'that way. But, the important point that I wanted to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f0ec896f73d581928f27187cfcf9c58'}>, <Document: {'content': 'make here is that if you needed to take like a five nearest neighbors approaches on two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '528264b78d7c7105a5c49ccd844da3e1'}>, <Document: {'content': 'input variables, you can still do that let us say you are interested in this particular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f995d8a9b11d8617ff7e8517c1421b1d'}>, <Document: {'content': 'point marked with the x then your nearest neighbors to this x again get defined on the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '759b6addccecd8845b5edd301e549b06'}>, <Document: {'content': 'two dimensions. So, it could be this point, this point, perhaps this point and what you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f775712850f7964bea02823b27f89e6a'}>, <Document: {'content': 'can see is we are taking some form of like may be Euclidean distance, because the distance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e703661c228ada60a8ec6b80fe638c8'}>, <Document: {'content': 'itself is not just on the x 1 axis, it is not just on the x 2 axis, it is on both x ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4402128b9d6dece359db531d6d1add8'}>, <Document: {'content': '1 and x 2 axis. So, you can still use the concept of the input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a8ed96abdbb742f0f9c272c6e9251c9'}>, <Document: {'content': 'space and once you have more than three year of when you have four or more input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '27d37eb4ac87c90a16a8975bfd5eceb3'}>, <Document: {'content': 'you are talking about hyper space. But, just you could still use some simple measures of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d875ce485fccf991f5678f31a0ba88b'}>, <Document: {'content': 'Euclidean distance or some measure or some other distances some famous once I call them ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f981941f34743eb280e7067b327a24b'}>, <Document: {'content': 'and Manhattan distance some of them are called the Mahalanobis distances, Mahalanobis distance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a7ec783db45e854bf97842bd62cdfc3'}>, <Document: {'content': 'as well. Now, the good thing with this approach is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4fa8dd4a66e1f49c73a9163d72d680d'}>, <Document: {'content': 'that you can even use it for classification problems, if you briefly remember in the previous ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '457f7be795a039d178d1600265f963d'}>, <Document: {'content': 'lecture we distinguish between supervised learning tasks, which basically just means ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a214693dd3611be57c75e4e12a741ad'}>, <Document: {'content': 'you have an output variable you need trying to predict that output variable from the input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1e2b82cd16b800413c77ce7de3b548e'}>, <Document: {'content': 'variables. But, this output variable can be continuous quantitative which is where we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '229673a12db023e977e08d6befcf073d'}>, <Document: {'content': 'primarily talked about regression and so on. But, it could also a categorical variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b9c34bc82afa8abb329f6576ded902b'}>, <Document: {'content': 'o, if taken an example out here on the right hand side, where you have two input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6a292083b2e6d14a26d6af89ec8661b'}>, <Document: {'content': 'x 1 and x 2 are two input variables and your output variables is categorical variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '738002529077618fd652938f7eca6937'}>, <Document: {'content': 'with two classes. So, think of it as male or female or buyer or non buyer in marketing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'afdea3c65a76e15eb92da8215e5f1a4e'}>, <Document: {'content': 'contexts, defective product, not defective product in a manufacturing context. So, let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c150e1cc821b71955ee4b3fb615b8d14'}>, <Document: {'content': 'us say this output variables which is a categorical variable is represented by either square circles ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3abbfd46f83819283314788266641ee4'}>, <Document: {'content': 'or squares. So, the orange circle are one class of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3b43a5f89870b116be67c5a6a4a2ef9c'}>, <Document: {'content': 'output, the blue squares are another class of the output and x 1 and x 2 are just your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e48ef2563d3f808b489d68972cc8015'}>, <Document: {'content': 'two input variables again out here you might be interested in for instances making an prediction ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a08a4816d8eccd9fac342f7d812e665'}>, <Document: {'content': 'out here and you might take the five nearest neighbors perhaps it will be this and may ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad922a441c9bddca7d7164273ac797d6'}>, <Document: {'content': 'be this. So, these might be the five nearest neighbors and because you need to predict, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34ff4f42cc34c0141442c99ccc526b8e'}>, <Document: {'content': 'whether it will be class A or class B you might want to take a voting approach or if ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c98e66dd163ffe9f3cb21dfb8ed995dc'}>, <Document: {'content': 'your approach is to predict the probability of it being circles or squares that is what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c8ace08266d967096f92ea7dc83ad20'}>, <Document: {'content': 'I am call class A and class B then you might just take the ratio of the circles to squares ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f68ba22e6a6cffa4a2ebde6f101ce0a4'}>, <Document: {'content': 'in your nearest neighbors. But, the idea is that this is also works perfectly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7579cecdc28a4d7d76c48bdcecaa10d6'}>, <Document: {'content': 'well, you do not need to just take an average, you can take you know ratios or you can just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5321f48d8673d97bb94d57c2e0227b4'}>, <Document: {'content': 'make them all vote essentially the majority win. So, you have three squares and two circles, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '311a1b01b60b0a2c0fb92d6a4ac5ab3d'}>, <Document: {'content': 'so I am going to predict this is going to be a square, you can using voting approach ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2038b0c4beb1ca8427de25d33c0f0d7e'}>, <Document: {'content': 'to say belongs to a particular class. So, I hope that gave you an idea of k nearest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1ee34e96463f467534be4dd5998c3c78'}>, <Document: {'content': 'neighbors, but also more importantly motivated to you the idea that you have this regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f39408a8905817377134a7d7f4afc395'}>, <Document: {'content': 'style approaches, where you got this explicit data model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e1b3ee73e39c406c5c273f18747e6e50'}>, <Document: {'content': 'So, this is your regression style approaches by you have a functional form and then you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7729600c2b5e53e6b26018654503dc09'}>, <Document: {'content': 'try and figure out the parameters. But, you can also take on very different approach to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75376f901605ee332f5f2a7bb0770f17'}>, <Document: {'content': 'the process of predictive analytics, which is through the process of prediction, where ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '41676f1a9c4c89785f5fefdb0c07454a'}>, <Document: {'content': 'the importance is not as much on the functional form which is cast and stone which is really ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5605b52bc34da43a3f7f17a5b927e8'}>, <Document: {'content': 'an assumption you are making about the relationship between your input and output variables. But, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e80597f7f3bf3eac75f091e230f687e1'}>, <Document: {'content': 'it really goes beyond that right like you do not want to make those assumptions and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1876659c6ebf9e7f68f80df6b157598'}>, <Document: {'content': 'you just want to take more algorithmic approach to this entire process. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '534effaf5237a3c0ba40e626676e6085'}>, <Document: {'content': 'You will get encounter a lot of machine learning techniques that we are going to be discussing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a7f4cc6e0805795031ec26f353e1c83b'}>, <Document: {'content': 'later in this course really belonging to that class. So, I hope that gave you an idea both ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cfef5d20d47b3a8a0f2feff6fe86302'}>, <Document: {'content': 'k nearest neighbors and this dichotomy that you be kind of seen machine learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b611a1f4e35b5fd9998a56fb2370b89b'}>, <Document: {'content': 'Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '170f7be97195bd40737de367c3c69a78'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Logistic Regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ec5dadcbbdb5bfd9622f74fca749e96c'}>, <Document: {'content': 'Hello and welcome to this module on Logistic Regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5265e21a74b73e1d4d78d83478beb27'}>, <Document: {'content': 'So, we have looked at the problem of classification earlier and here is an example from one of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5b2f9ea2b1634b02a6116c175e09f18'}>, <Document: {'content': 'the earlier modules. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e15892c9f4fb0091e7f4e622bf59537a'}>, <Document: {'content': 'So, the users not in brown here are those who bought a computer and those marked in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54c854f304481b51eca6bfef05096d84'}>, <Document: {'content': 'red are people, who did not buy computer. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '16f27339b6400aec34bb9a8b373aea59'}>, <Document: {'content': 'And the goal of classification we said earlier is to find a decision surface that would help ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '633361af4c7237d683a22f44d10de27c'}>, <Document: {'content': 'us separate people who buy computers from those who do not buy computers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6cbd7c72cc5eb50c9d0ff141c4aaf68f'}>, <Document: {'content': 'There are different ways in which you could have these decision surfaces and we looked ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6c5ebf8d090f51ba6a8deef75a14763'}>, <Document: {'content': 'at a few. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf069b09d0da6d44ea95d4a81a5050c4'}>, <Document: {'content': 'Now, let us step back and ask the question, what exactly does this decision surface mean. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f15acceb66f2fc33a2d1bdb3897f8f9e'}>, <Document: {'content': 'Specifically let me ask the question, what is the data point that lie on a decision surface ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8942dacf1c944668945626dedbb5ca4a'}>, <Document: {'content': 'belong to, is it buy computers or does not buy computers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7fecfb635d67ac6733b3fc1e9fd848b6'}>, <Document: {'content': 'So, one way of thinking about it is to say that this decision surface denotes all the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea5b8c63e4b68de825a16e318a59e45a'}>, <Document: {'content': 'data points for which the probability of it being red is equal to the probability of it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dfc07eaff194095b5e36e0638045441'}>, <Document: {'content': 'being brown. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '191de436a62fbb8a0a0e791a1857e0b7'}>, <Document: {'content': 'This essentially means that for the points on the boundary the decision boundary you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9afe071e9187f868733395f1efebbfef'}>, <Document: {'content': 'are not able to make a decision as to whether you will buy computer or does not buy a computer. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3242c2e880d6f4d73e3db1663bc9cb70'}>, <Document: {'content': 'So, what is it tell us about the points that lie to one side of the boundary? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80302ee29203249c37bfe950c795283a'}>, <Document: {'content': 'So, the points that lie to one side of the boundary are those, where the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9c6cb46a47eb9adc4072e437c0c3437'}>, <Document: {'content': 'that the person will not buy a computer in this case is higher than the probability that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd64777caee048fabfb40d5c7d796b48a'}>, <Document: {'content': 'he will buy a computer. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e06b4744c10f280c93f77d4263454a8'}>, <Document: {'content': 'So, the one way of thinking about the decision boundary is that it models all the points, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3cf62bc64f7bb3b00a663cdc7a0c1057'}>, <Document: {'content': 'where both the classes are equally likely or equally probable to occur. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '93ed93bf2ca8816d2754ed456cd3d811'}>, <Document: {'content': 'So, if you want to go beyond classification, so you might be interested in knowing what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29c49a7db2489e846cecf4b6e07fa2b7'}>, <Document: {'content': 'is the actual probability of a specific class given a data point. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '371b3c1df3184780338924125ca9c400'}>, <Document: {'content': 'Not just in finding the right classification, you really like to know what is the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ecc6b1d84d8df7d87c110d0ef8f7ec5a'}>, <Document: {'content': 'that the person buys a computer given the age and income of the person versus the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cefcc483f77eb2b0721c96fd0ce01913'}>, <Document: {'content': 'that the person will not buy a computer given the age and income of the person. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d4eb82d686e2a4e3fee164f9962158'}>, <Document: {'content': 'So, why would you want to know this kind of probability or the class label? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd6bbc69c8dfc6ff10e93005bfcb642f'}>, <Document: {'content': 'So, one example is you could think of in medical domain. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4122f4306544865bc7c5da78130e9b44'}>, <Document: {'content': 'Suppose I say that, you have a specific disease or the patient walks into the hospital and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3915a2807869402ed3cab70d9437de6d'}>, <Document: {'content': 'the doctor says that the patient has a specific disease and you would like to know if the, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ff5864765d0aa0020f972ecaa342b2d'}>, <Document: {'content': 'how confident is the Doctor of the prediction. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'db9bb4b0aff6882363359ebf9865dac0'}>, <Document: {'content': 'So, the Doctor says I am 95 percent sure that this patient has the disease, then you certainly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85f7c4a3c2d618317d995a78619485d0'}>, <Document: {'content': 'would go into the treatment. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9641d5575571fc2aa164a7742fe9fdc9'}>, <Document: {'content': 'So, like wise when you have a classifier that is going to give you a class label you would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '311e68eb612c6fb086a5be32387b39e5'}>, <Document: {'content': 'like to know, how sure the classifier is of the class label and that is one application, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e666495236ec5fd764669268276bdc8'}>, <Document: {'content': 'where you would like to see these kinds of probabilities. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aae8daffd3bc1f3794b15bdfafaf116d'}>, <Document: {'content': 'So, one way to approach predicting probabilities instead of just the class labels could be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bebc7c0e0abd69c41ec0977cf5cbcefa'}>, <Document: {'content': 'to treat it as a regression problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '53e5f1bbe14d85a40d4d74e64425c137'}>, <Document: {'content': 'So, let us stop and think about how you would treat classification as a regression problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '723a9257566eeb22afdef0a4e912a9a7'}>, <Document: {'content': 'So, normally in classification, so you have labels, who does not buy a computer or buys ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a1a8a223a81488d8bf0dd77a220ee5d'}>, <Document: {'content': 'a computer. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e3fa6eb1023032b24b0fd14fbe48d56'}>, <Document: {'content': 'So, instead of using these labels you could use an indicator variable for the class. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1c3f5bb0a702295002982a5662ba81'}>, <Document: {'content': 'So, if the user is or the customer is going to buy a computer I would say the output is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c291a5fcb295f1857b42cfa63d4cb9ee'}>, <Document: {'content': '1, if the customer is not going to buy a computer I would say the output is 0. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba7245f0d3a04dccea919ce961891a26'}>, <Document: {'content': 'Now, your data gets transformed into a regression problem now instead of a classification problem, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1dbe008d4828bf07ab4b95558b46d377'}>, <Document: {'content': \"where you have 0's and 1's as your response variables and the actual attributes of the \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a455e1897404e1f7b9817d3eb56f5c3b'}>, <Document: {'content': 'data has the predicted variables for the regression problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fd248aa386a04324094916daa25367d6'}>, <Document: {'content': 'And you could use linear regression here, we all know about linear regression now; you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3295c52f61938ae2635c0880adffe558'}>, <Document: {'content': 'could use linear regression here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34ff29ee33a6d81d44de8045c8ba765e'}>, <Document: {'content': 'And the finally, their function that if it f of x can be interpreted as the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b9691ae797cab2b2f8f950a97d24c89c'}>, <Document: {'content': 'that the output y will be 1 given the data x, that seems like a reasonable way of doing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5af20c35ef5492ea889e8e3538db559e'}>, <Document: {'content': 'classification. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '91258b5929a08c22401079002a172cfc'}>, <Document: {'content': 'So, whenever the probability is greater than 0.5, you would say that x belongs to class ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a95e4c04bd82bf28478fdfabc4c38fa'}>, <Document: {'content': '1, the probability is less than 0.5 you will say x belongs to class 0, that it is actually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26fd5e54d42604d198f49b7595903c5a'}>, <Document: {'content': 'a valid way of doing a classification using linear regression, but there are some problems ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f3006259e0b495789eaf884e178fe90e'}>, <Document: {'content': 'with that. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d6295be59ab12ff4ac06c518e03304e'}>, <Document: {'content': 'So, what are the problems? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df82b6c47e00d900a7ce788b7d15375'}>, <Document: {'content': 'So, linear regression is not really limited in range, the output can go from minus infinity ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a04623f3ecc8552cd77ff086f442d3c'}>, <Document: {'content': 'to plus infinity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cff89b0dba0c980dd5e36f7b405ce8d0'}>, <Document: {'content': 'So, typically this output cannot be interpreted as a probability, when you troublesome it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2fc71365d13de366152f2db50c791e11'}>, <Document: {'content': 'is the fact that the output can be negative and therefore, this certainly cannot be interpreted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ca7a291e993c04ebd8bc732df90bd0d'}>, <Document: {'content': 'as a probability even if you think of doing some kind of normalization. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22c71dbabc0e944eaf1709b688f45471'}>, <Document: {'content': 'Having said that I should say, it actually works in practice, if you do not really want ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd50b95987adb3b55876a0fa12680e88c'}>, <Document: {'content': 'to treat it as probability, but just as a classifier, you know if it is greater than ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f044e2256d0ddbe474ea55847e1ff2e8'}>, <Document: {'content': '0.5 it take it as 1 and lesser than 0.5 take it as 0 it works well, it works in practice, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ee48a6e58f5220d70c2890f02e2b131'}>, <Document: {'content': 'but not that well and there is way of doing better than just using simple linear regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '63b87433e140218ae0f2752fa6ed3cc6'}>, <Document: {'content': 'So, I want to use linear regression still, but I am going to do that on a transformed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '28c7aea80ca519e46eac5afe0d5a3a99'}>, <Document: {'content': 'function. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67a20ce5f10e705696e488d1e9f99b92'}>, <Document: {'content': 'If the transformation that we are going to talk about here is called the logistic function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b66df74aa69eaab5f17e55cff1004b2'}>, <Document: {'content': 'or the logit function, so let us have some notation here. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '787a6eacde362f7e77ca3436f13692bd'}>, <Document: {'content': 'Let p of x denote the probability that the output y is 1 given x, then the logit transformation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ae5a99dc08a80e488b0f740174834445'}>, <Document: {'content': 'is given by the logarithm of p of x divided by 1 minus p of x. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e20ee23ae00219fc44727e2a7deb747'}>, <Document: {'content': 'So, if you think about the binary problem, so p of x is the probability of the output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '865eb14f4e72da3508b0eb413a6e794d'}>, <Document: {'content': 'being 1 and 1 minus p of x is a probability of the output being 0. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd4e4a7a396969706409c3e9f982209a6'}>, <Document: {'content': 'So, essentially you are taking this ratio of the probability of success to the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b7ce9f99e382b5c895a2b9122b58f0ed'}>, <Document: {'content': 'of failure. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac20e25d0e2945f42daf2174c3934372'}>, <Document: {'content': 'So, this is known as an odds and so this sometimes known as the log odds function. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd0381fa8f8438b87972207e62e17599'}>, <Document: {'content': 'So, now, what are we going to do in logistic regression is essentially try to fit a linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d11f604dfe172721ba5ce1e452053b7'}>, <Document: {'content': 'regression model to this logistic function as the output. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '278b0c5f5a9c45ab52e62c32709a05b4'}>, <Document: {'content': 'So, essentially we end up saying that your log of p of x by 1 minus p of x can be modeled ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '41109d3b80e5a5bfd907b8529455f5a4'}>, <Document: {'content': 'as some linear function, which is beta naught plus x times some beta 1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f9b5cc176b39d64f714700e98e7dd3ae'}>, <Document: {'content': 'So, if you think about it you can solve for p of x from this kind of an expression and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9566e66271029091d6e0ed7bb017c2df'}>, <Document: {'content': 'then you end up having p of x looking like a sigmoid function. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18be78dd299d045032838623afb5a349'}>, <Document: {'content': 'So, e power of beta naught plus x beta divided by 1 plus e power beta naught x beta and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4653ef7b72a4807820985a01304bf68f'}>, <Document: {'content': 'can simplify that and the functional form that you are going to get is something like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a118a93365df66c206aa12ac43588925'}>, <Document: {'content': 'this. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3bf8e519672b4bda150dbd8d6b44c8e0'}>, <Document: {'content': 'So, you can see that the it behaves like a probability function. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e3a79882bd0f5db981b3181e6cebe69'}>, <Document: {'content': 'So, it transfer only from 0 to 1 and by varying the value of beta what you are going to do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e04d4b27b4af103329029029283250'}>, <Document: {'content': 'is your going to vary the slope and by varying the value of beta naught, you are going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6276e3c64c373083bcab917a4051c6dd'}>, <Document: {'content': 'vary where the function is going to rise. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c6d2b962ed579d8c34f807db3faf0a28'}>, <Document: {'content': 'So, this gives us a very valid way of fitting probabilities, there is no problem with interpreting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6636b6813d133de103217f29e101913'}>, <Document: {'content': 'p of x fitted in this fashion as a probability. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7a6612c3d696deee9f9383948c6ff342'}>, <Document: {'content': 'So, earlier we trying to interpret f of x in a linear regression model as a probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1088889ced719ec003e5d540afba836f'}>, <Document: {'content': 'had problems, so we could not do that, because it could be a negative as we saw earlier. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2dbc0e65b78d66e2ab118264df97f9ed'}>, <Document: {'content': 'But, in this case since p of x is going to be limited between 0 and 1 he might as well ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c47d5a34a3c0b179a66bc0cf2f66dbb'}>, <Document: {'content': 'interpreted as a probability is it that right model for doing it that is an open question, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ad81caddfedcde62a431b34a93e6d273'}>, <Document: {'content': 'it depends on the domain that your working in, but it is fairly widely used and it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cccfe96b89a74d47c281db630638b034'}>, <Document: {'content': 'very power in resolves in a very powerful classifier and which you can use in variety ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10a3d207c69cafb03c559283e9150efa'}>, <Document: {'content': 'of different settings, whether this assumption is actually supported by the data or not it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '187e3f5678db41ed897e854265a94e8a'}>, <Document: {'content': 'seems to work well in practice. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c41ea2d8d51ed8eea76d5871dfb3aab6'}>, <Document: {'content': 'So, that fig did with the linear regression case we will predict the classes 1 and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd6d83b295009cbc1c1a5fec229cf9b8'}>, <Document: {'content': 'probability of x is greater than 0.5 and 0 otherwise and this essentially you can show ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d8ac13cd75ec0b2c788b966abfbe879'}>, <Document: {'content': 'if this minimizes the misclassification rate given the form of the predictor that we had ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1df358780a4f2bb510fac0b49658d4d5'}>, <Document: {'content': 'on the previous line one thing to note. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b3b78a9f5169638997b130b27ba55ed'}>, <Document: {'content': 'So, even though p of x is given by this exponential function, the actual classification boundaryÉ ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4066dfc02bc4a83f39f60cf0c3728818'}>, <Document: {'content': 'So, what is the decision boundary? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb9be784f50c07c9ef002ad0263aec2'}>, <Document: {'content': 'Decision boundary is the point, where the probability of class 1 is equal to the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '39ba6918fe1ef8c0c4a0b93c85810f70'}>, <Document: {'content': 'of class 2 or class 1 and class 0 probabilities are equal. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8cfa5ba90fc173cff6eadcb105f6830f'}>, <Document: {'content': 'So, you with the little bit of thought you can see that the decision boundary is still ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '60b0f4a6cafc8a42b3f9819ad50ed03a'}>, <Document: {'content': 'given by a line which essentially beta naught plus x beta 1 equal to 0. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4102da156c0ed70a0f6366dacc7bc6d6'}>, <Document: {'content': 'So, that gives you the decision boundary of the logistic regression classified as well ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0ce05ce4cde017465081d84baea0ffb'}>, <Document: {'content': 'and hence this is also a linear classifier and I mentioned earlier it is pretty powerful ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6709f25d8134e3db2a08d9b57ead544'}>, <Document: {'content': 'and works well in practice. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13d7a3d1f93100a8f9401733489fce0c'}>, <Document: {'content': 'So, let us look at an example of what happens when we fit data using logistic regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b97682a4dfd110dde0902434e9a7831'}>, <Document: {'content': 'versus linear regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d16d61af3e8e933ca7311ebd4a50ae2'}>, <Document: {'content': 'So, here is a two class problem, so the data points or either in blue or in red and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f8171d1c90d6e7909abe913d5c07541'}>, <Document: {'content': 'shading in the region indicates, what is the class label that would be predicted by the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7a6e9bfb476e0a912f18531f87351568'}>, <Document: {'content': 'classifier in those regions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5802530366b0c12df481286371f3f33'}>, <Document: {'content': 'So, on the right hand side you have slides I mean you have the prediction made by fitting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6a0b8cff9b1368a318566f0b4b95c1d1'}>, <Document: {'content': 'a linear regression to the indicator variable on the left hand side you have the output ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7e82de25289168d695ef18686562827'}>, <Document: {'content': 'given by logistic regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9182d25158414ad6a0fdd94debd46d'}>, <Document: {'content': 'So, you can see that linear regression actually makes a certain errors closer to the boundary ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'af4d6b7dac18457272ea76c410637baa'}>, <Document: {'content': 'that is because linear regression is essentially limited at the rate at which the curves can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f73fdeb7abc4ff0690f93e41706ce4c6'}>, <Document: {'content': 'climb and when closer to the boundary when there are points that are bunch together from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7124d693228cefe3ca63f59e655ff9c'}>, <Document: {'content': 'one class, but little further away from the rest of the class linear regression is not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4fc80402c74b05dc9a1904f59926d09a'}>, <Document: {'content': 'able to model those successfully, while logistic regression by virtue of the fact that you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92d322ee03caf2b3ad863ca36cc2bb1'}>, <Document: {'content': 'could have a steep climb from 0 to 1 is able to capture those data points. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e649e9a44ec1059c64267e6fac2f6344'}>, <Document: {'content': 'So, this is essentially the difference between linear and logistic regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd38c0392aa1b2b949ace829e54fbddd2'}>, <Document: {'content': 'So, far I have been talking about binary classification problems, because they are easier to illustrate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5b8fd23c67e426776bb7ffcc1d54c4e'}>, <Document: {'content': 'and kind of understand the basics behind. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2d5779a054d070a6f9f968f5ff2fe64'}>, <Document: {'content': 'But, then logistic regression can be extended to multiple classes as well, suppose there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5332132af36ed7d3983c9f1ebec7bdd'}>, <Document: {'content': 'are k classes then I would say that each class gets the different set of parameters beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5c3cca77de9c89ca31a3b8e64e2cf5'}>, <Document: {'content': 'naught and beta for that specific class. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e0775e84fca70ad86b487f966947e8'}>, <Document: {'content': 'So, in that case what happens is your probability of... ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce144bb40051d9b64db17d66d5f9cd79'}>, <Document: {'content': 'So, the probability that particular class is the right class for a data point is given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ecb8b467157a17a4f0e168ed0f73b5ad'}>, <Document: {'content': 'by e power beta naught of c which is the e power beta naught of c plus x beta c it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '803b7b2845833390de700cfc67b6424e'}>, <Document: {'content': 'our essentially the parameters specific to the class and divided by the total the normalizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '325a4e563f34a00f5fe24fa420a1064'}>, <Document: {'content': 'factor, which is essentially the numerators sums for all the data points. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd1f6cade1a47bb5cfd2a4a39290ae33b'}>, <Document: {'content': 'To make the problem somewhat easier traditionally the parameters of one of the classes, it could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '401abdab3a6bcf9725abfae38b520596'}>, <Document: {'content': 'be either the first class by numbering from 0 to k or it could be the last class which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3622b31f2234ad608090e08ee913d133'}>, <Document: {'content': 'is k is set to 0 and you can think about it, it really does not affect what the classifier ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '942281ecf93ea73553fb0afadc4bc750'}>, <Document: {'content': 'the decision boundary that you are going to learn it will change the parameters that you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c4f2e938888eefcf5efebcbbb045936'}>, <Document: {'content': 'are learning, but the decision boundary that you learn will not be affected. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb8d5c1bb10bc60c5db5ca2dfc9c88a3'}>, <Document: {'content': 'So, in a sense you will be left with fewer parameters that you have to estimate that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58b0e74245157c4ff4bc19261c476e53'}>, <Document: {'content': 'is because you are talking about probability distributions here and we know that as soon ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1571f9976d154f00b00e11c7d0186f7e'}>, <Document: {'content': 'as you fix n outcomes in a discrete probability distribution of the n plus 1 the outcome is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f652d598494ac79248fb05cd603d7b54'}>, <Document: {'content': 'automatically fixed in total of n plus 1 outcome. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2826b6ed98124b89f06f5d589ca2edfe'}>, <Document: {'content': 'So, far we have been looking at the basic model and logistic regression and I will end ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fdf35f78811d8f59e9894aaca3b73150'}>, <Document: {'content': 'this module here and for the next module we will look at how will actually learn the parameters ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a04df440d68556fd06103fd71fad9dbf'}>, <Document: {'content': 'of this logistic regressions classifier. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5f57f9115a7cefd30ba59802ad38425'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Training a Logistic Regression Classifier ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5969a0e4b849ad7d41555766ca3a2e4e'}>, <Document: {'content': 'Hi, so we are looking at the module on Training Logistic Regression Classifier now. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abaf5f3524c7386d954c92172cc6513e'}>, <Document: {'content': 'So, in the previous module we looked at the basic idea behind logistic regression, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1a77a8bb4ddff526d6def081b8c7894'}>, <Document: {'content': 'is essentially to do linear regression with a logistic transformation. So, you took the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f4bbf3bd80636f57e925afdc40fa192'}>, <Document: {'content': 'log odds function, which is p of x by 1 minus p of x, took the logarithm of it and try to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b2ea4add7eca8bf99c12b03af836aeb2'}>, <Document: {'content': 'fit a linear curve to this transform function. So, how do we find these parameters beta, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a24425e9703eacf89d121d975fe54513'}>, <Document: {'content': 'beta 1 and beta naught? So, we optimize the likelihood of the training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b390213bf009996423c8202dc55a161a'}>, <Document: {'content': 'data with respect to the parameters beta, so that is essentially the way we are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1f1b7899eaca11932a4e53ce2788542'}>, <Document: {'content': 'to be training this. So, this is slightly different from some of the earlier methods ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30021101a0403ed67f932de475e0590'}>, <Document: {'content': 'we have looked at identifying the parameters, mainly because we are looking at here the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8d6ceb7dcce14a3f044a554489e960d'}>, <Document: {'content': 'probability of classification, not just getting the classifications right or wrong, but we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8194722082e53dfded99a0e145bccaa1'}>, <Document: {'content': 'are actually looking at the probability of classification, that makes more sense to try ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb99cd3801bd2fcf67b41be9205f0f66'}>, <Document: {'content': 'to optimize the probability of seeing the training data with respect to the parameter ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee38eb60155de69cb9be67515d947b19'}>, <Document: {'content': 'beta. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6a408dbbd973ab830484c7745c657e1'}>, <Document: {'content': 'So, what is the likelihood? So, the likelihood is the probability of a training data D given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fecf85fc742c38396f747e551bf4cfd1'}>, <Document: {'content': 'a particular parameter setting beta. So, you should note here that, it is the function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82eb59b948bc8a1d2a22bef984ccc103'}>, <Document: {'content': 'of the parameter setting, because the training data D that is given to you is usually fixed. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9335693342a66a71e08f8d122a8869f'}>, <Document: {'content': 'So, here is an example of the likelihood of some kind of classification tasks. So, I am ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '788a845cd0883601db997f2b8e7da610'}>, <Document: {'content': 'going to assume that the data is given to you in the form of x i, y i pairs as we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56dc3af6f4a68528ba0ec714900794bc'}>, <Document: {'content': 'done in the past. So, for each x i there is going to be an outcome ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d4191dff6affe3262750b07e8c246f1'}>, <Document: {'content': 'y i, which will be either 1 if it belongs to class 1 or 0 if it belongs to class 2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a054060153c99771993aa669f917973f'}>, <Document: {'content': 'So, let us look at one term in the product that I have written down there, is you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e93dc1b8d679438f1272851f3df38c0'}>, <Document: {'content': 'see if the output corresponding to x i is 1, then the first term in the product will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed2394b02bd5fedf616450880c456541'}>, <Document: {'content': 'be p of x i and the second term in the product will be 1, because y i is 1 and 1 minus p ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbf3827646a80965ea08658cc00faca'}>, <Document: {'content': 'of x i is going to raise to the power of 0, which essentially reduce to 1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd3efa812fb9c0a60c8eefc557d2704bd'}>, <Document: {'content': 'Likewise, if the output corresponding to x i is 0, then the first term in the product ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20a16296d00af231620a9755d8efb37'}>, <Document: {'content': 'is going to be 1 and the second term in the product will remain as 1 minus p of x i. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8a68a1e32616f2150a3244fc8fa74d5'}>, <Document: {'content': 'this essentially means that depending on, what the output variable is I am going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58fd25e308edda9ecc4d6b2cf3dccd75'}>, <Document: {'content': 'either take the probability of the data point occurring, probability of the data point having ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f238536f5fb481a3e6d28b2e1a12cf0'}>, <Document: {'content': 'a label of 1 or the probability of the data point having the label of 0. So, to do recall ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bfadaa914fc8b8db9464ff31a4a1a96f'}>, <Document: {'content': 'that p of x i is the probability that y equal to 1 given x i. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c048e4838c35fa673180bbe655fc629'}>, <Document: {'content': 'So, now, for this is for one data point and if I want to look at the probability of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb3309e8d48eb4540ba3bba6c5b15f97'}>, <Document: {'content': 'entire data, I just take the product over all the data points, so the product runs from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c78d8c7aeb02bf78b4e900146c4babe3'}>, <Document: {'content': '1 to n. So, this expression now gives me the probability of seeing the training data given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '848e19b714cc162654b9bb5449a1d41f'}>, <Document: {'content': 'a specific parameter setting. So, where do beta and beta naught appear on the expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8791c7d6fa1794bcecd3f1d7f30d581c'}>, <Document: {'content': 'on the right hand side, so p of x i is specified in terms of beta and beta naught. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f05b6e4c97d010fff7e16af98ebc85db'}>, <Document: {'content': 'So, implicitly, so beta and beta naught are appearing on the right hand side of the equation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4951921e81ad89cc05add7ef1bdf8605'}>, <Document: {'content': 'and like I said, likelihood is the function of the parameters and hence we denote it as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8a7fdec510e295de653494bec9a2b7f'}>, <Document: {'content': 'L of beta. So, now our goal is to optimize this likelihood and, so that, so we get a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26ff17cd60e571258c8fe96aecd19d'}>, <Document: {'content': 'good estimate of the parameter, so we have to find the right set of beta, so that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f8082eba096284e06f602bef5be840e0'}>, <Document: {'content': 'probability is maximized. So, now, we look the term, the term looks a little hard to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6c4a6433138549d3bdd737130b9f46b'}>, <Document: {'content': 'optimize, because lot of products here and, so we have to be little careful. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3818072f788de0ba9a13c9c795c5b2ce'}>, <Document: {'content': 'So, the usual way that we operate here is to take logarithms of this likelihood and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c91b4d5e2aaffbe8b77ced87a5792bc7'}>, <Document: {'content': 'you can see that lower case l here is used to denote the log of the likelihood function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '650b13a4e224ebedd8f516061958c048'}>, <Document: {'content': 'and then, you just walk through this log likelihood expression a little slowly. So, now, that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d3a425151254661b34f08c9ff1b9b8a'}>, <Document: {'content': 'I have taken the logarithms in the first step. In the first step, so I have taken logarithms ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77cb8d27daf290da028a9a4f013e03e0'}>, <Document: {'content': 'and therefore, the products that I had earlier have become summations and the exponentiation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd49274b99deddddd121b229bec15412'}>, <Document: {'content': 'that I had earlier have become products. So, that corresponds the original exponentiation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1369298d35e1a43e40fae0888e35e50a'}>, <Document: {'content': 'I had in my expression and now they have become products. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '702767bdbcaeae09a15b21ad150e6136'}>, <Document: {'content': 'Now, we can do a little bit of simplification here and you can see that, what I have essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '776006c06ad1fb692707f9e0988a7e01'}>, <Document: {'content': 'done is taken the… In the second step I have expanded the product term in the second ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9241e1468a1bceefec4f80323573f261'}>, <Document: {'content': 'term in this summation and then, I have gathered terms together which have a coefficient of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2e1737da927aab08711e6044745beea'}>, <Document: {'content': 'y i. So, that gives me the second term in the summation and the first term is just essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b8f7db80297d8130d9880b70326037c'}>, <Document: {'content': 'one times log of 1 minus p of x i. So, we know what log of p x i by 1 minus p x i is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33e56bd3e6c4e017574c86d51d5c1713'}>, <Document: {'content': 'and that is essentially the function that you are trying to fit from the beginning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '584793b5e32178b518426d351c855a93'}>, <Document: {'content': 'So, we replace that with our linear fit that we had, the linear regressions fit that we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46a047b70c3622660e434159df788eeb'}>, <Document: {'content': 'did. And then, we do further simplification in order to come up with the expression given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8626a8c49a8a86b1543f282be5730e1'}>, <Document: {'content': 'on the last line, that essentially writing out p x i and then, evaluating 1 minus p x ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fee621524b7d8a353b6ce841777d8087'}>, <Document: {'content': 'i and that gives me the negative logarithms term on the last line of the expressions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '307bff3b8d21d0a32be5080f8ffd7232'}>, <Document: {'content': 'So, now, what do we do? We have the log likelihood, so what we do to maximize this log likelihood. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2053fcf11ecf52239142bbe45bd31a1e'}>, <Document: {'content': 'We essentially take the derivatives of this log likelihood with respect to beta and then, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88da08ed61fd296fdf9a3b020cb73cc4'}>, <Document: {'content': 'we should be equating this to 0. So, the first line here is essentially taking the derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1eb4a9db28976fa33fad9345d52e20b'}>, <Document: {'content': 'of the log likelihood and it simplify to a very nice form, which is y i minus probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '649bc8ad2f14fe220e8a0f789b928c7d'}>, <Document: {'content': 'of x i times x i each individual component of x i and now, we set this equal to 0. But, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37a353bcb517ef80cdac76b267415c44'}>, <Document: {'content': 'we really cannot solve this that easily. Why? Because, p x i is actually a transcendental ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c64bf5cd3e336fb52a60d5b9a9fa2cc'}>, <Document: {'content': 'function, so it is not very easy to find the close form solution for these kinds of expressions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a45a3b10f887ed878a03586037e31f55'}>, <Document: {'content': 'So, we have to actually look at numerical methods for solving these kinds of optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f62527d172bc2453f662c0c7730f116'}>, <Document: {'content': 'problems and we essentially look at class of algorithms, which are known as interior ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69f0ebf21fe63a5ce4e70d8037f68bdd'}>, <Document: {'content': 'point methods. So, I am not really going to get into the math behind all of this, but ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5de0987b6270581170486583a4e170f'}>, <Document: {'content': 'I assume that many of you have actually come across very simple optimization technique ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b23385866bf9a4beee06492639bc4761'}>, <Document: {'content': 'called Newton Raphson method. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38ea131506a11a1cb1254cfcc12255f'}>, <Document: {'content': 'So and here is what the expression for Newton Raphson method is going to look like. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59049975547b59407b09676160e74691'}>, <Document: {'content': 'I start off with a guess for my initial solution, the beta and start of the guess beta naught ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c46ed2c07676146d9a9a07b71e9bf366'}>, <Document: {'content': 'and I would typically like my beta naught to be close to the true solution. And once ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d7dd003409f9b4d7daab0a41542eef0'}>, <Document: {'content': 'I have the guess for beta naught, then I keep updating the solution by essentially subtracting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b3451ad8746d9f2074079338aa7d9d8'}>, <Document: {'content': 'the first order derivative of the likelihood divided by the second order derivative of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '765b9e8140f788f873fce3872e514bc8'}>, <Document: {'content': 'the likelihood. Take the ratio and then, subtract it from beta in order to give me my next estimate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5bd9ca2ac6f60621893a906af6a40d2d'}>, <Document: {'content': 'So, this has fast convergence under certain regularity condition, so for one thing is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8dc0a7fd0069604d7f546021a971a7b3'}>, <Document: {'content': 'that second derivative should like this and should be positive. And as you can see the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0ad3bea951c59e087e67b3d5517b5fe'}>, <Document: {'content': 'first derivative is going to be 0 you are not going to be changing the value of your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '343faf4ea06689edc3276c8447cdd519'}>, <Document: {'content': 'guess and when would the first derivative be 0 it will be 0 at one of the optima whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5e76feec3ca4c56f8202a6e830f0539'}>, <Document: {'content': 'it is the maxima or the minima and you will also like this since the second derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fba8966c72e924b32afa162e5fb4588f'}>, <Document: {'content': 'is going to positive to approach the minima. So, you can till when you approach optima ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d2a7e5b95d9465a360113a77966b80b'}>, <Document: {'content': 'you can be sure there is going to be the minima. So, this is essentially the basic idea behind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '44d75ba79bde2976b62db361cebe5aa9'}>, <Document: {'content': 'the Newton Raphson method and when Newton Raphson method is applied specifically to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1605e0eff7b3abb957d14c1462f43126'}>, <Document: {'content': 'the logistic regression problem you come up with the iterative technique, which is called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c787e50318d79d6c46a66dccd2522673'}>, <Document: {'content': 'iterative re weighted least squares approach for training for finding the parameters in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd99caf517b0f593468ba4086cda78cf3'}>, <Document: {'content': 'logistic regression. And, so most of these statistical packages that we have especially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eedde7da87da031053992ee85966e2d9'}>, <Document: {'content': 'R in particular of front trust was have a very simple function that loves you to fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f31b896ca24de41f17dd384b01fe2a5d'}>, <Document: {'content': 'logistic regression to any data set that you have and they will essentially be using Newton ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eec94946a9e8183a3b94a3a6dac668e7'}>, <Document: {'content': 'Raphson by way of iterative re weighted least squares techniques. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd258b93079210070851121dab1225d7f'}>, <Document: {'content': 'To summarize this couple of modules logistic regression, so logistic regression is very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14904a950abfcc9c0c9bb7f0009102d'}>, <Document: {'content': 'powerful classifier build on the idea of doing linear regression on a logistic transformed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79f919704a7a5f08ed049e6fad72e7fc'}>, <Document: {'content': 'output variables and the logistic regression is related to exponential family of probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc023bebe280bef6d3823424f6a8f8f4'}>, <Document: {'content': 'distribution that rise in the variety of problems and that is the one of the reasons that make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68cb6c987386b9233af6c1faed1491'}>, <Document: {'content': 'them very, very popular classifier. And apart from that they work really well ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e129e10de13ae3849f9729c7c5633e9'}>, <Document: {'content': 'I mean, so that is the another reason that logistic regression is classifier of choice ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a21c35be92041db2398b264b3c97c411'}>, <Document: {'content': 'for many people’s especially in medical domains, because they allow you to perform ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3e5286c8e815623ec0f21c0e53018a3'}>, <Document: {'content': 'what you know as sensitive analysis. So, you can look at dependence of class labels on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22f2e70ba3128f06c623c0d4abd8e127'}>, <Document: {'content': 'features by looking at the, the regression coefficient of specific feature in the fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed436edb44e3e713fe09649fa508ea8f'}>, <Document: {'content': 'that you obtain. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17fa7d068b61155442942751586a7a3'}>, <Document: {'content': 'Training a Logistic Regression Classifier ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5969a0e4b849ad7d41555766ca3a2e4e'}>, <Document: {'content': 'Hi, so we are looking at the module on Training Logistic Regression Classifier now. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abaf5f3524c7386d954c92172cc6513e'}>, <Document: {'content': 'So, in the previous module we looked at the basic idea behind logistic regression, which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1a77a8bb4ddff526d6def081b8c7894'}>, <Document: {'content': 'is essentially to do linear regression with a logistic transformation. So, you took the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f4bbf3bd80636f57e925afdc40fa192'}>, <Document: {'content': 'log odds function, which is p of x by 1 minus p of x, took the logarithm of it and try to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b2ea4add7eca8bf99c12b03af836aeb2'}>, <Document: {'content': 'fit a linear curve to this transform function. So, how do we find these parameters beta, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a24425e9703eacf89d121d975fe54513'}>, <Document: {'content': 'beta 1 and beta naught? So, we optimize the likelihood of the training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b390213bf009996423c8202dc55a161a'}>, <Document: {'content': 'data with respect to the parameters beta, so that is essentially the way we are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1f1b7899eaca11932a4e53ce2788542'}>, <Document: {'content': 'to be training this. So, this is slightly different from some of the earlier methods ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30021101a0403ed67f932de475e0590'}>, <Document: {'content': 'we have looked at identifying the parameters, mainly because we are looking at here the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8d6ceb7dcce14a3f044a554489e960d'}>, <Document: {'content': 'probability of classification, not just getting the classifications right or wrong, but we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8194722082e53dfded99a0e145bccaa1'}>, <Document: {'content': 'are actually looking at the probability of classification, that makes more sense to try ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb99cd3801bd2fcf67b41be9205f0f66'}>, <Document: {'content': 'to optimize the probability of seeing the training data with respect to the parameter ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ee38eb60155de69cb9be67515d947b19'}>, <Document: {'content': 'beta. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6a408dbbd973ab830484c7745c657e1'}>, <Document: {'content': 'So, what is the likelihood? So, the likelihood is the probability of a training data D given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fecf85fc742c38396f747e551bf4cfd1'}>, <Document: {'content': 'a particular parameter setting beta. So, you should note here that, it is the function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82eb59b948bc8a1d2a22bef984ccc103'}>, <Document: {'content': 'of the parameter setting, because the training data D that is given to you is usually fixed. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9335693342a66a71e08f8d122a8869f'}>, <Document: {'content': 'So, here is an example of the likelihood of some kind of classification tasks. So, I am ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '788a845cd0883601db997f2b8e7da610'}>, <Document: {'content': 'going to assume that the data is given to you in the form of x i, y i pairs as we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56dc3af6f4a68528ba0ec714900794bc'}>, <Document: {'content': 'done in the past. So, for each x i there is going to be an outcome ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d4191dff6affe3262750b07e8c246f1'}>, <Document: {'content': 'y i, which will be either 1 if it belongs to class 1 or 0 if it belongs to class 2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a054060153c99771993aa669f917973f'}>, <Document: {'content': 'So, let us look at one term in the product that I have written down there, is you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e93dc1b8d679438f1272851f3df38c0'}>, <Document: {'content': 'see if the output corresponding to x i is 1, then the first term in the product will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed2394b02bd5fedf616450880c456541'}>, <Document: {'content': 'be p of x i and the second term in the product will be 1, because y i is 1 and 1 minus p ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbf3827646a80965ea08658cc00faca'}>, <Document: {'content': 'of x i is going to raise to the power of 0, which essentially reduce to 1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd3efa812fb9c0a60c8eefc557d2704bd'}>, <Document: {'content': 'Likewise, if the output corresponding to x i is 0, then the first term in the product ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20a16296d00af231620a9755d8efb37'}>, <Document: {'content': 'is going to be 1 and the second term in the product will remain as 1 minus p of x i. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8a68a1e32616f2150a3244fc8fa74d5'}>, <Document: {'content': 'this essentially means that depending on, what the output variable is I am going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58fd25e308edda9ecc4d6b2cf3dccd75'}>, <Document: {'content': 'either take the probability of the data point occurring, probability of the data point having ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f238536f5fb481a3e6d28b2e1a12cf0'}>, <Document: {'content': 'a label of 1 or the probability of the data point having the label of 0. So, to do recall ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bfadaa914fc8b8db9464ff31a4a1a96f'}>, <Document: {'content': 'that p of x i is the probability that y equal to 1 given x i. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c048e4838c35fa673180bbe655fc629'}>, <Document: {'content': 'So, now, for this is for one data point and if I want to look at the probability of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb3309e8d48eb4540ba3bba6c5b15f97'}>, <Document: {'content': 'entire data, I just take the product over all the data points, so the product runs from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c78d8c7aeb02bf78b4e900146c4babe3'}>, <Document: {'content': '1 to n. So, this expression now gives me the probability of seeing the training data given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '848e19b714cc162654b9bb5449a1d41f'}>, <Document: {'content': 'a specific parameter setting. So, where do beta and beta naught appear on the expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8791c7d6fa1794bcecd3f1d7f30d581c'}>, <Document: {'content': 'on the right hand side, so p of x i is specified in terms of beta and beta naught. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f05b6e4c97d010fff7e16af98ebc85db'}>, <Document: {'content': 'So, implicitly, so beta and beta naught are appearing on the right hand side of the equation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4951921e81ad89cc05add7ef1bdf8605'}>, <Document: {'content': 'and like I said, likelihood is the function of the parameters and hence we denote it as ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8a7fdec510e295de653494bec9a2b7f'}>, <Document: {'content': 'L of beta. So, now our goal is to optimize this likelihood and, so that, so we get a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26ff17cd60e571258c8fe96aecd19d'}>, <Document: {'content': 'good estimate of the parameter, so we have to find the right set of beta, so that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f8082eba096284e06f602bef5be840e0'}>, <Document: {'content': 'probability is maximized. So, now, we look the term, the term looks a little hard to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6c4a6433138549d3bdd737130b9f46b'}>, <Document: {'content': 'optimize, because lot of products here and, so we have to be little careful. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3818072f788de0ba9a13c9c795c5b2ce'}>, <Document: {'content': 'So, the usual way that we operate here is to take logarithms of this likelihood and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c91b4d5e2aaffbe8b77ced87a5792bc7'}>, <Document: {'content': 'you can see that lower case l here is used to denote the log of the likelihood function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '650b13a4e224ebedd8f516061958c048'}>, <Document: {'content': 'and then, you just walk through this log likelihood expression a little slowly. So, now, that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d3a425151254661b34f08c9ff1b9b8a'}>, <Document: {'content': 'I have taken the logarithms in the first step. In the first step, so I have taken logarithms ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77cb8d27daf290da028a9a4f013e03e0'}>, <Document: {'content': 'and therefore, the products that I had earlier have become summations and the exponentiation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd49274b99deddddd121b229bec15412'}>, <Document: {'content': 'that I had earlier have become products. So, that corresponds the original exponentiation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1369298d35e1a43e40fae0888e35e50a'}>, <Document: {'content': 'I had in my expression and now they have become products. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '702767bdbcaeae09a15b21ad150e6136'}>, <Document: {'content': 'Now, we can do a little bit of simplification here and you can see that, what I have essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '776006c06ad1fb692707f9e0988a7e01'}>, <Document: {'content': 'done is taken the… In the second step I have expanded the product term in the second ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9241e1468a1bceefec4f80323573f261'}>, <Document: {'content': 'term in this summation and then, I have gathered terms together which have a coefficient of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2e1737da927aab08711e6044745beea'}>, <Document: {'content': 'y i. So, that gives me the second term in the summation and the first term is just essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b8f7db80297d8130d9880b70326037c'}>, <Document: {'content': 'one times log of 1 minus p of x i. So, we know what log of p x i by 1 minus p x i is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33e56bd3e6c4e017574c86d51d5c1713'}>, <Document: {'content': 'and that is essentially the function that you are trying to fit from the beginning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '584793b5e32178b518426d351c855a93'}>, <Document: {'content': 'So, we replace that with our linear fit that we had, the linear regressions fit that we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46a047b70c3622660e434159df788eeb'}>, <Document: {'content': 'did. And then, we do further simplification in order to come up with the expression given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8626a8c49a8a86b1543f282be5730e1'}>, <Document: {'content': 'on the last line, that essentially writing out p x i and then, evaluating 1 minus p x ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fee621524b7d8a353b6ce841777d8087'}>, <Document: {'content': 'i and that gives me the negative logarithms term on the last line of the expressions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '307bff3b8d21d0a32be5080f8ffd7232'}>, <Document: {'content': 'So, now, what do we do? We have the log likelihood, so what we do to maximize this log likelihood. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2053fcf11ecf52239142bbe45bd31a1e'}>, <Document: {'content': 'We essentially take the derivatives of this log likelihood with respect to beta and then, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88da08ed61fd296fdf9a3b020cb73cc4'}>, <Document: {'content': 'we should be equating this to 0. So, the first line here is essentially taking the derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1eb4a9db28976fa33fad9345d52e20b'}>, <Document: {'content': 'of the log likelihood and it simplify to a very nice form, which is y i minus probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '649bc8ad2f14fe220e8a0f789b928c7d'}>, <Document: {'content': 'of x i times x i each individual component of x i and now, we set this equal to 0. But, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37a353bcb517ef80cdac76b267415c44'}>, <Document: {'content': 'we really cannot solve this that easily. Why? Because, p x i is actually a transcendental ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c64bf5cd3e336fb52a60d5b9a9fa2cc'}>, <Document: {'content': 'function, so it is not very easy to find the close form solution for these kinds of expressions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a45a3b10f887ed878a03586037e31f55'}>, <Document: {'content': 'So, we have to actually look at numerical methods for solving these kinds of optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f62527d172bc2453f662c0c7730f116'}>, <Document: {'content': 'problems and we essentially look at class of algorithms, which are known as interior ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69f0ebf21fe63a5ce4e70d8037f68bdd'}>, <Document: {'content': 'point methods. So, I am not really going to get into the math behind all of this, but ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5de0987b6270581170486583a4e170f'}>, <Document: {'content': 'I assume that many of you have actually come across very simple optimization technique ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b23385866bf9a4beee06492639bc4761'}>, <Document: {'content': 'called Newton Raphson method. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38ea131506a11a1cb1254cfcc12255f'}>, <Document: {'content': 'So and here is what the expression for Newton Raphson method is going to look like. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59049975547b59407b09676160e74691'}>, <Document: {'content': 'I start off with a guess for my initial solution, the beta and start of the guess beta naught ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c46ed2c07676146d9a9a07b71e9bf366'}>, <Document: {'content': 'and I would typically like my beta naught to be close to the true solution. And once ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d7dd003409f9b4d7daab0a41542eef0'}>, <Document: {'content': 'I have the guess for beta naught, then I keep updating the solution by essentially subtracting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b3451ad8746d9f2074079338aa7d9d8'}>, <Document: {'content': 'the first order derivative of the likelihood divided by the second order derivative of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '765b9e8140f788f873fce3872e514bc8'}>, <Document: {'content': 'the likelihood. Take the ratio and then, subtract it from beta in order to give me my next estimate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5bd9ca2ac6f60621893a906af6a40d2d'}>, <Document: {'content': 'So, this has fast convergence under certain regularity condition, so for one thing is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8dc0a7fd0069604d7f546021a971a7b3'}>, <Document: {'content': 'that second derivative should like this and should be positive. And as you can see the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0ad3bea951c59e087e67b3d5517b5fe'}>, <Document: {'content': 'first derivative is going to be 0 you are not going to be changing the value of your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '343faf4ea06689edc3276c8447cdd519'}>, <Document: {'content': 'guess and when would the first derivative be 0 it will be 0 at one of the optima whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f5e76feec3ca4c56f8202a6e830f0539'}>, <Document: {'content': 'it is the maxima or the minima and you will also like this since the second derivative ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fba8966c72e924b32afa162e5fb4588f'}>, <Document: {'content': 'is going to positive to approach the minima. So, you can till when you approach optima ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d2a7e5b95d9465a360113a77966b80b'}>, <Document: {'content': 'you can be sure there is going to be the minima. So, this is essentially the basic idea behind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '44d75ba79bde2976b62db361cebe5aa9'}>, <Document: {'content': 'the Newton Raphson method and when Newton Raphson method is applied specifically to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1605e0eff7b3abb957d14c1462f43126'}>, <Document: {'content': 'the logistic regression problem you come up with the iterative technique, which is called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c787e50318d79d6c46a66dccd2522673'}>, <Document: {'content': 'iterative re weighted least squares approach for training for finding the parameters in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd99caf517b0f593468ba4086cda78cf3'}>, <Document: {'content': 'logistic regression. And, so most of these statistical packages that we have especially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eedde7da87da031053992ee85966e2d9'}>, <Document: {'content': 'R in particular of front trust was have a very simple function that loves you to fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f31b896ca24de41f17dd384b01fe2a5d'}>, <Document: {'content': 'logistic regression to any data set that you have and they will essentially be using Newton ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eec94946a9e8183a3b94a3a6dac668e7'}>, <Document: {'content': 'Raphson by way of iterative re weighted least squares techniques. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd258b93079210070851121dab1225d7f'}>, <Document: {'content': 'To summarize this couple of modules logistic regression, so logistic regression is very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14904a950abfcc9c0c9bb7f0009102d'}>, <Document: {'content': 'powerful classifier build on the idea of doing linear regression on a logistic transformed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79f919704a7a5f08ed049e6fad72e7fc'}>, <Document: {'content': 'output variables and the logistic regression is related to exponential family of probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc023bebe280bef6d3823424f6a8f8f4'}>, <Document: {'content': 'distribution that rise in the variety of problems and that is the one of the reasons that make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68cb6c987386b9233af6c1faed1491'}>, <Document: {'content': 'them very, very popular classifier. And apart from that they work really well ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e129e10de13ae3849f9729c7c5633e9'}>, <Document: {'content': 'I mean, so that is the another reason that logistic regression is classifier of choice ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a21c35be92041db2398b264b3c97c411'}>, <Document: {'content': 'for many people’s especially in medical domains, because they allow you to perform ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3e5286c8e815623ec0f21c0e53018a3'}>, <Document: {'content': 'what you know as sensitive analysis. So, you can look at dependence of class labels on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '22f2e70ba3128f06c623c0d4abd8e127'}>, <Document: {'content': 'features by looking at the, the regression coefficient of specific feature in the fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed436edb44e3e713fe09649fa508ea8f'}>, <Document: {'content': 'that you obtain. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17fa7d068b61155442942751586a7a3'}>, <Document: {'content': 'Classification and Regression Trees ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79591b60a518374f390f0b46bf7d2078'}>, <Document: {'content': 'Hi and welcome to this module on Classification and Regression Trees. So, today we will look ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9300c9ba1c8d258e4f2efaf38316c730'}>, <Document: {'content': 'at a very simple, but powerful idea for building a both classifiers and regressors. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '95edf556d6785f995becffc4585ecfc3'}>, <Document: {'content': 'The basic idea is that, you are going to partition ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '755e7104d1a483c50b981d0d932927ad'}>, <Document: {'content': 'the input space into rectangles. So, let us imagine that you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a47ebd50f4107cec6535f270ca75f63'}>, <Document: {'content': 'have a two dimensional input space x 1 and x 2. So, you are going to try and partition ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a71760cb6aeb9495ce49e78bd5de6eb'}>, <Document: {'content': 'this into rectangles by drawing axis parallel lines. So, why are we drawing access parallel ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2966289b876f4a8374699006b891af2'}>, <Document: {'content': 'lines here? Because, these lines can be with specified very easily by just comparing against ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd191362bd5144a0beeb73449cd8bfa'}>, <Document: {'content': 'one of those dimensions of the input data. So, for example, to draw this line all I need ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2bee507d3e687047062b9b1e4ef55cd5'}>, <Document: {'content': 'to specify is the intercept at the x 2 axis. So, likewise to draw this line I need to specify ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '99e8fec1ac427ff6bc6d600401e01af3'}>, <Document: {'content': 'the intercept of the x 1 axis. So, one way of thinking about these kinds of partitioning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e58eaf15ad07ae959236ab981e8cfa08'}>, <Document: {'content': 'of the input space for using axis parallel lines is to think of making a series of decisions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cfba97714fb12ae56648f7f83e38a88b'}>, <Document: {'content': 'as to, which side of a specific line is your data point line. So, you can think of this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f89ab5ea33ef2b447bdd28e002960c61'}>, <Document: {'content': 'as following, we will call this say t 1, this point is t 2, this point is t 3, this is t ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '790e1c8f71da8c2638daf1e643e21b2c'}>, <Document: {'content': '4. These are the intercepts along the respective x 1 and x 2 axis. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b3af2a6701e2743cf61279dd9ba9441'}>, <Document: {'content': 'Then, one way of representing this is to think of this as a series of tests or decisions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45f6bafe0c61620d7e608bebe366593e'}>, <Document: {'content': 'that you are making, so I can start of by asking the question, is x 1 less than or equal ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ad1d995852fc03a9bf2cf0278695055'}>, <Document: {'content': 'to t 1. So, that is essentially saying here is a line that represents x 1 equal to t 1 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ba4628cc1a1a6d3e12b171e36bb06e4f'}>, <Document: {'content': 'that is your data point lie to the left of the line or to the right of the line. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a90f1580c4633cf2fc588d1ed26d51a6'}>, <Document: {'content': 'if it lies to the left of the line, so this will be an x, then I ask the second question, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '933255b56c29a881da8815c423ab1bed'}>, <Document: {'content': 'which is essentially is x 2 lesser than equal to t 2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1499e144743ce393a66ee05eaacc7977'}>, <Document: {'content': 'So, x 2 equal to t 2 is this line and I am asking the question if the data point is above ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '326f5a9e2f997e417a2cac7c4a683f7'}>, <Document: {'content': 'this line or if the data point is below this line. So, if it is below the line, so then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dfeba6e890bbfb5728f5f00ca4f379f'}>, <Document: {'content': 'I get a yes for this question as well and I will denote this by R 1, so this region ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '65460f268cacaee9b2ace0e12084d4ab'}>, <Document: {'content': 'is R 1. So, since this represents both x 1 being less than t 1 and x 2 being less than ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dad4d823a35dc0a30163f8929f458809'}>, <Document: {'content': 't 2, it is essentially bounded in this region. So, likewise if x, if the point is actually ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b6d24082f3cdbbba9756f6f64504340'}>, <Document: {'content': 'greater than t 2, so in this case this will evaluate to no and say I get to a region, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '916f45a03207c9c8a22dac1f4bff5053'}>, <Document: {'content': 'which is called R 2. So, what would this region be if you can think ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4af25158530da0f1c4806ef72e0a5fa'}>, <Document: {'content': 'about it? This is essentially the region, where x 1 is greater than t 1, but x 1 is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6907146bd6ad31ad0ce3a78e451352b0'}>, <Document: {'content': 'lesser than t 3. So, I am going to call this region R 3, so here is x 1 is greater than ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90b759ea342fa3e9e5e4ae406d85d7f2'}>, <Document: {'content': 't 1, but lesser than t 3, so that would be R 3 and if x 1 is greater than t 3, so in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a7571b9eb2e3be50136fe19ff25541ed'}>, <Document: {'content': 'this case I am again splitting in to two regions. So, essentially, so I am testing on x 2 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa91cb3f2e43318671679f4decfc939b'}>, <Document: {'content': 't 4. So, what is that you notice about this tree that we have drawn here? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2bdcdd2359174d424807245fdeb6209f'}>, <Document: {'content': 'So, every point I am asking you a binary question, is this yes or no. So, essentially it is a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '14323dc3d0088ee34ae4b3f17abbba77'}>, <Document: {'content': 'binary tree, at every point you divide into two branches. And once I have divided into ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd5c5aab62e401246499e8537ae1a14df'}>, <Document: {'content': 'one of these branches, once I go down one of these paths I am, from here on I am only ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2af78d1f554c623e440ec53621e1b55'}>, <Document: {'content': 'concerned about data points that has satisfy the first question that I asked. So, from ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '27014c9050334c1863994a2a63b403f6'}>, <Document: {'content': 'this point on in the tree I am only worried about data points that are to the left side ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5dc93c19264f9922b52d620908a5aee'}>, <Document: {'content': 'of the line here. At this point in the tree I am worried about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc5b45cdb057c5c936c8f628544f4852'}>, <Document: {'content': 'data points that are to the right of the line here. So, that is a couple of distinguishing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6eea9f545ed216dcc265890e49f7a9a7'}>, <Document: {'content': 'features of the decision tree that I am making binary decisions and I am also looking at ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3372958ef28c9ae27075460f32e69854'}>, <Document: {'content': 'some kind of a divide and conquer approach great. Now, what we have done is that we have ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55c679a01df0be52f2172cb55d10c1eb'}>, <Document: {'content': 'a representation that allows us to split the input space into different regions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c2b1c13d360fe22cdea94feb7b625f27'}>, <Document: {'content': 'So, depending upon the kind of problem that we are solving whether it is a classification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'acc4f1881ccf813bd25566e1296548d5'}>, <Document: {'content': 'problem or whether it is a regression problem, so we would like to fit a single value to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8eb9b0b13438983150250815a9b694b8'}>, <Document: {'content': 'each of this regions. So, if it is a regression problem, so regression problem we will be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '93dd15dec2b218ba5e29089bd9b4c917'}>, <Document: {'content': 'outputting a single value for the entire region. So, if your data point falls in R 1 regardless ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '97e953059b21632adbde2004b9e9c012'}>, <Document: {'content': 'of, where in R 1 it is falling, it is so the data point could fall here, it could fall ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c159650a0fcacf26d55c1c9aa9a206d'}>, <Document: {'content': 'here, it could fall here, it could fall here regardless of where in R 1 the data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45994bb24b3fda66223e1c3f19a2bfc9'}>, <Document: {'content': 'lands up, I am going to predict the same output. So, it would be the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23647239b0a490fbd7ed85a3d940a2c0'}>, <Document: {'content': 'same real valued output for each region, so in the case of classification, what you expect ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f02a9ed977793551e105a86acf4529d'}>, <Document: {'content': 'it to be, it will be the same class label for the region. So, regardless of where in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '798ed663ca5de8438458b4261169b0a8'}>, <Document: {'content': 'R 1 the data point falls I will always output the same class label for the classification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '669f3fa8b18175c5ef0446e9ac6dba0'}>, <Document: {'content': 'problem. So, now, so we have two questions that we have to answer in the case of decision ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '38ba78791d9cc5cbe5a89db317586a41'}>, <Document: {'content': 'trees. So, the first one is how do we form the regions and the second question is, having ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf2cbedcc25969bce5cdc025f0fcda8c'}>, <Document: {'content': 'formed the regions, how do I decide what is the output that I am going to produce for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a432d28311a161f413e35b8760dda2b8'}>, <Document: {'content': 'that region. So, we will look at each of these problems ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8a2222fdcfe5b726452873bd1ddc6757'}>, <Document: {'content': 'in turn, but the first thing if I wanted to mention before I go on to look at, how we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '558971b01ae5384497a6cda93ccc7008'}>, <Document: {'content': 'solve them is that decision tree is a fantastic, because they are the most interpretable of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '719d4e2d329eb0e065d5bff372b03cb1'}>, <Document: {'content': 'all of the classifiers that we are going to look at, even more so than linear regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6f8de36fb52428df29907fe44297fcef'}>, <Document: {'content': 'at some point. Because, if we think of the way we constructed the decision tree, it seems ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90d7da81ba91ec2b330f8cff4096fe0c'}>, <Document: {'content': 'like a very natural way to map it to how humans think about making decision. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd78b8f403009b208fc049c1897f54555'}>, <Document: {'content': 'So, that way the interpretability, so the interpretability of decision trees are very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '523458f59aef8e8621587ea77458a362'}>, <Document: {'content': 'high and in fact, that makes it one of the classifiers or regressors of choice in a very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58dfa3cf6c739e428e10d2781cb9e750'}>, <Document: {'content': 'wide varieties of problems. And the second advantage of decision trees is that they can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c6dc74bddf3904f3c92ee475dca512dd'}>, <Document: {'content': 'work well with mixed mode data. So, here the example I gave you assume that x 1 and x 2 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd435cf0c06717ca07219f0cde24d782a'}>, <Document: {'content': 'are actual numbers and you could pick arbitrary comparison points x 1, x 2 need not be numbers ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'be78b3bfe3e1cced11c64555ed84456b'}>, <Document: {'content': 'they could be a categorical variable like color or it could be age, but represented ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a2f237651df281af87b0df557c887f0'}>, <Document: {'content': 'as young old and middle age. It did not necessarily be a number on which, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e67da7446fe5b811637bd90f7129ed6'}>, <Document: {'content': 'you have to run this kind of test I could compare whether the color is red or not red ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '556742663732af869826a3068a83584c'}>, <Document: {'content': 'or I could look at whether the person is young or middle aged verses the person is old. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0ac2a993293e984646ecfa6613cae9e'}>, <Document: {'content': 'I could have any kind of binary test among categorical attributes and then, I can still ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '414a7be69335a8ad07e1503ee0dccaa6'}>, <Document: {'content': 'construct the decision tree. So, the first advantages one of interpretability the second ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '86941bde2af80b3cb7eded2c82fbafbc'}>, <Document: {'content': 'one the, which you can hand mixed mode data. So, now, let us step back and let us look ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98d88467b981f0f09a62c3068910c293'}>, <Document: {'content': 'at regression trees specifically, so what we know about regression. So, regression the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fbcacc25eac717dbd64be13d63c016c'}>, <Document: {'content': 'goal of regression is essentially to minimize some squared error. So, this is one of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2acbcfd93a4258651e7bdfa2242b9759'}>, <Document: {'content': 'goals of regression is to minimize some squared error will stick with that of course you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b7f8e6c584bc2cb4956215c47e82a90'}>, <Document: {'content': 'build regressors for whatever objective that you want optimize, So I want to fit a function ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18968b874472f637bf4074891f0c238'}>, <Document: {'content': 'f says that I minimize this some squared error. Let us, suppose that I have a tree that has ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d2937eb49609143b018c3703012248b'}>, <Document: {'content': 'split by input space into m regions, which I denote by R 1 to R m, so I have m regions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fdfdca40a19744b9a99a0bfb4f7d936d'}>, <Document: {'content': 'in the input space. And then, for each of these regions, so I am going to output a specific ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd3df55961a1b02a2ebccd98e86d2b53a'}>, <Document: {'content': 'value, which I denote by C m. So, C m is the value that have output for any data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1f851bac6acc232532ce2390a90956de'}>, <Document: {'content': 'that lies in region R m and I here is an indicator function that denotes whether the data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55dbaaee15c44caa58235c5c87a643da'}>, <Document: {'content': 'lies in region R m or R naught. So, essentially, what this summation tells ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '87414e4327128316b0418706f382060'}>, <Document: {'content': 'us is that if the data point input data point x that is come to us is going to lies in one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc6b6139faf697004ac6d2184d5e0dec'}>, <Document: {'content': 'of these regions 1 to m, 1 to capital M. I do not know, which region it is going to lies ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b547dc3ae2bfddfcf891bca92a121f1'}>, <Document: {'content': 'in, but this indicator function will tell me, which region it lies in. So, this summation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd0d646728b912509b12851b25035c7bf'}>, <Document: {'content': 'will be non zero for only one term essentially the region in which, the data point lies in. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2f3a5d5275a29389d4de6b3640eb051'}>, <Document: {'content': 'And therefore, the output will be the C value corresponding to the region in which, the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '583f104e14bd7a427ebe99105175f820'}>, <Document: {'content': 'data point lies in. Suppose x lies in R 2, then the output will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de962e04a82d2b1aef0419df2198960e'}>, <Document: {'content': 'be C 2, suppose I am given this tree already this region split has been decided for me, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9ccaa40b5a07ab72ba2eca5ee73585'}>, <Document: {'content': 'then we know what is the best value that we have to output for C m, so what would be the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bb85a6a6e3d33af8b5ed2d7ee1c3a2b'}>, <Document: {'content': 'best value you have to output for C m. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20eaeea5e479171650601a1b8d7e399b'}>, <Document: {'content': 'Essentially I go through my training data I pick out all those x is, which lies in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb0be358f31ce5a3fd12bffd9ef1b555'}>, <Document: {'content': 'm th region pickout all the x is that lie in the m th region look at the corresponding ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e77edbf8ef2d9bebb25267e2e855f7c9'}>, <Document: {'content': 'y and take the average of those and that will be the value that I output if the data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc50701c9e5e760bf4f1b73e530deb72'}>, <Document: {'content': 'lies in the region R m. So, why is this a reasonable choice well, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ae3f6e1d82486b5aaf8d1964b2c0daf2'}>, <Document: {'content': 'so one way to think about it is when I am trying to minimize the error in a specific ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2eb60090949110b2e82f5f406d7f28a'}>, <Document: {'content': 'region when I am trying to minimize the error in a specific region, let us say R 4 I do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b88282506b9ca042bb37e2666372a1e'}>, <Document: {'content': 'not have to worry about any of the training point that lie outside of R 4, because the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c9b563ad0234cde1615e0805c2caa5'}>, <Document: {'content': 'value I have to predict for them is completely independent of right all these other data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5f9e9351cc50aa5333af68c36cd6d61c'}>, <Document: {'content': 'point. So, I only have to worry about the data point that lie within R 4 when I am trying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5854681265ea9a4decb631ddbcb88b5'}>, <Document: {'content': 'to make a fit for the value there in output in R 4. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4a46d63c35be62d39558105939f8bda'}>, <Document: {'content': 'And among all the data points with lie in R 4 the best prediction that I can make is; ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b21c570ad4fe345152a8856b4fe0a9a8'}>, <Document: {'content': 'obviously, the average in terms of minimizing the some squared error. So, if I have a different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d2d2438fe849af4c3355f96b1befcbf'}>, <Document: {'content': 'criteria let us say among to minimize the median I mean I want to minimize absolute ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '374831846cbdee7554b03473a5fffc84'}>, <Document: {'content': 'deviation, then I probably have to predict the median not the average. So, this part ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23f763ddf89af154ad781ed808965b6c'}>, <Document: {'content': 'is fine we know we are solved one of the two problems. So, what were the two problems one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de9d30b7e7864ed1c398a02d391773ca'}>, <Document: {'content': 'is given the region split, what is the output that have to predict for each region. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a8fa158039fab3f0d459ac823e54e5b'}>, <Document: {'content': 'So, that we know how to do that at least in the case of regression, now comes the harder ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e14c8a5e72c178f0f64ebe5a29c451e'}>, <Document: {'content': 'question, how are you going to find the regions, how are you going to find the best R m’s. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '11fbc6c4a2feeb10932a1e9e87fa849b'}>, <Document: {'content': 'In fact, finding the best R m’s finding the best region split is actually a combinatorial ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33f895084c65a5e87be3115dee4b97b1'}>, <Document: {'content': 'problem when it is actually infeasible and it is going to take a very, very long time ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '20374d74a3ab571b6dbde231632b4445'}>, <Document: {'content': 'to find the exactly the right set of regions. So, quite often what people do is they adopt ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c49cad511b567936dbe4724d1175c77'}>, <Document: {'content': 'a greedy approach to finding this regions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35fc4042ee9e97fd08da332d3d3ee2e'}>, <Document: {'content': 'So, what is that greedy approach to, so you basically start of by considering ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7755d0f28b33f203c2d3b90fbf600c8'}>, <Document: {'content': 'basically start of by considering a split variable, so what is the split variable. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d6eef90584c296b18e5dcb2b2634e05'}>, <Document: {'content': 'in the case of the example tree here, so the split variable at this level is x 1 and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'adf3815b1e99229b71c085860e3eac42'}>, <Document: {'content': 'split variable at here is x 2 the split variable here was again x 1. So, essentially you consider ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7f13405bf87a4562d8030f84ea82243'}>, <Document: {'content': 'some split variable and then, try to find try to find the best split point. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb888ad13a2643d09c8fb46ad4d9807c'}>, <Document: {'content': 'So, what is the split point again, so in the tree that you saw earlier, so in the first ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f26247156ceab2439c7932eaececa4fc'}>, <Document: {'content': 'level the split point was t 1 likewise, so at each level we have to find out what is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b81d6c4dfac4c03beadf9d1e86890a2e'}>, <Document: {'content': 'the appropriate split point is. So, let us take us a simple example, so I am going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dd39e41a639fcd2821ee01c25b210e5c'}>, <Document: {'content': 'define going to define two sub regions R 1 and R 2. So, R 1 is that part of the space, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa07e0c10a3d52eff5c2418fe38521fb'}>, <Document: {'content': 'where the variable x the j th co ordinate of the variable x is lesser than or equal ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa06b78e4a05e1dc4eb28c3cf295a67b'}>, <Document: {'content': 'to some chosen value s. Likewise R 2 is that sub region, where the j th co ordinate of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cab69529475f68ee1df5f2de763e34c5'}>, <Document: {'content': 'the variable x is greater than some chosen value s. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4be8c5bb2d8a2defbfe9b38e43a9391'}>, <Document: {'content': 'So, now, what we are really trying to do is trying to find j and s; such that we can solve ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7ea773b04a52e146b452a9e1410a4c8'}>, <Document: {'content': 'for the best possible split just to give an intuition here. So, in this case think of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57c02ec04daf03f909475dccdc4f1699'}>, <Document: {'content': 'the original data, so I have chosen a split point, which is t 1. So, the s in my choice ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e1d50b9b3da8f4533edc585b1f9e9e0'}>, <Document: {'content': 'is t 1 and this part is wherever x was less than x 1 was less than t 1 and this part of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1f97794ae5541e793bfdfddacac0053'}>, <Document: {'content': 'the space was wherever x 1 was greater than t 1. So, in our new terminology this will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '335688e947856709a7dc62a796b0f02a'}>, <Document: {'content': 'correspond to and this will correspond to, so the one here is, because we are looking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45fc6ebe2f6fbcb5cc49110a2338105e'}>, <Document: {'content': 'at x 1 the and the t 1, because that is the split point of you are considering. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '437d57cc73d3f391330d48cb0795f7a7'}>, <Document: {'content': 'So, now, we have to find j and s both; such that this expression is minimize, what are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6c735e7ef8920eefd5201df2f876473'}>, <Document: {'content': 'this expression. So, we know this. So, C 1, is the prediction I am going to make if the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '65d6b4cf603fef9a76fe4d15bf44a9f4'}>, <Document: {'content': 'data point lies in the sub region R 1, so y i, so these are all data points that lie ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'defdcf08e7d5ac0aa7a79e7408a7843d'}>, <Document: {'content': 'in the sub region R 1, so prediction I make for this C 1. So, this is essentially the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '608a13a29f1cb526629eed0c5faa691a'}>, <Document: {'content': 'squared error for all the data points at lie in R 1 and this is like wise this squared ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a88ae4d272c9d4b099d110f3aabff6f'}>, <Document: {'content': 'error for all the data points at lie in R 2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66b32896af780a496f0d5d430e3518f6'}>, <Document: {'content': 'So, we already saw that, so we can basically find the C 1 that minimizes this error likewise ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b2bd83ea769e154ed4d3e2dce448e25b'}>, <Document: {'content': 'find the C 2 that minimizes that error. Now, my problem is to find j and s such that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42def3d8a3a30caa6bb3142ce7710ad5'}>, <Document: {'content': 'entire expression is minimized some on a little daunting in the beginning, but if you think ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b15d6b2675f1a6e19c5984e3988f8daa'}>, <Document: {'content': 'about it is not that hard, why because we are operating with the finite training set, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5fdcf9aa10a831b3f2cd6d745de3afc3'}>, <Document: {'content': 'like the data that is given to us at the beginning from, which we are going to build this tree ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75347db6f29e540ca0cef6454687c99e'}>, <Document: {'content': 'is going to be finite. So, what does it tell us for every x j that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2625d5a439b3d9b12c2b238e35552d56'}>, <Document: {'content': 'I can choose as my splitting variable there are only finitely many points at which, I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa625de695c4a923fbd68e56e1651be8'}>, <Document: {'content': 'have to consider a split. So, the essentially tells me for every j I choose there are only ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aca78adde2fa4a3667ec3a6e6e5985ed'}>, <Document: {'content': 'a finitely many s that I have to try, why is that because that can only be a finitely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9bbf64dd67dae31a152fcf039606d734'}>, <Document: {'content': 'many different values that the variable, x j can take in your training data. So, essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4944e77278ee43854c013bd383e5b012'}>, <Document: {'content': 'what we do is that at every level in your decision tree you basically look at this expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6fd4ba390de2c3655a1dafe5cc7d4969'}>, <Document: {'content': 'for all possible split points for every possible splitting variable that you have in your data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '692e08dd8c21e3fbbc5fefe92dacfef6'}>, <Document: {'content': 'and then, decide on which is the best possible splitting point. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c34f4dc6d74f3ad3544627558b80cf'}>, <Document: {'content': 'Once the best j and s is found, so what you do next you essentially go hide and split ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f35237d79ad170babd5b4394bdd971e3'}>, <Document: {'content': 'the data into two parts one corresponding to the R 1 of the best j and s and other corresponding ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55a7e290fa5cffdb1dfbc81ad1a8c69d'}>, <Document: {'content': 'to R 2 of best j and s. And now, you repeat this process in both R 1 and R 2. So, now, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4cc6503c262854972395400a1f0e1112'}>, <Document: {'content': 'if you think about it a problem is become much simpler, because the ranges that you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f85d32a92b358aeaed97596711bcb08'}>, <Document: {'content': 'have to the number of data point that you are looking at this much lesser and, so likewise ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a0c38e4c59fe4f30efa2bd7127bdeb32'}>, <Document: {'content': 'you just keep going until you come to a point where you are happy to stop. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f31740246a374fc6036a33fef9045b35'}>, <Document: {'content': 'So, this essentially I will stop here as in this module and the next module look at till ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a65cc1567221349747ed7e3146e890e4'}>, <Document: {'content': 'when we will grow the tree and how are you going to handle the classification. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8a43fe37df4676e18053d60e020fbde'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Classification and Regression Trees(contd) ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d2201c8b4948a9fdfd2818f4cbe93f'}>, <Document: {'content': 'Hi, so in this module will continue looking at Classification and Regression Trees. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45a247f05c6eb502a8cd7155d30f5379'}>, <Document: {'content': 'So, in the previous module we looked at how to build the tree, but we never looked at ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cff263a8ad19de43dfd4a8b792b750d9'}>, <Document: {'content': 'when to stop. So, if we continue growing a very large tree, so we might end up making ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7289cd1cf530ba2ba0b56629b4dbb5ce'}>, <Document: {'content': 'a lot of decisions and essentially end up specializing your tree to individual data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1d8753bab6b166f136859986450c1217'}>, <Document: {'content': 'points. So, it will lead to over fitting of your tree to the data and that is not a good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9a2b03b8df3a30840b430b49c9660437'}>, <Document: {'content': 'place to be in. On the other hand, if you stop growing your tree to soon, you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e41521d9a7e6190d0afb6f9e328859a4'}>, <Document: {'content': 'be sought on finding interesting patterns in the data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e509c2ac03e91cbd4d47f27812a64b2'}>, <Document: {'content': 'So, for example, a very famous XOR case, so there is no single attribute on which you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c450ac90484a3f5a0f2a13836964386b'}>, <Document: {'content': 'can split and get any kind of improvement in your performance, because whether you split ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '245c51aab0438f1cf0be253b49be15ce'}>, <Document: {'content': 'on x 1 or whether you split on x 2. So, half your data is going to be positive, the other ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21a092cafd10c63a258963444e5c14d1'}>, <Document: {'content': 'half is going to be negative. So, you really cannot hope to get any improvement by only ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d25b064222fd80e3b02e3fb37d7693e'}>, <Document: {'content': 'splitting on one attribute. So, you just kind of stop there saying that hey there are no ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '63ecec870fc9b980130615fd3ad58446'}>, <Document: {'content': 'one individual attribute that gives me an improvement in performance and therefore let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '743eff59813260de13a08d8c84138203'}>, <Document: {'content': 'me stop. So, you might lose out on interesting patterns ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c6a7188892ce9d3d113ca78fd0eae43'}>, <Document: {'content': 'if you are going to stop too early. So, you cannot leave a tree that is very, very specialized ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1474ea0b070ed2033327692f1561369a'}>, <Document: {'content': 'to small data points and you cannot stop early as well. So, what we do? So, you typically ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f9e8ce096a835c856f53511203f8891'}>, <Document: {'content': 'grow the tree until you come to a point, where there are few data points in the leaf. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6bc75af38b0dd39f8758a66f061a4a3'}>, <Document: {'content': 'the leaf here would correspond to region. So, I keep going the tree until I come to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82473e4f6098fae42730b918a8d21acf'}>, <Document: {'content': 'a point where each region has only a few data points, typically you would like to have bought ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5bd96efb877d4bfc4c38af66caa78f13'}>, <Document: {'content': 'four or five data points at least per region. So, for using some of the standard tools say ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '130f19977f9213d9897cc250240fb32d'}>, <Document: {'content': 'something like weka. So, weka actually stops by default, when the number of data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e4464b19e83fa07d265cafaa3c17dbd'}>, <Document: {'content': 'in a region go down to two. So, it will stop only at that point, that is still a significant ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '667019332e476a6eb9c3ca63d983fe3d'}>, <Document: {'content': 'over fretting. So, you grow a large tree, where there are very few points per region ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8f63e7b0c51b956ea67a8959667a1ce'}>, <Document: {'content': 'and then you prune the tree, you basically start trying ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13541d07013cc7158b8eaa8d5e1c16a4'}>, <Document: {'content': 'to collapse the internal nodes in the tree, such that you come up with the better tree. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '86e3e25ab5a4a27a5cf9949a60e09100'}>, <Document: {'content': 'So, we have looked at how you need a validation sets. So, you have a training data and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a4b6ba60e88aeccec50d598e849c4e84'}>, <Document: {'content': 'have a validation data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b256286de53d6fd840ba44c64de1571f'}>, <Document: {'content': 'So, one way of looking at pruning which is to say that, I will look at the performance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a4d5b8ed6bcdc33e5bd878c76c16be7'}>, <Document: {'content': 'of the tree on the validation data. So, look at the performance of the tree on the validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e0f00ea4aa694a0e29d85a5b3811ad53'}>, <Document: {'content': 'data and then, I will start collapsing some of the internal nodes of the tree. So, what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f76478e9b7d99b686acc32321c60dd2'}>, <Document: {'content': 'do I mean by that? Suppose that I have a tree that let us think of the tree that we had ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4fd290a7ad9107450481faeddd44514d'}>, <Document: {'content': 'earlier. So, this is the tree that we had earlier. So, one way of pruning something ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '515bfa5cb6a7772577c9edd3d203c2d'}>, <Document: {'content': 'like this could be to say that, I am going to collapse one of the internal nodes and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d8e16b1d241e231c63e4d6b267f2ed0'}>, <Document: {'content': 'replace it with a single region. So, there I have pruned the tree, so this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '539d9569bca3387b7f1263234fdf289c'}>, <Document: {'content': 'corresponds in the picture, this corresponds to doing something like, so that is our four ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f477f7be94f9f77eba49d4be8ccd48fb'}>, <Document: {'content': 'prime. So, now, I can look at the performance of this pruned tree on the validation data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e0f47d18b7514ed02cdcaaad7cc479c9'}>, <Document: {'content': 'versus the performance of the original tree on the validation data. So, now, if there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea83c5873d7278b375f913857e538982'}>, <Document: {'content': 'is not a significant dip in the performance, when I have removed one of the internal nodes, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b8b17d04fbb4eb70d455bc2957db1840'}>, <Document: {'content': 'then I will keep this new tree. So, there are essentially tells me that whatever ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50bd395fad83ddc253994aabd20e5207'}>, <Document: {'content': 'I did in terms of the previous test on t 4 or something that was peculiar to the training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd694e4b889d70f836ea2d70404d888e9'}>, <Document: {'content': 'data. So, now, I look at it in the validation set, where you really does not look like I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37b48be1b7ec00e9bfac7211a37e66e2'}>, <Document: {'content': 'need to make that split. Or on the other hand, there might be a slight decrease in performance, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b0841f407d73b5edb9b592f3919f4d1'}>, <Document: {'content': 'when I go from the original tree to the prune tree. Now, the question you have to ask yourself ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce88384db0f50a92017aa23d1b562b63'}>, <Document: {'content': 'is, given the fact that I have reduced the size of the tree is it to pay the penalty ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85a02a43cadd4d7f45aa46b859b0a25d'}>, <Document: {'content': 'in terms of the slight reduction in performance. So, there is trade off, so there is this what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '396397ee126a5f46efa0e0068c1bfd20'}>, <Document: {'content': 'would call ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c54dfbd64fc6f66bf176b5f6d6be7e98'}>, <Document: {'content': 'the cost complexity trade off. So, that is the cost that you incur in terms of the misclassification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8896c525c42053e36457d2ea1a4957dc'}>, <Document: {'content': 'that or the misprediction in this case in that you are going to make in terms of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8b17db11ca510c316c797fe162f35224'}>, <Document: {'content': 'reduced reduction in the number of regions. And the complexity is essentially the number ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e92f1dd3880476d1056600c11d863af7'}>, <Document: {'content': 'of test that you have to perform on the size of the decision tree that you are operating ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b4c65968f763384bacbb98e465fad1cd'}>, <Document: {'content': 'with. So, this is the cost complexity trade off and depending on which side of the cost ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c213b11de9e2307ea2a6ebdd08c08972'}>, <Document: {'content': 'complexity trade off that you want to be on you can get more complex or less complexities. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c3566bdac874d3d01c0d1291630b4ff'}>, <Document: {'content': 'So, that many ways in which this kind of pruning techniques can be implemented and one simple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '88cc59016b580a50389484b857729c6'}>, <Document: {'content': 'way which is essentially to choose a validation set and then have a tradeoff between the prediction ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca60fb049e59514bd59cce7484f6f5e1'}>, <Document: {'content': 'error of the pruned tree in the validation set verses some measure of complexity of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b6f9b71aa54beb5edd7ebb774df53e6'}>, <Document: {'content': 'tree. So, one measure of complexity of the tree is the number of internal nodes that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8cfdccc51f5b5b442c6ad09b8da791f5'}>, <Document: {'content': 'you have in the tree. So, you can basically trade off the two and try to come up with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a5f22399cfc5a8d4bd4ada24bad2655'}>, <Document: {'content': 'a good smaller tree. So, far we have been talking about regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1885b26b52ff0341c87b9a4ae0d919ba'}>, <Document: {'content': 'trees and we looked at how you would split regions and once having split the found the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5be08e32a3772aad78d79281a8382c98'}>, <Document: {'content': 'regions how do you fit, what is the best prediction that you have to make in each of those regions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '926c0aa147dc0d6c3a522f0aadfef915'}>, <Document: {'content': 'and how would be find the regions, we adopted a greedy approach by picking once variable ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4055bd14e7b97e976108f46ca8cb442'}>, <Document: {'content': 'at a time and then trying to find out which is the best point to split the variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b639263a0da60809102d653e47b7a49'}>, <Document: {'content': 'And we looked at the squared prediction error and essentially now if you want to look at ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bdf1a2c840f0c28b229bf758c833d6c'}>, <Document: {'content': 'classification trees, you only think we have to ask our self is, what is the appropriate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf629e010b220569b669614cf2330bab'}>, <Document: {'content': 'measure that we will have to look at in the greedy such procedure. So, once we have decide ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f16f1eec5404c8305aa474d912f59376'}>, <Document: {'content': 'on what the appropriate measure is, so the rest of the framework that we need to build ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '102fcd873b93cde771a99a1b92d68e66'}>, <Document: {'content': 'classification trees are already in place. So, we can do the greedy algorithm and then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6925ba2fbc3721089ac2c9005d79e9c'}>, <Document: {'content': 'once we know, what is the appropriate measure that we are going to use we can optimize that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '96f38bc204816799201627fbe9c03859'}>, <Document: {'content': 'measure in the greedy algorithm and we can use the same setup that we have for pruning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7cba3cdc7f5b1efffa5a3cd9ef5d6bb2'}>, <Document: {'content': 'So, we have a validation data and then we have a cost complexity trade off, where the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0a5084196c38e5cc5908080fe18641f'}>, <Document: {'content': 'cost instead of being measured by this squared error is going to be measured by whatever ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '48c3d02d25e81bcaf611581891a326ae'}>, <Document: {'content': 'metric we choose for a classification. So, I am going to say that I will build the classification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2221a26767b49538487d59c2e637b0d'}>, <Document: {'content': 'tree where the prediction. So, which we will called as p hat, so p hat here is the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f977eebe3759aaef79b7c2524b9f957'}>, <Document: {'content': 'that a data point in region m belongs to class k the data point in region m. So, one of these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5f6d8c0d59c3bd8854959083c0715bd'}>, <Document: {'content': 'regions, what is the probability that this data point belongs to class k. So, I am going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f9c7b8021ab984bc7a98f0b358fe0cb'}>, <Document: {'content': 'to look at all the data point that is fall in the region R m. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1f23908059a1e7c22b0d06ce2fd97e68'}>, <Document: {'content': 'So, I am going to look at all the data points if fall in the region m and look at whether ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9fa660a0ea412e073de0a3a1cd39e42b'}>, <Document: {'content': 'their class labels in the training data, whether it belongs to class k. So, if a point x i ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5e79b777cb45b8ff8078e31ee2f6548'}>, <Document: {'content': 'in region m, if the label is k then this will be 1. So, the summation is essentially going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e9661ce78f278a37c22ffba94b4765c'}>, <Document: {'content': 'to count the number of data points in region m that belongs to class one or number of data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3428d4ae03a6ad0e435abcb539bcb0e9'}>, <Document: {'content': 'points in region one that belong to class two and so on and so forth this expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3110a24eb15ce4016afa401ed02f100'}>, <Document: {'content': 'is for class k and then I am going to divide it by the total number of points that fall ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c2844b9c8aa325d3f741d2bdec4ce07'}>, <Document: {'content': 'in region m. So, N m is the total number of points in region ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5746248309c53b1107a5ecdcf022d763'}>, <Document: {'content': 'm. So, this is essentially gives me an empirical estimate of the probability of a data point ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b0a7387ebeab65f61df5cd2dcc724e20'}>, <Document: {'content': 'falling in region m belonging to class k and if I am interested in actually returning that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c7405dbeb63e61505577c5447f0941e0'}>, <Document: {'content': 'class label then... So, the class in region m is going to be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6386adc448bffff682f2d44eb1e6efc8'}>, <Document: {'content': 'the class that I will assign to data points in region is essentially that k that gives ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e363fcb40e3f931e78a79e00f4c770f8'}>, <Document: {'content': 'me the maximum p hat value that make sense. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aff39c6ddff23d93c1754a43bf29683'}>, <Document: {'content': 'So, you look at the different error measures that we can use. So, what is the most popular ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8bf3be5cac955609cd44a7430d986c90'}>, <Document: {'content': 'error measure in classification or which is the most appropriate error measure in classification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1aabf573ccce6b3e47ffd30d90c0b4d9'}>, <Document: {'content': 'is a simple thing called a misclassification error, the misclassification error is given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c8c27dfb6128d7c2e3985c1172d7a65'}>, <Document: {'content': 'by... So, the number of times the actual label does not match the prediction that we make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6bca8c027cb0924068f72fd30af725c5'}>, <Document: {'content': \"by our classifiers. So, class m is the prediction made by our classifier in the m'th region. \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a42e60f9311c346876d580b4b2b59652'}>, <Document: {'content': \"So, for all the data points in the m'th region whenever the class label the true class label \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e7814e242699bc010d95e8c6ccd542f'}>, <Document: {'content': 'does not match the predictor class label. So, I am going to sum it up divided by the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6d8ae0c9412c9fb846ebc4fb4e530bb'}>, <Document: {'content': 'total number of data points in that region. So, this gives me the misclassification error ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b727504653446f997ca8c0e37b2b2a45'}>, <Document: {'content': 'this for a specific region and I can do this over all regions and that gives me the total ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32bc9a38814268873f590e508b40f484'}>, <Document: {'content': 'misclassification error. So, if you think about it this is essentially... So, the number ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '85cac8881e3a05777bc5c124660c1ede'}>, <Document: {'content': 'of times this is should have been correct is given by p hat m and the class that I am ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7299086e18a564ce35d58faf6d0a61e0'}>, <Document: {'content': 'going to predict. So, this is the given this gives me the maximum from the previous expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c9a23716dd32a7ec129fe62f5f16b9d5'}>, <Document: {'content': 'that we had. So, the total misclassification error will be 1 minus p hat m. So, I can use ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64ea5d24896640e097d1136c950190ae'}>, <Document: {'content': 'this as my error measure. So, once I estimate p hat the once I estimate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6cb257d4145f783a9bb2369ada12f7de'}>, <Document: {'content': 'the arg max over k of p hat m k then 1 minus that gives me the misclassification error ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52ac968d10c184ba7d1f21e1bf8e957b'}>, <Document: {'content': 'for that region and I can sum this over all the regions. Whether there are a couple of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50e7e26a71226ee60a64928c33282318'}>, <Document: {'content': 'slightly more common error measures that are used especially in the decision tree literature. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6bc0fb9eecc1ea304a4867f71452be48'}>, <Document: {'content': 'So, one of these measures called cross entropy or deviance that leads us to something called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8cd7fdb9442a8e1b8070867844ad1a72'}>, <Document: {'content': 'information gained measure for looking at the splitting criteria. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a4d73aa10f356bec19aff0cde376981'}>, <Document: {'content': 'So, the cross entropy is given by... So, this is p hat m k is the label distribution that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ff66cd982fe545a7716ff78644cefadf'}>, <Document: {'content': 'we have inferred is a label distribution that we have inferred from the data that was given ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '79e1db0b2276dc88f79700141b02f7b0'}>, <Document: {'content': 'to us, but we stop and think about it, if you have sufficient training data. So, that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6444c5dab8ae29b92c01d3a9a6970f0'}>, <Document: {'content': 'p hat m k is actually the true output label distribution as well. So, this cross entropy ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c5acf73a1dbba6f67a26cd2260fe9222'}>, <Document: {'content': 'term actually gives us in some sense the amount of information that we need to encode the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30d6435bde796a808e75d5296670fcfb'}>, <Document: {'content': 'true label given that you are in the region m. So, this gives raise to this measure called ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c50ad8ed3a1d6359d9738a70131a514'}>, <Document: {'content': 'information gain that tells us that if I have not split into the following set of regions. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80ee22bc89e927b6914ecaaf799ad677'}>, <Document: {'content': 'How much information would I need to represent the labels of the data verses having split ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e079c56391f93091a2eaa98004c9126'}>, <Document: {'content': 'into all these regions? How much less information do I need? If I do not know anything about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'be727f078679373c1e031fb8003c9ea5'}>, <Document: {'content': 'where the data point lies on this entire plane then I have some amount of uncertainty about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4fd1c837eafd68c1cd384927495b2cc1'}>, <Document: {'content': 'what the label is. Once I know that the data points lies in R 1 or R 2 verses the rest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4a58656290933f82bea7415d4dc2073f'}>, <Document: {'content': 'of the plane have less uncertainty about what the label should be and... So, given that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '69204b08a70ea18d41c3da3283483784'}>, <Document: {'content': 'when I have split the data into two regions I have gained some information about what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66bfeb246578489fc7b3a95367d2f364'}>, <Document: {'content': 'the label should be and that leads to this notion of an information gain measure. And ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef444c215764c07721f99f4952b2cef9'}>, <Document: {'content': 'so it comes from this cross entropy term and we can use this also as a measure for a splitting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1dce19cb2f2aa703fd4edb9371105a62'}>, <Document: {'content': 'the tree in the greedy growing stage. Last one is a Gini index is given by the expression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9cdb9d554387209bd64ca2d9f6ffc2e9'}>, <Document: {'content': 'p hat m k into 1 minus p hat m k and it is actually a measure of the disparity in the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'be7ef586827889e9a2e5deaf1cc0f6e3'}>, <Document: {'content': 'population of distribution of variables. So, if you think about it, so this is the probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31112d72675808df956cfe7ff1a01771'}>, <Document: {'content': 'that the label k appears in the distribution in a particular region m and this a probability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7d6df4ba264c61fcc1fb33c774e46d82'}>, <Document: {'content': 'that the label k does not appear in the distribution m and this expression will be maximum when ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '30219a12d268eb6209cfeac525e2b652'}>, <Document: {'content': 'they are equally likely like the problem there label k appearing and the label k not appearing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '448d60bb330da3fd5cb36849e81d7c51'}>, <Document: {'content': 'in the region m or equally likely. So, that is really a bad situation for us to be in. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61e69ee139a533018fe973119295bb71'}>, <Document: {'content': 'So, in the some sense having a high value here essentially is a indicator that this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '99e278c7919354117889d11077f09ade'}>, <Document: {'content': 'is not a great way of splitting the data into different regions. So, when you could use ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c15e9709bfc3ee2a0206f95d018a1895'}>, <Document: {'content': 'one any of these three measures in terms of going your decision tree and they have their ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80adc7eefae17b36cfa7e534ec4982d4'}>, <Document: {'content': 'own advantages and disadvantages. And so the cross entropy and the Gini index or more sensitive ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92eb6d2d1063a828ff517acffc494bf'}>, <Document: {'content': 'to note probabilities than misclassification rate, but then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd274d82ea0615233867a8a9e93854c68'}>, <Document: {'content': 'quite often we find that these lead to much better trees than using the misclassification ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bd7e48a2321ce6d7ccd1586ad15e24f2'}>, <Document: {'content': 'rate directly. So, that brings us to the end of our discussion ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e2efc6bc2f58cfb359791e5bd9b0679'}>, <Document: {'content': 'on classification and regression trees, but there are few points that I would like to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7aac49a2e5fd3b29fc043d79978a14c4'}>, <Document: {'content': 'make about the use of tree based classifiers or regressors. So, the first thing is I glossed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80c7f76e880c4d27d8252437df0191ef'}>, <Document: {'content': 'over a little bit the problem of handling discrete valued attributes I said could choose ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd75efaa20c76d6fa43fa1fdacc1e112a'}>, <Document: {'content': 'red or not red, but then if you think about discrete valued attribute that has k possible ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ff1447064afbf76365a82d0d0991f66'}>, <Document: {'content': 'values, you can see that there is a combinatorial many ways of dividing these attributes into ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f563c9938adabb148d5be20249fc4987'}>, <Document: {'content': 'two groups. So, you need to have clever way of handling this and then many of the decision ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6cbb8147f96e05146a4c67906047df'}>, <Document: {'content': 'tree packages do handle this in a meaningful way. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0fcf54989da102eb23bfbf3f0a2139'}>, <Document: {'content': 'The other thing which we have to be very aware of when you are dealing with trees is that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed54542a8e089a2b4e57f0d3a0200ee1'}>, <Document: {'content': 'trees are notoriously unstable. So, what do I mean by unstable is that if there is a very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68770874cc716147043e2f4027644f98'}>, <Document: {'content': 'small change in the training data that you give to the trees the tree that you build ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ab2c1a96ecff572e1e8ec7d3ed07aa82'}>, <Document: {'content': 'out of the data could be very different. So, you could delete a few data points and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ab5a59b138443faf541a8878e932df80'}>, <Document: {'content': 'might end up having a tree that looks very different. So, this is in contrast to some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e602e6359ec39aa7100f4fff50cab4f6'}>, <Document: {'content': 'of the earlier thinks that we are looked at like logistic regression or support vector ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '54b1064e175fe914610823dfd1db5ae1'}>, <Document: {'content': 'machines, where these things stable to deletion of one or two data points, but decision trees ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8bd690152c3f7a008d7a108ce8bc4709'}>, <Document: {'content': 'can be notoriously unstable. So, one way that people get around this problem ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f54287eeb7dbd76cf496602aa90917e7'}>, <Document: {'content': 'of instability in decision trees is to make sure that you build many, many different decision ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3cd4ea13d6af1dced9d4db2e934d1720'}>, <Document: {'content': 'trees with slightly different views of the data and then combine the predictions made ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4c1d754943c01e7e1cbe10fd2fffe13f'}>, <Document: {'content': 'by all the decision trees into a single tree. So, that way we can actually end make sure ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7574d8f426aaecb81bbdce03156728e1'}>, <Document: {'content': 'that the overall classifier that you build is reasonably stable and another point that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef6becd2efab674559620d0568729f2'}>, <Document: {'content': 'we have to look at suppose you have fitting decision trees using regression. So, you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23b73e0458ddfe0254d8fee3d2dcfbf9'}>, <Document: {'content': 'see that for each region we will be outputting a specific constant value at no point or we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40562ae30a1782f1fccbc7a04ae233fd'}>, <Document: {'content': 'worried about how will the value change from R 1 to R 2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'db087941e8fb19666d781d4fe93fc67'}>, <Document: {'content': 'So, you are only worried about what is happening in R 1 and we are fitting a value in R 1 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd94aa34a229104c3822cd89e1fa24212'}>, <Document: {'content': 'therefore, there is no concern of smoothness in our faiths and therefore, the fits that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e141981211fed3e75bea1a0fec6803d'}>, <Document: {'content': 'given by decision tree can be pretty non smooth it is a smoothness is a criteria then we will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e4f51c167dc604437cbf98933c3f0478'}>, <Document: {'content': 'have to either add some more regularizing factors into decision trees which makes it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42fae6f4dd78c8ba2dc57716e280d35a'}>, <Document: {'content': 'more complicated or will have to look at other forms of regressions. So, just keep in mind, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9bf3dc134d5a10de3b7b7e5f6a68a4f4'}>, <Document: {'content': 'so decision trees or very powerful classifiers, very powerful regressors they are wonderful ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94aa5c3c469983b9001784c106a8c2b6'}>, <Document: {'content': 'in terms of interpretability, but they also come with their own caveats. See you later. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd83fa793f58f2e8b0b836a7dfe737cfd'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Bias Variance Dichotomy ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f8c904b4778ce326d79b0530be471c7'}>, <Document: {'content': 'Hello and welcome to our lecture on Bias-Variance Dichotomy, this is a conceptual lecture. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57567ba1fa475f886bcb1ac3d41a36c0'}>, <Document: {'content': 'you must be right now going through lectures, where you are learning many of the machine ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59dc0f4afd49652f56bd2713497c122e'}>, <Document: {'content': 'learning tools and techniques and this is not one of them. So, we not using a new technique ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d3e392a4189f64532cf8bbe3923b13f'}>, <Document: {'content': 'in this lecture, but we are introducing a very important concept that is machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '594f2a02856f8c5ecb4511f5de5a4f22'}>, <Document: {'content': 'and therefore, it applies to all the techniques I would say that you are learning and I would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f07a89aa1e072d75b48fd003b75ea681'}>, <Document: {'content': 'go in so for saying this is probably one of the most important concepts that you should ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9a09c9790789d468a9491fb340cc9650'}>, <Document: {'content': 'understand in machine learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5593aaa87fcf7091bf43ff2decf634ee'}>, <Document: {'content': 'So, let us derive into what the core concept, the concept can be said in very simple sentence. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56487c92dd0840797896488e40506eab'}>, <Document: {'content': 'The idea here is that while adding complexity to a model, while if you want to keep adding ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3dcceae7efb7553bb66cbd3eaf93137'}>, <Document: {'content': 'like say complexity to a model, you might improve the fit of the model. So, you might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8cc8e01567efa1139bf2f35d86a1a2d'}>, <Document: {'content': 'find out that you are better describing the data, but that need not improve the predictive ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8144f962e2fd0a1bcdd5dbf053ea3f5f'}>, <Document: {'content': 'accuracy of this model when you compare it to new data that you get and this concept ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df288d9bf6587406f572944d89e3a5a'}>, <Document: {'content': 'is to for whatever type of model that you look at. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '639da2d1ff1289b47a5b33c0e02eeb45'}>, <Document: {'content': 'So, you can take an example on linear regression and you can add complexity to it by either ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd20b57c4f690e52d14dc8779447db7cc'}>, <Document: {'content': 'adding more input variables or even with one input variable, you can add more complex transformations. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '668bdb053262fdce9d8f8c441636813c'}>, <Document: {'content': 'So, for instance one way of just adding complexity to a simple problem, where you have one input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd4907d4e2e629655eb445097b959af75'}>, <Document: {'content': 'variable, one output variable is that saying I am not only interested in looking it through ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b60389e2df8a589abeaee1667b302bbf'}>, <Document: {'content': 'the standard input variable, but I would like to look it as a polynomial. So, what is the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '805fea7792316c49f898f019f8150f6d'}>, <Document: {'content': 'model when you have x, x is the input variable, but you can also take x square, x cube and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6044c53dff85ddbc7cbccb975eae021'}>, <Document: {'content': 'so on. So, you can have a more complicated fit between ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0161230e15b633a99bad7977d1e67f7'}>, <Document: {'content': 'y and x, because in simple regression you always see y is equal to m x plus c, but what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b85661afeabcfb9835863f57ed34941b'}>, <Document: {'content': 'we have y is equal to m 1 x plus m 2 x square plus c. So, you can add complexity that way, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cea290f83ede0e7c984dbe10f5ba3b6d'}>, <Document: {'content': 'you can add complexity by adding more variables. Now, this is again not confined to the regression ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e36a1a31943a28d176ae8fc875015e7'}>, <Document: {'content': 'any more. Almost any method that you take, you will typically find that there is some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '26b77a4db63b3a86f6ae758b16982dd2'}>, <Document: {'content': 'way of getting more and more complex. So, for instance you might have already covered ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77dd82738924ad97db85e7beab54aafe'}>, <Document: {'content': 'trees, classification regression trees. You can add more complexity by, you know creating ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b40e9e71b140cb8d51be0e4014300916'}>, <Document: {'content': 'more and more and more branches to the point where you have such a complicated tree, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '151bed04f4e081f26d1450fdbb083d41'}>, <Document: {'content': 'have such a large tree where each terminal node or leaf is a single data point that we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2451c35109f8eac39e0cb53c1a6a8b18'}>, <Document: {'content': 'are using in your training set. So, you have really the more complexity, but k need choices ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50bb4ce9034613f050d0a89f94debd6c'}>, <Document: {'content': 'k could be assigned complexity. With neural networks something that you would learn the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '231d89f206c813a830390d26628c5886'}>, <Document: {'content': 'future, the number of layers in the neural networks can make the complexity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c91aa292124e0e2acd646ad6abd9c7a'}>, <Document: {'content': 'And this idea of complexity would become more clear as we, you know talk through some example. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9eaede9a288c6e8d786be0bfad88409b'}>, <Document: {'content': 'But, the idea is that you can make the model more and more and more complex, the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '45926e9877fed5ec1b1b1b0f2c1b344'}>, <Document: {'content': 'that you are going to use to create this relationship between input variables and the output variable, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b98a6983cb70dba4a21608c480669400'}>, <Document: {'content': 'nothing become more and more and more complex and the more and more complex it becomes you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6a03d33402f61385c982de413ee111d'}>, <Document: {'content': 'will do a better job of fitting the data that you have. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '75831acc99162480b45eb036f4050b26'}>, <Document: {'content': 'But, that does not mean you are creating a better model and the answer is might not, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce376b6c46485798d38d898a1b32c4fa'}>, <Document: {'content': 'because while in might fit a data better that you have, tomorrow we need to predict using ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ad11cf5396541c5b7cb89e3731f6d16'}>, <Document: {'content': 'this model, you might not do a better job of predicting and we are going to see how ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '479d2b8811171e065a4735c71b38df77'}>, <Document: {'content': 'that can happen that can possibly happen. Now, this core concept in machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77d5389206a66ac28b19a4e81da6c98b'}>, <Document: {'content': \"is also sometimes referred to as Occam's razor that is more of mathematical concept and it \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '783be2c7fb5c0c67a86c6401bd1e6078'}>, <Document: {'content': 'is definitely used permanently in machine learning, And the idea there is not which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52c81b1900cd2544c0282ce9740571f'}>, <Document: {'content': 'is that if there are two models with equal predictive accuracy, then you prefer the model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cca71f9f237c0caa8cf52182490e41c9'}>, <Document: {'content': 'that is simpler that is less complex. So, that is not go back, but you see how these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c57505fe80debd863775062f3d445f0e'}>, <Document: {'content': 'two highly related concepts, but the concept of bias variance dichotomy, you are essentially ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a6abf22b3fe1d9db4dcfb3ae63f7746f'}>, <Document: {'content': 'questions saying that I can add more complexity to the model and it will look like it is doing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f905e9b3f1555a7eb3b804d2e7551953'}>, <Document: {'content': 'a better job of fitting a data, but am I getting better predictive. So, let us actually you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '98c486562b73ae517a56e081c62f96f5'}>, <Document: {'content': 'know understand this through an example in this particular model in the two tables that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c8c934088e4d892f025782e9c2ae8359'}>, <Document: {'content': 'have shown you here is model one which is your good old linear regression that you know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a64c99e1a119796eaae461b361489bac'}>, <Document: {'content': 'and here is the data set. So, use the same data set which is the same ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33c45675aabab492402b0cd88ec662c5'}>, <Document: {'content': 'x on the right hand side and the same y, but I say that I do not necessary believe this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f933d5d1220df04c0451a86a2228dbe5'}>, <Document: {'content': 'is the right model that is model one is the right model. What are the relationship between ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5e11a8c820329113bfa2764881e9a9d'}>, <Document: {'content': 'x and y were more complex. So, I added an x square term and x square is nothing but ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cac6766ddaabd9101694ae6950fd658a'}>, <Document: {'content': 'it is really simple, it just I take x and i square it and I created new column and like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2908f03715fb62b7bfd2775a7ee64438'}>, <Document: {'content': 'that I keep on adding columns still x to the power 9 and finally, I have all these as potential ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd46e39ca5c124b4f63db6dd7fe288901'}>, <Document: {'content': 'input variables as described by this model. So, y is some function of all these parameters ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5182fc60c607cc296c7ebad1f529de7'}>, <Document: {'content': 'and I do a multiple regression on that. So, how does it work up? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7135a3125e3b4eb384e9bb9ab09e3cba'}>, <Document: {'content': 'So, here is my linear model, here are the data points that you saw before and here is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2978b660f9de1b3f30e8d33cd19dde03'}>, <Document: {'content': 'the that line that you see is the fitted line that goes to these data standard and so this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5548946f551a8712b2acab24a99a7906'}>, <Document: {'content': 'is what you get, how this what are you... So, the polynomial the one that I created ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f203ba7145a415ddfc1f8fc308285bcf'}>, <Document: {'content': 'before is actually an ninth order polynomial. So, how does that will work? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bda19412a02483c478ad5f6bc0e14f3'}>, <Document: {'content': 'Well it turns out that it does a really good job of fitting all the data points as we can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d4dd487c7e0c1a9b9f618c9bfe5f3c3'}>, <Document: {'content': 'see this ninth order polynomial described by this black curve goes through every single ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70eb69f5ca95910e9dd6f20c1d4fc8b3'}>, <Document: {'content': 'data point and that should not be surprised, you have ten data points and you using ninth ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '894eda0277ce0b5b030879c9fa8562a9'}>, <Document: {'content': 'order polynomial fit it there is enough flexibility in the model, in the coefficient is enough ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56a587723f90597fc2e9b87769607fca'}>, <Document: {'content': 'complexity in the model where you do not you can actual go through each data point, here ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '208bbf385b0f12939b6107f681b52fea'}>, <Document: {'content': 'in the first model it is a straight line, even if it wanted to it cannot even if I had ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7799d8970a416089fe4fc73b210f268b'}>, <Document: {'content': 'the flexibility to put this line wherever you had I can move this line up and down I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eaccbb792305fb19f091d3270dbcbab1'}>, <Document: {'content': 'can rotate this line, but the best job that I can windup doing is actually the line that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e40b78dec9433f5a47a8fb84baec13d4'}>, <Document: {'content': 'you see on this screen. So, I did a simple linear regression which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '382ccf23343ef9e763e4d7f4a4d64d59'}>, <Document: {'content': 'does try to you know fit as many data points as possible and this is as best as it could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '595337541e0f3ab8ca66361208c03394'}>, <Document: {'content': 'do now with the ninth order polynomial, where it tries to do that it does really good job ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '679c65c78d5dd601102d5aa8147eb275'}>, <Document: {'content': 'it fits all the data points. See you sitting there knowing like walk, so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '21efc2881104aef0056111d28ec97138'}>, <Document: {'content': 'may be my system is ninth order polynomial, but it is not I let you learn a secret, this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '728e2f16ecb5afbe89e9e352220979d5'}>, <Document: {'content': 'actual data between x and y was created by an actually a linear system with some amount ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c73607de15fbe97f8a96e2cfdfb8697'}>, <Document: {'content': 'of noise. So, you truly equation the true relationship which because I am the God out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c28c8aa41db587a5b6a4215e5aeb39d'}>, <Document: {'content': 'here, I am the one whose actually creating these data points. So, I have this oracle ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '376c689efd9549a3c261340a5077b4cd'}>, <Document: {'content': 'I am letting you on the secret that I actually created that this data points through some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90743034b246148f86afc71bb69642cc'}>, <Document: {'content': 'y is equal to beta naught to b naught plus b 1 x and not x square and x cube, but then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f413e526ec92dba3d605f9eab5c6abb1'}>, <Document: {'content': 'was some error, some noise like any regular system. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd894598f1188af02a2308497fc1e4800'}>, <Document: {'content': 'So, given this let us it looks like this the ninth order polynomial still developed better ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d767838be133571074872616ed853d0'}>, <Document: {'content': 'job of fitting, the fit is great. But, let us look at how these two models compare will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb42fd47c20fdba5e42ec545fb1da1e8'}>, <Document: {'content': 'they have to predict that another occurrence, here is the fitted model, the same graph has ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df57b0634b422097fc4839fbc26aa02a'}>, <Document: {'content': 'above in the linear model. But, I created a new data set I created new data set and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3753af00314afa22fbd583b8258201d1'}>, <Document: {'content': \"I see how well my line does as a job predicting what is going to happen next's and the answer \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b708da1d3e7234dda84554085c4fb3dd'}>, <Document: {'content': 'is it does not do too bad. So, this is the predicted line, the blue line is predicted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f77c9dc83c3142d02b343510ba0950a'}>, <Document: {'content': 'line that I got from my training data. Now, I am go get new data it looks like I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5b79b4d908888f08061e38eb4c76f2c9'}>, <Document: {'content': 'miss targeted couple of times and I probably should expect that given what I know right ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '514c78d1e97a8e009bbf7a43d4dc7faa'}>, <Document: {'content': 'now that there is some amount of noise in the system that is irreducible. But, both ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '238790c5faf31a2ae891a1914d155a40'}>, <Document: {'content': 'this basically means is that tomorrow when you come to me with saying that hey 4.5 what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35b1d7f3a6d4ef38748e71fd13e6afe0'}>, <Document: {'content': 'you predict I say I am going predict this value and in reality I windup seeing this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc68026c9cbd662fd2730246dc368802'}>, <Document: {'content': 'value that the line in red is what I windup seeing in the field when I makeup predicts ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66aa64c90d4d7d5793dc46dc7526d916'}>, <Document: {'content': 'in the prediction make is where I windup, what I windup using as the blue line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4d182c2357f67eb009c69cd35354a212'}>, <Document: {'content': 'Now, what do you see in terms of predicting with the ninth order polynomial, you actual ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd04cad140694194192b45d4d21cf1a78'}>, <Document: {'content': 'do quite terribly bad I mean look at this data point. So, this data point where it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c3104f49f774a7d38226db31dd90ea'}>, <Document: {'content': '1.5, my prediction at 1.5 would be something close to minus 40 minus 39 or whatever. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32c39ac3df3157ce02886ac2953a2d9c'}>, <Document: {'content': 'add 1.5 if I were to use this ninth order polynomial as might fit it model I would be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '789a91d4525322684f51be0351f85a61'}>, <Document: {'content': 'predicting minus 40, but look it what I actually got, I got plus 50. So, I was almost of by ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3bc7765ff7853e791241dd408b9c5e1'}>, <Document: {'content': 'x 55 units, whereas in that much with the linear model and you are going to see the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33a0825dd731e3bbe00d7aefe00a3850'}>, <Document: {'content': 'same kind of we are practitioner. In fact, while at something like 0.5 this model just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84f33d8b6a6af1193e049c3d9097b9c7'}>, <Document: {'content': 'goes through the roof I cannot even fit it inside the graphs. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca940a007d1bd101bd1848deed82409a'}>, <Document: {'content': 'So, clearly the ninth order polynomial while is doing a great job of fitting does it very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59fa262db1ee4eb1c7bda8c181b0c970'}>, <Document: {'content': 'bad job of predicted and if my goal more often than I should say bad, but I did go more often ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d7241e6b8865e88f1032c2723407c28'}>, <Document: {'content': 'than now is to really come up with good predicted accuracy. So, the most machinery from judges ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37167031b46edf0f0aa423cccc0126bb'}>, <Document: {'content': 'not trying to fit a model to the data that does not buy you the much, but you want to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '24e2c5f89d865a944cf430b90e0bfbd9'}>, <Document: {'content': 'pay to model that can be generalized to other situations. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f193378c16a48c790fe1cfc4aa154f3'}>, <Document: {'content': 'So, tomorrow when you get a data set, because what you going to do with the model here either ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5702ae41f253f40a97d1da0ff4a7dd2'}>, <Document: {'content': 'going to predict or either you going interpret the model in either case you need to acknowledge ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3661cac329b789e9a2dc2ee67372b41a'}>, <Document: {'content': 'that what you have the data that you have is nothing but, a sample and if you take another ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa686d59eee86b76ecfd94ce15f0f3f0'}>, <Document: {'content': 'sample I if it turns out that you would have told a completely different story they may ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8bbdea9803baf269a10e2d4fc9460c68'}>, <Document: {'content': 'be the way doing things is not really correct, my whole points is that for instance, if you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '631e6126baddcb20b130aa7704e688f1'}>, <Document: {'content': 'had seen the reds star data in the linear regression model you might have created a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62e1f588c075922bad72b98b075c1536'}>, <Document: {'content': 'slightly different line may be the line would have looked little bit like this. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '78224748b48ecda7154b2025f95987ee'}>, <Document: {'content': 'But, think about what you might done with the complex polynomial you might have created ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3f3bf38477b3f7e9633ef292b7740592'}>, <Document: {'content': 'a you know completely different polynomial function that look than again will go through ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd498192e5befc0786c29bab598f8274'}>, <Document: {'content': 'all the red data point, but if for one sample you create one story and for another sample ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '790af1b4443c83a33b6c649496698fc9'}>, <Document: {'content': 'you create completely different story can may be the way doing thing is not a really ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4161c7ddf1dfa2e2c68e1975d6f2ecd5'}>, <Document: {'content': 'accurate. So, here is what we shown you the system where it was truly a linear system ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f8de0b98cbbdcb02b79825693a46abeb'}>, <Document: {'content': 'and clearly how using a linear model made more sense than having a more complex model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '622e7a463063da7c50cc81ec3647f55'}>, <Document: {'content': 'Now, what happens when there is more complexity to this system, let us for instance say that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1f7482349cacebb65dfc2869f8b6ce40'}>, <Document: {'content': 'my true model was quadratic and that is what I going show here, I going take up quadratic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '910b15952e9ab46000b91797170a9eb1'}>, <Document: {'content': 'model and then see what happen when I try a linear fit. So, here is quadratic model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b12c5dad3b3e9475b91faf4e72f495e6'}>, <Document: {'content': 'this is the truth that the machine learning algorithm does not know that in the world ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc58fc4cd9193f9f44414528755c2b46'}>, <Document: {'content': 'I will never know that true system is quadratic. The only thing that I have is data show you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '71e435904c8cc2026c497bea53055486'}>, <Document: {'content': \"the data I am just setting you in a secret that for today's exercise I created this data. \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7ce37b8761c16ba1867cdfb22ae51c7e'}>, <Document: {'content': 'So, this is the data, this is the model that I created this data using this model and adding ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40acd2a54360de93fbaa58814e8221c5'}>, <Document: {'content': 'some amount of noise or uncertainty. So, the model that is creating is data has ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2cf25e25dc468c0a601bc2799048dd40'}>, <Document: {'content': 'actually you know is more plus beta 1 x 1 that it because it only 1 x and beta 2 x square. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80bdc28dc37753c6b86160bd4cce0187'}>, <Document: {'content': 'So, I am using some model like this which some b naught, b 1, b 2 plus some amount of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '74120371e12c28e6d77ab07e352bb4b1'}>, <Document: {'content': 'noise. Obviously, if you knew the this was the model then this is the model you going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '555cc409553d6208a5fd27fc8f8fca3d'}>, <Document: {'content': 'to try a fit, I mean if you knew this is the model then this is the model you should use ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2978204176b4e7c028e77049195e7098'}>, <Document: {'content': 'with that truly known b 1, b 2. So, you do not even need to do any kind of machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f07b120c42b01bdde8a942e1d108a1'}>, <Document: {'content': 'statistics excise. But, sadly you are only given the data and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '89fd1420ea4547b5d2b3a7e31d956e1b'}>, <Document: {'content': 'you not told which model it is, now let us see what happens if you hand this data and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b41c506a8d4626d5876a034b58c165e'}>, <Document: {'content': 'then you try to fit a line this is one fit. But, the kind of give you a feel for what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '48febb67948c5f0b2398b43a43844bba'}>, <Document: {'content': 'happens when you do this many times I generated another set of data points using this model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '17758b7c4a99b2aecf13e5cdd05e683c'}>, <Document: {'content': 'So, I have shown you only one set of blue dots, blue small mini circles, but effectively ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '73739b63d0775acc7a4e51e39aaa0105'}>, <Document: {'content': 'it will another set of blue mini circles and then fit a line, I fit another line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'efebfda346ef3a6076384af4dacfd937'}>, <Document: {'content': 'But, one thing you should note this is look all these lines are in general more or less ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c566373a315f4a2385cbb89c059f9052'}>, <Document: {'content': 'they trying to the same job and in general they wind up feeling chronically in certain ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ccbcb1348288df93cafa937faf5c0c0'}>, <Document: {'content': 'cases they always windup underestimating in this region, because their always under the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2933433d826ec45a1e1421193e578b6'}>, <Document: {'content': 'truth, they always windup over estimating. So, each time I do this exercise it looks ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '191b9c5f975319656382b42396fdf33c'}>, <Document: {'content': 'like chronically of in certain areas, but I am fairly consistent each time when we do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23e245e9b716d88e42a295b4318e2df7'}>, <Document: {'content': 'this exercise I windup kind of creating the same line. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3a48145ce3973d5327ab48e4955f3c3'}>, <Document: {'content': 'Now, what happens when you have a ninth order problem, what happens is you still not doing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b17e020829443a97c8a7c4bbfd1954ee'}>, <Document: {'content': 'to great and the reason you not doing to great is because this is a quadratic system and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc23d050111c1a947ed7f70710af88b2'}>, <Document: {'content': 'when you are trying to fit something so complex. So, you still over shooting a lot, but there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '200217ec8024d10d15a539d96e856ab4'}>, <Document: {'content': 'are couple of things note this one in general as expected as shown in the previous slide ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e2855e402f745c19fad7fb71168bbc6'}>, <Document: {'content': 'you not always telling the same story, one time you telling one story the next time you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e57d5f6052bdee4bdecdb654e037e07'}>, <Document: {'content': 'know you predicting vastly different this kind of extreme variance from one kind of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fa5a0a04ad9fa5b59abf05b412ca2ce5'}>, <Document: {'content': 'prediction to the other is not seen in the linear fit. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '89fe363fc63a6d3ff9b755a2bfd5398a'}>, <Document: {'content': 'But, take another look you are not; obviously, chronically off in certain areas, yes this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e004633624d8cf99179b51b6aa881913'}>, <Document: {'content': 'is only five such fits, but imagine if you had five thousand such fits. Even if you had ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32def6c8151b1f02008d6ab7ad507461'}>, <Document: {'content': 'five thousand fits in the linear model, you will always be overestimating some regions, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9b2faf668e55d4c8bb5244fd07606e24'}>, <Document: {'content': 'you always be underestimated some regions in that is called bias, whereas in ninth order ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e1d1724a57b90e9ba5b3c516354a915'}>, <Document: {'content': 'polynomial the idea is that yes there so much variability, but if you were to do many, many, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c5d7eb5388fcfedd3e18252ef8e5141'}>, <Document: {'content': 'many fits that on average you might not be off from the blue line and that useful in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '408767caa556a7990516df00bbc0713'}>, <Document: {'content': 'concept it is not useful in practice, because in reality you are going to get only one data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '11f8888c293d7e83826445308f5c0ddf'}>, <Document: {'content': 'set and you are going to try one fit. So, if it is off, it is off whether it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5fbe1863f4fa8e4295baf31a216970af'}>, <Document: {'content': 'because of whatever reason, but it really helps to understand why it is off, here in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6cacf00841a4330b275545c7f198fbb7'}>, <Document: {'content': 'the linear fit it is off because you might be chronically always going to be off. Because, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '446a70669cb6acba85bcbc8b41133c11'}>, <Document: {'content': 'you are trying to fit very rudimentary model, very simplistic model for something that little ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92a0175db651eaf41f556a027fd6f3f8'}>, <Document: {'content': 'more in reality more complex, the model the reality is more complex, so this is more complex ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bfd2ec284988c651fa9004eeb3b9ed30'}>, <Document: {'content': 'because of it is quadratic and the model you are trying fit is too simplistic, because ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ced6dc14e123ad5e94a86b73b3b24c0'}>, <Document: {'content': 'it can only here model can many be a line. So, it is a line which is you know simple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59c6d2eaa105e79c94a1af1a73e1eeb8'}>, <Document: {'content': 'whereas... So, here what you can to witness is a lot ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fb9147a962d4ab1d2056c32c2ec39c73'}>, <Document: {'content': 'of a some region you are going to be always off whereas out here in the ninth order polynomial ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77b0b5b1840a502ce9dd5c0b9c88f87e'}>, <Document: {'content': 'this is so much variability. Because, you are just getting fooled by the pure noise, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9991700b0b48601afd6ca393e8b882a'}>, <Document: {'content': 'the same thing that you saw in the previous equation, nothing is really in the previous ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56b018a015ec7c169fc4edc525cdef4e'}>, <Document: {'content': 'line nothing is really change, you still trying to fit something that is a model that is overly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea0bbf1d354bf5599bbcba4d21e2e3bc'}>, <Document: {'content': 'complex to a system that is not that complex we went from a linear data source to quadratic ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67fad3e2578dd8091e7ab7b962baf17a'}>, <Document: {'content': 'data source, but that is still does not in ninth order polynomial, so this is a lot of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd4ba3d886916933ae23f36e2763cbaae'}>, <Document: {'content': 'variability. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '179d31b2ac2a1c846b96b76b59abd4ae'}>, <Document: {'content': 'And this is the point that is get captured in what is offend, what is really described ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8cfcc71c86494259ef8cd56e4976e16'}>, <Document: {'content': 'as a bias variance dichotomy. This graph is taken from the ESL book Hastie and Tibshirani, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '826a023638e9defe6ac8eae3e6ad07cb'}>, <Document: {'content': 'and it captured what we kind of try to illustrate in the last two slides, which is as model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4663213a96bd8d134029cffe69f0c06e'}>, <Document: {'content': 'complexity goes increases it looks like you are doing a very good job of fitting the data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6739d5cf540160e50ee37816745b88fb'}>, <Document: {'content': 'this is nothing but, the fit. So, you take the data and look here error keeps you want ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70184eb917a6337f93609d628032a61f'}>, <Document: {'content': 'very low error. So, as you keep on making a more and more ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e5209f6b14da5fc6e6fba425426184b6'}>, <Document: {'content': 'and more and more complex model, you are going to go through more and more data points. But, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '860718f03ec9a89cac03f3f0190b2d77'}>, <Document: {'content': 'at some point your ability to predict this is prediction on new data, the red line is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8b8a674a7b28607793b28af3adac575'}>, <Document: {'content': 'prediction on new data, the blue line is the fit on the data that was given to you for ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fc58543d15e18347c2dd1c7d9ff62d29'}>, <Document: {'content': 'training. So, it is call the training sample and the prediction is done on the test sample, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61a15c9b4c58efdfc18d11f82b50457a'}>, <Document: {'content': 'you prediction keeps on getting low to a sweet spot for model complexity and then after that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13b5c30fe117fcc6af62ed3bd40cbae2'}>, <Document: {'content': 'it goes up and what is the sweet spot, you that sweet spot would be at a, if you are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f997b429b6c1d06391027d7bc9b4719e'}>, <Document: {'content': 'able to match the exact complexity of the system. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd91f8a7fdffb90eef24d30d04740a05d'}>, <Document: {'content': 'So, if you had a quadratic system for instance and you use a quadratic model that could might ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f9ced25430d7d90e2dbbb4562273c4cd'}>, <Document: {'content': 'be an sweet spot. And so if have a complexity that is perfect you know and what we are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '24efdac8c2e389332a0d74cf6f49c0df'}>, <Document: {'content': 'to do is, in the real world you don’t know what the true model is. So, how do you figure ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '76f7ebebf6ee0126c493b5a1215f1a1'}>, <Document: {'content': 'out what this complexity should be and that is going to be covered in or lectures on validation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8a7d82a88228c38f7824011ef67d028'}>, <Document: {'content': 'How do you validate a model, how do you fine tune some parameters of a particular model, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'caa9531cb0c2fcc5815c84aae18663a3'}>, <Document: {'content': 'again it can be K-nearest neighbors, it can be trees, it can be neural networks, it can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c318a70245d4231f263a75b3dff539d9'}>, <Document: {'content': 'be support vector machine, it can be a simple regression. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8f6eeaa9254da086252b2c17254bcac7'}>, <Document: {'content': 'But, if there is a some kind of tuning parameters that there can increase or decrease complexity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8c042cb88c4bd830b89c7b0bebab8c4b'}>, <Document: {'content': 'How do you go about increasing and decreasing complexity is seen what works best and then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '96a71382af780f514fa85763d1391336'}>, <Document: {'content': 'choosing the appropriate one that we captured in validation, the lectures and invalidation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9e8ba6a76892157835c32763b5542dd5'}>, <Document: {'content': 'when this lecture we want to create an appreciation that as model complexity increases, the fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '82ef7856d377607b290f3ec0df5506cd'}>, <Document: {'content': 'becomes better, the prediction you need to find the sweet spot of model complexity and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3681b070bc0508f246a68089a46846a'}>, <Document: {'content': 'that is the core idea. The other idea here is that yes when model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9127d68569e75e2c7dc3ceb052c957db'}>, <Document: {'content': 'complexity is low, you do not do too well in terms of here prediction, but the reason ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f4fd76942765cd58b5ebd028125b2707'}>, <Document: {'content': 'for that is because there is high bias meaning in that linear regression if you remember, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a5dec836d652e65b360b719c83e9e00e'}>, <Document: {'content': 'you always of when you trying to fit that linear regression to the quadratic function, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '90eca20daafb8c003026a2dd1c492990'}>, <Document: {'content': 'you will always often certain regions. So, you had a high bias in a low variance, what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5e1d55339639d462d9dc9b9d5ab08d5d'}>, <Document: {'content': 'we mean by that I go back to the slide is you are bias in certain regions. So, these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '930bafd45906ca8dc123c031b3e1419a'}>, <Document: {'content': 'are regions were you have bias, you are always going to be off, because of the nature of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5d7146a728e61c4f8a7626f708595b9d'}>, <Document: {'content': 'you try to fit a line through a curve, but you have a very low variance, if the variability ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c96404a05e9b5e34dbc81efeb9e8329a'}>, <Document: {'content': 'between many such fits is low. So, on any given day you get any given data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbe1a99f0d6f7774c1557d496c6d90a5'}>, <Document: {'content': 'set it is not like you are going to come up with the completely new equation. So, that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '51dcc7a5e4d15b643bc02f509d7dab90'}>, <Document: {'content': 'is what we call us high bias and low variance. Now, you step over to the ninth order polynomial ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f915f0f617732a2142cf98a3d3ef3024'}>, <Document: {'content': 'here the bias is not that high, it is not like that can tell you that you are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f70504028d3cc0e8a210797bcca20ca7'}>, <Document: {'content': 'to chronically be under predicting or over predicting in some regions. So, this has low ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '792a455c242fb61a7d9d1b92a632493d'}>, <Document: {'content': 'bias, so I cannot tell you upfront that you going to be always off in one direction, but ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9386088cfe5a447af788a442cbc58295'}>, <Document: {'content': 'it is got high variance. What you mean by that is on any given day ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2e23785b393495b95750a638bf33ae'}>, <Document: {'content': 'if I take any given data set I take the sample and I try to fit this polynomial I do not ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66bae28d3b2374352a716b0bd59776ba'}>, <Document: {'content': 'know which line I am going to get, I mean this line predicts some astronomical high ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5148784128d71e51831ff668e7f1bfd'}>, <Document: {'content': 'value out here, whereas this curve predicts some astronomical low value out here. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '39f0199767050945efc24662aad07912'}>, <Document: {'content': 'this like such high variance on a given data set I do not know how going to be predicting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e7c0fea2e5901b47b95ab66e24e874de'}>, <Document: {'content': 'and therefore, if I can have such high variance, it just means that I am probably not going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '80662326cdd793f00f2489590bcb2cb1'}>, <Document: {'content': 'to do very good job of predicting. I do not even know what I am going to be predicting. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d0edb2e502a05bdab452270bfc584ed'}>, <Document: {'content': 'So, that is the concept between the bias variance dichotomy, which is that when you go for lower ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'abf8507b608ca73ed80b919d2b60da52'}>, <Document: {'content': 'model complexity you get high bias and low variance and we go for a higher model complexity ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d5e57238c35d63c1c1e0bfc060bd43a'}>, <Document: {'content': 'you will get low bias and high variance and this is the very important concept to be internal ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2f2ce2e351522e8b7331cd86adbef1f1'}>, <Document: {'content': 'analyst with respect to Machine learning and it this going to be extremely important even ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '884692598d6f930891d5d8d873238865'}>, <Document: {'content': 'in terms of applying more advance techniques. So, I hope the bias variance dichotomy is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '487347565fbd46f8a11f82de430257ea'}>, <Document: {'content': 'clear. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '635ab02b54dc1eb5856a3f27bfe524eb'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Model Assessment and Selection ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8da54d62b123e62c08c8718f5ec7ef9f'}>, <Document: {'content': 'Hello and welcome to our lecture today on Model Assessment and Selection. So, this lecture ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ef6a486789d8a11b6b8ca2d65d30ac3a'}>, <Document: {'content': 'is in many ways similar both in style and content to our lecture on bias variance dichotomy. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc5ac90423d21503cff1055ed05aa924'}>, <Document: {'content': 'It is similar in style and that, we are not going to be teaching you a new techniques, but we are teaching you a concept in machine learning that is really important. And so ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56e0402ab51385d8d0fcae4d689ef751'}>, <Document: {'content': 'like the bias variance dichotomy, it is really applicable to almost every technique that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f37e3aa9b640edf83e2372b3e266bd32'}>, <Document: {'content': 'you could be using in machine learning and should not just say, every technique in machine learning more narrowly in every technique in supervised learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55bd8be96b494c5aadb06f5369887aea'}>, <Document: {'content': \"So, your regression, your neural networks, your cards, your decision trees, your SVM's \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b15c805f5ef485854f6f354af14b8e74'}>, <Document: {'content': 'you know, so whatever techniques you are learning right now and whatever you have learnt this is a very, very important concept. It is also very similar in content, because in the bias ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2651534a3b1ac9614fe9129c05ffe806'}>, <Document: {'content': 'variance dichotomy lecture you really learnt about how it is really important to understand ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '40c95910cc3fd8fade852f0e31ad9061'}>, <Document: {'content': 'that you can have highly simplistic models, which would have high degree of bias and low ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70e26a3016e7ad76ef70db1e5bdbeb71'}>, <Document: {'content': 'variance and therefore, not really good. And at the same time you can have highly complex ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'faf239bb78e291aef69da868cff9b9b3'}>, <Document: {'content': 'models, which tend to over fit the data and therefore, have a lot of variance, but very ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9904195adbb859578b1e6fe5e44a03ed'}>, <Document: {'content': 'low bias. And therefore, you are being introduces the problem there, saying that well you it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '545c2d71f03777ce8e67d67e3bf2712'}>, <Document: {'content': \"is not a good idea to go to either extreme. In today's lecture we will be answering coming \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d2c0ac94fffc7fe9ca26c4ff25a9e41'}>, <Document: {'content': 'up with solutions for that problem and not just that problem, for many other related ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5199d61ff6dc60d5c3bc5f6dad88aae'}>, <Document: {'content': 'problems, but somewhere the solution is in looking more carefully towards how do you access a particular model and therefore, how do you go about selecting between multiple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9ad968782bf0d3ee8f0d5a4e8e2c9960'}>, <Document: {'content': 'models. So, jumping into the subject in terms of when you have data and you are trying to relate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2a614b73561fce0231c673b082673fb0'}>, <Document: {'content': 'that, use the data towards models, there is one part which is actually using the data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94be1f55d51f4e4cd5fe60e45634a247'}>, <Document: {'content': 'to train the model. There is another part, which is you could use some of the data to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd9e878ec65b140506b6aae806c3861c4'}>, <Document: {'content': 'not just train the model, but to choose between models or to fine tune the model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '382f310f368e55d5d848298f7273a588'}>, <Document: {'content': 'So, using the data to train the model if model training, model selection the word selection ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd98fe92292ac074a6a692f64d2457060'}>, <Document: {'content': 'is use kind of loosely here could mean selecting between multiple models or it could mean, you know it is a meaning you could use that select between completely different models ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7f98c0579b62523ce7c446622b2bbf87'}>, <Document: {'content': 'or at the same time you could use that you can fix the model and you could just be like a parameters that define the model and you could go about figuring out what value to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'dc72ca8fe0abc82f83dcd68cf5ab2419'}>, <Document: {'content': 'set it to by using data in the model selection. And finally, you have the concept of model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4aa90bcea341509a4b01cc55faba9d21'}>, <Document: {'content': 'assessment, which is once you fixed everything out, if you want to get an idea of how good ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ea135f13b6ebb5922e01ba0d25be46a0'}>, <Document: {'content': 'this model is. So, that you know that when you take this model to the field, this is the performance you would expect that has to do with model ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46642db9739b49932a8ffd6719b67a3c'}>, <Document: {'content': 'assessment and corresponding to these three goals or objectives, you could break up your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '311c11365b7d7e4042b1f782b5acdf51'}>, <Document: {'content': 'data and this is something that you might not always have the luxury to do. But, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'baadb17e4d8b92621e477be1f2ea0061'}>, <Document: {'content': 'can if you have a data rich situation we have enough data, then you could potentially break ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5a8470ec645bf8d90efd2d45b06f6a26'}>, <Document: {'content': 'up the data and training, validation and test, where the training goes towards model training, the validation goes towards model selection and fine tuning and the test goes towards ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b5299cfb35c0046d21966d505f2e7643'}>, <Document: {'content': 'model assessment. And there is no formula exactly as to what ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'caa150ac6f7cfa17ac0457949d040e30'}>, <Document: {'content': 'this break up should be or what is enough data, that is a hard question to answer, it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '860e7c29c6358023a48ec767a5ac4914'}>, <Document: {'content': 'varies from case to case, but at the surface you know typically if you have enough data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '93b42188024fae7b5b8f07ae00d13537'}>, <Document: {'content': 'you tend to break it up as either 50 percent, 25 percent and finally, 25 percent out here ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61cc57c0853e71c53c0dcdb09519c830'}>, <Document: {'content': '50, 25, 25. But, you could also have you know 60, 20, 20 and again, even with this you cannot ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bc655f3961014224dc44792eeb0fd4d9'}>, <Document: {'content': 'say one is right, one is wrong or something is perfectly right or wrong I am just giving you some values that are typically seen. You can also see in the picture that I have, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ed6cf68eaafb1bfbaf899639222bdf60'}>, <Document: {'content': 'the representation I have below that sometimes you might want to do training and validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e93575268b263539c0ade020f6524c5d'}>, <Document: {'content': 'together and we will talk about that more specifically. But, if you, this sometimes ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b64301aa73bada1b8885c143b274d277'}>, <Document: {'content': 'comes up when you do not have as much data, where you do not have the luxury of just taking ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '10af75d74cf9b242cae48dff3a9e0430'}>, <Document: {'content': 'one 25 percent and calling it validation. So, somewhere you know you just use the same ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1833093072430f5695da5a097b8e7722'}>, <Document: {'content': 'data to perform training and validation and how you will do that is something that we are going to talk about in good detail in this lecture. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd904ccf847c7f26eacc56d693b57cf46'}>, <Document: {'content': 'But, you still try to keep some percentage for model assessment, then the final say on ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '24c87fe800eadaee2ff5fae1f907dbaf'}>, <Document: {'content': 'how good the model will be if you take it out to the field, if the model want to completely ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ecd2652d65bafc26559ced19eb8a94cc'}>, <Document: {'content': 'look at new data and try to predict how good will it work. So, what we are going to do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6306014466c3f74f42fa1444fd873146'}>, <Document: {'content': 'next is, we are going to take each of these phases and talk in detail about what really ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da0cbc8ff9352f339f90ce1fd2edb013'}>, <Document: {'content': 'we can do for each of these objectives. The first phase, which is model training or is one, where you take the data and you train. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '27795e3bd9e5bbaf8468beff4b469067'}>, <Document: {'content': 'What we mean by training out here is really that you fixed everything, you fixed all your ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c48856e238228c6a02b4e9dcae744c79'}>, <Document: {'content': 'steps, you chosen a particular model and you chosen all the parameters associated with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '638feeaede4dda3bb061aa8b5c8c97eb'}>, <Document: {'content': 'this model. So, at this phase you are not doing any model selection, in phase one we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c3eaa80e5d423f89f8bcf2d9f3bc1cf8'}>, <Document: {'content': 'are not doing any model selection, no selection, you also not trying to decide on what values ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '91a7302b8ebd13fe97beeab743039ee9'}>, <Document: {'content': 'that some of the modeling parameter should be, what I mean by that is let us see you are doing a ridge regression, in ridge regression you have a complexity parameter. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a90b452e0a366770d7f2c875360680aa'}>, <Document: {'content': 'If you go back to our lecture on ridge regression, you are doing this optimization where it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac3cc55dd667bc55bc8fd015ebdd0a89'}>, <Document: {'content': 'look like the ordinary least square. But, you have another term, this is the term associated ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5ad0aac73fa66d6753a3d330c2a89f54'}>, <Document: {'content': 'with regularization, where you penalize really large coefficients. So, for that you need to set a value, you need to set a particular lambda, you already decided that, in this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d28dc7ca02cc5d521cdd127cc4347e4'}>, <Document: {'content': 'phase when you are going for data for training, you fix the model and you fixed all the parameters ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '60bd3877d1cb26ce5572f715ef25decb'}>, <Document: {'content': 'associated with the model. If you for instance doing a neural network training, you are not using this data to decide how many intermediate node should be there, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a07e904f40375310b80a67bb9453b65'}>, <Document: {'content': 'that is not what is happening. Once you finalized on the exact model and its details, you are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b438a95b01f04cea7c2a91ac916e1af'}>, <Document: {'content': 'just using this data to figure out what the model should be, meaning take the case of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '31b04d1b05dae27cf98f6bf294c13a9c'}>, <Document: {'content': 'linier regression, you fixed everything, you fixed what the input variables are, you fixed ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a569cfab2131d811e86f79cc9f49f4fc'}>, <Document: {'content': 'what data you are going to process with respect to a simple linear regression or a multiple ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7bb9191b55e0f1caf4cb3ac8c3e51442'}>, <Document: {'content': 'regression, there are no other parameters to fine tune. So, it is you just plug in the data and you get the betas, so here you might just be getting ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '44789d3ff02813e9f0b0e750a60663cd'}>, <Document: {'content': 'the betas. So, that you have created the model, you are essentially creating the model in ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29ee1d3e135e41fbdd982cc9ade33b21'}>, <Document: {'content': 'this phase, if you are doing decision trees then let say you need to have fixed on the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5316b08296a6288221276b939edcade3'}>, <Document: {'content': 'algorithm for the decision trees need to have fixed, what the input variables are, need to have fixed other parameters that you could probably play around within decision trees ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56690fa6bfc8c9f088da277e9ca48960'}>, <Document: {'content': 'and we are going to talk about those parameter soon. But, once everything is fixed, you just ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23cf5d02d72312d54399ead528649087'}>, <Document: {'content': 'plug in the data and how to get or is the tree structure or network, in the case of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac1d1be97b1205e531d42c2951957b02'}>, <Document: {'content': 'neural network. So, this is essentially the last leg, once ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cb0d93cdca2a58b522334df52b66c2fd'}>, <Document: {'content': 'you fixed everything you are not doing any model assessment, you are not doing any model selection, you already selected your model, you already set the parameters for your model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23243df25b3408d65cd1fba0e8f6521b'}>, <Document: {'content': 'You just plugging the data into this finalized version and getting the actual model out of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fac990c481d1387c2f27101599bc9632'}>, <Document: {'content': 'it. So, that is what you are doing in this data for training. What happens in the validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bf120d02446b41c765ce44cae917a6c8'}>, <Document: {'content': 'phase? In the validation phase, you are essentially using this data. So, this is separate data, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd4c2dfa1075c1c1a145dfc9e9d165c59'}>, <Document: {'content': 'you use some data for training. Now, think about this, you using separate data and you using this data to make some decisions. You could be using this data to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '152e376b4c046bf381b5497be75e96ef'}>, <Document: {'content': 'select between multiple methods. So, you might say, hey I have this data I could do a linear ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '67eda9f318eb272c93bea6b52b9c904'}>, <Document: {'content': 'regression on it, I could do a regression tree on it or a random forest or something. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ca21865beb92352eef15ffb0188950a1'}>, <Document: {'content': 'I could do a neural network, how do I know which to do, why not to do all of them and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7451d69b2492e4af488aaf19668f4a25'}>, <Document: {'content': 'see, which one does better and how do you do, see which one does better. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '18d26f5321bda0ac0e6d12bbaf6a7623'}>, <Document: {'content': 'In the validation phase what you will do is, you will use the same training data that is you will use the same training data that you saw, we discussed in once and you will fit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ef2fe2d949c813e80a6dd64792341c0'}>, <Document: {'content': \"these three completely different approaches. You will create the data's for the regression, \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4f3881721d3ccf9e1643553c829e28ff'}>, <Document: {'content': 'you will create a tree for the regression tree, you will create a network for the neural network. Now, you go apply these three to the validation data, what I mean by that is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ebb992c6a7fbbcdee05444c6e60a11e7'}>, <Document: {'content': 'that you go take the input data in the validation and apply these three methods and you get ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c1187c8a330d5e21cc369265e78e4670'}>, <Document: {'content': 'some predicted outputs. Each of these methods are going to give you some completely different, I mean might not be completely different, but they are going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da69136128f57c15fca489f0eeeeb2f6'}>, <Document: {'content': 'to give you a different outputs, different predictions. Compare these predictions to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66a51ab04106a10f31388d508472913f'}>, <Document: {'content': 'the actual output that you have in the validation data, any. When we say data whether it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e24af933264109aa108c7cd99afcf2f3'}>, <Document: {'content': 'for training, validation or test, it means you would have input output pairs. So, if you have five input variables you will have values of each of those five input variables, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '83694be05a35b65d60b7e82d6d245109'}>, <Document: {'content': 'one data point is nothing but, the vector of all values of the five input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe4f5f5eab3a7a1c9686f9d166cc6e8f'}>, <Document: {'content': 'and the one output variables, so whatever value that is. So, in some sense when you have these input, output pairs what you can do in the validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9da23a2b50f141c2298fbc6ea128074f'}>, <Document: {'content': 'phase is, you would have created the models from the training phase and what you are interested ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '184e45c5c4b45ed057beb593000518f5'}>, <Document: {'content': 'is in comparing multiple different models. So, you might have completely different methods and then you apply the input of the validation phase to get predictions from these methods ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac1bfd62963535d92e2e9b48efce6b96'}>, <Document: {'content': 'and compare these predictions to the actual output and see how well they did. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56cfbc49146089e280f75b63107f43e0'}>, <Document: {'content': 'And you can do that by both in a regression context or in a classification context, meaning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ad8e55c1513e6db1c1f482eb6e8246'}>, <Document: {'content': 'that if you predicted 13 and it is 13.5 I can say are you miss by 0.5, if you predicted ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92ddf9484dce4711689566f9edae876c'}>, <Document: {'content': 'that is going to be a class like I am predicting that this is 70 percent chance it is male ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd25cf303ceb0109297822fa140e1cfc2'}>, <Document: {'content': 'and then it winds up being female you can say your you can use different kind of a metric to measure that performance like misclassification index or entropy or something. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3c07afa351574dbe9d0ab7625d0e34a3'}>, <Document: {'content': 'But, essentially the idea in the validation phase is to choose is make some decisions, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6eabeda769fdddbda4189eab49d0a957'}>, <Document: {'content': 'we spoke about how you can use that to choose between methods, but more often what we see ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61743fd02e623b8f1610b08a64d83e54'}>, <Document: {'content': 'is that it is huge extensively to find tune parameters for a given algorithm. So, let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa685eb7ac948b26c5e025ddd737d42c'}>, <Document: {'content': 'say I go about and I am decided right at the stage. So, we have already talked about let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '12d58e998e91bc59f473940af549e56f'}>, <Document: {'content': 'us call this a, b and c, so we already talk about two a into b I am really talking about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbe6925439c1e8ce948a587cb3cd59c3'}>, <Document: {'content': 'here is let say I have fixed I have decided that I am going to use a decision tree somebody ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2ce58579cc05ae40a2c16c98ddb4732c'}>, <Document: {'content': 'use the cart algorithm. Now, the cart algorithm might have a lot of parameters that I need to set one potential parameter could be then minimum number of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6f0fcb66abebff7ad44a7e14e4a1e79'}>, <Document: {'content': 'leaves in the terminal nodes what; that means, is the minimum number of data points in a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8c3bb68327d455ca979849fedb9343f'}>, <Document: {'content': 'terminal node of the tree. So, in a cart algorithm you can keep on splitting the tree into branches ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '56c6fd1fa18fcee96a4c093affc1f1a0'}>, <Document: {'content': 'and branches still the terminal nodes just have a one data point, the idea is where do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35e790cc69aee24348556133c42678dd'}>, <Document: {'content': 'you stop and one way of choosing where to stop just to have a limit you put in the limit ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3b48c24a8a2c1bfa396f6c83e7335c1'}>, <Document: {'content': 'saying do not create new branches, if they are going to result in the final nodes having ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '35d36bf13b5b0fe6a1193f48e4c755f5'}>, <Document: {'content': 'less than let us say 5000 data point or we calls them 5000 leaves. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f159d3996a6233ea396fd1ff601ad74a'}>, <Document: {'content': 'But, what should that be, should that be 5000, should that be 500, should that be 50,000 ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b942e4d83daecadff63918464ea35c50'}>, <Document: {'content': 'what we can do is you can run all of them, you can create three completely different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e256edc6484e8fb0e155c14def620cea'}>, <Document: {'content': 'trees, one with each version and again validate that use the validation data set to see which ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cbc236b1ea338739220c7c0e1dd0e224'}>, <Document: {'content': 'does better and for pretty much any method, any complex enough method you will find that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '13bf3fecbdcdc274dc6ead48d5a8c5e0'}>, <Document: {'content': 'the method itself will have some parameters that you need to finalize. So, in the case I guess you guys have finished you might not have started neural networks here, but you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '83ec74b4c89c8c6756c9b22fc82561f'}>, <Document: {'content': 'must have finish. So, we finish ridge regression. So, again let us think about ridge regression, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fed85866b6e71d0e432fdeb2ec1f92c'}>, <Document: {'content': 'in ridge regression we know that what ridge regression it does this process of regularization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '42dcd0ffeb1480370401227d0902c370'}>, <Document: {'content': 'by introducing a new parameter called lambda, lambda is what forces is kind lambda is kind ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '11a1355e656a40b89c7f3ad7b3d25621'}>, <Document: {'content': 'of like the way in which penalty is applied towards that optimization problem and constraints ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2c3980ca9353522d3b21361bc33405dd'}>, <Document: {'content': 'the size of the betas. So, if we have a very high lambda that is like a very harsh penalty ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1057b0a76dc6cb25eb4b90c394e82255'}>, <Document: {'content': \"towards large sized data's or coefficient that linear equation. So, a ridge regression \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '46991312c9f4aee8f54137c8feb1885c'}>, <Document: {'content': 'goes ahead and applies penalty. Now, if that lambda which is parameter is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6d0660c6ab94cb40164138e51462d594'}>, <Document: {'content': 'very low it is close to 0 then there is no penalty, the ridge regression becomes like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8ef440b3f0528725ae040e6e6eda8e8c'}>, <Document: {'content': 'the ordinary least square regression, it is actually identical if that lambda is exactly ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5810f678834d307c8c97960a3f2ebb55'}>, <Document: {'content': 'set to zero. But, then the question comes about what should I said lambda to. I understand the concept that I might want to apply some amount of penalty to really large betas and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b47f77f8d2fe0c1aab9306eed9904b66'}>, <Document: {'content': \"that was what we discuss and the course on regularization saying you know I do not if I have multicolinearity in my data I might have two data's that are off to opposing magnitudes \", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fdb8880f7446ad6e8c953104bbf36d05'}>, <Document: {'content': 'that are becoming really large and so on and so forth. But, what should I said the lambda to be in the ridge regression, in the process of ridge ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3263bd21a4f0d33088967c36e38f6986'}>, <Document: {'content': 'regression I need to said the parameter lambda to some value on what do I said it too is there right answer to it. The answer is you might not know that apriori what to said lambda ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cd4bdb1669c1484ad3a5be1864432de0'}>, <Document: {'content': 'to, but you can use this validation phase, you can said lambda to 0.25 set lambda to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e32f3f1aaa1944dc750f09e196a24b4'}>, <Document: {'content': '0.5 set lambda to 0.75 and create three different ridge regression equations, three different ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f071c6b5c211a59049e9a34ca9d83556'}>, <Document: {'content': 'predictors. Now, take the input data from the validation set enough and apply that and all three predictors, all these three predictors to come up with ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58bb39f628e116021c5a228367d2475c'}>, <Document: {'content': 'some predictions, three different sets of predictions of the validation data. Compare ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f62ea0e57f130b93f6de420ac6e50898'}>, <Document: {'content': 'the predictions of each of these predicators, compare these three predicts sets of predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1bd1b2f45cf34c152a6c8bf21472855e'}>, <Document: {'content': 'to the actual output data of the validation data set and you might and you can then compare ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6bd4e360998db93b476994081541f4e'}>, <Document: {'content': 'these three and say oh it is look like when I said penalty parameter lambda in ridge regression to 0.5 I do better than when I said that parameter to .25. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3ad97e9bcc5ae98f54be7dc8008df0cb'}>, <Document: {'content': 'So, again just you can go to the bank I know you finished a sub set of techniques and you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4df2019cd27455546a6dc868ec41fd32'}>, <Document: {'content': 'are still going to learned for instance neural networks and some other methods. But, what you should take you know commit memory and some sense is that almost any machine learning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd238a72f04b0c8f4c51f582172c298c5'}>, <Document: {'content': 'technique that you adopt you will have to choose some parameters and this validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94883a52c01d47120ffad87ffa019921'}>, <Document: {'content': 'can help you choose the parameters. Another example I can think of is for instance, the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '729e52165c41c9737ddada910481fa0'}>, <Document: {'content': 'neighbors what should be a value of K be? Am I suppose be a let me 5 nearest neighbors, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '65d82fc8ff9030be525d071883daa1db'}>, <Document: {'content': '6 nearest neighbors, 10 nearest neighbors, 1 nearest neighbor what should in K nearest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '104c52c87c7313377e33418802833946'}>, <Document: {'content': 'neighbor algorithm that we discuss in our class, where we compare regression to K nearest neighbors. What should the value of K be? and you can use the validation to figure that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '428ef1f567481e7a8af56cfc367433f1'}>, <Document: {'content': 'out. The last use case and sense of the validation phase is one that can also be used to choose ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8202a2335b653304aecadfec90b18128'}>, <Document: {'content': 'the correct number of input variables and to also say which ones those input variables ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c0c99854715c687297f185c26921bf3e'}>, <Document: {'content': 'should be such choosing the correct number and also choosing those which... So, I can say I need five input variables are not six, but which five should they be. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ebcef61414968eb0217e184cc5d94458'}>, <Document: {'content': 'So, in that sense and this kind of really links up with best some sets ridge regression where you are trying a whole combination of different inputs. Again the idea could be ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '547a5f6f2fbcafa41bcfcadd37c9b307'}>, <Document: {'content': 'that I can have a fixed model, I could say I am going to use ridge regression or like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9c439f2fa94301dcccaf94fd2a841f65'}>, <Document: {'content': 'I am going to use regression or I am going to use trees with this parameter, but which inputs should I will be using, should I use all my input variables, should I use input ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33203b458dce4713ad01158bd662b819'}>, <Document: {'content': 'variable 4, 7 and 9 you know. So, that decision also sometimes like we spoke earlier about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '539c1defb0654e0811d49c4afef5d303'}>, <Document: {'content': 'regression here R square will just keep on getting higher and higher as you keep on adding more inputs that does not mean you getting a better model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1020767f20d8dde86a1be3836a1ebdbf'}>, <Document: {'content': 'So, choosing those inputs says you can choose a model with inputs a, b, c, d another one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1e14cfd55d6f3ba28221b28a2c9daa82'}>, <Document: {'content': 'where you use inputs d, e, f, g and you can really compare the performance of these two ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29274a7158b26a75ba2f192841a6f257'}>, <Document: {'content': 'different models on the validation data set. Now, there are some techniques already there ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '925f862632569fdd8476d68d07ea2415'}>, <Document: {'content': 'that we have discussed which helps you choose what the input should be and we spoken about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6bf5aded3e4b427e530d3ad23193ac56'}>, <Document: {'content': 'that using metrics that are more complex and R square like for instance I adjusted R square which you know goes ahead and you know penalizes more complexity and there are a whole bunch ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '28cd147af856cba4c81c21fc21e0319a'}>, <Document: {'content': 'of a blanket of approaches to penalize more important. So, we adjusted R square there something called the AIC information criteria AIC and then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd7e78dbc136c72210eb6b3e3df604b57'}>, <Document: {'content': 'this Bayes Information Criteria (BIC). So, there are whole bunch of metrics that will just go way and say blanket I am going to penalize you for adding more input variables. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f7678255c1124bd34de11eb2b6e2a2e9'}>, <Document: {'content': 'But, we don’t need to do that by that might be computationally convenient and it is you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5bab38412649bc344338f77856e2c63d'}>, <Document: {'content': 'could if you have the luxury of creating this validation data set. A simple thing that you could do is you can just try these different combinations of inputs ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '50dd4119c7b73b37556ea3ca1a88c08f'}>, <Document: {'content': 'for the chosen method of doing the training and so on and apply them and see which approach ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6af60655d8b262b3c34bfcb83f68ca27'}>, <Document: {'content': 'does better. So, you could just untimely wind up using prediction error instead of say something ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a3f9669cc157e0af6dd8adf110a717a7'}>, <Document: {'content': 'like a adjusted R square, you can use the prediction error. Because, you are not just blindly looking at how good the model fix the data that you train the model on you now ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7fb959b12e2f692031b5e41c5d533f6f'}>, <Document: {'content': 'taking the data that the model has never seen when I trained and you using that to decide ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '114e52959b6c2217c1b7b74d118096cc'}>, <Document: {'content': 'which what your input variables should be or how many input variables you should have. So, the important thing is the validation itself and this whole process of model selection ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4aa41e0b31b2dfb0bc99d8f84e7a0a89'}>, <Document: {'content': 'is not just one thing, you could use validation really it is choose between multiple methods, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d8c9e9866e5ce6873a466f121d17853'}>, <Document: {'content': 'you can use at to find tune the parameters of the algorithm and you can also kind use that and some way to choose what your variables should be. Finally, you have the test phase, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4fb36ede6cc24b5d413bf0dde2cdfe6'}>, <Document: {'content': 'now we are at number 3, the test phase and we have three and now we are talking about ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b9f729bee4d5e5316419369b9752c3c7'}>, <Document: {'content': 'three the test phase. The test phase, the purpose is different it is not to find tune the model, it is not to help you come up with the best model, you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0daef577a8d6a2dccc84fc0b2b99ca8'}>, <Document: {'content': 'could come up with the good model, you could come up with the horrible model, the test phases I do not care, my job the job of the test phase is fairly simple. The job of the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c4eb9e1b44a42cf8c2276b3a0784158b'}>, <Document: {'content': 'test phase is to give an accurate idea of what the performance of this algorithm is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3465309aea6978e3af7584ad62a44454'}>, <Document: {'content': 'going to be and you can turn around and say why do not I just use you know something like ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ab73b82de80cf5db2b412bea79faf936'}>, <Document: {'content': 'my prediction error that I got in the validation phase. And the answer is, you cannot because you have or hell I mean you can go even one step ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70f4c6e04f351c437913e0c9ebcfd1d6'}>, <Document: {'content': 'for instance say why cannot I just look at the predicted the error, the residue will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '29cf1ba9f960fee9660ba7749ece8448'}>, <Document: {'content': 'some of the squares and some sense. The error that is comes from the training, you cannot do that for the simple reason that you are explicitly find tuning the model, find tuning ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cea3e157a4904cce96e7a01cdac90ca3'}>, <Document: {'content': 'the co-efficient, find tuning the parameters irrespective and this I am loosely using these ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcfb6f886924e6fdf42924fc2af692a6'}>, <Document: {'content': 'term, because it applies to any approach you take, but there is decision trees whether it is discriminant analysis, whether it is support vector machines, whether it is neural ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2e50be1ff3d6f3fa24df91966214cb46'}>, <Document: {'content': 'network, whatever approach you take, there is a whole bunch of optimization going on these approaches you have to sitting there and trying to make this model make sense out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd78d2ca5a6e83017469bed2eb7891a72'}>, <Document: {'content': 'of this data that process itself is going to make you do very good on this data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8d11ea4298300ee171aaf760bb92beb4'}>, <Document: {'content': 'Now, to get true picture of how well you do when you see a new data, you do not want data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3776e49f21f2187b603bcd4d9f63012c'}>, <Document: {'content': 'that was used to make this method good in the first place, you use some data and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4e29c9da3a3d4a96e32d84227ad5bdcb'}>, <Document: {'content': 'training data to figure out the co-efficiency. So, use some data and the training data to construct the actual tree or that actual network, you use some data and the validation data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '44e76f3f62e02cf101f7f0c13e9037ed'}>, <Document: {'content': 'said to find tune what your parameter should be why. So, that you do a good job of fitting. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b261960090534fde747861e78e172d26'}>, <Document: {'content': 'Now, you do not want to use the same data to tell me how good I will perform when I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c16ff9e864713a435c10f9e3fa60813'}>, <Document: {'content': 'go when tomorrow someone comes to me and says here is input data can you come up with the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34b1dabe3ac1da8cf0d3f940215b5da9'}>, <Document: {'content': 'prediction for me, to do that you want to really look at data that is never been looked ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '524001aef1a474468434ced1cd5e6e18'}>, <Document: {'content': 'at essentially this is you should think of this as data that is been kept in the vault ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f6322c5de059c5d1c502a5dc16f69b7d'}>, <Document: {'content': 'and never looked at and you brining it out in the end after all the modeling has been ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d6dbfa93549615efd8c56c405fb2ee4'}>, <Document: {'content': 'finished, you use the training data to construct the tree, you use the select the validation data to fine tune it, you now have end product the end product regression could be a set ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e8feb375526b78c004e2290d03a0d7fb'}>, <Document: {'content': 'of co efficient like the betas, then product in tree could be the tree structure, whatever ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '84c5bb3316918471f746f39e6894fc85'}>, <Document: {'content': 'it is. You basically have a finalized end product and you want to now see how good the end product works, now pull out this data, pull out this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '23ac88f62b3dc79815141758b652f964'}>, <Document: {'content': 'data provide the input data from the test data set ask the model to make predictions ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e6fe3ac5d003a6f2475511e578b08d12'}>, <Document: {'content': 'compare it to the actual output and you are just going to report that. You are not going ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4de8c92fd0edb372fb26a420a7f73243'}>, <Document: {'content': 'to use this data to make any more decisions in terms of which method to use or you are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e173d6d9c3bfa1577c85d40626f849b'}>, <Document: {'content': 'not going to use data to fine tune anything about your model. Because, if you are doing that you are using the data to make the model better, but you ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c43a599eceae3acc3653d650c6f9121b'}>, <Document: {'content': 'are not giving any longer an accurate impression of how successful you would be if you were ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4941a9caee23bda1285af3e99540744a'}>, <Document: {'content': 'to completely see this data fresh and you were to make predictions, you are not giving ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '34fe8eaf0e242cbf45fa52baa7168e14'}>, <Document: {'content': 'an assessment of the model. So, in order to give the assessment of the model pull out ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '337c2f435ba73b78334f1dd5d6b80587'}>, <Document: {'content': 'this data that is never been seeing by model in the end and the only purpose of this data is to see how good the model is not to construct the model or not to fine tune the model. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ae83733674ffb956b8adade9042188ae'}>, <Document: {'content': 'that is the test phase. Now, we are going to talk about in the last part and approach called cross validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce37ca3016d04d2569244aa047715063'}>, <Document: {'content': 'and it is really ties to this case that I showed you in the first slide, where training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5124d4c10672fae8032a4dc79c9cd5f8'}>, <Document: {'content': 'and validation are kind of put together, this is an approach that really kicks and when if you feel like do not have enough that much data for separate validation set and you want ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc56cb2716d9b344015460cbcc0398a9'}>, <Document: {'content': 'to kind a squeeze more out of it. So, a simple approach there is just have the training and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5c7eb7cbc4f20ad7b189207934a5700b'}>, <Document: {'content': 'validation set as a one large data set and you can break it up into n chunks. So, out here of broken it up into five chunks going between five to ten is typical and the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c63db8211c52f5bf9f6dd903f9dda84f'}>, <Document: {'content': 'approach is called actually k fold cross validations. So, it is k fold where you have to choose ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e2e030859f3dd90f79e7792ed264160f'}>, <Document: {'content': 'a particular k and here I have chosen five and so basically broken the data set into ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e620882ca9ffb34b07fd6ed7a262f088'}>, <Document: {'content': 'five chunks; obviously, very important guys is that this break up is it is random. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df23fa2cdd5bfdd63e7e18cc91602b77'}>, <Document: {'content': 'if you given the data in some order, you do not want to very you know conveniently just break it up into five chunks and because they might be some kind of you know implicit order ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68cc2c5bfe00485799bef77fe003442e'}>, <Document: {'content': 'may be the data was for instance chronologically order, you do not want to create training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b1702ebea33434924eb0197c65860a6b'}>, <Document: {'content': 'data set one of the early chronology data. So, you kind of want to shuffle the data up ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2722e7c3d714bac8d0c9a91916cee684'}>, <Document: {'content': 'and then break it up into five bins and the idea is to use any four bins to train four ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '68dc78a5360e18e85eeb77fbce0433e0'}>, <Document: {'content': 'to train and one to validate. So, you might sit there and say hay you know you just sounds ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bbf3330aab5101fddf02b709e585d36d'}>, <Document: {'content': 'like, you are breaking up the training and validation at the latest stage than at a earlier ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'befbf6c064f6da65c5cbddea2bcf86b7'}>, <Document: {'content': 'stage. But, the answer is no there is one more step, once you do that shuffle them around ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe36529c4b2a399619ec763c9a6905fc'}>, <Document: {'content': 'meaning, now in this particular graph I have shown you let us call them 1, 2, 3, 4 and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bea8dee6c3365cc91700965e32d3582f'}>, <Document: {'content': '5 in this graph you are using 1, 2, 3 and five to train and you are validating. So, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '939810b315fdbde3386842a2ef9fa38b'}>, <Document: {'content': 'this is train and you are validating with 4. Now, permute and the next step do 1, 2, 3, 4 and validate on 5 next step permute 2, 3, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '191ce077c936e1bd5b85e55010f0ecca'}>, <Document: {'content': '4, 5 and validate on 1 and so you like that you keep going till you done all the five ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'c95e2fa8b13a679d091d78350ff498aa'}>, <Document: {'content': 'combinations, where you would a validate on step and you take the cumulative validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '33153a1306f1f002c4e4d9c1c37d7339'}>, <Document: {'content': 'results. So, you will take the cumulative validation results to see how well to make ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd8b45f7c7398f99f05d3cde61caeccb3'}>, <Document: {'content': 'decisions in terms of validation and so this is called k fold cross validation. So, one question that comes up fairly frequently is what should the value of k be, in k fold ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fafb9ce18305bef3ceaf02c981614d30'}>, <Document: {'content': 'cross validation, given you tentative idea there it is typically some are between five and ten, but again that is not something that is cast in stone and it is more important ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a248d69d91e2b48e8a07ac62844d93b'}>, <Document: {'content': 'that you understand what it is means to choose high value of k verses low value of k. In ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e04fc981dbfd4566a1fad2b65b51195'}>, <Document: {'content': 'the simplest sense our high value of k in a k fold cross validation leads to a model, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fea57119ce33f45836621112898b39fd'}>, <Document: {'content': 'where leads to an assessment I should say which is of very low bias, but of high variance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '62d3ab1d52e2769e84f33b2953f97561'}>, <Document: {'content': 'and a very low k. So, k of like 2 or 3 or so. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7e1a9fa063670196c8a1d6c26d166e53'}>, <Document: {'content': 'So, essentially could have low variance, but a higher bias and one extension of for instance ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6c44136e1136d485413086cf66d4fae4'}>, <Document: {'content': 'the cross validation, where you have a very high k, where you like it to have an unbiased ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4dbda954e1e52abf9aa9a330e4b05f2d'}>, <Document: {'content': 'assessment, but assessment with lots of variance is the case of the leave one out cross validation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd286d264d4d9cddb64ea9ff6f6a9d1c6'}>, <Document: {'content': 'The idea here is that if you have n data points I am going to use n minus 1 data points to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8537e40ff41606f8a4d80f28624a7619'}>, <Document: {'content': 'do the training and I am going to validate on that one data point that I left out and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b53f3010e23a80c35c22d15786d15cad'}>, <Document: {'content': 'what I will do is I will keep just like in k fold cross validation, how you change the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fbc3f3e572b887e4ec85f750f71f420d'}>, <Document: {'content': 'training sets in the validation sets, how you shuffle them, how you permute them, the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '59a23969675643ca0c734104c0c1587f'}>, <Document: {'content': 'same way here going to leave one data point out for validation train on the others and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b6b3f3292d92c17d59f51c0f24c741d5'}>, <Document: {'content': 'then predict on this and then keep doing that interactively. So; obviously, with that if it is essentially like k is equal to n then the cross validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bcf01313766b211e0e53b563fba657a3'}>, <Document: {'content': 'is almost completely unbiased, but can have high variance, because the n training sets ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '32a49f81c51cbd88a7c8d1380bbde22e'}>, <Document: {'content': 'are so similar to one and other and some approach like that is also computationally fairly cumbersome. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '66a5e7756d133e0d5e4f2f7cd8342743'}>, <Document: {'content': 'But, if you have that approach then leave one out the cross validation is also something ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '19a77b986808d1aec899ea33afc9e83b'}>, <Document: {'content': 'that you might consider. So, I hope that gives you an idea of cross validation and more broadly the use of you know the idea behind validation as a means ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8e3c51abab214c02fce4ab19baeecbd'}>, <Document: {'content': 'of model selection and this whole thing of breaking up training for creating models validation ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '396fbd4b28c47cd8d633700d33b7cdfa'}>, <Document: {'content': 'for assessing and kind of you know selecting models and test for pure assessment, but no ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1ce21adbe390f3083593b7a0f112c69a'}>, <Document: {'content': 'for the selection and decision making. Thank you. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '541c51306c05e127dac8705c799e97e'}>, <Document: {'content': 'English - NPTEL Official ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6979ef20a0c434804bffc03d2e331dd'}>, <Document: {'content': 'Support Vector Machines ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6b2a92c6011cca1e06195a9687d6dc2b'}>, <Document: {'content': 'Hello and welcome to this module on Support Vector Machines. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cf189f7c3b95bd8590eaa950e494f9e7'}>, <Document: {'content': 'So, we have been looking at the variety of classifier so far and one of the things, let ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '27ed0ef364e89a473348b3469634c9fe'}>, <Document: {'content': 'us look at the linear classifier. So, the one of the thing is, if I have data points ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d0fb6fd6c285b164ab894055fb7549e'}>, <Document: {'content': 'that are even perfectly separable, here is a class and here is another class, you can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1a966fd464b8db2e1544813fba62df2c'}>, <Document: {'content': 'see that they are very clearly separated. But, when I train a linear classifier it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7add4eb30b3b648ca36498db49cb8f58'}>, <Document: {'content': 'not entirely clear, which of these many possible lines that could separate the data, would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '624d0093994a52e753b5579ce2a8a6f8'}>, <Document: {'content': 'your classifier end up learning. There are many, many different lines that could separate ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f3a76bd4b20e4142cf855c00c6628125'}>, <Document: {'content': 'the data and we are not sure, what your classifier would end up learning. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d8d68cc404de8eb6e1dd6e50a2b34a5'}>, <Document: {'content': 'So, support vector machines initially were born out of and need to answer this question. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '679919a0a0887087dadd0b2e08f57156'}>, <Document: {'content': 'Among all of these different lines or all of these different decision surfaces that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa3c6f5ff535b728096f8234e273241a'}>, <Document: {'content': 'you could use for separating the data given to you, which of those is the best decision ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe9a8072da5f9bf989112b76ca33a099'}>, <Document: {'content': 'surface? Some of all those alternatives which you think should be the best decision surface. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aa06cd0b4c2cd208ba3a1dc7b3061ea0'}>, <Document: {'content': 'So, one answer to this question is to define an optimal separating, if an optimal separating ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57eaf8161a89825e7dc812601aab43e8'}>, <Document: {'content': 'hyper plane as the surface, such that the nearest data point to the surface is as far ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f2757d6259f9c95d337dfe985a316e5e'}>, <Document: {'content': 'away as possible among all of it is surfaces. So, here is a separating line and the nearest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'df09de211d8e77f5d15d23b84daaf148'}>, <Document: {'content': 'data point to that is that or that or that. So, if you think about it, so the nearest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e200fd3e6bca3d3ba1cf15dd6b9663e'}>, <Document: {'content': 'data point cannot belong to just one class. So, I could draw a line like this, but then ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1320b92e45549c3d336ea061e653f685'}>, <Document: {'content': 'there would mean that I am reducing the distance of the data point to the separating surface ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77a2609e5cb2382ed2657ec15a3cee4b'}>, <Document: {'content': 'or if I go this way, again I will be the reducing the distance of the data point to the surface ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '65cb3921f2db49c7bbe8a61da94ba7fc'}>, <Document: {'content': 'in one class or the other. When I say that you are maximizing the distance of the closest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'de2888126c8df058b153df974d1c3702'}>, <Document: {'content': 'data point to the separating hyper plane, that essentially means that the closest data ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '94510f568f72fb05ed207ea20d8bd57b'}>, <Document: {'content': 'point from either class is at the same distance away from the hyper plane. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ac82c392b9e3af0e059880872a00e721'}>, <Document: {'content': 'So, this distance and being same as this distance would be the same as this distance and this. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '61f3bd5a33f6edeb4b8391f20bf0316f'}>, <Document: {'content': 'So, this distance of the closest data point to the separating surface is known as the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b0b2e4ba2a5fac541eb1e51e4e8a810'}>, <Document: {'content': 'margin of the classifier, which we will denote by ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1aaa21e1e3e27420e0ed2be5d95fb5a1'}>, <Document: {'content': 'm. So, the goal of finding a separating optimal separating hyper plane is essentially to find ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ce6b3a4c7eba0e110cf97b8dd90a50ca'}>, <Document: {'content': 'the classifier, such that this margin m is as large as possible. So, let us step back ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '463bfbad2bd64177f05cae399eaac00c'}>, <Document: {'content': 'and think about what such a line means. You know in all linear classifiers we have seen ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '589c737741d6522efd9177035018bae7'}>, <Document: {'content': 'so far, so we know that we are going to say something like. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '227e38ea926acbff5848d86ff3510038'}>, <Document: {'content': 'So, y is beta naught plus beta transpose x, for convenience sake here I will write it ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '541b427e2b1a65865e4a4ad9677e2238'}>, <Document: {'content': 'as x transpose beta, since we are taking inner products that is fine. So, a line like this ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aaeb4d21342cbb60b0925aafbc7719e4'}>, <Document: {'content': 'could essentially be obtained by setting this beta naught plus x transpose beta equal to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '57f334e1b9381782d6814b65d72f2ed4'}>, <Document: {'content': '0. So, all the data points on this line or those data points for which beta naught plus ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6ec09f680d7f9558753f5d4da9ab5989'}>, <Document: {'content': 'x transpose beta evaluates to 0, so that is the equation of the line here. So, if it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '733539af99e3aff407ef2bb26dd744b6'}>, <Document: {'content': 'negative beta naught plus beta transpose x is less than 0, so we are going to say that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'bb837ffc49f6e6991bdd5fc93e9adf75'}>, <Document: {'content': 'x is of class minus 1 and if beta naught plus beta transpose x is greater than 0, we will ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2909f1cfafc86b62f6f5bd7701e29617'}>, <Document: {'content': 'say that next class plus 1. So, remember that, so we will, we using some ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3a68446c8dc179f63e45ef2f83641b1a'}>, <Document: {'content': 'kind of encoding for the class. The class could be does not buy a computer or buys a ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e7254c7d71249aa243d01a72f3628a67'}>, <Document: {'content': 'computer, he is sick, he is healthy. I mean the classes could be many different things, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '70e8b5f74b5c7e429158ad1377853f18'}>, <Document: {'content': 'but numerically we are going to be assigning some encoding for the class and in this case, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '19b0a1c4c30aac06a2739afa1bc85e34'}>, <Document: {'content': 'I choose to use minus 1 and plus 1 as the encoded. There is a reason for that as we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9f29a77be55331604f0319889105c9e0'}>, <Document: {'content': 'will see shortly. So, if beta naught plus beta transpose x is less than 0 and I say, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b81046b16ada958798392c247d8d0a94'}>, <Document: {'content': 'it is class minus 1. But, in this case what I really want, I do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '612cf3c2853e458e95ad365da539d40'}>, <Document: {'content': 'not want it to be this less than 0, but I want it to be at least m away from the hyper ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '486670f385b5c71c28bb19d303b342a'}>, <Document: {'content': 'plane. I want it to be m away from the line beta naught plus beta transpose x equal to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '5df23c1d5856163df98a4c305345cde4'}>, <Document: {'content': '0. So, I might use x transpose and beta transpose x interchangeably at points, but as you know ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3e48d0be55302e057c82f1130d09768b'}>, <Document: {'content': 'they are inner products, so that is fine. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '763e45aeaae274e6b0bd063322e6f8db'}>, <Document: {'content': 'So, what I really want is, so y i is plus 1 I want beta naught plus x i transpose beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '240889cafee07477438053b56a91d794'}>, <Document: {'content': 'to be greater than m. What happens if y i is minus 1? I really want it to be at least ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '77c842ecabffa6a95b39b74f60d96818'}>, <Document: {'content': 'm away in that case as well, but then we know that beta naught plus x i transpose beta would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '4ea17b0b4c1309f66b5b3a2943f08b33'}>, <Document: {'content': 'be negative, when the class is minus 1. So, what I do is I essentially just multiplied ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9b37050e1a85b891496c678832de4db1'}>, <Document: {'content': 'by the actual class variable and I want this whole distance. Because, if y i is plus ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e0c12b7e1e694bbd4caa63a13becd99'}>, <Document: {'content': '1 I would like this also to be plus, the positive and I want it to be at least m away from the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd0d718f52c2d00c644d26ae66fcef87'}>, <Document: {'content': 'hyper plane and y i is minus 1, this is going to be negative. So, the product is going to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '616a19188c6697d1755fbc7c58a2965b'}>, <Document: {'content': 'be positive and I want that to be at least m away from the hyper plane. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '503e3b10bb0153f0df218e721f3e2e66'}>, <Document: {'content': 'So, this is thick and strained that we want to satisfy and what is our goal. If we remember, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a506d95d75c1df13b9e62eccf01781a8'}>, <Document: {'content': 'our goal is to make sure that this m is as large as possible. So, what will do is, we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'e9c558b8fcd6535f0a7aedc63a6a6ecd'}>, <Document: {'content': 'will say maximize m beta naught beta subject toÉ So, I am going to maximize the margin ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2d2924ab0a2747697b816dbd4807285'}>, <Document: {'content': 'm over beta naught and beta, subject to the constraints that y i times x i transpose beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '6fe3898fb6f003c6bb445ba6cd0c4e83'}>, <Document: {'content': 'plus beta naught is greater than or equal to m, for every data point in my training ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd6851c63241602710a5c446c8024c58a'}>, <Document: {'content': 'data. So, this kind be done assuming that all the ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd12985897c0ec46ffe10cb398cdb4880'}>, <Document: {'content': 'data is nicely separated. So, and I can actually draw a linear surface that separates the data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1b45c61770ed04679903b3872da9a5b0'}>, <Document: {'content': 'So, if a kind of linear surface that separates data, then I can come up with at least one ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd2e8245cbd16731e9f51a7f721b4b7c4'}>, <Document: {'content': 'surface that satisfies this constraints for some value of m and essentially, I have to ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '55e1e6485deca42b249d2cf2b1a9a89c'}>, <Document: {'content': 'find value of m that is maximum here. But, one thing if you look at this equation or ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3856ba77ff6450980051734dfb16b4a0'}>, <Document: {'content': 'the constraint that we have written, so I can arbitrarily increase the value of beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f886c97f995cf13a72c2ce542b5a1411'}>, <Document: {'content': 'and make this value as large as I want. So, I need to have some constraint on beta ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7de7dd28333fb3ec78e1517294b1c767'}>, <Document: {'content': 'as well. So, what we will do is, we will constraint the norm of beta to be equal to 1. So, we ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'f0a7b0b8170d57f46e258a6e184e0e54'}>, <Document: {'content': 'will not look at all possible weights beta naught and beta, we will only look at those ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'eaf49152971b47508a402f25a8206841'}>, <Document: {'content': 'weights insist that the size of beta is constraint to be 1. So, the norm of beta is, you could ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cc7eaa8a7e2b266a89c5e52c8156e70c'}>, <Document: {'content': 'take the Euclidean norm of beta, I am saying that the norm of beta should be 1. So, I hope ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2eeac5313e542538bc6fd198911505dc'}>, <Document: {'content': 'the formulation of the optimization problem so far is clear. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '58f1669fccf7079c51a1009915b8402'}>, <Document: {'content': 'So, it is essentially saying that I want all my data points to be at least a distance m ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'd51ba1d43b4c05866154e574f6dd0e25'}>, <Document: {'content': 'away from the hyper plane and subject to that constraint and subject to my beta being norm ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3eb14ae1f7499fd5b4921480941dd8c'}>, <Document: {'content': 'one, I want to maximize the margin. So, this is a pretty works and constraint, so we can ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'da88ff410acac4cc4eb5ac495dfdc9e7'}>, <Document: {'content': 'try to get rid of it by changing the other inequality constraints to by normalizing them ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '700dd01bca6f1b8622d7ccc6052d88d4'}>, <Document: {'content': 'with the beta. So, this again allows me to achieve the same effect of not getting a high ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a64867c87a1850fd19273df71cae1570'}>, <Document: {'content': 'value for m just by increasing the size of beta, because I am dividing by the size of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '720cd84cb594a0f382ed69726c8e006b'}>, <Document: {'content': 'beta. So, that achieves the same constraint and ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '64c003ccc8c77120c7f0cf6aaa356cf9'}>, <Document: {'content': 'you can essentially write it like that. So, one thing that we should note here is that, ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '497acd5f4ad6cbcfe667e2498f2c7e9c'}>, <Document: {'content': 'if a specific beta satisfies these constraints, any positively scale version of beta would ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7b10939a7667edd01d0c189b64019b2f'}>, <Document: {'content': 'also satisfies the constraints. I can just multiply by some positive number, if it is ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1429a29be9e055babfb2546de75ecb6f'}>, <Document: {'content': 'originally all, for all the exercise was giving me negative values larger than m or minus ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '52fc0b334a62a3a39b5aef73aca1bb2e'}>, <Document: {'content': 'm or positive values larger than m, just multiplying it by a positive quantity will not change ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '809ecc2b8c41998c57a6efc5616f8991'}>, <Document: {'content': 'anything. It will still give me negative values that are lesser than minus m or positive values ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '2dddfdc88e28f8f611478020e1b36809'}>, <Document: {'content': 'that are greater than m. Therefore I can essentially choose a specific value for beta, such that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1109515c9f9a2ea6d00b0909d98de48c'}>, <Document: {'content': 'this evaluates to 1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '92f4c134bb25666d1b226c01a49f5339'}>, <Document: {'content': 'So, I set, so accept norm beta equal to 1 by m, so that this constraint becomes y i ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '989057cf940aabacc4740b4c7edcb398'}>, <Document: {'content': 'x i transpose beta is greater than equal to 1 subject to the constraint that, you are ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'cfdd1e8830a414e913eee216a07d418e'}>, <Document: {'content': 'finding the smallest such beta. So, this optimization problem then becomes, this is optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'ec095a2d59de8f1145b69a03c8fcdf46'}>, <Document: {'content': 'problem of maximizing the margin, now essentially becomes the problem of finding the smallest ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'b54840647d36c405abc5ac8abfb96f17'}>, <Document: {'content': 'beta, such that this conditions are satisfied. So, this is essentially means that my margin ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'aeb4d06c8a985b00459e284a942a30e7'}>, <Document: {'content': 'here is going to be 1 over now beta. So, to make it mathematically more convenient ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe050db5520f94abd15cd8a49f2722cd'}>, <Document: {'content': 'I am going to minimize the quadratic form of that. So, essentially I will be minimizing ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7c309bfe092a6555064666e26b7b37eb'}>, <Document: {'content': 'this square of beta, since it is norm any way. So, this would be positively to begin ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fe940a0bbfafe748eaf96737cd38e7d1'}>, <Document: {'content': 'with, so I can minimize this square, that is not a problem and so that is my final optimization ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '7fbae32ff6c0b7e62830bf0058fec513'}>, <Document: {'content': 'problem. So, this is the final optimization problems, where I am saying that, so together ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '8a90b171759672e01c7469d92c6adc78'}>, <Document: {'content': 'with these constraints a kind of define a slab around the separating hyper plane, I ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '3d2d39dda815043aae4ae75360e56441'}>, <Document: {'content': 'define a slab around these separating hyper plane of with 1 by beta. So, making sure that ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1c3091ee25b82cbc38fecf4dae03d669'}>, <Document: {'content': 'there are no data points with in this region, so I am trying to now maximize the width of ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '95d5158430c48173acd8f572ff2f2d66'}>, <Document: {'content': 'this region, so that there are no data points in that region, that is essentially the idea ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '9d1a7107a31093bea43d4006f6d8224d'}>, <Document: {'content': 'behind this optimization problem. So, this defines the basic optimization problem ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'fd85b616fc65dd7214ae70d61c3380d1'}>, <Document: {'content': 'in the case of support vector machines. So, in the next module we will look at, how do ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '60783a0c9521bf8e011da3d3b61174cd'}>, <Document: {'content': 'you go about setting up a solution for this optimization problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '37c472f1fd7805a1f07c68c3bdab46b8'}>, <Document: {'content': \"\\nUndoubtedly, Data Science is the most revolutionary technology of the era. It's all about deriving useful insights from data\\nin order to solve real-world complex problems. Hi all I welcome you to this session\\non Data Science full course that contains everything that you need to know in order to master data science.\\nNow before we get started, let's take a look at the agenda. The first module is an reduction to data science\\nthat covers all the basic fundamentals of data science followed by this. We have statistics and probability module\\nwhere you'll understand the statistics and math behind data science and machine learning algorithms.\\nThe next module is the basics of machine learning where will understand what exactly machine learning is the different types\\nof machine learning the different machine learning algorithms and so on the next module is the supervised learning\\nalgorithms module where we'll start by understanding the most basic With them or which is linear regression.\\nThe next module is the logistic regression module where we will see how logistic regression can be used to solve\\nclassification problems. After this we'll discuss about decision trees and we'll see how decision trees can be used to solve\\ncomplex data-driven problems. The next module is random Forest here will understand\\nhow random Forest can be used to solve classification problems and regression problems with the help\\nof use cases and examples. The next module will be be discussing is the k-nearest neighbor module.\\nWe will understand how gain and can be used to solve complex classification problems followed by this.\\nWe look at the naive bias module, which is one of the most important algorithms in the Gmail spam detection.\\nThe next algorithm is support Vector machine where we will understand how svm's can be used\\nto draw a hyperplane between different classes of data. Finally. We move on to the unsupervised learning module where we\\nwill understand how genes can be used for clustering. And how you can perform Market Basket analysis by using\\nAssociation rule mining. The next module is reinforcement learning where we will understand the different concepts\\nof reinforcement learning along with a couple of demonstrations followed by this bill.\\nLook at the Deep learning module where we will understand what exactly deep learning is what our neural networks\\nwith different types of neural networks. And so on. The last module is the data science interview questions module\\nwhere we will understand the important concepts of data. Along with a few tips in order to Ace the interview now\\nbefore we get started make sure you subscribe to Adorama YouTube channel in order to stay updated\\nabout the most trending Technologies data science is one\\nof the most in-demand Technologies right now. Now this is probably because we're generating data at an Unstoppable pace.\\nAnd obviously we need to process and make sense out of this much data. This is exactly where data science comes in in today's session.\\nWe'll be talking about data science in depth. So let's move ahead and take a look at today's agenda. We're going to begin\\nwith discussing the various sources of data and how the evolution of technology and introduction of IOD\\nand social media have led to the need of data sign next. We'll discuss how Walmart is using insightful patterns\\nfrom their database to increase the potential of their business. After that. We will see what exactly data science is,\\nthen we'll move on and discuss who are data scientist is where we will also discuss the various skill sets.\\nNeeded to become a data scientist next we can move on to see the various data science job roles\\nsuch as data analyst data architect data engineer and so on after this we will cover the data life cycle where we will discuss\\nhow data is extracted processed and finally use as a solution. Once we're done with that. We'll cover the basics of machine learning\\nwhere we'll see what exactly machine learning is and the different types of machine learning next. We will move onto the K means algorithm\\nand we'll discuss a use case of the k-means clustering after which we Discuss the various steps involved\\nin the k-means algorithm and then we will finally move on to the Hands-On part where we use the k-means algorithm to Cluster movies\\nbased on their popularity on social media platforms, like Facebook at the end of today's session\\nwill also discuss about what a data science certification is and why you should take it up. So guys, there's a lot to cover in today's session.\\nLet's jump into the first topic. Do you guys remember the times when we have telephones and we\\nhad to go to PC your boots in order to make a phone call. Call now those things are very simple because we didn't generate a lot of data.\\nWe didn't even store the contacts and our phones or our telephones. We used to memorize phone numbers back then or you know,\\nthese have a diary of all our contact but these days we have smartphones with store a lot of data.\\nSo there's everything about us in our mobile phones. We have images we have contacts. We have various apps.\\nWe have games. Everything is stored on a mobile phones these days similarly the PCS that we use in the earlier times.\\nIt used to process very little data. All right, there was A lot of data processing needed because technology was an evolved that much.\\nSo if you guys remember we use floppy disk back then and floppy. This was used to store small amounts of data,\\nbut later on hard disks were created and those used to store GBS of data. But now if you look around there's data\\neverywhere around us. All right, we have a data stored in the cloud. We have data in each and every Appliance at our houses.\\nSimilarly. If you look at smart cars these days they're connected to the internet they connected to a mobile phones\\nand this also generates a lot of data. What we don't realize is that evolution of technology has generated a lot of data.\\nAll right. Now initially there was very little data and most of it was even structured only a small part\\nof the data was unstructured or semi-structured. And in those days you could use Simple bi Tools in order\\nto process all of this data and make sense out of it. But now we have way too much data and order to process this much data.\\nWe need more complex algorithms. We need a better process. All right, and this is where data science comes in now guys,\\nI'm not going to get into the depth of data science. Yet I'm sure all of you have heard of iot or Internet of things.\\nNow. Did you guys know that we produce 2.5 quintillion bytes of data each day.\\nAnd this is only accelerating with the growth of iot. Now iot or Internet of Things is just a fancy term\\nthat we use for network of tools or devices that communicate and transfer data through the internet.\\nSo various devices are connected to each other through the internet and they communicate with each other right\\nnow the communication happens by exchange of data or by. Generation of data now these devices include the vehicles.\\nWe drive the include our TVs of coffee machines refrigerators washing machines and almost everything else\\nthat we use in a daily basis. Now, these interconnected devices produce an unimaginable\\namount of data guys iot data is measured in zettabytes and one zettabyte is equal to trillion gigabytes.\\nSo according to a recent survey by Cisco. It's estimated that by the end of 2019,\\nwhich is almost here. The iot will generate more than five hundred zettabytes\\nof data per year. And this number will only increase through time. It's hard to imagine data in that much volume,\\nimagine processing analyzing and managing this much of data. It's only going to cause as a migraine\\nso guys having to deal with this much data is not something that traditional bi tools can do.\\nOkay. We no longer can rely on traditional data processing methods. That's exactly why we need data science.\\nIt's our only hope right now now let's not get into the details here. Yet moving on. Let's see how social media is adding on\\nto the generation of data. Now the fact that we are all in love with social media. It's actually generating a lot of data for us.\\nOkay. It's certainly one of the fuels for data creation Now all these numbers that you see on the screen are generated every minute\\nof the day. Okay, and this number is just going to increase so for Instagram it says that approximately 1.7 million pictures uploaded\\nin a minute and similarly on Twitter approximately. A hundred and forty eight thousand tweets are published\\nevery minute of the day. So guys imagine in one are how much that would be and then imagine in 24 hours.\\nSo guys, this is the amount of data that is generated through social media. It's unimaginable. Imagine processing this much\\ndata analyzing it and then trying to figure out, you know, the important insights from this much data analyzing\\nthis much data is going to be very hard with traditional tools or traditional methods. That's why data science was introduced data science\\nis a simple process that will just extract the useful information from data. All right, it's just going to process\\nand analyze the entire data and then it's just going to extract what is needed now guys apart from social media and iot,\\nthere are other factors as well which contribute to data generation these days all our transactions are done online, right?\\nWe pay bills online. We shop online. We even buy homes online these days you can even sell your pets on oil excuses.\\nNot only that when we stream music and Watch videos on YouTube all of this is generating a lot\\nof data not to forget. We've also brought Health Care into the internet wall. Now there are various watches like bit fit\\nwhich basically trans our heart rate and it generates data about a health conditions education is\\nalso an online thing right now. That's exactly what you are doing right now. So with the emergence of the internet,\\nwe now perform all our activities online. Okay, obviously, this is helping us, but we are unaware of how much data we are generating\\nwhat can be done with All of this data and what if we could use the data that we generated to our benefit?\\nWell, that's exactly what data science does data science is all about extracting the useful insights from data and using\\nit to grow your business. Now before we get into the details of data science, let's see how Walmart uses data science to grow that business.\\nSo guys Walmart is the world's biggest retailer with over 20,000 stores in just 28 countries.\\nOkay. Now, it's currently building the world's biggest. Good Cloud, which will be able to process two point five petabytes\\nof data every hour now. The reason behind Walmart success is how the user customer data\\nto get useful insights about customers shopping patterns. Now the data analyst and the data scientist at Walmart.\\nThey know every detail about their customers. They know that if a customer buys Pop-Tarts, they might also buy cookies, how do they know all of this?\\nLike how do they generate information like this now the user data that they get from their customers.\\nHours and the analyze it to see what a particular customer is looking for. Now. Let's look at a few cases\\nwhere Walmart actually analyze the data and they figured out the customer needs. So let's consider the Halloween\\nand the cookie sales example now during Halloween sales Analyst at Walmart took a look at the data.\\nOkay, and he found out that a specific cookie was popular across all Walmart stores.\\nSo every Walmart store was selling these cookies very well, but he found out that they would to stores which are not selling.\\nA DOT. Okay. So the situation was immediately investigated and it was found that there was a simple stocking oversight.\\nOkay, because of which the cookies were not put on the shelves for sale. So because this issue was immediately identified\\nthey prevented any further loss of sales now another such example, is that true Association rule mining Walmart found out\\nthat strawberry Pop-Tart sales increased by seven times before a hurricane. So a data analyst at Walmart identified the association\\nbetween ha Hurricane and strawberry pop tarts through data mining now guys. Don't ask me the relationship between Pop-Tarts\\nand Harry Caine, but for some reason whenever there was a hurricane approaching people really wanted to eat strawberry Pop-Tart.\\nSo what Walmart did was they place all the strawberry Pop-Tarts? I will check out before a hurricane would occur.\\nSo this way the increase sales of the Pop-Tarts Now, where's this is a natural thing. I'm not making it up.\\nYou can look it up on the internet. Not only that Walmart is analyzing the data generated\\nby Social media to find out all the training product so through social media. You can find out the likes and dislikes of a person right?\\nSo what Walmart did is they are quite smart the user data generated by social media to find out what products are trending\\nor what products are liked by customers. Okay an example of this is 1 mod analyze social media data to find out\\nthat Facebook users were crazy about cake pops. Okay, so Walmart immediately took a decision\\nand they introduced cake pops into the Walmart stores. So guys the only reason Walmart is so successful is\\nbecause the huge amount of data that they get they don't see it as a burden instead. They process this data analyze\\nit and then you try to draw useful insights from it. Okay, so they invest a lot of money a lot of effort\\nand a lot of time and data analysis. Okay, they spend a lot of time analyzing data in order to find any hidden patterns.\\nSo as soon as they find out hidden pattern or association between any two products, these are giving out offers\\nor Started having discount or something along that line. So basically Walmart uses data in a very effective manner the analyzer very, well.\\nThey process the data very well and they find out the useful insights that they need in order to get more customers or in order to improve their business.\\nSo guys, this was all about how Walmart uses data science now, let's move ahead and look at what is data set now\\nguys data science is all about uncovering findings from data. It's all about surfacing the hidden insights\\nthat can help. Ponies to make smart business decisions. So all these hidden insights or these hidden patterns can be used to make better decisions\\nin a business now an example of this is also Netflix. So Netflix, basically analyzes the movie viewing patterns\\nof users to understand what drives user interest and to see what users want to watch and then\\nonce they find out they give people what they want. So guys actually data has a lot of power.\\nYou should just know how to process this data and how to extract the useful information. From data.\\nOkay. That's what data science is all about. So guys a big question over here is how do data scientists get useful insights from data.\\nSo it's all starts with data exploration. Whenever a data scientist comes across any challenging question\\nor any sort of challenging situation, they become detectives so the investigative leads\\nand they try to understand the different patterns or the different characteristics of the data. Okay.\\nThey try to get all the information that they can from the data and then Then they use it for the betterment of the organization\\nor the business. Now, let's look at who is a data scientist. So guys the data scientists\\nhas to be able to view data through a quantitative lengths. So guys knowing math is one of the very important skills\\nof data scientists. Okay. So mathematics is important because in order to find a solution you're going to build a lot of predictive models\\nand these predictive models are going to be based on hard math. So you have to be able to understand all\\nthe Underlying mechanics with these models most of the predictive models most of the algorithms require mathematics.\\nNow, there's a major misconception that data science is all about statistics.\\nNow, I'm not saying that statistics is an important. It is very important, but it's not the only type of math that is utilized\\nin data science. There are actually many machine learning algorithms which are based on linear algebra.\\nSo guys overall you need to have a good understanding of math and apart from that data scientist.\\nEli's technology, so data scientists have to be really good with technology. Okay. So their main work is they utilize all the technology\\nso that they can analyze these enormous data sets and work with complex algorithms.\\nSo all of this requires tools, which are much more sophisticated than Excel so there's data scientist need to be very efficient\\nwith coding languages and few of the core language has associated with data science include SQL python R & sass.\\nIt is also important for a data scientist. Be a tactical business consultant. So guys business problems can be on a sword by data scientist\\nsince our data scientists work so closely with data they know everything about the business.\\nIf you have a business and you give the entire data set of your business stored data scientist,\\nhe know each and every aspect of your business. Okay? That's how data scientists work. They get the entire data set.\\nThey study the data set the analyze it and then we see where things are going wrong or what needs to be done more or what?\\nNeeds to be excluded. So guys having this business Acumen is just as important as having skills\\nin algorithms or being good with math and technology. So guys business is also as important as\\nthese other fields now, you know who our data scientist is. Let's look at the skill sets that a data scientist names.\\nOkay, it always starts with Statistics statistics will give you the numbers from the data.\\nSo a good understanding of Statistics is very important for becoming a data scientist. You have to be familiar with satisfaction.\\nContest distributions maximum likelihood estimators and all of that apart\\nfrom that you should also have a good understanding of probability Theory and descriptive statistics.\\nThese Concepts will help you make Better Business decisions. So no matter what type\\nof company or role you're interviewing for. You're going to be expected to know how to use the tools of the trade.\\nOkay. This means that you have to know a statistical programming language like our or Python and also you'll need to know or database.\\nWiring language like SQL now the main reason why people prefer our and python is because of the number of packages\\nthat these languages have and these predefined packages have most of the algorithms in them.\\nSo you don't have to actually sit down and code the algorithms instead. You can just load one of these packages\\nfrom their libraries and run it. So programming languages is a must at the minimum. You should know our\\nor python and a database query language now, let's move on to data extraction and processing.\\nSo guys That you have multiple data sources like mySQL database Mongo database.\\nOkay. So what you have to do is you have to extract from such sources and then in order to analyze\\nand query this database you have to store it in a proper format or a proper structure.\\nOkay, finally, then you can load the data in the data warehouse and you can analyze the data over here.\\nOkay. So this entire process is called extraction and processing. So guys extraction\\nand processing is all about getting data. From these different data sources and then putting it in a format\\nso that you can analyze it now next is data wrangling and exploration now guys data wrangling is one\\nof the most difficult tasks in data science. This is the most time-consuming task because data wrangling is all about cleaning the data.\\nThere are a lot of instances where the data sets have missing values or they have null values\\nor they have inconsistent formats or inconsistent values and you need to understand what to do with such values.\\nThis is Data wrangling or data cleaning comes into the picture then after you're done with that. You are going to analyze the data.\\nSo where's after data wrangling and cleaning is done. You're going to start exploring. This is where you try to make sense out of the data.\\nOkay, so you can do this by looking at the different patterns in the data the different Trends outliers\\nand various unexpected results in all of that. Next. We have machine learning. So guys if you're a large company\\nor with huge amounts of data or if you're working at a company. See where the product is data driven,\\nlike if you're working in Netflix or Google Maps, then you have to be familiar with machine learning methods, right?\\nYou cannot process large amount of data with traditional methods. So that's why you need a machine learning algorithms.\\nSo there are few algorithms. Like knok nearest neighbor does random Forest this K means algorithm this support Vector machines,\\nall of these algorithms. You have to be aware of all of these algorithms and let me tell you\\nthat most of these algorithms can be implemented. Using our or python libraries. Okay, you need to have an understanding\\nof machine learning. If you have large amount of data in front of you which is going to be the case for most of the people right now\\nbecause data is being generated at an Unstoppable Pace earlier in the session we discussed\\nhow much of data is generated. So for now knowing machine learning algorithms and machine learning Concepts is a very required skill\\nif you want to become a data scientist, so if you're sitting for an interview as a data scientist, you will be asked machine learning.\\nSeems you will be asked how good you are with these algorithms and how well you can Implement them. Next we have big data processing Frameworks.\\nSo guys, we know that we've been generating a lot of data and most of this data can be structured or unstructured as well.\\nSo on such data, you cannot use traditional data processing system. So that's why you need\\nto know Frameworks like Hadoop and Spark. Okay. These Frameworks can be used to handle big data lastly.\\nWe have data visualization. So guys data visualization is Is one of the most important part\\nof data analysis, it is always very important to present the data in an understandable and Visually appealing format.\\nSo data visualization is one of the skills that data scientists have to master. Okay, if you want to communicate the data with the end users\\nin a better way then data visualization is a must so guys are a lot of tools which can be used for data visualization tools like Diablo\\nand power bi are few the most popular visualization tools. So with this we sum up the entire skill set\\nthat is needed to become a data scientist apart from this you should also have data-driven problem solving approach.\\nYou should also be very creative with data. So now that we know the skills that are needed to become a data scientist.\\nLet's look at the different job roles just data science is a very vast field. There are many job roles under data science.\\nSo let's take a look at each role. Let's start off with a data scientist. So there's data scientists have to understand.\\nThe challenge is over business and they have to offer the best solution using data analysis\\nand data processing. So for instance if they are expected to perform predictive analysis, they should also be able to identify Trends and patterns\\nthat can have the companies in making better decisions to become a data scientist. You have to be an expert in our Matlab SQL Python and other\\ncomplementary Technologies. It can also help if you have a higher degree in mathematics\\nor computer engineering next we have data. An analyst so a data analyst is responsible\\nfor a variety of tasks, including visualization processing of massive amount of data and among them.\\nThey have to also perform queries on databases. So they should be aware of the different query languages\\nand guys one of the most important skills of a data analyst is optimization. This is because they have to create and modify algorithms\\nthat can be used to pull information from some of the biggest databases without corrupting the data\\nso to become Be done. You must know Technologies such as SQL our SAS and python.\\nSo certification in any of these Technologies can boost your job application. You should also have a good problem solving quality.\\nNext. We have a data architect. So a data architect creates the blueprints for a data management\\nso that the databases can be easily integrated centralized and protected with a best security measures.\\nOkay. They also ensure that the data Engineers have the best tools and systems to work with So to become a data architect,\\nyou have to have expertise and data warehousing data modeling extraction transformation and loan.\\nOkay. You should also be well versed in Hive Pig and Spark now apart from this there are data Engineers.\\nSo guys, the main responsibilities of a data engineer is to build and test scalable Big Data ecosystems.\\nOkay, they are also needed to update the existing systems with newer or upgraded versions\\nand they are also responsible for improving the efficiency. For database now. If you are interested in a career as a data engineer,\\nthen technologies that require hands-on experience include Hive nosql are Ruby Java C++ and Matlab,\\nit would also help if you can work with popular data apis and ETL tools next.\\nWe have a statistician. So as the name suggests you have to have a sound understanding\\nof statistical theories and data organization. Not only do they extract and offer valuable insights.\\nThey also create new. Methodologies for engineers to apply now. If you want to become a statistician then you have\\nto have a passion for logic. They are also good variety of database systems such as SQL Data Mining\\nand other various machine learning Technologies by that. I mean, you should be good with math and you should also\\nhave a good knowledge about the weight is database system such as SQL and also the various machine learning Concepts\\nand algorithms is the most next we have the database administrator. So guys the job profile of a database administrator\\nis Much self-explanatory, they are basically responsible for the proper functioning of all the databases\\nand they are also responsible for granting permission or the working in services to the employees of the company.\\nThey also have to take care of the database backups and recoveries. So some of the skills that are needed to become a database administrator include\\ndatabase backup and Recovery data security data modeling and design next. We have the business analyst now the role of a business analyst\\nis a little It different from all of the other data signs job now. Don't get me wrong. They have a very\\ngood understanding of the data oriented Technologies. They know how to handle a lot of data and process it\\nbut they are also very focused on how this data can be linked to actionable business inside.\\nSo they mainly focus on business growth. Okay. Now a business analyst acts like a link between the data engineers and the management Executives.\\nSo in order to become a business analyst you have to have an understanding of business finances business intelligence.\\nAnd also I did acknowledge, he's like data modeling data visualization tools and Etc\\nat last we have a data and analytics manager a data and analytics manager is responsible for the data science operations.\\nNow the main responsibilities of a data and analytics manager is to oversee the data science operation.\\nOkay, he's responsible for assigning the duties to the team according to their skills and expertise now their strength should include Technologies\\nlike SAS our SQL. And of course, they should have good management skills apart from that.\\nThey must have excellent social skills leadership qualities and and out-of-the-box thinking attitude.\\nAnd like I said earlier you need to have a good understanding of Technologies. Like pythons as our Java and Etc.\\nSo Guys, these were the different job roles in data science. I hope you all found this informative.\\nNow, let's move ahead and look at the data lifecycle. So guys are basically six steps in the data life cycle.\\nIt starts with a business requirement. Next is the data acquisition after that you would process the data\\nwhich is called data processing. Then there is data exploration modeling and finally deployment.\\nSo guys before you even start on a data science project. It is important that you understand the problem you're trying to solve.\\nSo in this stage, you're just going to focus on identifying the central objectives of the project and you will do this by identifying the variables\\nthat need to be predicted next up. We have data acquisition. Okay. So now that you have your objectives I find it's time\\nfor you to start Gathering the data. So data mining is the process of gathering your data from different sources\\nat this stage some of the questions you can ask yourself is what data do I need for my project?\\nWhere does it live? How can I obtain it? And what is the most efficient way to store\\nand access all of it? Next up there is data processing now usually all the data\\nthat you collected is a huge mess. Okay. It's not formatted. It's not structured. It's not cleaned.\\nSo if Find any data set that is cleaned and it's packaged well for you, then you've actually won the lottery\\nbecause finding the right data takes a lot of time and it takes a lot of effort and one of the major time-consuming task\\nin the data science process is data cleaning. Okay, this requires a lot of time. It requires a lot of effort\\nbecause you have to go through the entire data set to find out any missing values or if there are any inconsistent values\\nor corrupted data, and you also find the unnecessary data. Over here and you remove that data.\\nSo this was all about data processing next we have data exploration. So now that you have sparkling clean set of data,\\nyou are finally ready to get started with your analysis. Okay, the data exploration stage is basically the brainstorming\\nof data analysis. So in order to understand the patterns in your data, you can use histogram.\\nYou can just pull up a random subset of data and plot a histogram. You can even create interactive visualizations.\\nThis is the point where you Dive deep into the data and you try to explore the different models\\nthat can be applied to your data next up. We have data modeling. So after processing the data,\\nwhat you're going to do is you're going to carry out model training. Okay. Now model training is basically about finding a model\\nthat answers the questions more accurately. So the process of model training involves a lot of steps.\\nSo firstly you'll start by splitting the input data into the training data set and the testing data set.\\nOkay, you're going to take the entire data set and you're going to separate it into Two two parts one is the training and one is the testing data\\nafter that your build a model by using the training data set and once you're done with that, you'll evaluate the training\\nand the test data set now to evaluate the training and testing data. So you'll be using series\\nof machine learning algorithms after that. You'll find out the model which is the most suitable for your business requirement.\\nSo this was mainly data modeling. Okay. This is where you build a model out of your training data set\\nand then you evaluate this model by using the testing data set. You have deployment.\\nSo guys a goal of this stage is to deploy the model into a production or maybe a production like environment.\\nSo this is basically done for final user acceptance and the users have to validate the performance of the models\\nand if there are any issues with the model or any issues with the algorithm, then they have to be fixed in this stage.\\nSo guys with this we come to the end of the data lifecycle. I hope this was clear statistics and probability are essential\\nbecause these disciples form the basic Foundation of all machine learning algorithms deep learning artificial intelligence\\nand data science. In fact, mathematics and probability is behind everything around us from shapes patterns\\nand colors to the count of petals in a flower mathematics is embedded in each and every aspect of our lives with this in mind.\\nI welcome you all to today's session. So I'm going to go ahead and Scoffs the agenda for today\\nwith you all now going to begin the session by understanding what is data after that.\\nWe'll move on and look at the different categories of data, like quantitative and qualitative data,\\nthen we'll discuss what exactly statistics is the basic terminologies in statistics and a couple\\nof sampling techniques. Once we're done with that. We'll discuss the different types of Statistics\\nwhich involve descriptive and inferential statistics. Then in the next session will mainly be focusing\\non descriptive statistics here will understand the different measures of center measures\\nof spread Information Gain and entropy will also understand all of these measures with the help of a use case and finally we'll discuss\\nwhat exactly a confusion Matrix is once we've covered the entire descriptive statistics module\\nwill discuss the probability module here will understand what exactly probability is the different terminologies\\nin probability will also study the Different probability distributions, then we'll discuss the types of probability which include\\nmarginal probability joint and conditional probability. Then we move on and discuss a use case\\nwhere and we'll see examples that show us how the different types of probability work\\nand to better understand Bayes theorem. We look at a small example. Also, I forgot to mention\\nthat at the end of the descriptive statistics module will be running a small demo in the our language.\\nSo for those of you who don't know much about our I'll be explaining every line in depth, but if you want to have a more in-depth understanding\\nabout our I'll leave a couple of blocks. And a couple of videos in the description box\\nyou all can definitely check out that content. Now after we've completed the probability module will discuss\\nthe inferential statistics module will start this module by understanding what is point estimation.\\nWe will discuss what is confidence interval and how you can estimate the confidence interval.\\nWe will also discuss margin of error and will understand all of these concepts by looking at a small use case.\\nWe'd finally end the inferential Real statistic module by looking at what hypothesis testing is hypothesis.\\nTesting is a very important part of inferential statistics. So we'll end the session by looking at a use case\\nthat discusses how hypothesis testing works and to sum everything up. We'll look at a demo\\nthat explains how inferential statistics Works. Alright, so guys, there's a lot to cover today.\\nSo let's move ahead and take a look at our first topic which is what is data.\\nNow, this is a quite simple question if I ask any of You what is data? You'll see that it's a set of numbers\\nor some sort of documents that have stored in my computer now data is actually everything.\\nAll right, look around you there is data everywhere each click on your phone generates more data than you know,\\nnow this generated data provides insights for analysis and helps us make Better Business decisions.\\nThis is why data is so important to give you a formal definition data refers to facts and statistics.\\nCollected together for reference or analysis. All right. This is the definition of data in terms\\nof statistics and probability. So as we know data can be collected it can be measured and analyzed\\nit can be visualized by using statistical models and graphs now data is divided into two major subcategories.\\nAlright, so first we have qualitative data and quantitative data. These are the two different types of data\\nunder qualitative data. We have nominal and ordinal data and under quantitative data.\\nWe have discrete and continuous data. Now, let's focus on qualitative data.\\nNow this type of data deals with characteristics and descriptors that can't be easily measured\\nbut can be observed subjectively now qualitative data is further divided\\ninto nominal and ordinal data. So nominal data is any sort of data\\nthat doesn't have any order or ranking? Okay. An example of nominal data is gender.\\nNow. There is no ranking in gender. There's only male female or other right?\\nThere is no one two, three four or any sort of ordering in gender race is another example of nominal data.\\nNow ordinal data is basically an ordered series of information.\\nOkay, let's say that you went to a restaurant. Okay. Your information is stored in the form of customer ID.\\nAll right. So basically you are represented with a customer ID. Now you would have rated their service as\\neither good or average. All right, that's how no ordinal data is and similarly they'll have a record of other customers\\nwho visit the restaurant along with their ratings. All right. So any data which has some sort of sequence\\nor some sort of order to it is known as ordinal data. All right, so guys, this is pretty simple to understand now,\\nlet's move on and look at quantitative data. So quantitative data basically these He's\\nwith numbers and things. Okay, you can understand that by the word quantitative itself quantitative is\\nbasically quantity. Right Saudis will numbers a deals with anything that you can measure objectively.\\nAll right, so there are two types of quantitative data there is discrete and continuous data\\nnow discrete data is also known as categorical data and it can hold a finite number of possible values.\\nNow, the number of students in a class is a finite Number. All right, you can't have infinite number\\nof students in a class. Let's say in your fifth grade. They have a hundred students in your class. All right, there weren't infinite number but there\\nwas a definite finite number of students in your class. Okay, that's discrete data. Next.\\nWe have continuous data. Now this type of data can hold infinite number of possible values.\\nOkay. So when you say weight of a person is an example of continuous data\\nwhat I mean to see is my weight can be 50 kgs or it NB 50.1 kgs\\nor it can be 50.00 one kgs or 50.000 one or is 50.0 2 3 and so\\non right there are infinite number of possible values, right? So this is what I mean by a continuous data.\\nAll right. This is the difference between discrete and continuous data. And also I'd like to mention a few other things over here.\\nNow, there are a couple of types of variables as well. We have a discrete variable\\nand we have a continuous variable discrete variable is also known as a categorical variable\\nor and it can hold values of different categories. Let's say that you have a variable called message\\nand there are two types of values that this variable can hold let's say that your message can either be a Spam message\\nor a non spam message. Okay, that's when you call a variable as discrete or categorical variable.\\nAll right, because it can hold values that represent different categories of data now continuous variables are basically variables\\nthat can store infinite number of values. So the weight of a person can be denoted as\\na continuous variable. All right, let's say there is a variable called weight and it can store infinite number of possible values.\\nThat's why we will call it a continuous variable. So guys basically variable is anything that can store a value right?\\nSo if you associate any sort of data with a Able, then it will become either discrete variable\\nor continuous variable. There is also dependent and independent type of variables. Now, we won't discuss all of that in death because\\nthat's pretty understandable. I'm sure all of you know, what is independent variable and dependent variable right?\\nDependent variable is any variable whose value depends on any other independent variable?\\nSo guys that much knowledge I expect or if you do have all right. So now let's move on and look at our next topic which Which is\\nwhat is statistics now coming to the formal definition of statistics statistics is an area of Applied Mathematics,\\nwhich is concerned with data collection analysis interpretation and presentation now usually\\nwhen I speak about statistics people think statistics is all about analysis\\nbut statistics has other parts to it it has data collection is also a part of Statistics data interpretation presentation.\\nAll of this comes into statistics already are going to use statistical methods to visualize data to collect data to interpret data.\\nAlright, so the area of mathematics deals with understanding how data can be used to solve complex problems.\\nOkay. Now I'll give you a couple of examples that can be solved by using statistics. Okay, let's say\\nthat your company has created a new drug that may cure cancer. How would you conduct a test to confirm\\nthe As Effectiveness now, even though this sounds like a biology problem. This can be solved\\nwith Statistics already will have to create a test which can confirm the effectiveness of the drum\\nor a this is a common problem that can be solved using statistics. Let me give you another example you\\nand a friend are at a baseball game and out of the blue. He offers you a bet\\nthat neither team will hit a home run in that game. Should you take the BET?\\nAll right here you just discuss the probability of I know you'll win or lose. All right, this is another problem\\nthat comes under statistics. Let's look at another example. The latest sales data has just come in\\nand your boss wants you to prepare a report for management on places where the company could improve its business.\\nWhat should you look for? And what should you not look for now? This problem involves a lot\\nof data analysis will have to look at the different variables that are causing your business to go down\\nor the you have to look at a few variables. That are increasing the performance of your models\\nand thus growing your business. Alright, so this involves a lot of data analysis and the basic idea\\nbehind data analysis is to use statistical techniques in order to figure out the relationship\\nbetween different variables or different components in your business. Okay. So now let's move on and look at our next topic\\nwhich is basic terminologies in statistics. Now before you dive deep into statistics,\\nit is important that you understand basic terminologies used in statistics.\\nThe two most important terminologies in statistics are population and Sample.\\nSo throughout the statistics course or throughout any problem that you're trying to stall with Statistics.\\nYou will come across these two words, which is population and Sample Now population is a collection\\nor a set of individuals or objects or events. Events whose properties are to be analyzed.\\nOkay. So basically you can refer to population as a subject that you're trying to analyze now a sample is just\\nlike the word suggests. It's a subset of the population. So you have to make sure that you choose the sample\\nin such a way that it represents the entire population. All right. It shouldn't Focus add one part of the population instead.\\nIt should represent the entire population. That's how your sample should be chosen.\\nSo Well chosen sample will contain most of the information about a particular population parameter.\\nNow, you must be wondering how can one choose a sample that best represents the entire population now\\nsampling is a statistical method that deals with the selection of individual observations\\nwithin a population. So sampling is performed in order to infer statistical knowledge about a population.\\nAll right, if you want to understand the different statistics of a population like the mean the median Median the mode or the standard deviation\\nor the variance of a population. Then you're going to perform sampling. All right, because it's not reasonable for you to study\\na large population and find out the mean median and everything else. So why is sampling performed you might ask?\\nWhat is the point of sampling? We can just study the entire population now guys, think of a scenario\\nwhere in your asked to perform a survey about the eating habits of teenagers in the US.\\nSo at present there are over 42 million teens in the US and this number is growing\\nas we are speaking right now, correct. Is it possible to survey each of these 42 million individuals\\nabout their health? Is it possible? Well, it might be possible\\nbut this will take forever to do now. Obviously, it's not it's not reasonable to go around\\nknocking each door and asking for what does your teenage son eat and all of that right? This is not very reasonable.\\nThat's By sampling is used. It's a method wherein a sample of the population is studied\\nin order to draw inferences about the entire population. So it's basically a shortcut to studying\\nthe entire population instead of taking the entire population and finding out all the solutions.\\nYou just going to take a part of the population that represents the entire population and you're going to perform all your statistical analysis\\nyour inferential statistics on that small sample. All right, and that sample basically here Presents the entire population.\\nAll right, so I'm short of made this clear to y'all what is sample and what is population now?\\nThere are two main types of sampling techniques that are discussed today. We have probability sampling and non-probability\\nsampling now in this video will only be focusing on probability sampling techniques\\nbecause non-probability sampling is not within the scope of this video. All right will only discuss the probability part\\nbecause we're focusing on statistics and probability, correct. Now again under probability sampling.\\nWe have three different types. We have random sampling systematic and stratified sampling.\\nAll right, and just to mention the different types of non-probability sampling,\\n's we have no bald Kota judgment and convenience sampling. All right now guys in this session.\\nI'll only be focusing on probability. So let's move on and look at the different types of probability sampling.\\nSo what is probability sampling it is a sampling technique in which samples\\nfrom a large population are chosen by using the theory of probability.\\nAll right, so there are three types of probability sampling. All right first we have the random sampling now\\nin this method each member of the population has an equal chance of being selected in the sample.\\nAll right, so each and every individual or each and every object in the population has an equal John's\\nof being a part of the sample. That's what random sampling is all about. Okay, you are randomly going to select any individual\\nor any object. So this Bay each individual has an equal chance of being selected.\\nCorrect? Next. We have systematic sampling now in systematic sampling every nth record is chosen\\nfrom the population to be a part of the sample. All right. Now refer this image\\nthat I've shown over here out of these six. Groups every second group is chosen as a sample.\\nOkay. So every second record is chosen here and this is our systematic sampling works.\\nOkay, you're randomly selecting the nth record and you're going to add that to your sample.\\nNext. We have stratified sampling now in this type of technique a stratum is used to form samples\\nfrom a large population. So what is a stratum a stratum is basically a subset\\nof the population that shares at One common characteristics. So let's say\\nthat your population has a mix of both male and female so you can create to straightens\\nout of this one will have only the male subset and the other will have the female subset. All right, this is what stratum is.\\nIt is basically a subset of the population that shares at least one common characteristics.\\nAll right in our example, it is gender. So after you've created a stratum you're going to use random sampling\\non these stratums and you're going to choose. Choose a final sample. So random sampling meaning\\nthat all of the individuals in each of the stratum will have an equal chance of being selected in the sample.\\nCorrect. So Guys, these were the three different types of sampling techniques. Now, let's move on and look at our next topic\\nwhich is the different types of Statistics. So after this, we'll be looking at the more advanced concepts of Statistics,\\nright so far we discuss the basics of Statistics, which is basically what is statistics the Friend\\nsampling techniques and the terminologies and statistics. All right. Now we look at the different types of Statistics.\\nSo there are two major types of Statistics descriptive statistics and inferential statistics in today's session.\\nWe will be discussing both of these types of Statistics in depth. All right, we'll also be looking at a demo\\nwhich I'll be running in the our language in order to make you understand what exactly\\ndescriptive and inferential statistics is soaked. As which is going to look at the basic, so don't worry.\\nIf you don't have much knowledge, I'm explaining everything from the basic level. All right, so guys descriptive statistics is a method\\nwhich is used to describe and understand the features of specific data set by giving a short summary of the data.\\nOkay, so it is mainly focused upon the characteristics of data. It also provides a graphical summary of the data now\\nin order to make you understand what descriptive statistics is. Let's suppose that you want to gift all\\nyour classmates or t-shirt. So to study the average shirt size of a student in a classroom.\\nSo if you were to use descriptive statistics to study the average shirt size of students in your classroom,\\nthen what you would do is you would record the shirt size of all students in the class and then you would find out the maximum minimum and average\\nshirt size of the cloud. Okay. So coming to inferential statistics inferential.\\nSix makes inferences and predictions about a population based on the sample of data taken from the population.\\nOkay. So in simple words, it generalizes a large data set and it applies probability\\nto draw a conclusion. Okay. So it allows you to infer data parameters based on a statistical model by using sample data.\\nSo if we consider the same example of finding the average shirt size of students in a class\\nin infinite real statistics. We'll take a sample set of the class which is basically a few people from the entire class.\\nAll right, you already have had grouped the class into large medium and small. All right in this method you basically build\\na statistical model and expand it for the entire population in the class. So guys, there was a brief understanding of descriptive\\nand inferential statistics. So that's the difference between descriptive and inferential now in the next section,\\nwe will go in depth about descriptive statistics. Right. So let's discuss more about descriptive statistics.\\nSo like I mentioned earlier descriptive statistics is a method that is used to describe and understand the features\\nof a specific data set by giving short summaries about the sample and measures of the data.\\nThere are two important measures in descriptive statistics. We have measure of central tendency,\\nwhich is also known as measure of center and we have measures of variability. This is also known as Measures of spread\\nso measures of center include mean median and mode now what is measures\\nof center measures of the center are statistical measures that represent the summary of a data set?\\nOkay, the three main measures of center are mean median and mode coming to measures of variability\\nor measures of spread. We have range interquartile range variance and standard deviation.\\nAll right. So now let's discuss each of these measures. Has in a little more depth starting\\nwith the measures of center. Now, I'm sure all of you know, what the mean is mean is basically the measure\\nof the average of all the values in a sample. Okay, so it's basically the average of all\\nthe values in a sample. How do you measure the mean I hope all of you know how the main is measured\\nif there are 10 numbers and you want to find the mean of these 10 numbers. All you have to do is you have to add up all the 10 numbers\\nand you have to divide it by 10 then. Represents the number of samples in your data set.\\nAll right, since we have 10 numbers, we're going to divide this by 10. All right, this will give us the average\\nor the mean so to better understand the measures of central tendency. Let's look at an example.\\nNow the data set over here is basically the cars data set and it contains a few variables.\\nAll right, it has something known as cars. It has mileage per gallon cylinder type displacement\\nhorsepower and relax. Silver ratio. All right, all of these measures are related to cars.\\nOkay. So what you're going to do is you're going to use descriptive analysis and you're going to analyze each of the variables\\nin the sample data set for the mean standard deviation median more and so on.\\nSo let's say that you want to find out the mean or the average horsepower of the cars among the population of cards.\\nLike I mentioned earlier what you'll do is you'll check the average of all the values. So in this case we will take The sum of the horsepower\\nof each car and we'll divide that by the total number of cards. Okay, that's exactly what I've done here in the calculation part.\\nSo this hundred and ten basically represents the horsepower for the first car.\\nAll right. Similarly. I've just added up all the values of horsepower for each of the cars\\nand I've divided it by 8 now 8 is basically the number of cars in our data set.\\nAll right, so hundred and three point six two five is what army mean is or the average of horsepower is all right.\\nNow, let's understand what median is with an example. Okay. So to Define median median is basically a measure\\nof the central value of the sample set is called the median. All right, you can see that it is the middle value.\\nSo if we want to find out the center value of the mileage per gallon among the population\\nof cars first, what we'll do is we'll arrange the MGP values in ascending\\nor descending Order and choose a middle value right in this case since we have eight values, right?\\nWe have eight values which is an even entry. So whenever you have even number of data points\\nor samples in your data set, then you're going to take the average of the two middle values.\\nIf we had nine values over here. We can easily figure out the middle value and you know choose that as a median.\\nBut since they're even number of values we are going to take the average of the two middle values. All right.\\nRight. So 22.8 and 23 are my two middle values and I'm taking the mean\\nof those 2 and hence I get twenty two point nine, which is my median. All right, lastly,\\nlet's look at how mode is calculated. So what is mode the value that is most recurrent\\nin the sample set is known as mode or basically the value that occurs most often.\\nOkay, that is known as mode. So let's say that we want to find out the most common type of cylinder\\namong the population of cards. What we have to do is we will check the value which is repeated the most number of times here.\\nWe can see that the cylinders come in two types. We have cylinder of Type 4 and cylinder of type 6, right?\\nSo take a look at the data set. You can see that the most recurring value is 6 right.\\nWe have one two, three four and five. We have five six and we have one two, three.\\nYeah, we have three four types of lenders and five six types of lenders.\\nSo basically we have three four type cylinders and we have five six type cylinders.\\nAll right. So our mode is going to be 6 since 6 is more recurrent than 4 so guys\\nthose were the measures of the center or the measures of central tendency. Now, let's move on and look at the measures of the spread.\\nAll right. Now, what is the measure of spread a measure of spread? Sometimes also called\\nas measure of dispersion is Used to describe the variability in a sample or population.\\nOkay, you can think of it as some sort of deviation in the sample. All right, so you measure this with the help\\nof the different measure of spreads. We have range interquartile range variance and standard deviation.\\nNow range is pretty self-explanatory, right? It is the given measure of how spread apart the values\\nin a data set are the range can be calculated as shown in this formula.\\nYou basically going to subtract the maximum value in your data set from the minimum value in your data set.\\nThat's how you calculate the range of the data. Alright, next we have interquartile range.\\nSo before we discuss interquartile range, let's understand. What a quartile is red.\\nSo quartiles basically tell us about the spread of a data set by breaking the data set into different quarters.\\nOkay, just like how the median breaks the data into two parts the court is We'll break it into different quarters.\\nSo to better understand how quartile and interquartile are calculated. Let's look at a small example.\\nNow this data set basically represents the marks of hundred students ordered from the lowest\\nto the highest scores red. So the quartiles lie in the following ranges the first quartile,\\nwhich is also known as q1 it lies between the 25th and 26th observation.\\nAll right. So if you look at this I've highlighted Add the 25th and the 26th observation.\\nSo how you can calculate Q 1 or first quartile is by taking the average of these two values.\\nAlright, since both the values are 45 when you add them up and divide them by two you'll still get 45 now the second quartile\\nor Q 2 is between the 50th and the 51st observation. So you're going to take the average of 58 and 59\\nand you will get a value of 58.5. Now, this is my second quarter the third quartile.\\nAh Q3 is between the 75th and the 76th observation here. Again, we'll take the average of the two values\\nwhich is the 75th value and the 76 value right and you'll get a value of 71.\\nAll right, so guys this is exactly how you calculate the different quarters. Now, let's look at what is interquartile range.\\nSo IQR or the interquartile range is a measure of variability based on dividing a data set\\ninto quartiles now the The interquartile range is calculated by subtracting the q1 from Q3.\\nSo basically Q3 minus q1 is your IQ are so your IQR is your Q3 minus q1?\\nAll right. Now this is how each of the quartiles are each core tile represents a quarter,\\nwhich is 25% All right. So guys, I hope all of you are clear with interquartile range and what our quartiles now,\\nlet's look at variance covariance is basically a measure that shows How much a random variable the first\\nfrom its expected value? Okay. It's basically the variance in any variable now variance\\ncan be calculated by using this formula right here x basically represents any data point in your data set\\nn is the total number of data points in your data set and X bar is basically the main of data points.\\nAll right. This is how you calculate variance variance is basically a Computing the squares of deviations.\\nOkay. That's why it says s Square there. Now let's look at what is deviation deviation is just the difference\\nbetween each element from the mean. Okay, so it can be calculated by using this simple formula\\nwhere X I basically represents a data point and mu is the mean of the population\\nor add this is exactly how you calculate the deviation Now population variance and Sample variance are very specific to\\nwhether you're calculating the variance in your population data set or in your sample data set.\\nThat's the A difference between population and Sample variance. So the formula for population variance is pretty explanatory.\\nSo X is basically each data point mu is the mean of the population n is the number of samples in your data set.\\nAll right. Now, let's look at sample. Variance Now sample variance is the average of squared differences from the mean.\\nAll right here x i is any data point or any sample in your data set X bar is the mean\\nof your sample. All right. It's not the main of your population. Ation, it's the mean of your sample.\\nAnd if you notice n here is a smaller n is the number of data points in your sample.\\nAnd this is basically the difference between sample and population variance. I hope that is clear coming\\nto standard deviation is the measure of dispersion of a set of data from its mean.\\nAll right, so it's basically the deviation from your mean. That's what standard deviation is now to better understand\\nhow the measures of spread are calculated. Let's look at a small use case. So let's see Daenerys has 20 dragons.\\nThey have the numbers nine to five four and so on as shown on the screen,\\nwhat you have to do is you have to work out the standard deviation or at in order to calculate the standard deviation.\\nYou need to know the mean right? So first you're going to find out the mean of your sample set.\\nSo how do you calculate the mean you add all the numbers in your data set and divided by the total number of samples in your data set\\nso you get a value of 7. Here then you calculate the rhs of your standard deviation formula.\\nAll right. So from each data point you're going to subtract the mean and you're going to square that. All right.\\nSo when you do that, you will get the following result. You'll basically get this 425 for 925\\nand so on so finally you will just find the mean of the squared differences. All right.\\nSo your standard deviation will come up to two point nine eight three once you take the square root.\\nSo guys, it's pretty simple. It's a simple At the magic technique, all you have to do is you have to substitute the values\\nin the formula. All right. I hope this was clear to all of you. Now let's move on\\nand discuss the next topic which is Information Gain and entropy now. This is one of my favorite topics in statistics.\\nIt's very interesting and this topic is mainly involved in machine learning algorithms,\\nlike decision trees and random forest. All right, it's very important for you to know how Information Gain and entropy really work and why they are\\nso essential in building machine learning models. We focus on the statistic parts of Information Gain\\nand entropy and after that we'll discuss a use case. And see how Information Gain and entropy is used in decision trees.\\nSo for those of you who don't know what a decision tree is it is basically a machine learning algorithm.\\nYou don't have to know anything about this. I'll explain everything in depth. So don't worry. Now. Let's look at what exactly entropy\\nand Information Gain Is Now guys entropy is basically the measure of any sort of uncertainty that is present in the data.\\nAll right, so it can be measured by using this formula. So here s is the set of all instances in the data set\\nor all the data items in the data set n is the different type of classes in your data set\\nPi is the event probability. Now this might seem a little confusing to y'all but when we go through the use case,\\nyou'll understand all of these terms even better. All right cam. The information gained\\nas the word suggests Information Gain indicates how much information a particular feature\\nor a particular variable gives us about the final outcome. Okay, it can be measured by using this formula.\\nSo again here heads of s is the entropy of the whole data set s SJ is the number\\nof instances with the J value of an attribute a s is the total number of instances in the data set V is the set of distinct values\\nof an attribute a h of s j is the entropy of subsets of instances\\nand hedge of a comma s is the entropy of an attribute a even though this seems confusing.\\nI'll clear out the confusion. All right, let's discuss a small problem statement where we will understand\\nhow Information Gain and entropy is used to study the significance of a model.\\nSo like I said Information Gain and entropy are very important statistical measures\\nthat let us understand the significance of a predictive model. Okay to get a more clear understanding.\\nLet's look at a use case. All right now suppose we are given a problem statement.\\nAll right, the statement is that you have to predict whether a match can be played or Not by studying the weather conditions.\\nSo the predictor variables here are outlook humidity wind day is also a predictor variable.\\nThe target variable is basically played or a the target variable is the variable that you're trying to protect.\\nOkay. Now the value of the target variable will decide whether or not a game can be played.\\nAll right, so that's why The play has two values. It has no and yes, no, meaning that the weather conditions are not good.\\nAnd therefore you cannot play the game. Yes, meaning that the weather conditions are good and suitable\\nfor you to play the game. Alright, so that was our problem statement. I hope the problem statement is clear to all of you now\\nto solve such a problem. We make use of something known as decision trees. So guys think of an inverted tree\\nand each branch of the tree denotes some decision. All right, each branch is Is known as the branch known\\nand at each branch node, you're going to take a decision in such a manner that you will get an outcome at the end of the branch.\\nAll right. Now this figure here basically shows that out of 14 observations 9 observations result in a yes,\\nmeaning that out of 14 days. The match can be played only on nine days.\\nAlright, so here if you see on day 1 Day 2 Day 8 day 9 and 11.\\nThe Outlook has been Alright, so basically we try to plaster a data set\\ndepending on the Outlook. So when the Outlook is sunny, this is our data set when the Outlook is overcast.\\nThis is what we have and when the Outlook is the rain this is what we have. All right, so\\nwhen it is sunny we have two yeses and three nodes. Okay, when the Outlook is overcast.\\nWe have all four as yes has meaning that on the four days when the Outlook was overcast.\\nWe can play the game. All right. Now when it comes to rain, we have three yeses and two nodes.\\nAll right. So if you notice here, the decision is being made by choosing the Outlook variable\\nas the root node. Okay. So the root node is basically the topmost node in a decision tree.\\nNow, what we've done here is we've created a decision tree that starts with the Outlook node.\\nAll right, then you're splitting the decision tree further depending on other parameters like Sunny overcast and rain.\\nAll right now like we know that Outlook has three values. Sunny overcast and brain so let me explain this\\nin a more in-depth manner. Okay. So what you're doing here is you're making the decision Tree by choosing the Outlook variable\\nat the root node. The root note is basically the topmost node in a decision tree.\\nNow the Outlook node has three branches coming out from it, which is sunny overcast and rain.\\nSo basically Outlook can have three values either it can be sunny. It can be overcast or it can be rainy.\\nOkay now these three values Use are assigned to the immediate Branch nodes and for each\\nof these values the possibility of play is equal to yes is calculated. So the sunny\\nand the rain branches will give you an impure output. Meaning that there is a mix of yes and no right.\\nThere are two yeses here three nodes here. There are three yeses here and two nodes over here,\\nbut when it comes to the overcast variable, it results in a hundred percent pure subset.\\nAll right, this shows that the overcast baby. Will result in a definite and certain output.\\nThis is exactly what entropy is used to measure. All right, it calculates the impurity or the uncertainty.\\nAlright, so the lesser the uncertainty or the entropy of a variable more significant is that variable?\\nSo when it comes to overcast there's literally no impurity in the data set. It is a hundred percent pure subset, right?\\nSo be want variables like these in order to build a model. All right now, we don't always Ways get lucky and we don't always find\\nvariables that will result in pure subsets. That's why we have the measure entropy. So the lesser the entropy of a particular variable the most\\nsignificant that variable will be so in a decision tree. The root node is assigned the best attribute\\nso that the decision tree can predict the most precise outcome meaning that on the root note.\\nYou should have the most significant variable. All right, that's why we've chosen Outlook or and now some of you might ask me why haven't you chosen\\novercast Okay is overcast is not a variable. It is a value of the Outlook variable.\\nAll right. That's why we've chosen our true cure because it has a hundred percent pure subset\\nwhich is overcast. All right. Now the question in your head is how do I decide which variable\\nor attribute best Blitz the data now right now, I know I looked at the data and I told you that,\\nyou know here we have a hundred percent pure subset, but what if it's a more complex problem\\nand you're not able to understand which variable will best split the data, so guys when it comes\\nto decision tree Information and gain and entropy will help you understand which variable will best split the data set.\\nAll right, or which variable you have to assign to the root node because whichever variable is assigned to the root node.\\nIt will best let the data set and it has to be the most significant variable. All right. So how we can do this is we need to use\\nInformation Gain and entropy. So from the total of the 14 instances that we saw nine\\nof them said yes and five of the instances said know that you cannot play on that particular day.\\nAll right. So how do you calculate the entropy? So this is the formula you just substitute the values in the formula.\\nSo when you substitute the values in the formula, you will get a value of 0.9940. All right.\\nThis is the entropy or this is the uncertainty of the data present in a sample.\\nNow in order to ensure that we choose the best variable for the root node. Let us look at all the possible combinations\\nthat you can use on the root node. Okay, so these are All the possible combinations\\nyou can either have Outlook you can have windy humidity or temperature. Okay, these are four variables\\nand you can have any one of these variables as your root note. But how do you select\\nwhich variable best fits the root node? That's what we are going to see by using Information Gain and entropy.\\nSo guys now the task at hand is to find the information gain for each of these attributes.\\nAll right. So for Outlook for windy for humidity and for temperature, we're going to find out the information.\\nNation gained all right. Now a point to remember is that the variable that results in the highest Information Gain must be chosen\\nbecause it will give us the most precise and output information. All right. So the information gain for attribute windy will calculate\\nthat first here. We have six instances of true and eight instances of false.\\nOkay. So when you substitute all the values in the formula, you will get a value of zero point zero four eight.\\nSo we get a value of You 2.0 for it. Now. This is a very low value for Information Gain.\\nAll right, so the information that you're going to get from Windy attribute is pretty low. So let's calculate the information gain\\nof attribute Outlook. All right, so from the total of 14 instances, we have five instances with say Sunny for instances,\\nwhich are overcast and five instances, which are rainy. All right for Sonny. We have three yeses\\nand to nose for overcast we have Or the for as yes for any we have three years and two nodes.\\nOkay. So when you calculate the information gain of the Outlook variable will get a value\\nof zero point 2 4 7 now compare this to the information gain of the windy attribute.\\nThis value is actually pretty good. Right we have zero point 2 4 7 which is a pretty good value\\nfor Information Gain. Now, let's look at the information gain of attribute humidity now over here.\\nWe have seven instances with say hi and seven instances with same. Right and under the high Branch node.\\nWe have three instances with say yes, and the rest for instances would say no similarly\\nunder the normal Branch. We have one two, three, four, five six seven instances would say yes\\nand one instance with says no. All right. So when you calculate the information gain\\nfor the humidity variable, you're going to get a value of 0.15 one. Now.\\nThis is also a pretty decent value, but when you compare it to the Information Gain, Of the attribute Outlook it is less right now.\\nLet's look at the information gain of attribute temperature. All right, so the temperature can hold repeat.\\nSo basically the temperature attribute can hold hot mild and cool. Okay under hot.\\nWe have two instances with says yes and two instances for no under mild. We have four instances of yes and two instances of no\\nand under col we have three instances of yes and one instance of no. All right.\\nWhen you calculate the information gain for this attribute, you will get a value of zero point zero to nine,\\nwhich is again very less. So what you can summarize from here is if we look at the information gain for each of these variable will see\\nthat for Outlook. We have the maximum gain. All right, we have zero point two four seven,\\nwhich is the highest Information Gain value and you must always choose a variable with the highest Information Gain to split the data\\nat the root node. So that's why we assign The Outlook variable at the root node.\\nAll right, so guys. I hope this use case was clear. If any of you have doubts. Please keep commenting those doubts now,\\nlet's move on and look at what exactly a confusion Matrix is the confusion Matrix is the last topic\\nfor descriptive statistics read after this. I'll be running a short demo where I'll be showing you\\nhow you can calculate mean median mode and standard deviation variance and all of those values\\nby using our okay. So let's talk about confusion Matrix now guys.\\nWhat is the confusion Matrix now don't get confused. This is not any complex topic now confusion.\\nMatrix is a matrix that is often used to describe the performance of a model.\\nRight? And this is specifically used for classification models or a classifier\\nand what it does is it will calculate the accuracy or it will calculate the performance of your classifier\\nby comparing your actual results and Your predicted results. All right. So this is what it looks like to prosit\\nof true- and all of that. Now this is a little confusing. I'll get back to what exactly true positive\\nto negative and all of this stands for for now. Let's look at an example and let's try and understand what exactly confusion Matrix is.\\nSo guys. I made sure that I put examples after each and every topic because it's important you understand the Practical part of Statistics.\\nAll right statistics has literally nothing to do with Theory you need to understand how Calculations\\nare done in statistics. Okay. So here what I've done is let's look at a small use case.\\nOkay, let's consider that your given data about a hundred and sixty-five patient's out of which hundred and five patients have a disease\\nand the remaining 50 patients don't have a disease. Okay. So what you're going to do is you will build a classifier\\nthat predicts by using these hundred and sixty five observations your feed all of these 165 observations to your classifier\\nand It will predict the output every time a new patients detail is fed to the classifier right now\\nout of these 165 cases. Let's say that the classifier predicted.\\nYes hundred and ten times and no 55 times. Alright, so yes basically stands for yes.\\nThe person has a disease and no stands for know. The person has not have a disease. All right, that's pretty self-explanatory.\\nBut yeah, so it predicted that a hundred and ten times. Patient has a disease and 55 times that\\nnor the patient doesn't have a disease. However in reality only hundred and five patients\\nin the samples have the disease and 60 patients who do not have the disease, right?\\nSo how do you calculate the accuracy of your model? You basically build the confusion Matrix?\\nAll right. This is how the Matrix looks like and basically denotes the total number of observations\\nthat you have which is 165 in our case actual denotes the actual use\\nin the data set and predicted denotes the predicted values by the classifier.\\nSo the actual value is no here and the predicted value is no here. So your classifier was correctly able\\nto classify 50 cases as no. All right, since both of these are no so 50\\nit was correctly able to classify but 10 of these cases it incorrectly classified meaning\\nthat your actual value here is no but you classifier predicted it as yes or a\\nthat's why this And over here similarly it wrongly predicted that five patients do not have diseases\\nwhereas they actually did have diseases and it correctly predicted hundred patients,\\nwhich have the disease. All right. I know this is a little bit confusing. But if you look at these values no,\\nno 50 meaning that it correctly predicted 50 values No\\nYes means that it wrongly predicted. Yes for the values are it was supposed to predict.\\nNo. All right. Now what exactly is? Is this true positive to negative and all of that?\\nI'll tell you what exactly it is. So true positive are the cases in which we predicted a yes\\nand they do not actually have the disease. All right, so it is basically this value\\nalready predicted a yes here, even though they did not have the disease.\\nSo we have 10 true positives right similarly true- is we predicted know\\nand they don't have the disease meaning that this is correct. False positive is be predicted.\\nYes, but they do not actually have the disease. All right. This is also known as type 1 error falls- is we predicted.\\nNo, but they actually do not have the disease. So guys basically false negative and true negatives are basically\\ncorrect classifications. All right. So this was confusion Matrix and I hope this concept is clear again guys.\\nIf you have doubts, please comment your doubt in the comment section. So guys that was descriptive statistics now,\\nBefore we go to probability. I promised all that will run a small demo in our all right,\\nwe'll try and understand how mean median mode works in our okay, so let's do that first.\\nSo guys again what we just discussed so far was descriptive statistics. All right, next we're going to discuss probability\\nand then we'll move on to inferential statistics. Okay in financial statistics is basically the second type of Statistics.\\nOkay now to make things more clear of you, let me just zoom in.\\nSo guys it's always best to perform practical implementations in order to understand the concepts in a better way.\\nOkay, so here will be executing a small demo that will show you how to calculate the mean median mode variance standard deviation\\nand how to study the variables by plotting a histogram. Okay. Don't worry. If you don't know what a histogram is.\\nIt's basically a frequency plot. There's no big signs behind it. Alright, this is a very simple demo\\nbut it also forms a foundation that everything. Machine learning algorithm is built upon.\\nOkay, you can say that most of the machine learning algorithms actually all the machine learning algorithms and deep learning algorithms have\\nthis basic concept behind them. Okay, you need to know how mean median mode and all of that is calculated.\\nSo guys am using the our language to perform this and I'm running this on our studio. For those of you who don't know our language.\\nI will leave a couple of links in the description box. You can go through those videos. So what we're doing is we are randomly generated.\\nEating numbers and Miss storing it in a variable called data, right? So if you want to see the generated numbers\\njust to run the line data, right this variable basically stores all our numbers.\\nAll right. Now, what we're going to do is we're going to calculate the mean now. All you have to do in our is specify the word mean\\nalong with the data that you're calculating the mean of and I was assigned this whole thing into a variable called mean\\nJust hold the mean value of this data. So now let's look at the mean\\nfor that abuser function called print and mean.\\nAll right. So our mean is around 5.99. Okay. Next is calculating the median.\\nIt's very simple guys. All you have to do is use the function median or write and pass the data as a parameter to this function.\\nThat's all you have to do. So our provides functions for each and everything. All right statistics is very easy when it comes to R\\nbecause R is basically a statistical language. Okay. So all you have to do is just name the function\\nand that function is Ready in built in your art. Okay, so your median is around 6.4.\\nSimilarly. We will calculate the mode. All right. Let's run this function.\\nI basically created a small function for calculating the mode. So guys, this is our mode meaning\\nthat this is the most recurrent value right now. We're going to calculate the variance and the standard deviation for that.\\nAgain. We have a function in are called as we're all right. All you have to do is pass the data to that function.\\nOkay, similarly will calculate the standard deviation, which is basically the square root of your variance\\nright now will Rent the standard deviation, right? This is our standard deviation value.\\nNow. Finally, we will just plot a small histogram histogram is nothing but it's a frequency plot already in\\nshow you how frequently a data point is occurring. So this is the histogram that we've just created it's quite simple in our\\nbecause our has a lot of packages and a lot of inbuilt functions that support statistics.\\nAll right. It is a statistical language that is mainly used by data scientists or by data\\nand analysts and machine learning Engineers because they don't have to student code these functions.\\nAll they have to do is they have to mention the name of the function and pass the corresponding parameters.\\nSo guys that was the entire descriptive statistics module and now we will discuss about probability.\\nOkay. So before we understand what exactly probability is, let me clear out a very common misconception people\\noften tend to ask me this question. What is the relationship between statistics and probability?\\nSo probability and statistics are related fields. All right. So probability is a mathematical method used\\nfor statistical analysis. Therefore we can say that a probability and statistics are interconnected\\nbranches of mathematics that deal with analyzing the relative frequency of events.\\nSo they're very interconnected feels and probability makes use of statistics and statistics makes use\\nof probability or a they're very interconnected Fields. So that is the relationship between said It is six and probability.\\nNow. Let's understand what exactly is probability. So probability is the measure\\nof How likely an event will occur to be more precise. It is the ratio\\nof desired outcome to the total outcomes. Now, the probability of all outcomes always sum up to 1 the probability will always\\nsum up to 1 probability cannot go beyond one. Okay. So either your probability can be 0 or it can be 1\\nor it can In the form of decimals like 0.5 to or 0.55 or it can be in the form of 0.5 0.7 0.9.\\nBut it's valuable always stay between the range 0 and 1 okay, another famous example of probability is rolling\\na dice example. So when you roll a dice you get six possible outcomes, right? You get one two, three four and five six phases of a dies now each possibility\\nonly has one outcome. So what is the probability that on rolling a dice? You will get 3 the probability is 1 by 6 right\\nbecause there's only one phase which has the number 3 on it out of six phases.\\nThere's only one phase which has the number three. So the probability of getting 3\\nwhen you roll a dice is 1 by 6 similarly. If you want to find the probability of getting\\na number 5 again, the probability is going to be 1 by 6. All right. So all of this will sum up to 1.\\nAll right, so guys, this is exactly what Ability is it's a very simple concept.\\nWe all learnt it in 8 standard onwards right now. Let's understand the different terminologies\\nthat are related to probability. Now that three terminologies that you often come across when we talk about probability.\\nWe have something known as the random experiment. Okay. It's basically an experiment or a process for which\\nthe outcomes cannot be predicted with certainty. All right. That's why you use probability.\\nYou're going to use probability in order to predict the outcome with Some sort of certainty sample space is the entire possible set of outcomes\\nof a random experiment and event is one or more outcomes of an experiment.\\nSo if you consider the example of rolling a dice now, let's say that you want to find out the probability\\nof getting a to when you roll the dice. Okay. So finding this probability is the random experiment\\nthe sample space is basically your entire possibility. Okay. So one two, three,\\nfour five six Is are there and out of that you need to find the probability of getting a 2 right?\\nSo all the possible outcomes will basically represent your sample space gives a 1 to 6 are all your possible outcomes.\\nThis represents your sample space now event is one or more outcome of an experiment.\\nSo in this case my event is to get a tattoo when I roll a dice, right? So my event is the probability of getting a to\\nwhen I roll a dice, so guys, this is basically what random experiment samples.\\nAll space and event really means alright now, let's discuss the different types of events.\\nThere are two types of events that you should know about there is disjoint and non disjoint events.\\nDisjoint events are events that do not have any common outcome. For example,\\nif you draw a single card from a deck of cards, it cannot be a king and a queen correct it can either be king\\nor it can be Queen now a non disjoint events are events that have common out.\\nFor example a student can get hundred marks in statistics and hundred marks in probability.\\nAll right, and also the outcome of a ball delivered can be a no ball and it can be a 6 right.\\nSo this is what non disjoint events are or n? These are very simple to understand right now.\\nLet's move on and look at the different types of probability distribution. All right, I'll be discussing\\nthe three main probability distribution functions. I'll be talking about probability density.\\nAaron normal distribution and Central limit theorem. Okay probability density function also known\\nas PDF is concerned with the relative likelihood for a continuous random variable to take on a given value.\\nAlright, so the PDF gives the probability of a variable that lies between the range A and B.\\nSo basically what you're trying to do is you're going to try and find the probability of a continuous random variable\\nover a specified range. Okay. Now this graph denotes the PDF of a continuous variable.\\nNow this graph is also known as the bell curve right? It's famously called the bell curve because of its shape and the three important properties\\nthat you need to know about a probability density function. Now the graph of a PDF will be continuous\\nover a range this is because you're finding the probability that a continuous variable lies between the ranges A and B,\\nright the second property. Is that the area bounded by By the curve of a density function\\nand the x-axis is equal to 1 basically the area below the curve is equal to 1 all right,\\nbecause it denotes probability again the probability cannot arrange more than one it has to be\\nbetween 0 and 1 property number three is that the probability that our random variable assumes a value between A\\nand B is equal to the area under the PDF bounded by A and B. Okay. Now what this means,\\nis that the probability You is denoted by the area of the graph. All right, so whatever value that you get here,\\nwhich basically one is the probability that a random variable will lie between the range A and B.\\nAll right. So I hope all of you have understood the probability density function. It's basically the probability of finding the value\\nof a continuous random variable between the range A and B. All right.\\nNow, let's look at our next distribution, which is normal distribution now.\\nNormal distribution, which is also known as the gaussian distribution is a probability distribution\\nthat denotes the symmetric property of the mean right meaning that the idea behind this function.\\nIs that the data near the mean occurs more frequently than the data away from the mean.\\nSo what it means to say is that the data around the mean represents the entire data set.\\nOkay. So if you just take a sample of data around the mean it can represent the entire data set now similar\\nto Probability density function the normal distribution appears as a bell curve right now\\nwhen it comes to normal distribution. There are two important factors. All right, we have the mean of the population\\nand the standard deviation. Okay, so the mean and the graph determines the location of the center of the graph,\\nright and the standard deviation determines the height of the graph. Okay. So if the standard deviation is large the curve is going\\nto look something like this. All right, it'll be short and wide. I'd and if the standard deviation is small the curve\\nis tall and narrow. All right. So this was it about normal distribution. Now, let's look at the central limit theorem.\\nNow the central limit theorem states that the sampling distribution of the mean of any independent random variable will be normal\\nor nearly normal if the sample size is large enough now, that's a little confusing.\\nOkay. Let me break it down for you now in simple terms if we had a large population\\nand be Why did it in too many samples, then the mean of all the samples from the population will be almost equal to the mean\\nof the entire population right? Meaning that each of the sample is normally distributed.\\nRight? So if you compare the mean of each of the sample, it will almost be equal to the mean of the population.\\nRight? So this graph basically shows a more clear understanding of the central limit theorem red you can see each sample here\\nand the mean of each sample. Oil is almost along the same line, right?\\nOkay. So this is exactly what the central limit theorem States now the accuracy or the resemblance to the normal distribution depends\\non two main factors, right? So the first is the number of sample points that you consider. All right,\\nand the second is the shape of the underlying population. Now the shape obviously depends on the standard deviation\\nand the mean of a sample, correct. So guys the central limit theorem basically states\\nthat eats Bill will be normally distributed in such a way that the mean of each sample will coincide with the mean\\nof the actual population. All right in short terms. That's what central limit theorem States. All right, and this holds true only for a large data set mostly\\nfor a small data set and there are more deviations when compared to a large data set is because of\\nthe scaling Factor, right? The small is deviation in a small data set will change the value vary drastically,\\nbut in a large data set a small deviation will not matter at all. Now, let's move.\\nVaughn and look at our next topic which is the different types of probability. This is a important topic\\nbecause most of your problems can be solved by understanding which type of probability should I use to solve this problem?\\nRight? So we have three important types of probability. We have marginal joint and conditional probability.\\nSo let's discuss each of these now the probability of an event occurring unconditioned\\non any other event is known as marginal. Or unconditional probability.\\nSo let's say that you want to find the probability that a card drawn is a heart.\\nAll right. So if you want to find the probability that a card drawn is a heart The Profit will be 13 by 52\\nsince there are 52 cards in a deck and there are 13 hearts in a deck of cards.\\nRight and there are 52 cards in a total deck. So your marginal probability will be 13 by 52.\\nThat's about marginal probability. Now, let's understand what is joint probability. And now joint probability\\nis a measure of two events happening at the same time. Okay, let's say that the two events are A and B.\\nSo the probability of event A and B occurring is the intersection of A and B.\\nSo for example, if you want to find the probability that a card is a four and a red that would be joint probability.\\nAll right, because you're finding a card that is 4 and the card has to be red in color. So for the answer to this would be to Biceps you do\\nbecause we have 1/2 in heart and we have 1/2 and diamonds, correct. So both of these are red and color therefore.\\nOur probability is to by 52 and if you further down it is 1 by 26, right?\\nSo this is what joint probability is all about moving on. Let's look at what exactly conditional probability is.\\nSo if the probability of an event or an outcome is based on the occurrence of a previous event or an outcome.\\nThen you call it as a conditional probability. Okay. So the conditional probability of an event B is the probability\\nthat the event will occur given that an event a has already occurred. Right?\\nSo if a and b are dependent events, then the expression for conditional probability is given by this.\\nNow this first term on the left hand side, which is p b of a is basically the probability\\nof event B occurring given that event a has already occurred.\\nSo like I said, if a and b are dependent events than this is the expression but if a and b are independent events,\\nand the expression for conditional probability is like this, right? So guys P of A and B of B is obviously the probability\\nof a and probability of B right now, let's move on now in order\\nto understand conditional probability joint probability and marginal probability.\\nLet's look at a small use case. Okay now basically we're going to Take a data set\\nwhich examines the salary package and training undergone my candidates. Okay. Now in this there are 60 candidates a without training\\nand forty five candidates, which have enrolled for Adder Acres training right. Now the task here is you have to assess the training\\nwith a salary package. Okay. Let's look at this in a little more depth. So in total,\\nwe have hundred and five candidates out of which 60 of them have not enrolled Frederick has training\\nand 45 of them have enrolled for a deer Acres. Inning. All right. This is the small survey that was conducted\\nand this is the rating of the package or the salary that they got right? So if you read through the data,\\nyou can understand there were five candidates without Eddie record training who got a very poor salary package.\\nOkay. Similarly, there are 30 candidates with Ed Eureka training who got a good package, right?\\nSo guys, basically you're comparing the salary package of a person depending on whether or not they've enrolled for a A core training right?\\nThis is our data set. Now. Let's look at our problem statement find the probability\\nthat a candidate has undergone editor Acres training quite simple, which type of probability is this.\\nThis is marginal probability. Right? So the probability that a candidate has undergone Edge rakers training is\\nobviously 45 divided by a hundred and five since 45 is the number of candidates with Eddie record raining\\nand hundred and five is the total number of candidates, so you Value of approximately 0.4 to or\\nI that's the probability of a candidate that has undergone a Judaica straining next question find the probability\\nthat a candidate has attended edger a constraining and also has good package.\\nNow. This is obviously a joint probability problem, right? So how do you calculate this now?\\nSince our table is quite formatted we can directly find that people who have gotten a good package\\nalong with Eddie record raining or 30, right? So out of hundred and five people 30 people\\nhave education training and a good package, right? They specifically asking for people with Ado Rekha training remember that right?\\nThe question is find the probability that a candidate has attended editor Acres training and also has a good package.\\nAlright, so we need to consider two factors that is a candidate who's addenda deaderick has training\\nand who has a good package. So clearly that number is 30 30 divided by total number of candidates,\\nwhich is 1 0 Five, right. So here you get the answer clearly. Next we have find the probability\\nthat a candidate has a good package given that he has not undergone training.\\nOkay. Now this is clearly conditional probability because here you're defining a condition you're saying\\nthat you want to find the probability of a candidate who has a good package given that he's not undergone.\\nAny training, right? The condition is that he's not undergone any training. All right. So the number of people\\nwho have not undergone training are 60 and out of that five of them have got a good package, right?\\nSo that's why this is Phi by 60 and not 5 by hundred and five because here they have clearly mentioned has\\na good package given that he has not undergone training. You have to only consider people\\nwho have not undergone training, right? So only five people who have not undergone training have gotten\\na good package, right? So 5 divided by 60 you get a probability of around 208\\nwhich is pretty low, right? Okay. So this was all about the different types of probability.\\nNow, let's move on and look at our last Topic in probability, which is base theorem.\\nNow guys Bayes theorem is a very important concept when it comes to statistics and probability.\\nIt is majorly used in knife bias algorithm. Those of you who aren't aware. Now I've bias is\\na supervised learning classification algorithm and it is mainly Used in Gmail spam filtering,\\nright a lot of you might have noticed that if you open up Gmail, you'll see that you have a folder called spam right\\nor that is carried out through machine learning and the algorithm used there is knife bias, right?\\nSo now let's discuss what exactly the Bayes theorem is and what it denotes the bias theorem is used\\nto show the relation between one conditional probability and it's inverse. All right, basically Nothing,\\nbut the probability of an event occurring based on prior knowledge of conditions\\nthat might be related to the same event. Okay. So mathematically the bell's theorem\\nis represented like this, right like shown in this equation. The left-hand term is referred to as the likelihood ratio,\\nwhich measures the probability of occurrence of event B, given an event a okay on the left hand side is\\nwhat is known as the posterior right is referred to as posterior. Are which means that the probability\\nof occurrence of a given an event B, right? The second term is referred to as the likelihood ratio\\nor a this measures the probability of occurrence of B, given an event a now P of a is also known as the prior\\nwhich refers to the actual probability distribution of A and P of B is again, the probability of B, right.\\nThis is the bias theorem in order to better understand the base theorem. Let's look at a small example.\\nLet's say that we Three balls we have about a bowel be and bouncy okay barley contains two blue balls\\nand for red balls bowel be contains eight blue balls and for red balls baozi contains one blue ball\\nand three red balls. Now if we draw one ball from each Bowl,\\nwhat is the probability to draw a blue ball from a bowel a if we know that we drew exactly a total\\nof two blue balls right if you didn't Understand the question. Please. Read it.\\nI shall pause for a second or two. Right. So I hope all of you have understood the question.\\nOkay. Now what I'm going to do is I'm going to draw a blueprint for you and tell you how exactly to solve the problem.\\nBut I want you all to give me the solution to this problem, right? I'll draw a blueprint. I'll tell you what exactly the steps are\\nbut I want you to come up with a solution on your own right the formula is also given to you. Everything is given to you.\\nAll you have to do is come up with the final answer. Right? Let's look at how you can solve this problem.\\nSo first of all, what we will do is Let's consider a all right, let a be the event of picking a blue ball from bag in and let\\nX be the event of picking exactly two blue balls, right because these are the two events\\nthat we need to calculate the probability of now there are two probabilities that you need to consider here.\\nOne is the event of picking a blue ball from bag a and the other is the event of picking exactly two blue balls.\\nOkay. So these two are represented by a and X respectively Lee so what we want is the probability of occurrence\\nof event a given X, which means that given that we're picking exactly two blue balls,\\nwhat is the probability that we are picking a blue ball from bag? So by the definition of conditional probability,\\nthis is exactly what our equation will look like. Correct. This is basically a occurrence of event a given an event X\\nand this is the probability of a and x and this is the probability of X alone, correct?\\nAnd what we need to do is we need to find these two probabilities which is probability of a and X occurring together\\nand probability of X. Okay. This is the entire solution. So how do you find P probability\\nof X this you can do in three ways. So first is white ball from a either white from be\\nor read from see now first is to find the probability of x x basically represents the event\\nof picking exactly two blue balls. Right. So these are the three ways in which it is possible.\\nSo you'll pick one blue ball from bowel a and one from bowel be in the second case.\\nYou can pick one from a and another blue ball from see in the third case.\\nYou can pick a blue ball from Bagby and a blue ball from bagsy. Right? These are the three ways in which it is possible.\\nSo you need to find the probability of each of this step two is that you need to find the probability of a\\nand X occurring together. This is the sum of terms 1 and 2. Okay, this is\\nbecause in both of these events, we are picking a ball from bag, correct. So there is find out this probability and let\\nme know your answer in the comment section. All right. We'll see if you get the answer right? I gave you the entire solution to this.\\nAll you have to do is substitute the value right? If you want a second or two, I'm going to pause on the screen so that you can go through this\\nin a more clear away. Right? Remember that you need to calculate two.\\nTease the first probability that you need to calculate is the event of picking a blue ball\\nfrom bag a given that you're picking exactly two blue balls. Okay, II probability you need to calculate\\nis the event of picking exactly two blue bonds. All right. These are the two probabilities.\\nYou need to calculate so remember that and this is the solution. All right, so guys make sure you mention your answers\\nin the comment section for now. Let's move on and Look at our next topic, which is the inferential statistics.\\nSo guys, we just completed the probability module right now. We will discuss inferential statistics,\\nwhich is the second type of Statistics. We discussed descriptive statistics earlier.\\nAlright, so like I mentioned earlier inferential statistics also known as statistical inference is a branch of Statistics\\nthat deals with forming inferences and predictions about a population based on a sample of data.\\nAre taken from the population. All right, and the question you should ask is how does one form inferences or predictions on a sample?\\nThe answer is you use Point estimation? Okay. Now you must be wondering what is point estimation one estimation is concerned\\nwith the use of the sample data to measure a single value which serves as an approximate value\\nor the best estimate of an unknown population parameter. That's a little confusing. Let me break it down to you for Camping\\nin order to calculate the mean of a huge population. What we do is we first draw out the sample of the population\\nand then we find the sample mean right the sample mean is then used to estimate the population mean this is basically Point estimate,\\nyou're estimating the value of one of the parameters of the population, right? Basically the main\\nyou're trying to estimate the value of the mean. This is what point estimation is the two main terms\\nin point estimation. There's something known as as the estimator and the something known as the estimate estimator is a function of the sample\\nthat is used to find out the estimate. Alright in this example. It's basically the sample mean right so a function\\nthat calculates the sample mean is known as the estimator and the realized value of the estimator is the estimate right?\\nSo I hope Point estimation is clear. Now, how do you find the estimates? There are four common ways in which you can do this.\\nThe first one is method of Moment you'll what you do is you form an equation in the sample data set\\nand then you analyze the similar equation in the population data set as well like the population mean population variance and so on.\\nSo in simple terms, what you're doing is you're taking down some known facts about the population\\nand you're extending those ideas to the sample. Alright, once you do that, you can analyze the sample and estimate more\\nessential or more complex values right next. We have maximum likelihood.\\nBut this method basically uses a model to estimate a value. All right. Now a maximum likelihood is majorly based on probability.\\nSo there's a lot of probability involved in this method next. We have the base estimator this works by minimizing\\nthe errors or the average risk. Okay, the base estimator has a lot to do with the Bayes theorem.\\nAll right, let's not get into the depth of these estimation methods. Finally. We have the best unbiased estimators in this method.\\nThere are seven unbiased estimators that can be used to approximate a parameter.\\nOkay. So Guys these were a couple of methods that are used to find the estimate\\nbut the most well-known method to find the estimate is known as the interval estimation.\\nOkay. This is one of the most important estimation methods or at this is where confidence interval also comes into the picture right\\napart from interval estimation. We also have something known as margin of error. So I'll be discussing all of this.\\nIn the upcoming slides. So first let's understand. What is interval estimate? Okay, an interval or range of values,\\nwhich are used to estimate a population parameter is known as an interval estimation, right?\\nThat's very understandable. Basically what they're trying to see is you're going to estimate the value of a parameter.\\nLet's say you're trying to find the mean of a population. What you're going to do is you're going to build a range\\nand your value will lie in that range or in that interval. All right. So this way your output is going to be more accurate\\nbecause you've not predicted a point estimation instead. You have estimated an interval\\nwithin which your value might occur, right? Okay. Now this image clearly shows\\nhow Point estimate and interval estimate or different. So where's interval estimate is obviously more accurate\\nbecause you're not just focusing on a particular value or a particular point\\nin order to predict the probability instead. You're saying that the value might be\\nwithin this range between the lower confidence limit and the upper confidence limit.\\nAll right, this is denotes the range or the interval. Okay, if you're still confused about interval estimation,\\nlet me give you a small example if I stated that I will take 30 minutes to reach the theater.\\nThis is known as Point estimation. Okay, but if I stated that I will take between 45 minutes\\nto an hour to reach the theater. This is an example of Will estimation all right.\\nI hope it's clear. Now now interval estimation gives rise to two important statistical terminologies one is known as confidence interval\\nand the other is known as margin of error. All right. So there's it's important that you pay attention\\nto both of these terminologies confidence interval is one of the most significant measures\\nthat are used to check how essential machine learning model is. All right. So what is confidence interval confidence interval is\\nthe measure of your confidence that the interval estimated contains the population parameter or the population mean\\nor any of those parameters right now statisticians use confidence interval to describe the amount\\nof uncertainty associated with the sample estimate of a population parameter now guys,\\nthis is a lot of definition. Let me just make you understand confidence interval with a small example.\\nOkay. Let's say that you perform a survey and you survey a group of cat owners.\\nThe see how many cans of cat food they purchase in one year. Okay, you test\\nyour statistics at the 99 percent confidence level and you get a confidence interval\\nof hundred comma 200 this means that you think that the cat owners by between hundred to two hundred cans in a year and also\\nsince the confidence level is 99% shows that you're very confident that the results are, correct.\\nOkay. I hope all of you are clear with that. Alright, so your confidence interval here will be\\na hundred and two hundred and your confidence level will be 99% Right? That's the difference between confidence interval\\nand confidence level So within your confidence interval your value is going to lie and your confidence level will show\\nhow confident you are about your estimation, right? I hope that was clear. Let's look at margin of error.\\nNo margin of error for a given level of confidence is a greatest possible distance\\nbetween the Point estimate and the value of the parameter that it is estimating you can say\\nthat it is a deviation from the actual point estimate right. Now. The margin of error can be calculated\\nusing this formula now zc her denotes the critical value or the confidence interval\\nand this is X standard deviation divided by root of the sample size.\\nAll right, n is basically the sample size now, let's understand how you can estimate the confidence intervals.\\nSo guys the level of confidence which is denoted by C is the probability that the interval estimate contains a population parameter.\\nLet's say that you're trying to estimate the mean. All right. So the level of confidence is the probability\\nthat the interval estimate contains a population parameter. So this interval between minus Z and z\\nor the area beneath this curve is nothing but the probability that the interval estimate contains a population parameter.\\nYou don't all right. It should basically contain the value that you are predicting right.\\nNow. These are known as critical values. This is basically your lower limit and your higher limit confidence level.\\nAlso, there's something known as the Z score now. This court can be calculated by using the standard normal table, right?\\nIf you look it up anywhere on Google you'll find the z-score table or the standard normal table get to understand\\nhow this is done. Let's look at a small example. Okay, let's say that the level of Vince is 90% This means\\nthat you are 90% confident that the interval contains the population mean. Okay, so the remaining 10% which is out of hundred percent.\\nThe remaining 10% is equally distributed on these Dale regions. Okay, so you have 0.05 here and 0.05 over here, right?\\nSo on either side of see you will distribute the other leftover percentage now these these scores are calculated from the table\\nas I mentioned before. All right one. N64 5 is get collated from the standard normal table.\\nOkay. So guys how you estimate the level of confidence. So to sum it up. Let me tell you the steps that are involved\\nin constructing a confidence interval first. You'll start by identifying a sample statistic.\\nOkay. This is the statistic that you will use to estimate a population parameter. This can be anything like the mean\\nof the sample next you will select a confidence level now the confidence level describes the uncertainty\\nof a Sampling method right after that you'll find something known as the margin of error, right?\\nWe discuss margin of error earlier. So you find this based on the equation that I explained in the previous slide,\\nthen you'll finally specify the confidence interval. All right. Now, let's look at a problem statement\\nto better understand this concept a random sample of 32 textbook prices is taken from a local College Bookstore.\\nThe mean of the sample is so so and so and the sample standard deviation is\\nThis use a 95% confident level and find the margin of error for the mean price\\nof all text books in the bookstore. Okay. Now, this is a very straightforward question. If you want you can read the question again.\\nAll you have to do is you have to just substitute the values into the equation.\\nAll right, so guys, we know the formula for margin of error you take the Z score\\nfrom the table. After that we have deviation Madrid's 23.4 for right\\nand that's standard deviation and n stands for the number of samples here. The number of samples is 32 basically 32 textbooks.\\nSo approximately your margin of error is going to be around 8.1 to this is a pretty simple question.\\nAll right. I hope all of you understood this now that you know, the idea behind confidence interval.\\nLet's move ahead to one of the most important topics in statistical inference,\\nwhich is hypothesis testing, right? So Sigelei statisticians use hypothesis testing\\nto formally check whether the hypothesis is accepted or rejected. Okay, hypothesis.\\nTesting is an inferential statistical technique used to determine whether there is enough evidence in a data sample to infer\\nthat a certain condition holds true for an entire population. So to understand\\nthe characteristics of a general population, we take a random sample, and we analyze the properties of the sample right we test.\\nWhether or not the identified conclusion represent the population accurately and finally we interpret their results now\\nwhether or not to accept the hypothesis depends upon the percentage value that we get from the hypothesis.\\nOkay, so to better understand this, let's look at a small example before that. There are few steps that are followed in hypothesis,\\ntesting you begin by stating the null and the alternative hypothesis. All right. I'll tell you what exactly these terms are\\nand then you formulate. Analysis plan right after that you analyze the sample data\\nand finally you can interpret the results right now to understand the entire hypothesis testing.\\nWe look at a good example. Okay now consider for boys Nick jean-bob\\nand Harry these boys were caught bunking a class and they were asked to stay back at school\\nand clean the classroom as a punishment, right? So what John did is he decided that four of them would take turns to clean their classrooms.\\nHe came up with a plan of writing each of their names on chits and putting them in a bout now every day.\\nThey had to pick up a name from the bowel and that person had to play in the clock, right? That sounds pretty fair enough now it is been three days\\nand everybody's name has come up except John's assuming that this event is completely random\\nand free of bias. What is a probability of John not treating right or is the probability\\nthat he's not actually cheating this can Solved by using hypothesis testing.\\nOkay. So we'll Begin by calculating the probability of John not being picked for a day.\\nAlright, so we're going to assume that the event is free of bias. So we need to find out the probability\\nof John not cheating right first we'll find the probability that John is not picked for a day, right?\\nWe get 3 out of 4, which is basically 75% 75% is fairly high.\\nSo if John is not picked for three days in a row the Probability will drop down to approximately 42% Okay.\\nSo three days in a row meaning that is the probability drops down to 42 percent.\\nNow, let's consider a situation where John is not picked for 12 days in a row\\nthe probability drops down to Tea Point two percent. Okay, that's the probability\\nof John cheating becomes fairly high, right? So in order for statisticians to come to a conclusion,\\nthey Define what is known as the threshold value. Right considering the above situation\\nif the threshold value is set to 5 percent. It would indicate that if the probability lies below 5% then John is cheating\\nhis way out of detention. But if the probability is about threshold value then John it just lucky and his name isn't getting picked.\\nSo the probability and hypothesis testing give rise to two important components of hypothesis testing,\\nwhich is null hypothesis and alternative hypothesis. Null. Hypothesis is based.\\nBasically approving the Assumption alternate hypothesis is when your result disapproves the Assumption right therefore\\nin our example, if the probability of an event occurring is less than 5% which it is then the event is biased hence.\\nIt proves the alternate hypothesis.\\nUndoubtedly machine learning is the most in-demand technology in today's market.\\nIt's applications. From Seth driving cause to predicting deadly diseases such as ALS the high demand\\nfor machine learning skills is the motivation behind today's session. So let me discuss the agenda with you first.\\nNow, we're going to begin the session by understanding the need for machine learning and why it is important after that.\\nWe look at what exactly machine learning is and then we'll discuss a couple of machine learning definitions.\\nOnce we're done with that. We'll look at the machine learning process and how you can solve a problem by using Using\\nthe machine learning process next we will discuss the types of machine learning which includes supervised unsupervised\\nand reinforcement learning. Once we're done with that. We'll discuss the different types of problems\\nthat can be solved by using machine learning. Finally. We will end this session by looking at a demo\\nwhere we'll see how you can perform weather forecasting by using machine learning. All right, so guys,\\nlet's get started with our first topic. So what is the importance or what is the need for machine learning now?\\nSince the technical Revolution, we've been generating an immeasurable amount of data as for research\\nwith generating around 2.5 quintillion bytes of data every single day\\nand it is estimated that by 2020 1.7 MB of data will be created every second\\nfor every person on earth. Now that is a lot of data right now. This data comes from sources such as\\nthe cloud iot devices social media and all of that. Since all of us are very interested\\nin the internet right now with generating a lot of data. All right, you have no idea how much data we generate\\nthrough social media all the chatting that we do and all the images that we post on Instagram the videos\\nthat we watch all of this generates a lot of data. Now how does machine learning fit into all of this\\nsince we're producing this much data, we need to find a method that can analyze process and interpret this much data.\\nAll right, and we need to find a method. That can make sense out of data. And that method is machine learning.\\nNow the lot of talk tire companies and data driven company such as Netflix and Amazon\\nwhich build machine learning models by using tons of data in order to identify any profitable opportunities.\\nAnd if they want to avoid any unwanted risk it make use of machine learning. Alright, so through machine learning You can predict risk\\nYou can predict profits you can identify opportunities, which will help you grow your business. Business so now I'll show you\\na couple of examples of where in machine learning is used. All right, so I'm sure all of you have been watch on Netflix.\\nNow the most important thing about Netflix is its recommendation engine. All right.\\nMost of Netflix's Revenue comes from its recommendation engine. So the recommendation engine\\nbasically studies the movie viewing patterns of its users and then recommends relevant movies to them.\\nAll right, it recommends movies depending on users interests. Depending on the type\\nof movies the user watches and all of that. Alright, so that is how Netflix uses machine learning.\\nNext. We have Facebook's Auto tagging feature. Now the logic behind Facebook's\\nAuto tagging feature is machine learning and neural networks. I'm not sure how many of you know this but Facebook\\nmakes use of deepmind face verification system, which is based on machine learning natural language processing\\nand neural networks. So deep mine basically studies the facial features in an image and it tag your friends and family.\\nAnother such example is Amazon's Alexa now Alexa is basically an advanced level virtual assistant\\nthat is based on natural language processing and machine learning. Now, it can do more than just play music for you.\\nAll right, it can book your Uber it can connect with other I/O devices that your house it can track your health.\\nIt can order food online and all of that. So data, and machine learning are basically the main factors\\nbehind Alex has power another such example is the Google spam filter.\\nSo guys Gmail basically makes use of machine learning to filter out spam messages.\\nIf any of you just open your Gmail inbox, you'll see that there are separate sections.\\nThere's one for primary this social the spam and the Joe general made now basically Gmail makes use\\nof machine learning algorithms and natural language processing to an Is emails in real time\\nand then classify them as either spam or non-spam now, this is another famous application of machine learning.\\nSo to sum this up, let's look at a few reasons. Why machine learning is so important.\\nSo the first reason is obviously increase in data generation. So because of excessive production of data,\\nwe need a method that can be used to structure and lies and draw useful insights from data.\\nThis is where machine learning comes as in it uses data to solve problems and find solutions\\nto the most complex tasks faced by organizations. Another important reason is that it improves decision-making.\\nSo by making use of various algorithms machine learning can be used to make Better Business decisions.\\nFor example machine learning is used to forecast sales. It is used to predict any downfalls in the stock market.\\nIt is used to identify risks anomalies and so on now the next reason Is it uncovers patterns\\nand Trends in data finding hidden patterns and extracting key insights from data is the most essential part\\nof machine learning. So by building predictive models and using statistical techniques machine learning\\nallows you to dig beneath the surface and explore the data at a minut scale now understanding data\\nand extracting patterns manually will take a lot of days. Now, if you do this through machine learning algorithms,\\nyou can perform such computations. Nations in less than a second. Another reason is\\nthat it's solved complex problems. So from detecting genes that are linked to deadly ALS disease\\nis to building self-driving cars and building phase detection systems machine learning\\ncan be used to solve the most complex problems. So guys now that you know,\\nwhy machine learning is so important. Let's look at what exactly machine learning is.\\nThe term machine learning was first coined by Arthur Samuel in the year 1959 now looking back\\nthat your was probably the most significant in terms of technological advancements.\\nThere is if you browse through the net about what is machine learning you'll get at least a hundred different definitions.\\nNow the first and very formal definition was given by Tom and Mitchell now,\\nthe definition says that a computer program is set to learn from experience e\\nwith respect to some class. Of caste and performance measure P if its performance at tasks in D\\nas measured by P improves with experience e all right. Now I know this is a little confusing.\\nSo let's break it down into simple words. Now in simple terms machine learning is a subset\\nof artificial intelligence which provides machines the ability to learn automatically\\nand improve from experience without being explicitly programmed to do so in the sense.\\nIt is the practice of getting machines to solve problems by gaining the ability to think but wait now\\nhow can a machine think or make decisions? Well, if you feel a machine a good amount of data,\\nit will learn how to interpret process and analyze this data by using machine learning algorithm.\\nOkay. Now guys, look at this figure on top. Now this figure basically shows how a machine learning algorithm\\nor how the machine learning process really works. So the machine learning Begins by feeding the machine lots\\nand lots of data okay by using this data. The machine is trained to detect hidden insights and Trends.\\nNow these insights are then used to build a machine learning model by using an algorithm\\nin order to solve a problem. Okay. So basically you're going to feed a lot of data to the machine.\\nThe machine is going to get trained by using this data. It's going to use this data and it's going to draw useful insights\\nand patterns from it, and then it's going to build a model by Using machine learning algorithms.\\nNow this model will help you predict the outcome or help you solve any complex problem or any business problem.\\nSo that's a simple explanation of how machine learning works. Now, let's move on and look at some of the most commonly used machine learning terms.\\nSo first of all, we have algorithm. Now, this is quite self-explanatory. Basically algorithm is a set of rules\\nor statistical techniques, which are used to learn patterns from data now an algorithm is The logic behind a machine learning model.\\nAll right, an example of a machine learning algorithm is linear regression. I'm not sure how many of you have heard of linear regression.\\nIt's the most simple and basic machine learning algorithm. All right. Next we have model now model is the main component\\nof machine learning. All right. So model will basically map the input to your output\\nby using the machine learning algorithm and by using the data that you're feeding the machine.\\nSo basically the model is a representation of the entire machine learning process.\\nSo the model is basically fed input which has a lot of data and then it will output a particular result\\nor a particular outcome by using machine learning algorithms. Next we have something known as predictor variable.\\nNow predictor variable is a feature of the data that can be used to predict the output.\\nSo for example, let's say that you're trying to predict the weight of a person depending\\non the person's height and their age. All right. So over here the predictor variables are your height\\nand your age because you're using height and age of a person to predict the person's weight.\\nAlright, so the height and the A's are the predictor variables now, Wait on the other hand is the response\\nor the target variable. So response variable is a feature or the output variable that needs to be predicted by using the predictor variables.\\nAll right, after that we have something known as training data. So guys the data that is fed to a machine learning model is always split\\ninto two parts first. We have the training data and then we have the testing data now training\\ndata is basically used to build the machine learning model. So usually training data is much larger.\\nThan the testing data because obviously if you're trying to train the machine then you're going to feed it a lot more data.\\nTesting data is just used to validate and evaluate the efficiency of the model.\\nAlright, so that was training data and testing data. So Guys, these were a few terms that I thought you should know\\nbefore we move any further. Okay. Now, let's move on and discuss the machine learning process.\\nNow, this is going to get very interesting because I'm going to give you an example and make you understand how the machine learning.\\nprocess works So first of all, let's define the different stages or the different steps involved in the machine learning process.\\nSo machine learning process always begins with defining the objective or defining the problem\\nthat you're trying to solve next is is data Gathering or data collection. Now the data that you need to solve this problem\\nis collected at this stage. This is followed by data preparation or data processing after that.\\nYou have data exploration and Analysis. Isis and the next stage is building a machine learning model.\\nThis is followed by model evaluation. And finally you have prediction or your output.\\nNow, let's try to understand this entire process with an example. So our problem statement here is to predict the possibility\\nof rain by studying the weather conditions. So let's say that you're given a problem statement\\nand you're asked to use a machine learning process to solve this problem statement. So let's get started.\\nAlright, so the first step is to Find the objective of the problem statement. Our objective here is to predict the possibility\\nof rain by studying the weather conditions. Now in the first stage of a machine learning process.\\nYou must understand what exactly needs to be predicted. Now in our case the objective is to predict the possibility\\nof rain by studying weather conditions, right? So at this stage, it is also essential to take mental notes on what kind\\nof data can be used to solve this problem or the type of approach that you can follow to get.\\nGet to the solution. All right, a few questions that are worth asking during this stage is\\nwhat are we trying to predict? What are the Target features or what are the predictor variables?\\nWhat kind of input data do we need? And what kind of problem are we facing? Is it a binary classification problem or is it\\na clustering problem now, don't worry. If you don't know what classification and clustering is I'll be explaining this\\nin the upcoming slides. So guys this was the first step of a machine learning process, which is Define the Double the problem.\\nAll right. Now, let's move on and look at step number two. So step number two is basically data collection\\nor data Gathering now at this stage. You must be asking questions such as what kind of data\\nis needed to solve the problem is the data available and if it is available, how can I get the data?\\nOkay. So once you know the type of data that is required, you must understand how you can derive this data data collection\\ncan be done manually or by web scraping, but if you're a beginner Nor and you're just looking to learn\\nmachine learning you don't have to worry about getting the data. OK there are thousands of data resources on the web.\\nYou can just go ahead and download the datasets from websites such as kaggle. Okay, now coming back to the problem\\nat hand the data needed for weather forecasting includes measures such as humidity level temperature pressure locality\\nwhether or not you live in a hill station and so on so guys such data must be collected\\nand stored for analysis. Now the next stage in machine learning is preparing your data\\nthe data you collected is almost never in the right format. So basically you'll encounter a lot of inconsistencies\\nin the data set. Okay, this includes missing values redundant variables duplicate values\\nand so on removing such values is very important because they might lead to wrongful computations\\nand predictions. So that's why at this stage you must can the entire data set for any inconsistencies.\\nYou have to fix them at this stage. Now. The next step is exploratory data analysis.\\nNow data analysis is all about diving deep into data and finding all the hidden data Mysteries.\\nOkay. This is where you become a detective. So edu or exploratory data analysis is like a brainstorming\\nof machine learning data exploration involves understanding the patterns and the trends in your data.\\nSo at this stage all the useful insights are drawn and all the correlations. Turns between the variables are understood.\\nSo you might ask what sort of correlations are you talking about? For example in the case of predicting rain fall.\\nWe know that there is a strong possibility of rain if the temperature has fallen low. Okay.\\nSo such correlations have to be understood and mapped at this stage. Now. This stage is followed by stage number 5,\\nwhich is building a machine learning model. So all the insights and the patterns that you derive\\nduring data exploration are used to build the machine learning. So this stage always Begins by splitting the data set\\ninto two parts training data and the testing data. So earlier in the session. I already told you what training\\nand testing data is now the training data will be used to build and analyze the model and the logic of the model\\nwill be based on the machine learning algorithm that is being implemented. Okay. Now in the case of predicting rainfall\\nsince the output will be in the form of true or false we can use a classification algorithm like logistically.\\nRegression now choosing the right algorithm depends on the type of problem. You're trying to solve the data set you have\\nand the level of complexity of the problem. So in the upcoming sections will be discussing different types of problems that can be solved by using machine learning.\\nSo don't worry. If you don't know what classification algorithm is and what logistic regression in.\\nOkay. So all you need to know is at this stage, you'll be building a machine learning model by using machine learning algorithm\\nand by using the training data set the next But in on machine learning process is model evaluation\\nand optimization. So after building a model by using the training data set it is finally time to put the model to a test.\\nOkay. So the testing data set is used to check the efficiency of the model and how accurately it can predict the outcome.\\nSo once you calculate the accuracy any improvements in the model have to be implemented in this stage.\\nOkay, so methods like parameter tuning and cross-validation can be used to improve the The performance\\nof the model this is followed by the last stage, which is predictions. So once the model is evaluated\\nand improved it is finally used to make predictions. The final output can be a categorical variable\\nor it can be a continuous quantity in our case for predicting the occurrence of rainfall the output will be a categorical variable\\nin the sense. Our output will be in the form of true or false. Yes or no. Yes, basically represents\\nthat is going to rain and no will represent that. It wondering okay as simple as that,\\nso guys that was the entire machine learning process.\\nA linear regression is one of the easiest algorithm in machine learning. It is a statistical model that attempts to show the relationship\\nbetween two variables. So the linear equation, but before we drill down to linear regression algorithm in depth,\\nI'll give you a quick overview of today's agenda. So we'll start a session with a quick overview of what is regression\\nas linear regression is one of a type of regression algorithm. Once we learn about regression,\\nits use case the various types of it next. We'll learn about the algorithm from scratch where I live\\nTo its mathematical implementation first, then we'll drill down to the coding part and Implement linear regression using python\\nin today's session will deal with linear regression algorithm using least Square method checketts goodness of fit\\nor how close the data is to the fitted regression line using the R square method and then finally\\nwhat we'll do well optimized it using the gradient descent method in the last part on the coding session.\\nI'll teach you to implement linear regression using Python and the coding session. Would be divided into two parts the first part would consist\\nof linear regression using python from scratch where you will use the mathematical algorithm\\nthat you have learned in this session. And in the next part of the coding session will be using scikit-learn for direct implementation\\nof linear regression. All right. I hope the agenda is clear to you guys are like so let's begin our session with what is regression.\\nWell regression analysis is a form of predictive modeling technique which investigates the relationship between a dependent and independent.\\nAble a regression analysis involves graphing a line over a set of data points\\nthat most closely fits the overall shape of the data or regression shows the changes\\nin a dependent variable on the y-axis to the changes in the explanatory variable on the x-axis fine.\\nNow you would ask what are the uses of regression? Well, they are major three uses of regression analysis\\nthe first being determining the strength of predicator, 's the regression might be used to identify the strength of the effect\\nthat the independent. Variables have on the dependent variable. For example, you can ask question. Like what is the strength of relationship between sales\\nand marketing spending or what is the relationship between age and income second is forecasting\\nan effect in this the regression can be used to forecast effects or impact of changes.\\nThat is the regression analysis help us to understand how much the dependent variable changes with the change\\nin one or more independent variable fine. For example, you can ask question like how Additional\\nseal income will I get for each thousand dollars spent on marketing third is Trend forecasting\\nin this the regression analysis to predict Trends and future values. The regression analysis can be used to get\\nPoint estimates in this you can ask questions. Like what will be the price of Bitcoin and next six months, right?\\nSo next topic is linear versus logistic regression by now. I hope that you know, what a regression is.\\nSo let's move on and understand its type. So there are various kinds of regression like linear. Session logistic regression polynomial regression\\nand others. All right, but for this session will be focusing on linear and logistic regression.\\nSo let's move on and let me tell you what is linear regression. And what is logistic regression then what we'll do we'll compare both of them.\\nAll right. So starting with linear regression in simple linear regression. We are interested in things like y equal MX plus C.\\nSo what we are trying to find is the correlation between X and Y variable this means\\nthat every value of X has a corresponding value of y in it if it is continuous.\\nI like however in logistic regression we are not fitting our data to a straight line like linear regression instead\\nwhat we are doing. We are mapping Y versus X to a sigmoid function in logistic regression.\\nWhat we find out is is y 1 or 0 for this particular value of x so thus we are essentially deciding true or false value\\nfor a given value of x fine. So as a core concept of linear regression You can say\\nthat the data is modeled using a straight line where in the case of logistic regression the data is model using a sigmoid function.\\nThe linear regression is used with continuous variables on the other hand the logistic regression.\\nIt is used with categorical variable the output or the prediction of a linear regression is the value of the variable\\non the other hand the output of production of a logistic regression is the probability of occurrence of the event.\\nNow, how will you check the accuracy and goodness of fit in case of linear regression? We are various methods.\\nTake measured by loss r squared adjusted r squared Etc while in the case of logistic regression you\\nhave accuracy precision recall F1 score, which is nothing but the harmonic mean of precision\\nand recall next is Roc curve for determining the probability threshold for classification\\nor the confusion Matrix Etc. There are many all right. So summarizing the difference between linear and logistic regression.\\nYou can say that the type of function you are mapping to is the main point of difference between linear\\nand regression a linear regression Maps a continuous X2 a continuous fi\\non the other hand a logistic regression Maps a continuous x to the bindery why so we can use logistic regression to make category\\nor true false decisions from the data find so let's move on ahead. Next is linear regression selection criteria,\\nor you can say when will you use linear regression? So the first is classification and regression capabilities regression models predict\\na continuous variable such as the Don't a day or predict the temperature of a city their Reliance\\non a polynomial like a straight line to fit a data set poses a real challenge when it comes towards building a classification capability.\\nLet's imagine that you fit a line with the training points that you have now imagine you add some more data points to it.\\nBut in order to fit it, what do you have to do? You have to change your existing model that is maybe you have to change the threshold itself.\\nSo this will happen with each new data point you add to the model, hence. The linear regression is not good for classification.\\nAll's fine. Next is data quality each missing value removes one data point that could optimize the regression\\nin simple linear regression. The outliers can significantly disrupt the outcome just for now. You can know that if you remove the outliers your model\\nwill become very good. All right. So this is about data quality. Next is computational complexity a linear regression is often\\nnot computationally expensive as compared to the decision tree or the clustering algorithm the order\\nof complexity for n training example and X features. Usually Falls in either Big O of x square or big of xn next is comprehensible\\nand transparent the linear regression are easily comprehensible and transparent in nature.\\nThey can be represented by a simple mathematical notation to anyone and can be understood very easily.\\nSo these are some of the criteria based on which you will select the linear regression algorithm.\\nAll right. Next is where is linear regression used first is evaluating Trends and sales estimate.\\nWell linear regression can be used in Business to evaluate Trends and make estimates or focused for example,\\nif a company sales have increased steadily every month for past few years then conducting a linear analysis\\non the sales data with monthly sales on the y axis and time on the x axis.\\nThis will give you a line that predicts the upward Trends in the sale after creating the trendline the company could use the slope\\nof the lines too focused sale in future months. Next is analyzing. The impact of price changes will linear regression\\ncan be To analyze the effect of pricing on consumer behavior. For instance. If a company changes\\nthe price on a certain product several times, then it can record the quantity itself for each price level\\nand then perform a linear regression with sold quantity as a dependent variable and price as the independent variable.\\nThis would result in a line that depicts the extent to which the customer reduce their consumption of the product\\nas the prices increasing. So this result would help us in future pricing decisions. Next is assessment of risk and fine.\\nFinancial services and insurance domain. Well linear regression can be used to analyze the risk,\\nfor example health insurance company might conduct a linear regression algorithm how it can do it can do it by plotting the number of claims\\nper customer against its age and they might discover that the old customers then to make more health insurance claim.\\nWell the result of such analysis might guide important business decisions. All right, so by now you have just a rough idea of\\nwhat linear regression algorithm as like, What it does where it is used when you should use it early now,\\nlet's move on and understand the algorithm and depth. So suppose you have independent variable on the x-axis\\nand dependent variable on the y-axis. All right suppose. This is the data point on the x axis.\\nThe independent variable is increasing on the x axis. And so does the dependent variable on the y-axis?\\nSo what kind of linear regression line you would get you would get a positive linear regression line. All right as the slope would be positive.\\nNext is suppose. You have an independent variable on the x-axis which is increasing and on the other hand the dependent variable on the y-axis\\nthat is decreasing. So what kind of line will you get in that case? You will get a negative regression line.\\nIn this case as the slope of the line is negative. And this particular line that is line of y equal MX\\nplus C is a line of linear regression which shows the relationship between independent variable and dependent variable\\nand this line is only known as line of linear regression. Okay? So let's add some data points to our graph.\\nSo these are some observation or data points on our graphs. Let's plot some more. Okay.\\nNow all our data points are plotted now our task is to create a regression line or the best fit line.\\nAll right now once our regression line is drawn now, it's the task of production now suppose.\\nThis is our estimated value or the predicted value and this is our actual value. Okay.\\nSo what we have to do our main goal is to reduce this error. That is to reduce the distance between the estimated\\nor the predicted value and the actual value. The best fit line would be the one which had the least error\\nor the least difference in estimated value and the actual value. All right, and other words we have to minimize the error.\\nThis was a brief understanding of linear regression algorithm soon. We'll jump towards mathematical implementation.\\nAll right, but for then let me tell you this suppose you draw a graph with speed on the x-axis\\nand distance covered. On the y axis with the time demeaning constant, if you plot a graph between the speed travel\\nby the vehicle and the distance traveled in a fixed unit of time, then you will get a positive relationship.\\nAll right. So suppose the equation of line as y equal MX plus C. Then in this case Y is the distance traveled\\nin a fixed duration of time x is the speed of vehicle m is the positive slope of the line and see is the y-intercept of the line.\\nAll right suppose the distance remaining constant. You have to plot a graph between the Rid of the vehicle\\nand the time taken to travel a fixed distance then in that case you will get a line with a negative relationship.\\nAll right, the slope of the line is negative here the equation of line changes to y equal minus of MX plus C\\nwhere Y is the time taken to travel a fixed distance X is the speed of vehicle m is the negative slope\\nof the line and see is the y-intercept of the line. All right. Now, let's get back to our independent and dependent variable.\\nSo in that term why is our dependent variable and That is our independent variable.\\nNow, let's move on and see the mathematical implementation of the things. Alright, so we have x\\nequal 1 2 3 4 5 let's plot them on the x-axis. So 0 1 2 3 4 5 6 alike and we have y as 3 4 2 4 5.\\nAll right. So let's plot 1 2 3 4 5 on the y-axis now, let's plot our coordinates 1 by 1 so x equal 1 and y equal 3,\\nso We have here x equal 1 and y equal 3. So this is the point 1 comma 3 so similarly\\nwe have 1 3 2 4 3 2 4 4 & 5 5. All right. So moving on ahead.\\nLet's calculate the mean of X and Y and plot it on the graph. All right, so mean of X is 1\\nplus 2 plus 3 plus 4 plus 5 divided by 5. That is 3. All right, similarly mean of Y is 3 plus 4 plus 2\\nplus 4 plus 5 that is 18. So it in divided by 5. That is nothing but 3.6 aligned so next\\nwhat we'll do we'll plot our mean that is 3 comma 3 .6 on the graph. Okay. So there's a point 3 comma 3 .6\\nsee our goal is to find or predict the best fit line using the least Square Method All right.\\nSo in order to find that we first need to find the equation of line, so let's find the equation of our regression line.\\nAll right. So let's suppose this is our regression line y equal MX plus C.\\nNow. We have an equation of line. So all we need to do is find the value of M and see\\nwhere m equals summation of x minus X bar X Y minus y bar upon the summation of x\\nminus X bar whole Square don't get confused. Let me resolve it for you. All right. So moving on ahead as a part of formula.\\nWhat we are going to do will calculate x minus X bar. So we have X as 1 minus X bar as 3 so 1 minus 3\\nthat is minus 2 next. We have x equal to minus its mean 3\\nthat is minus 1 similarly. We have 3 minus 3 is 0 4 -\\n3 1 5 - 3 2 alight so x minus X bar. It's nothing but the distance of all the point\\nthrough the line y equal 3 and what does this y minus y bar implies it implies\\nthat distance of all the point from the line x equal 3 .6 fine. So let's calculate the value of y minus y bar.\\nSo starting with y equal 3 - value of y. A bar that is 3.6.\\nSo it is three minus 3.6 how much - of 0.6 next is 4 minus 3.6 that is 0.4 next to minus 3.6\\nthat is minus of 1 point 6 next is 4 minus 3.6 that is 0.4 again,\\n5 minus 3.6 that is 1.4. Alright, so now we are done with Y minus y bar fine now next\\nwe will calculate x minus X bar whole Square Let's calculate x minus X bar whole Square.\\nSo it is minus 2 whole square. That is 4 minus 1 whole square. That is 1 0 squared is 0 1 Square 1 2 square for fine.\\nSo now in our table we have x minus X bar y minus y bar and x minus X bar whole Square.\\nNow what we need. We need the product of x minus X bar X Y minus y bar.\\nAlright, so let's see the product of x minus X bar X Y minus y bar that is minus of 2 x minus of 0.6.\\nThat is one. Point 2 minus of 1 x 0 point 4 that is minus of 0 point 4 0 x\\nminus of 1.6. That is 0 1 multiplied by zero point four that is 0.4.\\nAnd next 2 multiplied by 1 point for that is 2.8. All right.\\nNow almost all the parts of our formula is done. So now what we need to do is get the summation\\nof last two columns. All right, so the summation of x minus X bar whole square is 10\\nand the summation of x minus X bar. X Y minus y bar is 4 so the value of M will be equal\\nto 4 by 10 fine. So let's put this value of m equals zero point 4 and our line y equal MX plus C.\\nSo let's file all the points into the equation and find the value of C. So we have y as 3.6 remember the mean by m as 0.4\\nwhich we calculated just now X as the mean value of x that is 3 and we have the\\nin as 3 point 6 equals 0 point 4 x 3 plus C. Alright\\nthat is 3.6 equal 1 Point 2 plus C. So what is the value of C that is 3.6 minus 1 Point 2.\\nThat is 2 point 4. All right. So what we had we had m equals zero point four see\\nas 2.4 and then finally when we calculate the equation of the regression line what we get is y equal zero point four times of X\\nplus two point four. So there is the regression line. Like so there's how you're plotting your points.\\nThis is your actual point. All right. Now for given m equals zero point four and SQL 2.4.\\nLet's predict the value of y for x equal 1 2 3 4 & 5. So when x equal 1 the predicted value\\nof y will be zero point four x one plus two point four that is 2.8.\\nSimilarly when x equal to predicted value of y will be zero point 4 x 2 plus 2 point 4 that equals to 3 point.\\nTwo similarly x equal 3 y will be 3 point 6 x equal 4 y will be 4 point 0\\nx equal 5 y will be four point four. So let's plot them on the graph and the line passing through all these predicting point\\nand cutting y-axis at 2.4 as the line of regression. Now your task is to calculate the distance between the actual\\nand the predicted value and your job is to reduce the distance. All right, or in other words, you have to reduce the error between the actual\\nand the predicted. The line with the least error will be the line of linear regression or regression line and it will also be the best fit line.\\nAlright, so this is how things work in computer. So what it do it performs a number of iteration\\nfor different values of M for different values of M. It will calculate the equation of line\\nwhere y equals MX plus C. Right? So as the value of M changes the line is changing so iteration will start from one.\\nAll right, and it will perform a number of iteration so after Every iteration what it will do it will calculate the predicted value\\naccording to the line and compare the distance of actual value to the predicted value and the value of M\\nfor which the distance between the actual and the predicted value is minimum will be selected\\nas the best fit line. All right. Now that we have calculated the best fit line now,\\nit's time to check the goodness of fit or to check how good a model is performing. So in order to do that,\\nwe have a method called R square method. So what is this R square? Well r-squared value is a statistical measure of\\nhow close the data are to the fitted regression line in general. It is considered that a high r-squared value model is a good model,\\nbut you can also have a lower squared value for a good model as well or a higher Squad value for a model\\nthat does not fit at all. All right. It is also known as coefficient of determination or the coefficient of multiple determination.\\nLet's move on and see how a square is calculated. So these are our actual values plotted on the graph.\\nWe had calculated the predicted values of Y as 2.8 3.2 3.6 4.0 4.4.\\nRemember when we calculated the predicted values of Y for the equation Y predicted equals 0 1 4 x\\nof X plus two point four for every x equal 1 2 3 4 & 5 from there.\\nWe got the power. Good values of Phi. All right. So let's plot it on the graph. So these are point and the line passing\\nthrough these points are nothing but the regression line. All right. Now, what you need to do is\\nyou have to check and compare the distance of actual - mean versus the distance of predicted - mean.\\nAlright. So basically what you are doing you are calculating the distance of actual value to the mean to distance of predicted value to the mean.\\nAll right, so there is nothing but a square in mathematically you can represent our school. Whereas summation of Y predicted values minus y\\nbar whole Square divided by summation of Y minus y bar whole Square where Y is the actual value y p is the predicted value\\nand Y Bar is the mean value of y that is nothing but 3.6. Remember, this is our formula.\\nSo next what we'll do we'll calculate y minus y bar. So we have y is 3y bar as 3 point 6 so we'll calculate\\nit as 3 minus 3.6 that is nothing but minus of 0.6 similarly for y equals 4 and Y Bar equal 3.6.\\nWe have y minus y bar as zero point 4 then 2 minus 3.6. It has 1 point 6 4 minus 3.6 again\\nzero point four and five minus 3.6 it is 1.4. So we got the value of y minus y bar.\\nNow what we have to do we have to take it Square. So we have minus of 0.6 Square as 0.36 0.4 Square as 0.16 -\\nof 1.6 Square as 2.56 0.4 Square as 0.16 and 1.4 squared\\nis 1.96 now is a part of formula what we need. We need our YP minus y BAR value.\\nSo these are VIP values and we have to subtract it from the No, right. So 2 .8 minus 3.6 that is minus 0.8.\\nSimilarly. We will get 3.2 minus 3.6 that is 0.4 and 3.6 minus 3.6\\nthat is 0 for 1 0 minus 3.6 that is 0.4. Then 4 .4 minus 3.6 that is 0.8.\\nSo we calculated the value of YP minus y bar now, it's our turn to calculate the value of y b minus\\ny bar whole Square next. We have - of 0.8 Square as 0.64 - of Point four square as 0.160 Square\\n0 0 point 4 Square as again 0.16 and 0.8 Square as 0.64.\\nAll right. Now as a part of formula what it suggests it suggests me to take the summation of Y P\\nminus y bar whole square and summation of Y minus y bar whole Square. All right. Let's see.\\nSo on submitting y minus y bar whole Square what you get is five point two and summation of Y P minus\\ny bar whole Square you get one point six. So the value of R square can be calculated as\\n1 point 6 upon 5.2 fine. So the result which will get is approximately equal to 0.3.\\nWell, this is not a good fit. All right, so it suggests that the data points are far away from the regression line.\\nAlright, so this is how your graph will look like when R square is 0.3\\nwhen you increase the value of R square to 0.7. So you'll see that the actual value would like closer to the regression line\\nwhen it reaches to 0.9 it comes. More clothes and when the value of approximately equals\\nto 1 then the actual values lies on the regression line itself, for example, in this case.\\nIf you get a very low value of R square suppose 0.02. So in that case what you'll see that the actual values are\\nvery far away from the regression line, or you can say that there are too many outliers in your data.\\nYou cannot focus anything from the data. All right. So this was all about the calculation of R square now,\\nyou might get a question like are low values of Square always bad. Well in some field it is entirely expected that I ask\\nwhere value will be low. For example any field that attempts to predict human behavior such as psychology\\ntypically has r-squared values lower than around 50% through which you can conclude that humans are simply harder\\nto predict the under physical process furthermore. If you are squared value is low, but you have statistically significant predictors,\\nthen you can still draw important conclusion about how changes in the predicator values associated.\\nOh sated with the changes in the response value regardless of the r-squared the significant coefficient still represent the mean change\\nin the response for one unit of change in the predicator while holding other predators in the model constant,\\nobviously this type of information can be extremely valuable. All right. All right.\\nSo this was all about the theoretical concept now, let's move on to the coding part and understand the code in depth.\\nSo for implementing linear regression using python, I will be using Anaconda with jupyter notebook installed on it.\\nSo I like there's a jupyter notebook and we are using python 3.01 it alright, so we are going to use a data set consisting\\nof head size and human brain of different people. All right. So let's import our data set percent matplotlib and line.\\nWe are importing numpy as NP pandas as speedy and matplotlib and from matplotlib.\\nWe are importing pipe out of that as PLT. Alright next we will import our data had brain dot CSV\\nand store it in the data variable. Let's execute the Run button and see the armor. But so this asterisk symbol it symbolizes\\nthat it still executing. So there's a output or dataset consists of two thirty seven rows\\nand four columns. We have columns as gender age range head size in centimeter Cube\\nand brain weights and Graham fine. So there's our sample data set that is how it looks it consists of all these data set.\\nSo now that we have imported our data, so as you can see they are 237 values in the training set\\nso we can find a linear. Relationship between the head size and the Brain weights. So now what we'll do we'll collect X & Y\\nthe X would consist of the head size values and the Y would consist of brain with values. So collecting X and Y. Let's execute the Run.\\nDone next what we'll do we need to find the values of b 1 or B not or you can say m and C.\\nSo we'll need the mean of X and Y values first of all what we'll do we'll calculate the mean of X and Y so mean x\\nequal NP dot Min X. So mean is a predefined function of Numb by similarly mean\\nunderscore y equal NP dot mean of Y, so what it will return if you'll return the mean values of Y\\nnext we'll check the total number of values. So m equals. Well length of X. Alright,\\nthen we'll use the formula to calculate the values of b 1 and B naught or fnc. All right, let's execute the Run button and see\\nwhat is the result. So as you can see here on the screen we have got b 1 as 0 point 2 6 3 +\\nB not as three twenty five point five seven. Alright, so now that we have a coefficient.\\nSo comparing it with the equation y equal MX plus C. You can say that brain weight equals\\nzero point 2 6 3 X Head size plus three twenty five point five seven so you can say\\nthat the value of M here is 0.26 3 and the value of C. Here is three twenty five point five seven.\\nAll right, so there's our linear model now, let's plot it and see graphically.\\nLet's execute it. So this is how our plot looks like this model is not so bad.\\nBut we need to find out how good our model is. So in order to find it the many methods\\nlike root means Square method the coefficient of determination or the a square method. So in this tutorial,\\nI have told you about our score method. So let's focus on that and see how good our model is.\\nSo let's calculate the R square value. All right here SS underscore T is the total sum of square SS.\\nOur is the total sum of square of residuals and R square as the formula is 1 minus total sum\\nof squares upon total sum of square of residuals. All right next when you execute it,\\nyou will get the value of R square as 0.63 which is pretty very good. Now that you have implemented simple linear regression model\\nusing least Square method, let's move on and see how will you implement the model using machine learning library\\ncalled scikit-learn. All right. So this scikit-learn is a simple machine. Young Library in Python welding machine learning model are\\nvery easy using scikit-learn. So suppose there's a python code.\\nSo using the scikit-learn libraries your code shortens to this length like so let's execute the Run button and see you\\nwill get the same our to score as Well, this was all for today's discussion.\\nMost of the entities in this world are related in one way or another at times finding relationship between entities\\ncan help you take valuable business decisions today. I'm going to talk about logistic regression,\\nwhich is one such approach towards predicting relationships. Now, let us see what all we are going to cover in today's training.\\nSo we'll start off the session by getting a quick introduction to what is regression. Then we'll see the different types of regression\\nand we'll be discussing the what and by of logistic regression. So in this part, we'll discuss what exactly it is.\\nIt is used why it is used and all those things moving ahead will compare linear regression\\nversus logistic regression along with the various real-life use cases and finally towards the end. I will be practically\\nimplementing logistic regression algorithm. So let's quickly start off with the very first topic\\nwhat is regression. The regression analysis is a predictive modeling technique. So it always involves predictions.\\nSo in this session, we'll just talk about predictive analysis and not prescriptive analysis. Now why because\\nif descriptive analysis you Need to have a good base and a stronghold on the predictive part first.\\nNow, it estimates relationship between the dependent variable and an independent variable. So for those of you\\nwho are not aware of these terminologies, let me give you a quick summary of it. So dependent variable is nothing but a variable\\nwhich you want to predict now, let's say I want to know what will be the sales on 26th of this month.\\nSo sales becomes a dependent variable or you can see the target variable. Now this dependent variable\\nor Target variable are going to depend on a lot of actors. The number of products you sold till date\\nor what is the season out there? Is there the availability of product or how is the product quality and all these things?\\nSo these are the NeverEnding factors which are nothing but the different features that leads to sail so these variables are called as an independent variable\\nor you can say the predictor now if you look at the graph over here, we have some values of X and we have values of Y now\\nas you can see over here if X increases the value of by also increases so let me explain you this\\nwith an example. Let's say we have until the value of x which is six point seven five and somebody asked you.\\nWhat was the value of y when the value of x is 7 so the way that you can do it or how regression comes into the picture is\\nby fitting a straight line by all these points and getting the value of M and C. So this is straight line guys\\nand the formula for the straight line is y is equal to MX plus C. So using this we can try to predict the value of y so here\\nif you notice the X variable can increase as much as it can but the Y variable will increase according to x\\nso Why is basically dependent on your X variable? So for any arbitrary value of x You can predict the value\\nof y and this is always done through regression. So that is how regression is useful. Now regression is basically classified into three types\\nyour linear regression, then your logistic regression and polynomial regression. So today we will be discussing logistic regression.\\nSo let's move forward and understand the what and by of logistic regression. Now this algorithm is most widely used\\nwhen the dependent variable or you can see the output is in the binary. A format. So here you need to predict the outcome\\nof a categorical dependent variable. So the outcome should be always discreet or categorical\\nin nature Now by discrete. I mean the value should be binary or you can say you just have two values it can either be 0\\nor 1 it can either be yes or a no either be true or false or high or low. So only these can be the outcomes so the value\\nwhich you need to create it should be discrete or you can say categorical in nature. Whereas in linear regression.\\nWe have the value of by or you can see Val you need to predict within a range that is how there's a difference between linear regression\\nand logistic regression. We must be having question. Why not linear regression now guys in linear regression the value of by or the value,\\nwhich you need to predict is in a range, but in our case as in the logistic regression, we just have two values it can be either 0\\nor it can be one. It should not entertain the values which is below zero or above one. But in linear regression,\\nwe have the value of y in the range so here in order to implement logic regression we need To clip this part\\nso we don't need the value that is below zero or we don't need the value which is above 1 so since the value of y will be between only 0 and 1\\nthat is the main rule of logistic regression. The linear line has to be clipped at 0 and 1 now.\\nOnce we clip this graph it would look somewhat like this. So here you're getting the curve which is nothing but three different straight lines.\\nSo here we need to make a new way to solve this problem. So this has to be formulated into equation.\\nAnd hence we come up with logistic regression. So here the outcome is either 0 Or one which is the main rule of logistic regression.\\nSo with this our resulting curve cannot be formulated. So hence our main aim to bring the values to 0\\nand 1 is fulfilled. So that is how we came up with large stick regression now here once it gets formulated into an equation.\\nIt looks somewhat like this. So guys, this is nothing but an S curve or you can say the sigmoid curve a sigmoid function curve.\\nSo this sigmoid function basically converts any value from minus infinity to Infinity to your discrete values,\\nwhich a Logitech regression wants or it Can say the values which are in binary format either 0 or 1.\\nSo if you see here the values as either 0 or 1 and this is nothing but just a transition of it,\\nbut guys there's a catch over here. So let's say I have a data point that is 0.8. Now, how can you decide\\nwhether your value is 0 or 1 now here you have the concept of threshold which basically divides your line.\\nSo here threshold value basically indicates the probability of either winning or losing so here by winning.\\nI mean the value is equal. One and by losing I mean the values equal to 0 but how does it do that?\\nLet's have a data point which is over here. Let's say my cursor is at 0.8. So here I check\\nwhether this value is less than the threshold value or not. Let's say if it is more than the threshold value.\\nIt should give me the result as 1 if it is less than that, then should give me the result is zero. So here my threshold value is 0.5.\\nI need to Define that if my value let's is 0.8. It is more than 0.5. Then the value shall be rounded of two one.\\nOne and let's say if it is less than 0.5. Let's I have a value 0.2 then should reduce it to zero.\\nSo here you can use the concept of threshold value to find output. So here it should be discreet.\\nIt should be either 0 or it should be one. So I hope you caught this curve of logistic regression.\\nSo guys, this is the sigmoid S curve. So to make this curve we need to make an equation.\\nSo let me address that part as well. So let's see how an equation is formed to imitate this functionality so over here,\\nwe have an equation of a straight. Line, which is y is equal to MX plus C. So in this case, I just have only one independent variable but let's say\\nif we have many independent variable then the equation becomes m 1 x 1 plus m 2 x 2 plus m 3 x 3 and so on till M NX n now,\\nlet us put in B and X. So here the equation becomes Y is equal to b 1 x 1 plus beta 2 x 2 plus b 3 x 3 and so on\\ntill be nxn plus C. So guys equation of the straight line has a range from minus infinity to Infinity.\\nYeah, but in our case or you can say largest equation the value which we need to predict or you can say\\nthe Y value it can have the range only from 0 to 1. So in that case we need to transform this equation.\\nSo to do that what we had done we have just divide this equation by 1 minus y so now Y is equal\\nto 0 so 0 over 1 minus 0 which is equal to 1 so 0 over 1 is again 0\\nand if we take Y is equals to 1 then 1 over 1 minus 1 which is 0 so 1 over 0 is infinity.\\nSo here are my range is now. Between 0 to Infinity, but again, we want the range from minus infinity to Infinity.\\nSo for that what we'll do we'll have the log of this equation. So let's go ahead and have the logarithmic of this equation.\\nSo here we have this transform it further to get the range between minus infinity to Infinity so over here we have log of Y\\nover 1 minus 1 and this is your final logistic regression equation. So guys, don't worry. You don't have to write this formula or memorize\\nthis formula in Python. You just need to call this function which is logistic regression and Everything will be automatically for you.\\nSo I don't want to scare you with the maths in the formulas behind it. But it is always good to know how this formula was generated.\\nSo I hope you guys are clear with how logistic regression comes into the picture next. Let us see what are the major differences\\nbetween linear regression was a logistic regression the first of all in linear regression, we have the value\\nof y as a continuous variable or the variable between need to predict are continuous in nature. Whereas in logistic regression.\\nWe have the categorical variable so here the value which you need to Should be discrete in nature. It should be either 0\\nor 1 or should have just two values to it. For example, whether it is raining or it is not raining\\nis it humid outside or it is not humid outside. Now, how's it going to snow and it's not going to snow.\\nSo these are the few example, we need to predict where the values are discrete or you can just predict\\nwhere this is happening or not. Next linear equation solves your regression problems. So here you have a concept of independent variable\\nand a dependent variable. So here you can calculate the value of y which you need to Plate it. Using the value of x.\\nSo here your y variable or you can see the value that you need to predict are in a range. But whereas in logistic regression,\\nyou have discrete values. So logistic regression basically solves a classification problem so it can basically classify it and it can just give you result\\nwhether this event is happening or not. So I hope it is pretty much Clear till now next in linear regression.\\nThe graph that you have seen is a straight line graph so over here, you can calculate the value of y\\nwith respect to the value of x where as in logistic regression. Glad that we got was a Escobar.\\nYou can see the sigmoid curve. So using the sigmoid function You can predict your y values.\\nSo I hope you guys are clear with the differences between the linear regression and logistic regression moving the a little see\\nthe various use cases where in logistic regression is implemented in real life. So the very first is weather prediction now\\nlargest aggression helps you to predict your weather. For example, it is used to predict whether it is raining or not whether it is sunny.\\nIs it cloudy or not? So all these things things can be predicted using logistic regression. Where as you need to keep in mind\\nthat both linear regression and logistic regression can be used in predicting the weather. So in that case linear regression helps you to predict\\nwhat will be the temperature tomorrow whereas logistic regression will only tell you which is going to rain or not or whether it's cloudy or not,\\nwhich is going to snow or not. So these values are discrete. Whereas if you apply linear regression, you will predicting things like what is the temperature tomorrow\\nor what is the temperature day after tomorrow and all those thing? So these are the slight? Is between linear regression\\nand logistic regression the moving ahead. We have classification problem. So python performs multi-class classification,\\nso here it can help you tell whether it's a bird. It's not a board. Then you classify different kind of mammals.\\nLet's say whether it's a dog or it's not a dog similarly, you can check it for reptile whether it's a reptile or not a reptile.\\nSo in logistic regression, it can perform multi-class classification. So this point I've already discussed\\nthat it is using classification problems next. It also helps you to determine the illnesses. Where so let me take an example.\\nLet's say a patient goes for a routine check up in hospital. So what doctor will do it, it will perform various tests on the patient and we'll check\\nwhether the patient is actually a law or not. So what will be the features so doctor can check the sugar level\\nthe blood pressure then what is the age of the patient? Is it very small or is it the old person then?\\nWhat is the previous medical history of the patient and all of these features will be recorded by the doctor\\nand finally, dr. Checks the patient data and Data - the outcome of Illness and the severity of illness.\\nSo using all the data of a doctor can identify whether a patient is ill or not. So these are the various use cases\\nin which you can use logistic regression now, I guess enough of theory part. So let's move ahead and see some of the Practical implementation\\nof logistic regression so over here, I be implementing two projects when I have the data set\\nof a Titanic so over here will predict what factors made people more likely to survive the sinking\\nof the Titanic ship anime. Second project will see the data analysis. On the SUV cars so over here.\\nWe have the data of the SUV cars who can purchase it and what factors made people more interested in buying SUV.\\nSo these will be the major questions as to why you should Implement logistic regression and what output will you get by it?\\nSo let's start by the very first project that is Titanic data analysis. So some of you might know\\nthat there was a ship called as Titanic with basically hit an iceberg and sank to the bottom of the ocean and it was a big disaster at that time\\nbecause it was the first voyage of the ship. It was supposed to be really really strongly built and one\\nof the best ships of that time. So it was a big disaster of that time. And of course there is a movie about this as well.\\nSo many of you might have washed it. So what we have we have data of the passengers those who survived and those\\nwho did not survive in this particular tragedy. So what you have to do you have to look at this data and analyze which factors would have been contributed\\nthe most to the chances of a person survival on the ship or not. So using the logistic regression, we can predict\\nwhether the person survived or the person died. Now apart from this we also have a look with the various features along with that.\\nSo first it is explore the data set so over here, we have the index value then the First Column\\nis passenger ID, then my next column is survived so over here, we have two values a 0 and a 1 so 0 stands\\nfor did not survive and one stands for survive. So this column is categorical where the values are discrete next.\\nWe have passenger class so over here, we have three values 1 2 and 3. So this basically tells you that whether a I think\\na stabbing in the first class second class or third class. Then we have the name of the passenger. We have the six or you can see the gender of the passenger\\nwhere the passenger is a male or female. Then we have the age we have the Sip SP. So this basically means the number of siblings\\nor the spouses aboard the Titanic so over here, we have values such as 1 0 and so on then we have\\nParts apart is basically the number of parents or children aboard the Titanic so over here,\\nwe also have some values then we I have the ticket number. We have the fear. We have the cabin number and we have the embarked column.\\nSo in my inbox column, we have three values we have SC and Q. So s basically stands\\nfor Southampton C stands for Cherbourg and Q stands for Queenstown. So these are the features\\nthat will be applying our model on so here we'll perform various steps and then we'll be implementing logistic regression.\\nSo now these are the various steps which are required to implement any algorithm. So now in our case we are implementing\\nlogistic regression, so, Very first step is to collect your data or to import the libraries that are used for collecting your data\\nand then taking it forward then my second step is to analyze your data so over here, I can go to the various fields and then I can analyze the data.\\nI can check did the females or children survive better than the males or did the rich passenger survived more\\nthan the poor passenger or did the money matter as in who paid more to get into the shape\\nwith the evacuated first? And what about the workers does the worker survived or what is the survival rate?\\nIf you were the worker in the ship and not just a traveling passenger, so all of these are very very interesting questions\\nand you would be going through all of them one by one. So in this stage, you need to analyze our data\\nand explore your data as much as you can then the third step is to Wrangle your data now data wrangling basically means cleaning your data so over here,\\nyou can simply remove the unnecessary items or if you have a null values in the data set. You can just clear that data and then you can take it forward.\\nSo in this step you can build your model using the train data. And then you can test it using a test so over here you will be performing a split\\nwhich basically split your data set into training and testing data set and find you will check the accuracy.\\nSo as to ensure how much accurate your values are. So I hope you guys got these five steps that you're going to implement in autistic regression.\\nSo now let's go into all these steps in detail. So number one. We have to collect your data or you can say import the libraries.\\nSo it may show you the implementation part as well. So I just open my jupyter notebook and I just Implement all of these steps.\\nIt's side-by-side. So guys this is my jupyter notebook first. Let me just rename jupyter notebook to let's say\\nTitanic data analysis. Now our first step was to import all the libraries\\nand collect the data. So let me just import all the libraries first. So first of all, I'll import pandas.\\nSo pandas is used for data analysis. So I'll say input pandas as PD then I will be importing numpy.\\nSo I'll say import numpy as NP so numpy is a library in Python which basically stands for numerical Python\\nand it is widely used to perform any scientific computation. Next. We will be importing Seaborn.\\nSo c 1 is a library for statistical brought think so. Say import Seaborn as SNS.\\nI'll also import matplotlib. So matplotlib library is again for plotting.\\nSo I'll say import matplotlib dot Pi plot as PLT now to run this library in jupyter Notebook all I have\\nto write in his percentage matplotlib in line. Next I will be importing one module as well.\\nSo as to calculate the basic mathematical functions, so I'll say import mats. So these are the libraries\\nthat I will be needing in this Titanic data analysis. So now let me just import my data set. So I will take a variable.\\nLet's say Titanic data and using the pandas. I will just read my CSV or you can see the data set.\\nI like the name of my data set that is Titanic dot CSV. Now. I have already showed you the data set so over here.\\nLet me just print the top 10 rows. So for that I will just say I take the variable Titanic data dot head\\nand I'll say the top ten rules. So now I'll just run this so to run these fellows have to press shift + enter\\nor else you can just directly click on this cell so over here. I have the index. We have the passenger ID, which is nothing.\\nBut again the index which is starting from 1 then we have the survived column which has a category. Call values or you can say the discrete values,\\nwhich is in the form of 0 or 1. Then we have the passenger class. We have the name of the passenger 6 8\\nand so on so this is the data set that I will be going forward with next let us bring the number of passengers\\nwhich are there in this original data set for that. I'll just simply type in print. I'll say a number of passengers.\\nAnd using the length function, I can calculate the total length. So I'll say length and inside this I will be passing this variable because Titanic data,\\nso I'll just copy it from here. I'll just paste it dot index and next set me just bring this one.\\nSo here the number of passengers which are there in the original data set we have is 891\\nso around this number were traveling in the Titanic ship so over here, my first step is done\\nwhere you have just collected data imported all the libraries and find out the total number of passengers,\\nwhich are Titanic so now let me just go back to presentation and let's see. What is my next step. So we're done with the collecting data.\\nNext step is to analyze your data so over here, we will be creating different plots to check the relationship\\nbetween variables as in how one variable is affecting the other so you can simply explore your data set by making use\\nof various columns and then you can plot a graph between them. So you can either plot a correlation graph.\\nYou can plot a distribution curve. It's up to you guys. So let me just go back to my jupyter notebook and let me analyze some of the data.\\nOver here. My second part is to analyze data. So I just put this in headed to now to put this in here to I just have to go\\nand code click on mark down and I just run this so first let us plot account plot where you can pay between the passengers\\nwho survived and who did not survive. So for that I will be using the Seabourn Library so over here I have imported Seaborn as SNS\\nso I don't have to write the whole name. I'll simply say SNS dot count plot.\\nI say axis with the survive and the data that I'll be using is the Titanic data or you can say the name\\nof variable in which you have store your data set. So now let me just run this so who were here as you can see I have survived column on my x\\naxis and on the y axis. I have the count. So 0 basically stands for did not survive and one stands for the passengers\\nwho did survive so over here, you can see that around 550 of the passengers who did not survive and they were around 350 passengers\\nwho only survive so here you can basically compute. There are very less survivors than on survivors.\\nSo this was the very first floor now that is not another plot to compare the sex as to whether\\nout of all the passengers who survived and who did not survive. How many were men and how many were female\\nso to do that? I'll simply say SNS dot count plot.\\nI add the Hue as six so I want to know how many females and how many male survive\\nthen I'll be specifying the data. So I'm using Titanic data set and let me just run this you have done a mistake\\nover here so over here you can see I have survived column on the x-axis and I have the count on the why now.\\nSo here your view color stands for your male passengers and orange stands for your female. So as you can see here the passengers\\nwho did not survive that has a value 0 so we can see that. Majority of males did not survive and if we see the people\\nwho survived here, we can see the majority of female survive. So this basically concludes the gender of the survival rate.\\nSo it appears on average women were more than three times more likely to survive than men next.\\nLet us plot another plot where we have the Hue as the passenger class so over here we can see which class at the passenger was traveling in\\nwhether it was traveling in class one two, or three so for that I just tried the same command.\\nI'll say SNS dot count plot. I keep my x-axis as\\nsubtly I'll change my you to passenger class. So my variable named as PE class.\\nAnd the data said that I'll be using is Titanic data. So this is my result so over here you can see I have blue for first-class orange\\nfor second class and green for the third class. So here the passengers who did not survive a majorly of the third class\\nor you can say the lowest class or the cheapest class to get into the dynamic and the people who did survive majorly belong to the higher classes.\\nSo here 1 & 2 has more eyes than the passenger who were traveling in the third class. So here we have concluded that the passengers\\nwho did not survive a majorly of third class. Us all you can see the lowest class and the passengers who were traveling in first and second class\\nwould tend to survive more next. I just got a graph for the age distribution over here. I can simply use my data.\\nSo we'll be using pandas library for this. I will declare an array and I'll pass in the column. That is age.\\nSo I plot and I want a histogram so I'll say plot da test.\\nSo you can notice over here that we have more of young passengers, or you can see the children between the ages 0 to 10\\nand then we have the average people and if you go ahead Lester would be the population.\\nSo this is the analysis on the age column. So we saw that we have more young passengers and more mediocre eight passengers,\\nwhich are traveling in the Titanic. So next let me plot a graph of fare as well. So I'll say Titanic data.\\nI say fair. And again, I got a histogram so I'll say haste.\\nSo here you can see the fair size is between zero to hundred now. Let me add the bin size.\\nSo as to make it more clear over here, I'll say Ben is equals to let's say 20 and I'll increase the figure size as well.\\nSo I'll say fixed size. Let's say I'll give the dimensions as 10 by 5.\\nSo it is bins. So this is more clear now next. It is analyzed the other columns as well.\\nSo I'll just type in Titanic data and I want the information as to what all columns are left.\\nSo here we have passenger ID, which I guess it's of no use then you have see how many passengers survived\\nand how many did not we also see the analysis on the gender basis. We saw when the female tend to survive more\\nor the maintain to survive more then we saw the passenger class where the passenger is traveling in the first class second class\\nor third class. Then we have the name. So in name, we cannot do any analysis. We saw the sex we saw the age as well.\\nThen we have sea bass P. So this stands for the number of siblings or the spouses which Are aboard the Titanic so let us do this as well.\\nSo I'll say SNS dot count plot. I mentioned X SC SP.\\nAnd I will be using the Titanic data so you can see the plot over here so over here you\\ncan conclude that. It has the maximum value on zero so you can conclude that neither children nor a spouse was\\non board the Titanic now second most highest value is 1 and then we have various values for 2 3 4 and so on next\\nif I go above the store this column as well. Similarly can do four parts. So next we have part\\nso you can see the number of parents or children which were aboard the Titanic so similarly can do. As well then we have the ticket number.\\nSo I don't think so. Any analysis is required for Ticket. Then we have fears of a we have already discussed as\\nin the people would tend to travel in the first class. You will be the highest view then we have the cable number\\nand we have embarked. So these are the columns that will be doing data wrangling on so we have analyzed the data\\nand we have seen quite a few graphs in which we can conclude which variable is better than another\\nor what is the relationship the whole third step is my data wrangling so data wrangling basically\\nmeans Cleaning your data. So if you have a large data set, you might be having some null values\\nor you can say Nan values. So it's very important that you remove all the unnecessary items that are present in your data set.\\nSo removing this directly affects your accuracy. So I'll just go ahead and clean my data by removing all the n n values and unnecessary columns,\\nwhich has a null value in the data set the next time you're performing data wrangling.\\nSupposed to fall I check whether my data set is null or not. So I'll say Titanic data, which is the name of my data set and I'll say is null.\\nSo this will basically tell me what all values are null and will return me a Boolean result. So this basically checks the missing data\\nand your result will be in Boolean format as in the result will be true or false so Falls mean if it is not null and prove means\\nif it is null, so let me just run this. Over here you can see the values as false or true.\\nSo Falls is where the value is not null and Drew is where the value is none. So over here you can see in the cabin column.\\nWe have the very first value which is null so we have to do something on this so you can see\\nthat we have a large data set. So the counting does not stop and we can actually see the some of it.\\nWe can actually print the number of passengers who have the Nan value in each column. So I'll say Titanic underscore data is null\\nand I want the sum of it all. Same thought some so this is basically print the number of passengers\\nwho have the n n values in each column so we can see that we have missing values in each column that is 177.\\nThen we have the maximum value in the cave in column and we have very Less in the Embark column.\\nThat is 2 so here if you don't want to see this numbers, you can also plot a heat map and then you can visually analyze it let me just do\\nthat as well. So I'll say SNSD heat map.\\nAnd save I take labels. False Choice run this as we have already seen\\nthat there were three columns in which missing data value was present. So this might be age so over here almost 20%\\nof each column has a missing value. Then we have the cabling columns. So this is quite a large value\\nand then we have two values for embark column as well. Add a see map for color coding.\\nSo I'll say see map. So if I do this\\nso the graph becomes more attractive so over here yellow stands for Drew or you can say the values are null.\\nSo here we have computed that we have the missing value of H. We have a lot of missing values in the cabin column\\nand we have very less value, which is not even visible in the Embark column as well. So to remove these missing values,\\nyou can either replace the values and you can put in some dummy values to it or you can simply drop the column.\\nSo here let us suppose pick the age column. So first, let me just plot a box plot and they will analyze with having a column as H.\\nSo I'll say SNS dot box plot. I'll say x is equals to passenger class.\\nSo it's p class. I'll say Y is equal to H and the data set that I'll be using is Titanic side.\\nSo I'll say three times goes to Titanic data. You can see the edge in first class and second class tends to be more older rather\\nthan we have it in the third class. Well that depends on The Experience how much you earn or might be there any number of reasons so here we concluded\\nthat passengers who were traveling in class one and class two a tend to be older than what we have in the class 3\\nso we have found that we have some missing values in EM. Now one way is to either just drop the column\\nor you can just simply fill in some values to them. So this method is called as imputation now\\nto perform data wrangling or cleaning it is for spring the head of the data set. So I'll say tightening knot head.\\nSo it's Titanic. Data, let's say I just want the five rows. So here we have survived which is again categorical.\\nSo in this particular column, I can apply logic to progression. So this can be my y value or the value\\nthat you need to predict. Then we have the passenger class. We have the name. Then we have ticket number.\\nWe're taping so over here. We have seen that in keeping. We have a lot of null values or you can say that any invalid\\nwhich is quite visible as well. So first of all, we'll just drop this column for dropping it. I'll just say Titanic underscore data.\\nAnd I'll simply type in drop and the column which I need to draw so I have to drop the cable column.\\nI mention the access equals to 1 and I'll say in place also to true.\\nSo now again, I just print the head and let us see whether this column has been removed from the data set or not.\\nSo I'll say Titanic dot head. So as you can see here, we don't have given column anymore.\\nNow, you can also drop the na values. So I'll say Titanic data dot drop\\nall the any values or you can say Nan which is not a number and I will say in place is equal to True its Titanic.\\nSo over here, let me again plot the heat map and let's say for the values we should before showing a lot of null values.\\nHas it been removed or not. So I'll say SNS dot heat map. I'll pass in the data set.\\nI'll check it is null. I'll say why tick labels is equal to false.\\nAnd I don't want color coding. So again I say false. So this will basically help me to check\\nwhether my values has been removed from the data set or not. So as you can see here, I don't have any null values.\\nSo it's entirely black now. You can actually know the some as well. So I'll just go above So I'll just copy this part\\nand I just use the sum function to calculate the sum. So here the tells me that data set is clean as\\nin the data set does not contain any null value or any Nan value. So now we have R Angela data.\\nYou can see cleaner data. So here we have done just one step in data wrangling that is just removing one column out of it.\\nNow you can do a lot of things you can actually fill in the values with some other values or you can just calculate the mean\\nand then you can just fit in the null values. But now if I see my data set, so I'll say Titanic data dot head.\\nBut now if I see you over here I have a lot of string values. So this has to be converted to a categorical variables\\nin order to implement logistic regression. So what we will do we will convert this to categorical variable\\ninto some dummy variables and this can be done using pandas because logistic regression just take two values.\\nSo whenever you apply machine learning you need to make sure that there are no string values present because it won't be taking these as your input variables.\\nSo using string you don't have to predict anything but in my case I have the survived columns 2210 how many?\\nPeople tend to survive and how many did not so CEO stands for did not survive and one stands for survive.\\nSo now let me just convert these variables into dummy variables. So I'll just use pandas and a say PD not get dummies.\\nYou can simply press tab to autocomplete and say Titanic data and I'll pass the six\\nso you can just simply click on shift + tab to get more information on this. So here we have the type data frame\\nand we have the passenger ID survived and passenger class. So if Run this you'll see that 0 basically stands for not a female and one stand\\nfor it is a female similarly for male 0 Stanford's not made and one Stanford may now we don't require both these columns\\nbecause one column itself is enough to tell us whether it's male or you can say female or not.\\nSo let's say if I want to keep only male I'll say if the value of mail is 1 so it is definitely a maid and is not a female.\\nSo that is how you don't need both of these values. So for that I just remove the First Column,\\nlet's say a female so I'll say drop first. Andrew it has given me just one column\\nwhich is male and has a value 0 and 1. Let me just set this as a variable hsx so\\nover here I can say sex dot head and just want to see the first five rows.\\nSorry, it's dot. So this is how my data looks like now here. We have done it for sex.\\nThen we have the numerical values in age. We have the numerical values in spouses. Then we have the ticket number.\\nWe have the pair and we have embarked as well. So in Embark the values are in. C and Q so here also we can apply this get dummy function.\\nSo let's say I will take a variable. Let's say embark. I'll use the pandas Library.\\nI'll enter the column name that is embarked.\\nLet me just print the head of it. So I'll say Embark dot head so over here. We have c q and s now here also we can drop the First Column\\nbecause these two values are enough with the passenger is either traveling for Q. That is Q in stone S4 sound time\\nand if both the values are 0 then definitely the passenger is from Cherbourg. That is the third value\\nso you can again drop the first value. So I'll say drop and true.\\nLet me just run this. So this is how my output looks like now similarly you can do it for The class as well.\\nSo here also we have three classes one two, and three so I'll just copy the whole statement.\\nSo let's say I want the variable name. Let's say PCL. I'll pass in the column name\\nthat is PE class and I'll just drop the First Column. So here also the values will be 1 2 or 3\\nand I'll just remove the First Column. So here we just left with two and three so if both the values are 0 then definitely\\nthe passengers travelling in the first class now, we have made the values as categorical now,\\nmy next step would be to concatenate all these new rules into a data set. We can see Titanic data using the pandas will just concatenate\\nall these columns. So I'll Superior. One cat and then say if we have to concatenate sex,\\nwe have to concatenate embarked and PCL and then I will mention the access to one.\\nI'll just run this can you to print the head so over here you can see that these columns have been added over here.\\nSo we have the mail column with basically tells where the person is male or it's a female then we have the Embark\\nwhich is basically q and s so if it's traveling from Queenstown value would be one else it would be 0 and If both of these values are zeroed,\\nit is definitely traveling from Cherbourg. Then we have the passenger class as 2 and 3.\\nSo the value of both these is 0 then passengers travelling in class one. So I hope you got this\\ntill now now these are the irrelevant columns that we have it over here so we can just drop these columns will drop\\nin PE class the embarked column and the sex column. So I'll just type\\nin Titanic data dot drop and mention the columns that I want to drop. So I say I even read the passenger ID\\nbecause it's nothing but just the index value which is starting from one. So I'll drop this as well then I don't want name as well.\\nSo I'll delete name as well. Then what else we can drop we can drop the ticket as well.\\nAnd then I'll just mention the axis. I'll say in place is equal to True.\\nOkay. So now my column name starts uppercase. So these has been dropped now,\\nlet me just bring my data set again. So this is my final leader said guys, we have the survived column which has the value 0\\nand 1 then we have the passenger class or we forgot to drop this as well. So no worries.\\nI'll drop this again.\\nSo now let me just run this. So over here we have the survive. We have the age.\\nWe have the same SP. We have the part. We have Fair mail and these we have just converted.\\nSo here we have just performed data angle. You can see clean the data and then we have just converted the values of gender\\nto male then embarked to q and s and the passenger Class 2 2 & 3. So this was all about my data wrangling\\nor just cleaning the data then my next up is training and testing your data. So here we will split the data set into train subset\\nand test steps. And then what we'll do we'll build a model on the train data and then predict the output on your test data set.\\nSo let me just go back to Jupiter and it is implement this as well over here. I need to train my data set.\\nSo I just put this indeed heading 3. So over here, you need to Define your dependent variable\\nand independent variable. So here my Y is the output for you can say the value that you need to predict so over here,\\nI will write Titanic data. I'll take the column which is survive. So basically I have to predict this column\\nwhether the passenger survived or not. And as you can see we have the discrete outcome, which is in the form of 0 and 1 and rest all the things we\\ncan take it as a features or you can say independent variable. So I'll say Titanic data.\\nNot drop so we just simply drop the survive and all the other columns will be my independent variable.\\nSo everything else as a features which leads to the survival rate. So once we have defined the independent variable\\nand the dependent variable next step is to split your data into training and testing subset. So for that we will be using SK loan.\\nI just type in from sklearn dot cross validation. import train test plate Now here\\nif you just click on shift and tab, you can go to the documentation and you can just see the examples over here.\\nI second class to open it and then I just go to examples and see how you can split your data.\\nSo over here you have extra next test wide range why test and then using this train test platelet\\nand just passing your independent variable and dependent variable and just Define a size and a random straight to it.\\nSo, let me just copy this and I'll just paste over here. Over here we will train test\\nthen we have the dependent variable train and test and using the split function will pass in the independent\\nand dependent variable and then we'll set a split size. So let's say I'll put it up 0.3.\\nSo this basically means that your data set is divided in 0.3 that is in 70/30 ratio, and then I can add any random straight to it.\\nSo let's say I'm applying one this is not necessary. If you want the same result as that of mine,\\nyou can add the random shape. So this will basically take exactly the same sample every Next I have to train and predict by creating a model.\\nSo here logistic regression will graph from the linear regression. So next I'll just type in from SK loan dot linear model import logistic regression.\\nNext I'll just create the instance of this logistic regression model. So I'll say log model is equals to largest aggression now.\\nI just need to fit my model. So I'll say log model dot fit and I'll just pass in my ex train.\\nand white rain Alright, so here it gives me all the details\\nof logistic regression. So here it gives me the class way dual fit intercept and all those things then what I need to do,\\nI need to make prediction. So I'll take a variable and checked addictions and I'll pass on the model to it.\\nSo I'll say log model dot predict and I'll pass in the value that is X test.\\nSo here we have just created a model fit that model and then we had made predictions. So now to evaluate how my model has been performing.\\nSo you can simply calculate the accuracy or you can also calculate a classification report.\\nSo don't worry guys. I'll be showing both of these methods. So I'll say from sklearn dot matrix input classification report.\\nIt's all here are used as fiction report. And inside this I'll be passing in white test\\nand the predictions. So guys this is my classification report.\\nSo over here, I have the Precision. I have the recall. We have the advanced code and then we have support.\\nSo here we have the value of decision as 75 72 and 73 which is not that bad now\\nin order to calculate the accuracy as well. You can also use the concept of confusion Matrix.\\nSo if you want to print the confusion Matrix, I will simply say from sklearn dot matrix import confusion Matrix first of all,\\nand then we just print this So how my function has been imported successfully so I'll say confusion Matrix.\\nAnd again passing the same variables which is why test and predictions. So I hope you guys already know the concept of confusion Matrix.\\nSo I just tell you in a brief what confusion Matrix is all about? So confusion Matrix is nothing but a 2 by 2 Matrix\\nwhich has a four outcomes. This basically tells us that how accurate your values are. So here we have the column as predicted.\\nNo predicted. Why? And we have actual no and then actually yes.\\nSo this is the concept of confusion Matrix. So here let me just fade in these values which we have just calculated.\\nSo here we have 105. 105 2125 and 63 So as you can see here,\\nwe have got four outcomes now 105 is the value where a model has predicted.\\nNo, and in reality. It was also a no so where we have predicted know an actual know similarly.\\nWe have 63 as a predicted. Yes. So here the model predicted. Yes, and actually also it was a yes.\\nSo in order to calculate the accuracy, you just need to add the sum of these two values and divide the whole by the some.\\nSo here these two values tells me where the order has actually predicted the correct output.\\nThis value is also called as true- This is called as false positive. This is called as true positive\\nand this is called a false negative. Now in order to calculate the accuracy. You don't have to do it manually.\\nSo in Python, you can just import accuracy score function and you can get the results from that.\\nSo I'll just do that as well. So I'll say from sklearn dot-matrix import accuracy score\\nand I'll simply print the accuracy and we'll pass in the same variables. That is why it is and predictions so over.\\nHere, it tells me the address. He has 78 which is quite good so over here if you want to do it\\nmanually, we have 2 plus these two numbers, which is 105 263. So this comes out to almost 168 and then you have to divide\\nby the sum of all the phone numbers. So 105 plus 63 plus 21 plus 25,\\nso this gives me a result of to 1/4. So now if you divide these two number, you'll get the same accuracy\\nthat is 78 percent or you can say point seven eight. So that is how you can calculate the See,\\nso now let me just go back to my presentation. I let's see what all we have covered till now. So here we have first plate our data into train\\nand test subset then we have build a model on the train data and then predicted the output on the test data set\\nand then my fifth step is to check the accuracy. So here we have calculator accuracy to almost 78 percent\\nwhich is quite good. You cannot say that accuracy is bad. So here it tells me how accurate your results are so him accuracy score defines\\nthat and hence got a good accuracy. So now moving ahead. Let us see the second project that is SUV data analysis.\\nSo in this a car company has released new SUV in the market and using the previous data about the sales of their SUV.\\nThey want to predict the category of people who might be interested in buying this. So using the logistic regression,\\nyou need to find what factors made people more interested in buying this SUV. So for this let us hear data set where I have user ID.\\nI have gender as male and female then we have the age we have the estimated. Melody and then we have the purchased column.\\nSo this is my discreet column or you can see the categorical column. So here we just have the value that is 0 and 1 and this column we need to predict\\nwhether a person can actually purchase a SUV or Not. So based on these factors, we will be deciding\\nwhether a person can actually purchase a SUV or not. So we know the salary of a person we know the age\\nand using these we can predict whether person can actually purchase SUV or not. So, let me just go to my jupyter notebook\\nand it is Implement a logistic regression. So guys, I I will not be going through all the details of data cleaning and analyzing the part start part.\\nI'll just leave it on you. So just go ahead and practice as much as you can. Alright, so the second project is SUV predictions.\\nSo first of all, I have to import all the libraries so I say import numpy as NP and similarly.\\nI'll do the rest of it.\\nAlright, so now let me just print the head of this data set. So this we have already seen that we have columns as user ID.\\nWe have gender. We have the H we have the salary and then we have to calculate whether person can actually purchase a SUV or not.\\nSo now let us just simply go on to the algorithm part. So we'll directly start off with the logistic regression\\non how you can train a model. So for doing all those things, we first need to Define your independent variable\\nand dependent variable. So in this case, I want my ex at is an independent variable is a data set.\\nI lock so here I will be specifying all the School and basically stands for that and in the columns,\\nI want only two and three dot values. So here we should fetch me all the rows\\nand only the second and third column which is age and estimated salary. So these are the factors which will be used to predict the dependent variable\\nthat is purchase. So here my dependent variable is purchase and independent variable is of age and salary\\nso I'll say Lena said dot I love I'll have all the rows and add just one fourth column.\\nThat is my purchased column. You don't values. All right, so I just forgot when one square bracket over here.\\nAlright, so over here. I have defined my independent variable and dependent variable. So here my independent variable is age and salary\\nand dependent variable is the column purchase. Now, you must be wondering what is this? I lock function.\\nSo I look function is basically an index of a panda's data frame and it is used for integer based indexing\\nor you can also say selection by index now, let me just bring these independent variables\\nand dependent variable. If I bring the independent variable I have age as well as a salary next.\\nLet me print the dependent variable as well. So over here you can see I just have the values in 0\\nand 1 so 0 stands for did not purchase next. Let me just divide my data set into training and test subset.\\nSo I'll simply write in from sklearn dot cross plate not cross-validation.\\nImport drain test next I'll just press shift + Tab and over here.\\nI'll go to the examples and just copy the same line. So I'll just copy this.\\nAs move the points now, I want to text size to be let's see 25, so I have divided the train in tested in 75/25 ratio.\\nNow, let's say I'll take the random set of 0 So Random State basically ensures the same result\\nor you can say the same samples taken whenever you run the code. So let me just run this now.\\nYou can also scale your input values for better performing and this can be done using standard scalar.\\nSo let me do that as well. So I'll say from sklearn Dot pre-processing.\\nImport standard scale now. Why do we scale it now? If you see a data set we are dealing with large numbers.\\nWell, although we are using a very small data set. So whenever you're working in a prod environment, you'll be working with large data set we\\nwill be using thousands and hundred thousands of you pulls so they're scaling down will definitely affect the performance by a large extent.\\nSo here let me just show you how we can scale down these input values and then the pre-processing contains all your methods & functionality,\\nwhich is Required to transform your data. So now let us scale down for test as well as a training data set.\\nSo else First Make an instance of it. So I'll say standard scalar. Then I have Extreme sasc Dot fit fit underscore transform.\\nI'll pass in my Xtreme video.\\nAnd similarly I can do it for test wherein I'll pass the X test. All right.\\nNow my next step is to import logistic regression. So I'll simply apply logistic regression\\nby first importing it. So I'll say from sklearn sklearn the linear model import logistic regression over here.\\nI'll be using classifier. So I said classifier dot is equals to logistically aggression so over here,\\nI just make an instance of it. So I'll say logistic regression and over here. I just pass in the random state,\\nwhich is 0 No, I simply fit the model.\\nAnd I simply passing next rain and white rain. So here it tells me all the details\\nof logistic regression. Then I have to predict the value. So I'll say why I prayed it's equal to classifier.\\nThen predict function and then I just pass in X test. So now we have created the model.\\nWe have scaled down our input values. Then we have applied logistic regression. We have predicted the values\\nand now we want to know the accuracy. So now the accuracy first we need to import accuracy scores.\\nSo I'll say from sklearn dot matrix input accuracy school\\nand using this function we can calculate the accuracy or you can manually do that by creating a confusion Matrix.\\nSo I'll just pass. my lightest and my y predicted All right, so over here I get the accuracy\\nas 89% So we want to know the accuracy in percentage. So I just have to multiply it by a hundred and if I run this\\nso it gives me 89% So I hope you guys are clear with whatever I have taught you today. So here I have taken my independent variable as age\\nand salary and then we have calculated that how many people can purchase SUV and then we have calculated our model by checking\\nthe accuracy so over here we get the accuracies 89 which is great. Alright guys that is it for today.\\nSo I'll Scoffs what all we have covered in today's training. First of all, we had a quick introduction to what is regression\\nand where the regression is actually use then we have understood the types of regression and then got into the details\\nof what and why of logistic regression of compared linear was in logistic regression. We have also seen the various use cases\\nwhere you can Implement logistic regression in real life and then we have picked up two projects that is Titanic data analysis\\nand SUV prediction so over here we have seen how we can collect your data analyze your data then perform.\\nModeling on that data train the data test the data and then finally have calculated the accuracy.\\nSo in your SUV prediction, you can actually analyze clean your data and you can do a lot of things\\nso you can just go ahead pick up any data set and explore it as much as you can open your eyes and see\\naround you will find dozens of applications of machine learning which you are using and interacting with in your daily life peed\\nbe using the phase detection. And Facebook are getting the recommendation for similar products from Amazon machine learning\\nis applied almost everywhere. So hello and welcome all to this YouTube session will learn about how to build a decision tree.\\nThis session is designed in a way that you get most out of it. Alright. So this decision tree is a type of classification algorithm\\nwhich comes under these supervised learning technique. So before learning about decision tree, I'll give you a short introduction to classification\\nwhere we'll learn about. What is classification what I'd say, Various types where it is used or what I'd see use cases now,\\nonce you get your fundamental clear will jump to the decision tree part under this. First of all, I will teach you to mathematically\\ncreate a decision tree from scratch then once you get your Concepts clear, we'll see how you can write a decision tree classifier\\nfrom scratch in Python using the card algorithm. All right. I hope the agenda is scared you guys what is classification?\\nI hope every one of you must have used Gmail. So how do you think the male is getting classified as Spam\\nor not spam mail. Well, there's nothing but classification So What It Is Well classification is the process\\nof dividing the data set into different categories or groups by adding label. In other way, you can say\\nthat it is a technique of categorizing the observation into different category. So basically what you are doing is you are taking\\nthe data analyzing it and on the basis of some condition you finely divided into various categories.\\nNow, why do we classify it? Well, we classify it to perform predictive analysis on it. Like when you get the mail\\nthe machine predicts it to be a Spam or not spam mail and on the basis of that prediction it add the irrelevant or spam mail\\nto the respective folder in general this classification. Algorithm handle questions. Like is this data belongs to a category or B category?\\nLike is this a male or is this a female something like that now the question arises where will you use it?\\nWell, you can use this of protection order to check whether the transaction is genuine or not suppose I am using.\\nA credit card here in India now due to some reason I had to fly to Dubai now. If I'm using the credit card over there,\\nI will get a notification alert regarding my transaction. They would ask me to confirm about the transaction.\\nSo this is also kind of predictive analysis as the machine predicts that something fishy is in the transaction as very for our ago.\\nI made the transaction using the same credit card and India and 24 hour later. The same credit card is being used for the payment in Dubai.\\nSo the Machine predicts that something fishy is going on in the transaction. So in order to confirm it it sends you a notification alert.\\nAll right. Well, this is one of the use case of classification you can even use it to classify different items\\nlike fruits on the base of its taste color size overweight a machine. Well trained using the classification algorithm\\ncan easily predict the class or the type of fruit whenever new data is given to it. Not just the fruit.\\nIt can be any item. It can be a car. It can be a house. It can be a I'm bored or anything.\\nHave you noticed that while you visit some sites or you try to login into some you get a picture capture for that right\\nwhere you have to identify whether the given image is of a car or its of a pole or not? You have to select it for example that 10 images\\nand you're selecting three Mages out of it. So in a way you are training the machine right you are telling\\nthat these three are the picture of a car and rest are not so who knows you are training at for something big right?\\nSo moving on ahead. Let's discuss the types. S of classification online. Well, there are several different ways\\nto perform the same tasks like in order to predict whether a given person is a male or a female the machine had to be trained first.\\nAll right, but there are multiple ways to train the machine and you can choose any one of them just for Predictive Analytics.\\nThere are many different techniques but the most common of them all is the decision tree, which we'll cover in depth in today's session.\\nSo as a part of classification algorithm we have decision tree random Forest name buys k-nearest neighbor.\\nLogistic regression linear regression support Vector machines and so on there are many. Alright, so let me give you an idea about few\\nof them starting with decision tree. Well decision tree is a graphical representation of all the possible solution\\nto a decision the decisions which are made they can be explained very easily. For example here is a task,\\nwhich says that should I go to a restaurant or should I buy a hamburger you are confused on that.\\nSo for that what you will do, you will create a dish entry for it starting with the root node will be first of all,\\nyou will check whether you are hungry or not. All right, if you're not hungry then just go back to sleep.\\nRight? If you are hungry and you have $25 then you will decide to go to restaurant. And if you're hungry and you don't have $25,\\nthen you will just go and buy a hamburger. That's it. All right. So there's about decision tree now moving on ahead.\\nLet's see. What is a random Forest. Well random Forest build multiple decision trees\\nand merges them together to get a more accurate and stable production. All right, most of the time random Forest is trained\\nwith a bagging method. The bagging method is based on the idea that the combination\\nof learning model increases the overall result. If you are combining the learning from different models\\nand then clubbing it together what it will do it will Increase the overall result fine.\\nJust one more thing. If the size of your data set is huge. Then in that case one single decision tree would lead\\nto our Offutt model same way like a single person might have its own perspective on the complete population as a population is very huge.\\nRight? However, if we implement the voting system and ask different individual to interpret the data,\\nthen we would be able to cover the pattern in a much meticulous way even from the diagram.\\nYou can see that in section A we have Howard large training data set what we do. We first divide our training data set\\ninto n sub-samples on it and we create a decision tree for each cell sample. Now in the B part\\nwhat we do we take the vote out of every decision made by every decision tree. And finally we Club the vote to get\\nthe random Forest dition fine. Let's move on ahead. Next.\\nWe have neighbor Buys. So named by is is a classification technique, which is based on Bayes theorem.\\nIt assumes that It's of any particular feature in a class is completely unrelated to the presence\\nof any other feature named buys is simple and easy-to-implement algorithm and due to a Simplicity\\nthis algorithm might out perform more complex model when the size of the data set is not large enough.\\nAll right, a classical use case of name bias is a document classification. And that what you do you determine\\nwhether a given text corresponds to one or more categories in the text case,\\nthe features used might be the presence or absence. Absence of any keyword. So this was about Nev from the diagram.\\nYou can see that using neighbor buys. We have to decide whether we have a disease or not. First what we do we check the probability\\nof having a disease and not having the disease right probability of having a disease is 0.1\\nwhile on the other hand probability of not having a disease is 0.9. Okay first, let's see\\nwhen we have disease and we go to the doctor. All right, so when we visited the doctor\\nand the test is positive Adjective so probability of having a positive test when you're having a disease is 0.8 0 and probability\\nof a negative test when you already have a disease that is 0.20. This is also a false negative statement as the test\\nis detecting negative, but you still have the disease, right? So it's a false negative statement.\\nNow, let's move ahead when you don't have the disease at all. So probability of not having a disease is 0.9.\\nAnd when you visit the doctor and the doctor is like, yes, you have the disease. But you already know that you don't have the disease.\\nSo it's a false positive statement. So probability of having a disease when you actually know there is no disease is 0.1 and probability\\nof not having a disease when you actually know there is no disease. So and the probability of it is around 0.90 fine.\\nIt is same as probability of not having a disease in the test is showing the same results a true positive statement.\\nSo it is 0.9. All right. So let's move on ahead and discuss about kn n algorithm.\\nSo this KNN algorithm or the k-nearest neighbor, it stores all the available cases\\nand classifies new cases based on the similarity measure the K in the KNN algorithm as the nearest neighbor,\\nwe wish to take vote from for example, if k equal 1 then the object is simply assigned to the class\\nof that single nearest neighbor from the diagram. You can see the difference in the image when k equal 1 k equal 3 and k equal 5, right?\\nWell the And systems are now able to use the k-nearest neighbor for visual pattern recognization to scan\\nand detect hidden packages in the bottom bin of a shopping cart at the checkout if an object is detected\\nwhich matches exactly to the object listed in the database. Then the price of the spotted product could even\\nautomatically be added to the customers Bill while this automated billing practice is not used\\nextensively at this time, but the technology has been developed and is available for use if you want you can just use It and yeah,\\none more thing k-nearest neighbor is also used in retail to detect patterns in the credit card\\nuses many new transaction scrutinizing software application use Cayenne algorithms\\nto analyze register data and spot unusual pattern that indicates a species activity.\\nFor example, if register data indicates that a lot of customers information is being entered manually rather\\nthan to automated scanning and swapping then in that case. This could indicate that the employees were using the register.\\nAre in fact stealing customers personal information or if I register data indicates that a particular good is being returned\\nor exchanged multiple times. This could indicate that employees are misusing the return policy\\nor trying to make money from doing the fake returns. Right? So this was about KNN algorithm\\nsince our main focus for this session will be on decision tree. So starting with what is decision tree,\\nbut first, let me tell you why did we choose the Gentry to start with? Well, these decision tree are really very easy.\\nEasy to read and understand it belongs to one of the few models that are interpretable where you can understand exactly\\nwhy the classifier has made that particular decision right? Let me tell you a fact that for a given data set.\\nYou cannot say that this algorithm performs better than that. It's like you cannot say that the Asian trees better than a buys\\nor name biases performing better than decision tree. It depends on the data set, right? You have to apply hit and trial method with all\\nthe algorithms one by one and then compare the The model which gives the best result as a model\\nwhich you can use at for better accuracy for your data set. All right. So let's start with what is decision tree.\\nWell a decision tree is a graphical representation of all the possible solution to our decision based on certain conditions.\\nNow, you might be wondering why this thing is called as decision tree. Well, it is called so\\nbecause it starts with the root and then branches off to a number of solution just like a tree right even the trees.\\nStarts from a roux and it starts growing its branches once it gets bigger and bigger similarly in a decision tree.\\nIt has a roux which keeps on growing with increasing number of decision and the conditions now,\\nlet me tell you a real life scenario. I won't say that all of you, but most of you must have used it.\\nRemember whenever you dial the toll-free number of your credit card company. It redirects you to his intelligent computerised assistant\\nwhere it asks you questions like, press one for English or press 2 for Henry, press 3 for this press 4 for that.\\nGreat now once you select one now again, it redirects you to a certain set of questions like press 1 for this press 1 for that\\nand similarly, right? So this keeps on repeating until you finally get to the right person, right?\\nYou might think that you are caught in a voicemail hell but what the company was actually doing it was just using a decision tree to get you to the right person.\\nI lied. I'd like you to focus on this particular image for a moment on this particular slide. You can see I image where the task is.\\nShould I accept a new job offer? Or not. All right, so you have to decide that for that what you did you created a decision tree starting\\nwith the base condition or the root node. Was that the basic salary or the minimum salary should be $50,000\\nif it is not $50,000. Then you are not at all accepting the offer. All right. So if your salary is greater than $50,000,\\nthen you will further check whether the commute is more than one hour or not. If it is more than one are you will just decline the offer\\nif it is less than one hour, then you are getting closer to accepting the job offer. Photo what you will do you will check\\nwhether the company is offering free coffee or not. Right if the company is not offering the free coffee,\\nthen you will just declined off and if it is offering the free coffee and yeah, you will happily accept the offer right there\\nare just an example of a decision tree. Now, let's move ahead and understand a decision tree.\\nWell, here is a sample data set that I will be using it to explain you about the decision tree. Alright in this data set each row is an example\\nand the first two columns provide features. Attributes that describes the data and the last column\\ngives the label or the class we want to predict and if you like you can just modify this data by adding additional features\\nand more example and our program will work in exactly the same way fine. Now this data set is pretty straightforward\\nexcept for one thing. I hope you have noticed that it is not perfectly separable. Let me tell you something more about that as\\nin the second and fifth examples, they have the same features, but different labels, both are Yellow as a Colour and diameter as three,\\nbut the labels are mango and lemon right? Let's move on and see how our decision tree handles this case.\\nAll right, in order to build a tree will use a decision tree algorithm called card this card algorithm\\nstands for classification and regression tree algorithm online. Let's see a preview of how it works.\\nAll right to begin with We'll add a root note for the tree and all the nodes receive a list\\nof rows as input and the root will receive the entire. Training data set now each node will ask true and false question\\nabout one other feature. And in response to that question will split or partition the data set into two different subsets\\nthese subsets then become input to child node. We are to the tree and the goal of the question is to finally unmix the labels\\nas we proceed down or in other words to produce the purest possible distribution of the labels at each node.\\nFor example, the input of this node contains only one single type of label. So we See that it's perfectly unmixed.\\nThere is no uncertainty about the type of label as it consists of only grapes right\\non the other hand the labels in this node are still mixed up. So we would ask another question to further drill it down.\\nRight but before that we need to understand which question to ask and when and to do\\nthat we need to conduct by how much question helps to unmix the label and we can quantify the amount of uncertainty\\nat a single node using a metric. Called gini impurity and we can quantify how much a question reduces\\nthat uncertainty using a concept called Information Gain will use these to select the best question to ask at each point.\\nAnd then what we'll do we'll iterate the steps will recursively build the tree on each of the new node will continue dividing the data\\nuntil they are no further question to ask and finally we reach to our Leaf. Alright, alright.\\nSo this was about decision tree. So in order to create a decision tree, first of all what you have to do you have to identify\\nA different set of questions that you can ask to a tree like is this color green and what will be these question?\\nThese questions will be decided by your data set like as this colored green is the diameter greater than equal to 3 is the color yellow\\nright questions resembles to your data set remember that? All right. So if my color is green,\\nthen what it will do it will divide into two parts. First. The Green Mango will be in the true while on the false.\\nWe have lemon and the Mac. All right if the color is green or the diameter. Meter is greater than equal to 3\\nor the color is yellow Asian tree terminologies. So starting with root node root node is a base node\\nof a tree the entire tree starts from a root node. In other words. It is the first node of a tree it represents\\nthe entire population or sample and this entire population is further segregated\\nor divided into two or more homogeneous set fine. Next is the leaf node.\\nWell Leaf node is the one when you reach at the The tree right that is you cannot further segregated down\\nto any other level that is the leaf node. Next is splitting splitting is dividing your root node\\nor node into different sub part on the basis of some condition. All right, then comes the branch or the sub tree.\\nWell, this Branch or subtree gets formed when you split the tree suppose when you split a root node,\\nit gets divided into two branches or two subtrees. Right? Next is the concept of pruning.\\nWell you can Say that pruning is just opposite of splitting what we are doing here. We are just removing the sub node of a decision tree\\nwill see more about pruning later in this session. All right, let's move on ahead. Next is parent or child node.\\nWell, first of all root node is always the parent node and all other nodes associated\\nwith that is known as chalky node. Well, you can understand it in a way that all the top node\\nbelongs to a parent node and all the bottom node, which are derived from a top node is a child node.\\nNode producing a further note is a child node and the node which is producing it as a parent node\\nsimple concept, right? It's use the cart algorithm and design a tree manually. So first of all\\nwhat you will do you decide which question to ask and when so how will you do that? So let's first of all visualize the decision tree.\\nSo there's the decision tree which will be creating manually or like first of all, let's have a look at the data set.\\nYou have Outlook temperature humidity and windy as your different attribute on the basis of that you have to predict\\nthat whether you can play or not. So which one among them should you pick first answer determine\\nthe best attribute that classifies the training data? All right. So how will you choose the best attribute\\nor how does a tree decide where to split or how the tree will decide its root node? Well before we move on\\nand split a tree there are some terminologies that you should know. All right, first being the gini index.\\nSo what is this gini index? The gini index is the measure of impurity or Purity used in building a day.\\nGentry and cart algorithm. All right. Next is Information Gain this Information Gain is\\nthe decrease in entropy after data set is split on the basis of an attribute constructing a decision tree is all about finding an attribute\\nthat Returns the highest Information Gain. All right, so you will be selecting the node that would give you the highest Information Gain.\\nAlright next is reduction in variance. This reduction in variance is an algorithm, which is used for continuous Target variable\\nor regression problems the split With lower variance is selected as a criteria to let the population see in general term.\\nWhat do you mean by variance? Variance is how much your data is wearing? Right? So if your data is less impure or is more pure\\nthan in that case the variation would be less as all the data almost similar, right? So there's also a way of setting a tree the split\\nwith lower variance is selected as the criteria to split the population. Alright. Next is the chi Square C Square.\\nIt is an algorithm which is used to find out these statistical significance between the Is between sub nodes and the parent nodes fine.\\nLet's move ahead. Now. The main question is how will you decide the best attribute\\nfor now just understand that you need to calculate something known as Information Gain the attribute\\nwith the highest Information Gain is considered the best. Yeah. I know your next question might be like, what is this information again?\\nBut before we move on and see what exactly Information Gain Is let me first introduce you\\nto a term called entropy because this term will be used in calculating the Information Gain. Mmmmmm.\\nWell entropy is just a metric which measures the impurity of something or in other words, you can say that as the first step to do\\nbefore you solve the problem of a decision tree as I mentioned is something about impurity. So let's move on and understand what is impurity suppose.\\nYou are a basket full of apples and another Bowl which is full of same label, which says Apple now\\nif you are asked to pick one item from each basket and ball then the probability of getting the apple\\nand it's correct label is 1 so in this case, You can see that impurities zero. All right.\\nNow what if there are four different fruits in the basket and four different labels in the bowl,\\nthen the probability of matching the fruit to a label is obviously not one. It's something less than that.\\nWell, it could be possible that I picked banana from the basket and when I randomly picked the label from the ball,\\nit says a cherry any random permutation and combination can be possible. So in this case I'd say that impurities is nonzero.\\nI hope the concept of impurities care. Are so coming back to entropy as I said entropy is the measure of impurity\\nfrom the graph on your left. You can see that as the probability is zero or one that has either they are highly impure\\nor they are highly pure than in that case the value of entropy is zero. And when the probability is 0.5,\\nthen the value of entropy is maximum. Well, what is impurity impurities the degree\\nof Randomness how random data is so if the data is completely pure in that case the randomness equals 0 or\\nif the Dies completely Empire even in that case the value of impurity will be zero question.\\nLike why is it that the value of entropy is maximum at 0.5 might arise in a mine, right?\\nSo let me discuss about that. Let me derive at mathematically as you can see here on the slide,\\nthe mathematical formula of entropy is - of probability of yes, let's move on and see\\nwhat this graph has to say mathematically suppose s is our total sample space and it's divided into two parts.\\nYes, and no. No, like in our data set the result for playing was divided into two parts. Yes or no,\\nwhich we have to predict either we have to play or not. Right? So for that particular case, you can Define the formula of entropy as entropy\\nof total sample space equals negative of probability of e is multiplied by log\\nof probability of years with a base 2 minus probability of no X log of probability\\nof no with base to where s is your total sample space and P of v s is the probability\\nof E. And be of known as the probability of no, well, if the number of yes equal number of know\\nthat is probability of s equals 0.5 right since you have equal number of yes,\\nand no so in that case value of entropy will be one just put the value over there.\\nAll right. Let me just move to the next slide. I'll show you this. Alright next is if it contains all Yes,\\nor all know that is probability of a sample space is either 1 or 0 then in that case entropy will be equal to 0\\nLet's see the mathematically one by one. So let's start with the first condition\\nwhere the probability was 0.5. So this is our formula for entropy, right?\\nSo there's our first case right which we discuss the art when the probability of vs equal probability of node\\nthat is in our data set. We have equal number of yes, and no. All right. So probability of yes equal probability of no\\nand that equals 0.5 or in other words, you can say that yes plus no equal to Total sample.\\nHe's all right, since the probability is 0.5. So when you put the values\\nin the formula you get something like this and when you calculate it, you will get the entropy of the total sample space as one.\\nAll right. Let's see for the next case. What is the next case either you have totally us\\nor you have totally know so if you have total, yes, let's see the formula when we have totally as so\\nyou have all yes and 0 no fine. So probability of e s equal 1 and yes.\\nYes as the total sample space obviously. So in the formula when you put that thing up here,\\nyou get entropy of sample space equal negative X of 1 multiplied by log of 1 as the value of log 1 equals 0.\\nSo the total thing will result to 0 similarly is the case with no even in that case, you will get the entropy of total sample space as 0\\nso this was all about entropy. All right. Next is what is Information Gain?\\nWell Information Gain what it does is it measures the reduction in entropy? It decides which attributes\\nshould be selected as the decision node. If s is our total collection than Information Gain equals entropy,\\nwhich we calculated just now that - weighted average X entropy of each feature.\\nDon't worry. We'll just see how it to calculate it with an example. Let's manually build a decision tree\\nfor our data set. So there's our data set which consists of 14 different instances out of which we have nine.\\nYes and five know I like so we have the formula for entropy just put over that since 9 years.\\nSo total probability of e s equals 9 by 14 and total probability of no equals Phi by 14\\nand when you put up the value and calculate the result, you will get the value of entropy as 0.94.\\nAll right. So this was your first step that is compute the entropy for the entire data set only now,\\nyou have to select that out of Outlook temperature humidity and windy, which of the node should you select as the root node\\nbig question right? I will Decide that this particular node should be chosen at the base note. And on the basis of that only I will be creating\\nthe entire tree. I will select that. Let's see. So you have to do it one by one you have to calculate the entropy\\nand Information Gain for all of the different nodes. So starting with Outlook. So Outlook has\\nthree different parameters Sunny overcast and rainy. So first of all select how many number of years\\nand no are there in the case of Sunny like when it is sunny how many number of years and how many number of knows?\\nAre there so in total we have to yes and three Nos and case of sunny in case of overcast.\\nWe have all yes. So if it is overcast then we will surely go to play. It's like that. Alright and next it is rainy then total number\\nof vs equal 3 and total number of no equals 2 fine next what we do we calculate the entropy\\nfor each feature for here. We are calculating the entropy when Outlook equals Sunny.\\nFirst of all, we are assuming that Outlook is our root node and for that we are calculating the Can gain for it.\\nAll right. So in order to calculate the Information Gain remember the formula it was entropy of the total sample space -\\nweighted average X entropy of each feature. All right. So what we are doing here, we are calculating the entropy of Outlook\\nwhen it was sunny. So total number of yes, when it was Sonny was to and total number of know\\nthat was three fine. So let's put up in the formula since the probability of yes is 2 by 5\\nand the probability of no is 3 by 5. So you will get something like this. All right. So you are getting the entropy\\nof sunny as zero point nine seven one fine. Next we will calculate the entropy for overcast\\nwhen it was overcast. Remember it was all yes, right. So the probability of e is equal 1 and when you put over\\nthat you will get the value of entropy as 0 fine and when it was rainy rainy has 3s and to nose.\\nSo probability of e s in case of Sonny's 3 by 5 and probability of know in case of Sonny's 2 by 5\\nand when you add the You of probability of vs and probability of note the formula you get the entropy\\nof sunny as zero point nine seven one point. Now, you have to calculate how much information you are getting from Outlook\\nthat equals weighted average. All right. So what was this weighted average total number of years\\nand total number of no fine. So information from Outlook equals 5 by 14 from where does this 5 came over?\\nWe are calculating the total number of sample space within that particular Outlook when it was sunny, right?\\nSo in case of Sunny there was two years and three NOS. All right. So weighted average for Sonny would be equal to 5 by 14.\\nAll right, since the formula was five by 14 x entropy of each feature. All right, so\\nas calculated the entropy for Sonny is zero point nine seven one, right?\\nSo what we'll do we'll multiply five by 14 with 0.97 one, right?\\nWell, this was the calculation for information when Outlook equal sunny, but Outlook even equals overcast and rainy.\\nIn that case, what we'll do again similarly will calculate for everything for overcast and sunny\\nfor overcast weighted averages for by 14 x its entropy. That is 0 and for Sonny it is same 5i 14-3.\\nYes and two nodes X its entropy that is zero point nine seven one. And finally we'll take the sum of all of them which equals\\nto 0.693 right next. We will calculate the information gained this\\nwhat we did earlier was Malaysian taken from Outlook. Now. We are calculating. What is the information?\\nWe are gaining from Outlook right. Now this Information Gain that equals to Total entropy minus the information\\nthat is taken from Outlook. All right. So total entropy we had 0.94 -\\ninformation we took from Outlook as 0.693. So the value of information gained from Outlook results\\nto zero point two four seven. All right. So next what we have to do. Let's assume that Wendy is our root node.\\nSo Wendy consists of two parameters false and true. Let's see how many years\\nand how many nodes are there in case of true and false. So when Wendy has Falls as its parameter,\\nthen in that case, it has six years and two nodes and when it as true as its parameter,\\nit has 3 S and 3 nodes. All right. So let's move ahead and similarly calculate the information taken from Wendy\\nand finally calculate the information gained from Wendy. Alright, so first of all, what we'll do we'll calculate the entropy of each feature.\\nER starting with windy equal true. So in case of true we had equal number of yes\\nand equal number of know. We'll remember the graph when we had the probability as 0.5 as total number of years\\nequal total number of know and for that case the entropy equals 1 so we can directly write entropy of room\\nwhen it's windy is one as we had already proved it when probability equals 0.5 the entropy is the maximum\\nthat equals to 1. All right. Next is entropy of false when it is Vending. I like so similarly just put the probability of yes\\nand no in the formula and then calculate the result since you have six years and to nose.\\nSo in total, you'll get the probability of yes 6 by 8 and probability of no as 2 by 8.\\nAll right, so when you will calculate it, you will get the entropy of false as zero point eight one one.\\nAlright now, let's calculate the information from windy. So total information collected from Windy\\nequals information taken when Wendy equal true plus Action taken when Wendy equal false.\\nSo we'll calculate the weighted average for each one of them and then we'll sum it up to finally get the total information taken from windy.\\nSo in this case, it equals to 8 by 14 multiplied by 0.8 1 1 plus 6 by 14 x 1.\\nWhat is this? 8 it is total number of yes, and no in case when when D equals false, right?\\nSo when it was false, so total number of BS that equals to 6 and total more of know that equal to 2\\nthat some UPS to 8. Alright, so that is why the waiter. Resul results to Aid by 14 similarly information taken\\nwhen windy equals true equals to 3 plus 3 that is 3 S and 3 no equal 6 divided by total number\\nof sample space that is 14 x 1 that is entropy of true. All right.\\nSo it is 8 by 14 multiplied by 0.8 1 1 plus 6 by 14 x one\\nwhich results to 0.89 to this is information taken from Windy.\\nAll right. Now how much information you are gaining from Wendy? So for that what you will do,\\nso total information gained from Windy that equals to Total entropy - information taken from Windy.\\nAll right, that is 0.94 - 0.89 to that equals to zero point zero four eight.\\nSo 0.048 is the information gained from Windy. Similarly. We calculated for the rest too.\\nSo for Outlook as you can see, the information was 0.693, and it's Information Gain was zero point two four seven in\\ncase of temperature the information was around. Zero point nine one one and the Information Gain\\nthat was equal to 0.02 9 in case of humidity. The information gained was 0.15 to and in the case of windy.\\nThe information gained was 0.048. So what we'll do we'll select the attribute with the maximum fine.\\nNow, we are selected Outlook as our root node, and it is further subdivided into three different parts Sunny overcast and rain,\\nso in case of overcast we have seen that it consists of all ears so we can consider it as a Leaf node,\\nbut in case of sunny and rainy it's doubtful as it consists of both. Yes and both know\\nso you need to recalculate the things right again for this node. You have to recalculate the things.\\nAll right, you have to again select the attribute which is having the maximum Information Gain. All right, so there is\\nhow your complete tree will look like. All right. So, let's see when you can play so you can play\\nwhen Outlook is overcast. All right in that case. You can always play if the Outlook is sunny.\\nYou will further drill. Time to check the humidity condition. All right, if the humidity is normal, then you will play\\nif the humidity is high then you won't play right when the Outlook predicts that it's raining then further you will check\\nwhether it's windy or not. If it is a week went then you will go and offer play but if it has strong wind,\\nthen you won't play right? So this is how your entire decision tree would look like at the end. Now comes the concept of pruning say is\\nthat what should I do to play? Well you have to do pruning pruning will decide how you will play.\\nSay what is this pruning? Well, this pruning is nothing but cutting down the nodes and order to get the optimal solution.\\nAll right. So what pruning does it reduces the complexity? All right, as are you can see on the screen that it showing only the result\\nfor yes that is it showing all the result which says that you can play before we drill down to a practical session\\na common question might come in your mind. You might think that our tree based model better than linear model right?\\nYou can think like if I can Was a logistic regression for classification problem and linear regression for regression problem.\\nThen why there is a need to use the tree. Well, many of us have this question in their mind and well there's a valid question too.\\nWell actually as I said earlier, you can use any algorithm. It depends on the type of problem.\\nYou're solving let's look at some key factor, which will help you to decide which algorithm to use and\\nwhen so the first point being if the relationship between dependent and independent variable as well approximated by By a linear model,\\nthen linear regression will outperform tree base model second case if there is a high non-linearity\\nand complex relationship between dependent and independent variables at remodel will outperform a classical regression model\\nin third case. If you need to build a model which is easy to explain to people a decision tree model\\nwill always do better than a linear model as the decision tree models are simpler to interpret then linear regression.\\nAll right. Now let's move on ahead and see how you can write it as Gentry classifier from scratch\\nand python using the cart algorithm. All right for this. I will be using jupyter notebook with python 3.0 installed on it.\\nAlright, so let's open the Anaconda and the jupyter notebook. Where is that? So this is our Anaconda Navigator\\nand I will directly jump over to jupyter notebook and hit the launch button. I guess everyone knows that jupyter.\\nNotebook is a web-based interactive Computing notebook environment where you can run your python codes.\\nSo my Jupiter notebook it opens on my Local Host w89 1 so I will be using this jupyter notebook\\nin order to write my decision tree classifier using python for this decision tree classifier. I have already written the set of codes.\\nLet me explain you just one by one. So we'll start with initializing our training data set.\\nSo there's our sample data set for which each row is an example. The last column is a label\\nand the first two columns are the features. If you want you can add some more features an example for your practice interesting fact is\\nthat This data set is design and way that the second and fifth example have almost the same features, but they have different labels.\\nAll right, so let's move on and see how the tree handles this case as you can see here. Both of them II and the fifth column have the same features.\\nWhat did different is just their label? Right? So let's move ahead. So this is our training data set next what we are doing we\\nare adding some column labels. So they are used only to print the trees fine. So what we'll do we'll add header to the columns\\nlike the First Column is of Close second is of diameter and third is a label column. All right, next\\nwhat we'll do we'll Define a function as unique values in which will pass the rows and the columns.\\nSo this function what it will do it will find the unique values for a column in the data set.\\nSo there's an example for that. So what we are doing here, we are passing training data Hazard row\\nand column number as 0 so what we are doing we are finding unique values in terms of color.\\nAnd in this since the row is training data and the column is 1 so what you are doing here, so we are finding the you Values in terms of diameter fine.\\nSo this is just an example next what we'll do we'll Define a function as class count and we'll pass the rows into it.\\nSo what it does, it counts the number of each type of example within data set. So in this function\\nwhat you are basically doing we are counting the number of each type for example in the data set or what we are doing we are counting the unique values\\nfor the label in the data set as a sample. You can see here we can pass that entire training data set to this particular function as class underscore count\\nwhat it will do it will find all the different types of Label within the training data set as you can see here the unique label consists\\nof mango grape and lemon. So next what we'll do. We'll Define a function is numeric and we'll pass\\na value into it. So what it will do it will just test if the value is numeric or not and it will return if the value is an integer or a float.\\nFor example, you can see is numeric. We are passing 7 so it is an integer so it will return in value and if we are passing red,\\nit's not a numeric value, right? So moving on ahead where you define a class named as question,\\nso This question does this question is used to partition the data set. This class voted does it just records a column number?\\nFor example 0 for color a light and a column value for example, green next what we are doing we are defining a match method\\nwhich is used to compare the feature value in the example to the feature values stored in the question.\\nLet's see how first of all what you are doing. We are defining an init function and inside that we are passing the self column\\nand the value as parameter. So next what we do we Define a function as match what it Does it compares the feature value\\nin an example to the feature value in this question when next we'll Define a function as re PR,\\nwhich is just a helper method to print the question in a readable format next what we are doing we are defining a function partition.\\nWell, this function is used to partition the data set each row in the data set it checks if it match the question or not\\nif it does so it adds it to the true rose or if not then it adds to the false Rose. All right, for example,\\nas you can see, it's partition the training data. Based on whether the roses are red or not here.\\nWe are calling the function question and we are passing a value of zero and read to it. So what did we do it will assign all the red rose\\nto True underscore Rose and everything else will be assigned to false underscore rose fine.\\nNext. What we'll do we'll Define a gini impurity function and inside that will pass the list of rows.\\nSo what it will do it will just calculate the gini impurity for the list of rows.\\nNext what we are doing here. We defining a function as Information Gain. So what this Information Gain function does it calculates\\nthe information game using the uncertainty of the starting node - the weighted impurity of the child node.\\nThe next function is find the best plate. Well, this function is used to find the best question to ask\\nby iterating over every feature of value and then calculating the Information Gain. But the detail explanation on the code,\\nyou can find the code in the description given below. All right next we'll define a class as leave\\nfor classifying the data. It holds a dictionary of glass like mango for how many times it appears in the row from the training data\\nthat reaches the sleeve. Alright, next is the decision node. So this decision node, it will ask a question.\\nThis holds a reference to the question and the two child nodes on the base of it. You are deciding which node to add further to which branch.\\nAlright so next. What we are doing we are defining a function of build tree and inside that we are passing our number of rows.\\nSo this is the function that is used to build the tree. So initially what we did we Define all the various function\\nthat we'll be using in order to build a tree. So let's start by partitioning the data set for each unique attribute,\\nthen we'll calculate the information gain and then return the question that produces the highest gain and on the basis of that will split the tree.\\nSo what we are doing here, we are partitioning the data set calculating the Information Gain. And then what this is returning it is returning the question\\nthat is producing the highest gain. All right. Now if gain equals 0 return Leaf Rose,\\nso what it will do. So if you are getting no for the gain that is gain equals 0 then in that case\\nsince no further question could be asked so what it will do it will return a leaf fine now true\\nor underscore Rose or false underscore Rose equal partition with rose and the question.\\nSo if we are reaching till this position, then you have already found. A feature of value which will be used to partition the data set then\\nwhat you will do you will recursively build the true branch and similarly recursively build the false Branch.\\nSo return Division and Discord node and side that will be passing question to branch and false front.\\nSo what it will do it will return a question node. Alice question owed this recalls the best feature\\nor the value to ask at this point fine. Now that we have built our tree next what we'll do we'll Define a print underscore tree function\\nwhich will be used to print the tree fine. So finally what we are doing in this particular function\\nthat we are printing our tree next is the classify function which will use it to decide whether to follow the true Branch or the false branch\\nand then compared to the feature values stored in the node to the example. We are considering and last\\nwhat we'll do we'll finally print the production at Leaf. So let's execute it and see okay,\\nso there's our testing data. All right. So we printed all Leaf\\nas well now that we have trained our algorithm with our training data set now it's time to test it.\\nSo there's our testing data set. So let's finally execute it and see what is the result.\\nSo this is the result you will get so first question, which is asked by the algorithm is is diameter greater\\nthan equal to 3 if it is true, then it will further ask if the color is yellow again,\\nif it is true, then it will predict mango as one and lemon with one.\\nAnd in case it is false, then it will just predict the mango. Now. This was the true part.\\nNow next coming to diameter is not greater than or equal to 3 then in that case it's false\\nand what it will do it will just predict the grape fine. Okay. So this was all about the coding part now,\\nlet's conclude this session. But before concluding let me just show you one more thing. Now, there's a scikit-learn algorithm cheat sheet,\\nwhich explains you which algorithm you should use and when all right, let's build in a decision tree format.\\nLet's see how it is built. So first condition it will check whether you have 50 samples or not. If your samples are greater than 50,\\nthen we'll move ahead if it is less than 50, then you need to collect more data if you sample is greater than 50,\\nthen you have to decide whether you want to predict a category or not. If you want to predict a category,\\nthen further you will see that whether you have labeled data or not. If you have label data, then that would be a classification\\nalgorithm problem. If you don't have the label data, then it would be a clustering problem. Now if you don't want to Category then what?\\nDo you want to predict predict a quantity? Well, if you want to predict a quantity, then in that case, it would be a regression problem.\\nIf you don't want to predict a quantity and you want to keep looking further, then in that case, you should go for dimensionality reduction problems and still\\nif you don't want to look and the predicting structure is not working. Then you have tough luck for that.\\nI hope this doesn't recession clarifies all your doubt over decision tree algorithm.\\nLet's begin this tutorial by looking at the topics that we'll be covering today.\\nSo first of all, we'll start Away by getting a brief introduction of random forest and then we'll go\\nas to see why we actually need random Forest right? Why not anything else but actually random Forest.\\nSo once we understand it's need at first place, then we'll go on to learn more about what is random forest\\nand we'll also look at various. Examples of random Forest so that we get a very clear understanding of it.\\nSo for the will also delve inside in to understand the working of random Forest as to\\nhow exactly random Forest Works will also watch out the random Forest algorithm step by step,\\nright so that you are able to write any piece of code any domain specific algorithm on your own now,\\nI personally believe that any learning is really incomplete. If it's not put into application\\nso for its completion will also Implement random forest in r with a very simple use case that is diabetes prevention.\\nSo let's get started with the introduction then. No, random Forest is actually one of the classifiers\\nwhich is used for solving classification problems. Now since some of you might not be really aware\\nof what classification is. So let's quickly understand classification first,\\nand then we'll try to related to the random Forest. So basically classification is a machine learning technique\\nin which you already have predefined categories under which you can classify your data.\\nSo it's nothing but to supervised learning model where you already have a data based on which you can train\\nyour machine, right? So your machine actually learns from this data. So whatever all that predefined data\\nthat you already have it actually works as a fuel for your machine, right?\\nSo let's say for an example ever wondered\\nhow your Gmail gets to know about the spam emails and filters it out from the rest of the genuine emails any guesses.\\nAll right. I'll give you a hint try to think something on the line that what would it actually look for what can be\\nthe possible parameters based on which you can decide or read. This is a genuine email or this is a spam email.\\nSo there are certain parameters that your classifier will actually look for like The subject line\\nor the text or the HTML tags and also the IP address of the source from where is this mail getting\\nfrom so it will analyze all these variables and then it will classify them into this Pam\\nor the genuine folder. So let's say for an example if your subject line States like mad\\nor cute or pretty and some other absurd keywords. Your classifier is smart enough\\nand it's trained in such a manner that it will Get to know. All right, this is a spam email and it\\nwill automatically filter it out from your genuine emails. So that is how you classify it works basically,\\nso that's pretty much about the classification now, let's move forward and see what always can be there\\nthrough which you can actually perform classification. So we have three classifiers\\nnamely decision tree random forest and a base, right so speaking briefly about Season 3 at first\\nso decision tree actually splits your entire data set in this structure of a tree\\nand it makes decision at every node and hence called decision tree. So no big bang theory, right?\\nSo you have certain data set. There are certain nodes at each node. It will for the split into the child nodes\\nand at each node. It will make a decision. So final decision will be in the form of positive\\nand negative, right? So let's say for an example you want to purchase a car, right?\\nSo what all will be the parameters? Let's say I have a go and I want to purchase a car\\nand I will keep certain parameters in my mind. That would be what exactly is my income.\\nWhat is my budget? What is the particular brand that I want to go for? What is the mileage of the car?\\nWhat is the cylinder capacity of the car and so on and so forth, right? So I'll make my decision based on.\\nAll these parameters, right and that is how you make decisions and further.\\nIf you really want to know more about decision tree as to how it exactly works. You can also check out our decision tree tutorial as well.\\nSo let's begin now to the random Forest now. So Random Forest isn't in simple classifier.\\nActually now, let's understand what this war in symbol means.\\nSo in simple methods actually. Use multiple machine learning algorithms to obtain\\nbetter predictive performance. So particularly talking about random Forest So Random forests uses\\nmultiple decision trees for prediction, right? So you are in assembling a lot of decision trees to come up\\nto your final outcome. As you can also look here in the image that your entire data set is actually for the split\\ninto three subsets, right and each subset for Leads to a particular decision tree.\\nSo here you have three decision trees and each decision tree will lead to certain outcome.\\nNow what random Forest will do is it will compile the results from all the decision trees\\nand then it will lead to a final outcome. Right? So it's compiled a section of all the multiple decision trees.\\nThat's all about the random Forest now, let's see what's lies there in a pace, right?\\nSo naive Bayes is very famous classifier, which is made on a very famous rule called Bayes theorem.\\nYou might have studied about Nee Bayes theorem in your 10 standard as well. So let's just see what Bayes theorem describes.\\nSo based on actually describes the probability of an event based on certain prior knowledge of conditions\\nthat might be related to the event, right? So for example, if cancer is related to age, right,\\nso then person's age can be used to more accurately assess probability of having a cancer\\nthan without having the knowledge of age. So if you know the age then it will become handy in addicting\\nthe occurrence of cancer for a particular person. Right? So the outcome of first event here is actually affecting\\nyour final outcome, isn't it? Yeah. So this is how naive Bayes classifier actually works.\\nSo that was all to give an overview of Nave Bayes classifier.\\nAnd this were pretty much about the types of classifiers now,\\nwe'll try to find out the answer to this particular question as to why we need random Forest fine.\\nSo like human beings learn from the past experiences. So unlike human beings a computer does not have\\nexperiences then how does machine takes decisions? Where does it learn from?\\nUm, well a computer system actually learns from the data which represents\\nsome past experiences of an application domain. So now let's see how random Forest helps in building up in learning model\\nwith a very simple use case of credit risk detection. Now needless to say\\nthat credit card companies have a very nested interest in identifying Financial transactions\\nthat are illegitimate and criminal in nature. And also I would like to mention this point\\nthat according to the Federal Reserve payment study Americans used credit cards to pay\\nfor twenty six point two million purchases in 2012, and the estimated loss due to unauthorized transactions\\nthat here was us six point 1 billion dollars now in the banking industry measuring risk is very critical\\nbecause the stakes are too high. So the overall goal is actually to figure out Out\\nwho all can be fraudulent before too much Financial damage has been done.\\nSo for this a credit card company receives thousands of applications for new cards\\nand each application contains information about an applicant, right?\\nSo so here as you can see that from all those applications\\nwhat we can actually figure out is that predictor variables. Like what is the marital status of the person?\\nWhat is the gender of the person? The age of the person and the status\\nwhich is actually whether it is a default pair or a non-default pair. So default payments are basically when payments\\nare not made in time and according to the agreement signed by the cardholder. So now that account is actually set to be in the default.\\nSo you can easily figure out the history of the particular card holder from this then we can also look\\nat the time of payment whether he has been a regular pair or not. Regular one, what is the source of income\\nfor that particular person? And so and so forth. So to minimize loss\\nthe back actually needs certain decision rule to predict whether to approve a particular loan of that particular person or not.\\nNow here is where the random Forest actually comes into the picture right now.\\nLet's see how random Forest can actually help us in this particular scenario.\\nNow, we have taken randomly two parameters. Out of all the predictive variables\\nthat we saw previously now, we have taken two predictor variables here.\\nThe first one is the income and the second one is the H right and similarly parallel\\nit to decision trees have been implemented upon those predicted variables and let's first assume the case\\nof the income variable, right? So here we have divided our income into three categories\\nthe first one being the person earning over 35,000. And dollars second from 15 to 35 thousand dollars the third one running\\nin the range of 0 to 15 thousand dollars. Now if a person is earning over $35,000,\\nwhich is a pretty good income pretty decent. So now we'll check out for the credit history.\\nNow the here the probability is that if a person is earning a good amount then there is very low risk\\nthat he won't be able to pay back already earning good. So the It is\\nthat his application of loan will get approved. Right? So there is actually low risk or moderate risk,\\nbut there's no real issue of high risk as such we can approve the applicants request here.\\nNow, let's move on and watch out for the second category where the person is actually earning from 15 to 35 thousand dollars right now here the person may\\nor may not pay back. So in such scenarios will look for the credit. History as to what has been his previous history.\\nNow if his previous history has been bad like he has been a default. ER in the previous transactions will definitely not consider\\napproving his request and he will be at the high risk in which is not good for the bank.\\nIf the previous history of that particular applicant is really good then we\\nwill just to clarify our doubt will consider another pair. Dress. Well, that will be on depth.\\nI have his already in really high depth then the risks again increases and there are chances\\nthat he might not pay repay in the future. So here will not accept the request of the person\\nhaving high dipped if the person is in the low depth and he has been a good pair in his past history.\\nThen there are chances that he might be back and we can consider approving the request of this particular applicant.\\nAnd let's look at the third category, which is a person earning from 0 to 15 thousand dollars.\\nNow, this is something which actually raises I broke and this person will actually lie\\nin the category of high risk. All right. So the probability is\\nthat his application of loan would probably get rejected now, we'll get one final outcome from this income parameter, right?\\nNow let us look at our second variable that is age which will lead into the second decision tree.\\nNow. Let us say if the person is Young, right? So now we will look forward to if it is a student now\\nif it is a student then the chances are high that he won't be able to repay back because he has no learning Source, right?\\nSo here the risks are too high and probability is that his application of loan will get rejected fine.\\nNow if the person is Young And he's not a student then we'll probably go on and look for another variable.\\nThat is pan balance. Now. Let's look if the bank balance is less than 5 lakhs. So again the risk arises and the probabilities\\nthat his application of loan will get rejected. Now if the person is Young is not a student\\nand his bank balance of greater than 5 lakhs is got a pretty good and stable and balanced then the probabilities\\nthat his zone of application will get approved. Of not let us take another scenario\\nif he's a senior, right? So if he is a senior will probably go and check out for this credit history.\\nHow well has he been in his previous transactions? What kind of a person he is like\\nwhether he's a defaulter or is Ananda falter now if he is a very fair kind of person\\nin his previous transactions then again the risk arises and the probability of his application\\ngetting rejected actually increases right now. If he has been an excellent person as\\nper his transactions in the previous history. So now again here there is least risk\\nand the probabilities that his application of loan will get approved. So now here these two variables income and age have led\\nto two different decision trees. Right and these two different decision trees actually led\\nto two different results. Now what random forest does is it will actually compile\\nthese two different results from these two different. Decision trees and then finally, it will lead to a final outcome.\\nThat is how random Forest actually works. Right? So that is actually the motive of the random Forest.\\nNow let us move forward and see what is random Forest right?\\nYou can get an idea of the mechanism from the name itself random forests. So a collection of trees is a fortress\\nthat's why I called for is probably and here also the trees are actually because being trained on subsets\\nwhich are being selected at random. And therefore they are called random forests.\\nSo a random forests is a collection or an in symbol of decision.\\nEat straight head a decision trees actually built using the whole data set considering all features,\\nbut actually in random Forest only a fraction of the number of rows is selected and that too at random and a particular number of features,\\nwhich are actually selected at random are trained upon and that is how the decision trees are built upon.\\nRight? So similarly number of decision trees will be grown and each decision tree will result in two.\\nWith a certain final outcome and random Forest will do nothing, but actually just compiled the results\\nof all those decision trees to bring up the final result. As you can see in this particular figure\\nthat a particular instance actually has resulted into three different decision trees right sonar tree\\none results into a final outcome called Class A and tree to results into class B. Similarly tree three results into class P\\nSo Random Forest will compile the results of all these decision trees. And it will go by the goal of the majority voting now\\nsince head to decision trees have actually voted into the favor of the Class B that is decision tree two,\\nand three therefore the final outcome will be in the favor of the Class B. And that is how random Forest actually works upon.\\nNow one really beautiful thing about this particular algorithm is that it is one of the versatile algorithms\\nwhich is capable of Performing both regression as well as classification.\\nNow, let's try to understand random Forest further with a very beautiful example or a this is my favorite one.\\nSo let's say you want to decide if you want to watch edge of tomorrow or not, right?\\nSo in this particular scenario, you will have two different actions to work Bond either.\\nYou can just straight away go to your best friend asked him about or read. Whether should I go for Edge of Tomorrow?\\nAnd what will I like this movie or you can ask a bunch? Your friends and take their opinion consideration\\nand then based on the final results. You can go out and watch Edge of Tomorrow, right?\\nSo now let's just take the first scenario. So where you go to your best friend asked about\\nwhether you should go out to watch edge of tomorrow or not. So your friend will probably ask you certain questions\\nlike the first one being here Jonah. So so let's say your friend asks you\\nif you really like The Adventurous kind of movies or not. So you say yes, definitely I would love to watch it Venture kind of movie.\\nSo the probabilities that you will like edge of tomorrow as well. Since it's of Tomorrow is also a movie of Adventure\\nand sci-fi kind of Jonah, right? So let's say you do not like the adventure John a movie.\\nSo then again the probability reduces that you might really not like edge of Morrow right.\\nSo from here you can come to a certain conclusion right? Let's say your best friend puts you into another situation\\nwhere he'll ask you or a do you like Emily plant? And you see definitely I like Emily Blunt\\nand then he puts another question to you. Do you like Emily Blunt to be in the main lead\\nand you say yes, then again, the probability arises that you will definitely like edge of tomorrow as\\nwell because Edge of Tomorrow is Has the Emily plant in the main lead cast so\\nand if you say oh I do not like Emily Blunt then again, the probability reduces\\nthat you would like Edge of Tomorrow to write. So this is one way\\nwhere you have one decision tree and your final outcome. Your final decision will be based on your one decision tree,\\nor you can see your final outcome will be based on just one friend. No, definitely not really convinced.\\nYou want to consider the options of your other friends also so that you can make very precise and crisp\\ndecision right you go out and you approach some other bunch of friends of yours.\\nSo now let's say you go to three of your friends and you ask them the same question\\nwhether I would like to watch Age of Tomorrow or not. So you go out and approach three or four friends friend one friend twin friend three.\\nNow, you will consider each of their Sport and then you will your decision now will be dependent\\non the compiled results of all of your three friends, right? Now here, let's say you go to your first friend\\nand you ask him whether you would like to watch it if tomorrow not and your first friend puts you to one question.\\nDid you like Top Gun? And you say yes, definitely I did like the movie Top Gun and the probabilities\\nthat you would like edge of tomorrow as well because topgun is actually a military action drama,\\nwhich is also Tom Cruise. So now again the probability Rises that yes, you will like edge of tomorrow as well and\\nIf you say no I didn't like Top Gun then again. The chances are that you wouldn't like Edge of Tomorrow, right?\\nAnd then another question that he puts you across is that do you really like to watch action movies?\\nAnd you say yes, I would love to watch them. Then again. The chances are that you would like to watch Edge of Tomorrow.\\nSo from your friend when you can come to one conclusion, I hear since the ratio of liking the movie\\nto don't like is actually 2 is to 1 so the final result. Actually, you would like Edge of Tomorrow.\\nNow you go to your second friend and you ask the same question. So now you are second friend asks you did you like far\\nand away when we went out and did the last time when we washed it and you say no I really didn't like far and away\\nthen you would say then you are definitely going to like Edge of Tomorrow. Why does so because far and away is actually\\nsince most of whom might not be knowing it so far in a ways Johner of romance and it revolves around a girl\\nand a guy Guy falling in love with each other and so on. So the probability is\\nthat you wouldn't like edge of tomorrow. So he ask you another question. Did you like Bolivian\\nand to really like to watch Tom Cruise? And you say Yes, again. The probability is\\nthat you would like to watch Edge of Tomorrow. Why because Oblivion again is a science fiction\\ncasting Tom Cruise full of strange experiences. And where Tom Cruise is the savior of the masses.\\nKind well, that is the same kind of plot in edge of tomorrow as well.\\nSo here it is pure yes that you would like to watch edge of tomorrow. So you get\\nanother second decision from your second friend. Now you go to your third friend and ask him so\\nprobably our third friend is not really interesting in having any sort of conversation with you say just simply asks you did you\\nlike Godzilla and you say no I didn't like Godzilla's we say definitely you wouldn't like\\nEdge of Tomorrow why so because Godzilla is also actually Fiction movie\\nfrom the adventure Jonah. So now you have got three results from three different decision trees from three different friends.\\nNow you compile the results of all those friends and then you make a final call that yes,\\nwould you like to watch edge of tomorrow or not? So this is some very real time and very interesting example\\nwhere you can actually Implement random Forest into ground reality.\\nNow let us look at various domains where random Forest is actually used. So because of its diversity random Forest is actually used\\nin various diverse to means like so beat banking beat medicine beat land use beat marketing name it\\nand random Forest is there so in banking particularly random Forest is being actually used to make it out\\nwhether the applicant will be a default a pair or it will be Older one\\nso that it can accordingly approve or reject the applications of loan, right?\\nSo that is how random Forest is being used in banking talking about medicine.\\nRandom. Forest is widely used in medicine field to predict beforehand. What is the probability\\nif a person will actually have a particular disease or not? Right? So it's actually used to look at the various disease Trends.\\nLet's say you want to figure out what is the probability that a person will have diabetes or not?\\nIt and so what would you do? It'd probably look at the medical history of the patient and then you will see.\\nAll right. This has been the glucose concentration. What was the BMI? What was the insulin levels\\nin the patient in the past previous three months. What is the age of this particular person and do it'll make a different decision trees based on each one\\nof these predictor variables and then you'll finally compiled the results of all those variables and then you will make a final decision.\\nAs to whether the person will have diabetes in the near future or not.\\nThat is how random Forest will be used in medicine sector now move. Random Forest is also actually used to find out the land use.\\nFor example, I want to set up a particular industry in certain area. So what would I probably look for a look for?\\nWhat is the vegetation over there? What is the Urban population over there? Right and how much is the distance\\nfrom the nearest modes of Transport like from the bus station or the railway station and accordingly.\\nI will split my parameters and I will make decision on each one of these parameters and finally I'll compile my decision of all\\nthese parameters in that will be my final outcome. So that is how I am finally going to predict\\nwhether I should put my industry at this particular location or not. Right?\\nSo these three examples have actually been of majorly Classification problem\\nbecause we are trying to classify whether or not with actually trying to answer this question\\nwhether or not right now, let's move forward and look how marketing is revolving around random Forest.\\nSo particularly in marketing we try to identify the customer churn.\\nSo this is particularly the regression kind of problem right now how let's see so customer churn\\nis nothing but actually the number of people which are actually on the number. Of customers who are losing out.\\nSo we're going out of your market. Now you want to identify what will be your customer churn in near future.\\nSo you'll most of them e-commerce Industries are actually using this like Amazon Flipkart Etc.\\nSo they particularly look at your each Behavior as to what has been your past history.\\nWhat has been your purchasing history. What do you like based on your activity around certain things around certain ads\\naround certain discounts? And I'm certain kind of materials right if you would like a particular top your activity will be more\\naround that particular top. So that is how they track each and every particular move\\nof yours and then they try to predict whether you will be moving out or not. So that is how they identify the customer churn.\\nSo these all are various domains where random Forest is used. And this is not the only list so there are\\nnumerous other examples, which actually Lee are using random forests that makes it so special actually.\\nNow, let's move forward and see how random Forest actually works. Right. So let us start with the random Forest algorithm first.\\nLet's just see it step by step as to how random Forest algorithm works.\\nSo the first step is to actually select certain M features from T. Where m is less than T.\\nSo here T is the total number of the predictor variables that you have in your data set and out of those total predictor variables.\\nYou will select some random Lisa. Um few features out of those now why we are actually selecting\\na few features only. The reason is that if you will select all the predictive variables\\nor the total predictor variables then each of your decision tree will be same\\nso we model is not actually learning something new. It is learning the same previous thing because all those decision trees will be similar right\\nif you actually split your predicted variables and you select randomly a few predicted variables.\\nNeed let's say there are 14 total number of variables and out of those who randomly pick just three right?\\nSo every time you will get a new decision tree, so there will be a variety right?\\nSo the classification model will be actually much more intelligent than the previous one.\\nNow. It has got very yet experiences. So definitely it will make different decisions each time.\\nAnd then when you will compile all those different decisions, it will be a new more. Are accurate and efficient result, right?\\nSo the first important step is to select certain number of features out of all the features now,\\nlet's move on to the second step. Let's say for any node D. Now. The first step is to calculate the best plate at that point.\\nSo, you know that decision tree how decision trees actually implemented so\\nyou pick up a the most significant variable right? And then you will split that particular node.\\nFor the child nodes, that is how the split takes place, right? So you will do it for M number of variables\\nthat you've selected. Let's say you have selected three so you will implement the split at all.\\nThose three nodes in one particular decision tree, right the third step is split up the node\\ninto two daughter nodes. So now you can split your root note into as many notes\\nas you want to but here we'll split our node into 2.2 notes as to this or that so it will be an answer.\\nIn terms of this or that right at fourth step will be to repeat all these three steps\\nthat we've done previously and we'll repeat all this splitting until we have reached all the N number of nodes, right?\\nSo we need to repeat until we have reached till the leaf nodes of a decision tree that is\\nhow we will do it right now after these four steps. We will have our one decision tree.\\nBut random Forest is actually about Decision trees. So here our fifth step will come into the picture\\nwhich will actually repeat all these previous steps for D number of times now hit these the the number\\nof decision trees. Let's say I want to implement five decision trees. So my first step will be to implement\\nall the previous steps 5 times. So the head the eye tration is 4/5 number of times right now.\\nOnce I have created these five decision trees still my task is not completed.\\nPleat yet. Now. My final task will be to compile the results of all these five different decision trees\\nand I will make a call in the majority voting right here. As you can see in this picture.\\nI had in different instances. Then I created indifferent decision trees.\\nAnd finally, I will compile the result of all these n different decision trees and I will take my call on the majority voting right.\\nSo whatever my majority vote says It will be my final result. So this is basically an overview of the random Forest algorithm\\nhow it actually works. Let's just have a look at this example to get\\nmuch better understanding of what we have learnt. So let's say I have this data set\\nwhich consists of four different instances, right? So basically it consists of the weather information\\nof previous 14 days right from D1 tildy 14, and this basically Outlook humidity and Win,\\nthis basically gives me the weather condition of those 14 days. And finally I have play\\nwhich is my target variable weather match did take place on that particular day or not right.\\nNow. My main goal is to find out whether the match will actually take place\\nif I have following these weather conditions with me on any particular day.\\nLet's say the Outlook is rainy that day and humidity is high and the wind is very weak.\\nSo now I need to predict whether I will be able to play The match that they are not all right. So this is a problem statement fine.\\nNow, let's see how random Forest is used in this to sort it out now here the first step is to actually\\nsplit my entire data set into subsets here. I have split my entire 14 variables into further\\nsmaller subsets right now these subsets may or may not overlap like there is certain overlapping between d 1\\ntill D3 and D3 till D6. Fine, so there is an overlapping of D3. So it might happen\\nthat there might be overlapping so you need not really worry about the overlapping but you have to make sure\\nthat all those subsets are actually different right? So here I have taken three different subsets\\nmy first sub set consists of D1 till D3 Mexican subset consists of D3\\ntill D6 and methods subset consists of D7 tildy. Now now I will first be focusing on my first subset now here,\\nlet's say that particular day the out It was overcast fine. If yes, it was overcast then the probabilities\\nthat the match will take place. So overcast is basically when your weather is too cloudy.\\nSo if that is the condition then definitely the match will take place and let's say it wasn't overcast.\\nThen you will consider these second most probable option that will be the wind and we will make a decision based on this now\\nwhether wind was weak or strong if wind was weak, then you And play the match else you would not. So now the final outcome out of this decision tree will be Play\\nBecause here the ratio between the play and no play is to is to 1 so we get to a certain decision from a first decision tree.\\nNow, let us look at the second subset now since second subset has different number of variables.\\nSo that is why this decision trees absolutely different from what we saw in our four subsets.\\nSo let's say if it was overcast then you will play the match. If it isn't the overcast\\nand you would go and look out for humidity now further, it will get split into two whether it was high or normal.\\nNow, we'll take the first case if the humidity was high and wind was week.\\nThen you will play the match else if humidity was high but wind was too strong,\\nthen you would not go out and play the match right now. Let us look at the second dot to node of humidity\\nif the humidity was Oil and the wind was weak then you will definitely go out and play the match\\nas you want go out and play the match. So here if you look at the final result,\\nthen the ratio of placed no play is 3 is to 2 then again. The final outcome is actually play, right?\\nSo from second subset, we get the final decision of play now, let us look at our third subset\\nwhich consists of D7 till D9 here if again the overcast is yes,\\nthen you will A match it's you will go and check out for humidity. And if the humidity\\nis really high then you won't play the match and you will play the match again the probability\\nof playing the matches. Yes, because the ratio of no play is Twist one, right?\\nSo three different subsets three different decision trees three different outcomes\\nand one final outcome after compiling all the results from these three different decision trees are so I hope\\nthis gives a better perspective a bit understanding of random Forest like how it really works.\\nAll right. So now let's just have a look at various features of random Forest Ray. So the first and the foremost feature is\\nthat it is one of the most accurate learning algorithms, right? So why it is so\\nbecause single decision trees are actually prone to having high variance\\nor Hive bias and on the contrary actually. Random Forest it averages the entire variance\\nacross the decision trees. So let's say if the variances say X4 decision tree,\\nbut for random Forest, let's say we have implemented n number of decision trees parallely.\\nSo my entire variance gets averaged to upon and my final variance actually becomes X upon n so\\nthat is how the entire variance actually goes down as compared to other algorithms.\\nThumbs right now second most important feature is that it works? Well for both classification and regression problems\\nand by far I have come across this is one and the only algorithm which works equally well for both of them.\\nBeh classification kind of problem or a regression kind of problem, right?\\nThen it's really runs efficient on large databases. So basically it's really scalable.\\nEven if you work for the lesser amount of database or if you work for really huge volume of data, right?\\nSo that's a very good part about it. Then the fourth most important point is that it requires almost no input preparation.\\nNow, why am I saying this is because it has got certain implicit methods,\\nwhich actually take care. And remove all the outliers and all the missing data and you really don't have to take care\\nabout all that thing while you are in the stages of input preparations. So Random Forest is all here to take care\\nof everything else and next. Is it performs implicit feature selection, right?\\nSo while we are implementing multiple decision trees, so it has got implicit method\\nwhich will automatically pick up some random features. Result of all your parameters and then it will go\\non and implementing different decision trees. So for example, if you just give one simple command\\nthat all right, I want to implement 500 decision trees no matter how so Random Forest will automatically take care\\nand it will Implement all those 500 decision trees and those all 500 decision trees will be different\\nfrom each other and this is because it has got implicit methods which will automatically collect different parameters.\\nHas itself out of all the variables that you have right, then it can be easily grown in parallel why it is so\\nbecause we are actually implementing multiple decision trees and all those decision trees are running\\nor all those decisions trees are actually getting implemented parallely. So if you say I want thousand trees to be implemented.\\nSo all those thousand trees are getting implemented parallely. So that is how the computation time reduces.\\nRight, and the last point is that it has got methods for balancing error\\nin unbalanced it as it's now what exactly unbalanced data sets are let me just give you an example of that.\\nSo let's say you're working on a data set fine and you create a random forest model and get\\n90% accuracy immediately. Fantastic you think right. So now you start diving deep you go a little Little deeper\\nand you discovered that ninety percent of that data actually belongs to just one class tan your entire data set\\nyour entire decision is actually biased to just one particular class.\\nSo Random Forest actually takes care of this thing and it is really not biased\\ntowards any particular decision tree or any particular variable or any class. So it has got methods which looks after it\\nand they Does all the balance of errors in your data sets? So that's pretty much\\nabout the features of random forests.\\nK-nearest neighbor is a simple algorithm which uses entire data set in its training phase when our prediction is required for unseen data.\\nWhat it does is it searches through the entire training data set for kaymu similar instances and the data with the most similar instance is finally\\nreturned as the prediction. So hello. Oh and welcome all to this YouTube session and in today's session will be dealing with KNN algorithm.\\nSo without doing any further, let's move on and discuss agenda for today's session. So we'll start our session with what is KN\\nwhere I'll brief you about the topic and we'll move ahead to see what its popular use cases\\nor how the industry is using KN for their benefit. Once we are done with it. We will drill down to the working of algorithm\\nand while learning the algorithm you will also understand the significance of K, or what does this case stands\\nfor in the nearest neighbor algorithm? Then we'll see how the prediction is made using Canon algorithm manually or mathematically.\\nAll right. Now once we are done with the theoretical concept will start the Practical or the demo session where we'll learn\\nhow to implement KNN algorithm using python. So let's start our session. So starting with what\\nis KNN algorithm will k-nearest neighbor is a simple algorithm that stores all the available cases\\nand classify the new data or case based on a similarity measure. It suggests that if you are similar to your neighbors,\\nthen you have one of them right for example, if apple looks more similar to banana orange\\nor Melon rather than a monkey rat or a cat that most likely Apple belong to the group of fruits.\\nAll right. Well in general Cayenne is used in Search application where you are looking for similar items\\nthat is when your task is some form of fine items similar to this one. Then you call this search as a Cayenne in search.\\nBut what is this KN KN? Well this K denotes the number of nearest neighbor which are voting class of the new data\\nor the testing data. For example, if k equal 1 then the Sting data are given the same label\\nas a close this example in the training set similarly. If k equal 3 the labels are the three closes classes are checked\\nand the most common label is assigned to then testing data. So this is what a KN KN algorithm means so moving on ahead.\\nLet's see some of the example of scenarios where KN is used in the industry. So, let's see the industrial application\\nof KNN algorithm starting with recommender system. Well the biggest use case of cayenne and search is a recommender system.\\nThus recommended system is like an automated. Good form of a shop counter guy when you asked him for a product not only shows you the product\\nbut also suggest you or displays your relevant set of products, which are related to the item. You're already interested in buying this KNN algorithm\\napplies to recommending products like an Amazon or for recommending media, like in case of Netflix or even for recommending advertisement\\nto display to a user if I'm not wrong almost all of you must have used Amazon for shopping, right?\\nSo just to tell you more than 35% of amazon.com revenue is generated by its recommendation engine.\\nSo what's the strategy Amazon uses recommendation as a targeted marketing tool in both the email campaigns around most\\nof its website Pages Amazon will recommend many products from different categories based on what you have browser\\nand it will pull those products in front of you which you are likely to buy like the frequently bought together option\\nthat comes at the bottom of the product page to tempt you into buying the combo. Well, this recommendation has just one main goal\\nthat is increase average order value or to upsell and cross-sell customers by providing product suggestions.\\nEastern items in the shopping cart or based on the product. They're currently looking at on site.\\nSo next industrial application of KNN algorithm is concept search or searching semantically similar documents\\nand classifying documents containing similar topics. So as you know, the data on the Internet is increasing exponentially\\nevery single second. There are billions and billions of documents on the internet each document on the internet contains multiple Concepts,\\nthat could be a potential concept. Now, this is a situation where the main problem is to Extract concept from a set of documents\\nas each page could have thousands of combination that could be potential Concepts an average document could have\\nmillions of concept combined that the vast amount of data on the web. Well, we are talking about an enormous amount\\nof data set and Sample. So what we need is we need to find a concept from the enormous amount of data set and samples, right?\\nSo for this purpose, we will be using KNN algorithm more advanced example could include handwriting detection like an OCR\\nor image recognization or even video. Organization. All right. So now that you know various use cases\\nof KNN algorithm. Let's proceed and see how does it work. So how does a KNN algorithm work?\\nLet's start by plotting these blue and orange point on our graph. So these Blue Points the belong to class A\\nand the orange ones they belong to class B. Now you get a star as a new pony and your task is to predict\\nwhether this new point it belongs to class A or it belongs to the class B. So to start the production the very first thing\\nthat you have to do is select the Value of K. Just as I told you KN KN algorithm refers to the number\\nof nearest neighbors that you want to select. For example, in this case k equal to 3.\\nSo what does it mean it means that I am selecting three points which are the least distance to the new point\\nor you can say I am selecting three different points which are closest to the star. Well at this point of time you can ask\\nhow will you calculate the least distance? So once you calculate the distance, you will get one blue\\nand two orange points which are closest to this star now. Since in this case as we have a majority of orange points,\\nso you can say that for k equal 3D star belongs to class B, or you can say that the star is more similar to the orange points\\nmoving on ahead. Well, what if k equal to 6 well for this case, you have to look for six different points\\nwhich are closest to this star. So in this case after calculating the distance, we find that we have four blue points\\nand two Orange Point which are closest to the star now as you can see that the blue points are in majority,\\nso you Can say that for k equals 6 this star belongs to class A or the star is more similar to Blue Points.\\nSo by now, I guess you know how a KNN algorithm work and what is the significance of gain KNN algorithm.\\nSo how will you choose the value of K? So keeping in mind this case the most important parameter\\nin KNN algorithm. So, let's see when you build a k nearest neighbor classifier. How will you choose a value of K?\\nWell, you might have a specific value of K in mind or you could divide up your data and use something like cross-validation technique to test several values\\nof K in order. To determine which works best for your data, for example, if n equal 2,000 cases then in that case the optimal value\\nof K lies somewhere in between 1 to 19. But yes, unless you try it you cannot be sure of it.\\nSo, you know how the algorithm is working on a higher level. Let's move on and see how things are predicted using KNN algorithm.\\nRemember I told you the KNN algorithm uses the least distance measure in order to find its nearest neighbors.\\nSo, let's see how these distance is calculated. Well, there are several distance measure which can be used.\\nSo to start with Will mainly focus on euclidean distance and Manhattan distance in this session.\\nSo what is this euclidean distance? Well, this euclidean distance is defined as the square root\\nof the sum of difference between a new point x and an existing Point why so for example here we have Point P1 and P2 Point\\nT. 1 is 1 1 and point p 2 is 5 for so what is the euclidean distance between both of them?\\nSo you can see that euclidean distance is a direct distance between two points. So what is the distance between the point P1 and P2\\nso we can calculate it as 5 minus 1 whole square plus 4 minus 1 whole square\\nand we can route it over which results to 5. So next is the Manhattan distance.\\nWell, this Manhattan distance is used to calculate the distance between real Vector using this some of their absolute difference\\nin this case the Manhattan distance between the point P1 and P2 is Mode of 5 minus 1\\nplus mod value of 4 minus 1 which results to 3 plus 4 that is 7 so this slide shows the difference\\nbetween euclidean and Manhattan distance from point A to point B. So euclidean distance is nothing but the direct\\nor the least possible distance between A and B. Whereas the Manhattan distance is a distance between A\\nand B measured along the axis at right angle. Let's take an example and see how things are predicted using KNN algorithm\\nor how the cannon algorithm is working. Suppose we have a data set which consists of height weight\\nand T-shirt size of some customers. Now when a new customer come we only have his height and weight as the information now our task is to predict.\\nWhat is the T-shirt size of that particular customer so for this will be using the KNN algorithm.\\nSo the very first thing what we need to do, we need to calculate the euclidean distance. So now that you have a new data of height 160 one centimeter\\nand weight are 61 kg. So the very first thing that we'll do is we'll calculate the euclidean distance. Stance which is nothing but the square root\\nof 160 1 minus 158 whole square plus 61 minus 58 whole square and square root of that is 4.24.\\nLet's drag and drop it. So these are the various euclidean distance of other points. Now, let's suppose k equal to 5 then the algorithm\\nwhat it does is it searches for the five customer closest to the new customer that is most similar to the new data in terms\\nof its attribute for k equal 5. Let's find the top five minimum euclidian distance. So these are the distance\\nwhich we are going to use Two three four and five. So let's rank them in the order first.\\nThis is second. This is third then this one is for again. This one is 5 so there is our order.\\nSo for k equal 5 we have for t-shirts which commanders size M and one t-shirt which comes under size l\\nso obviously best guess for the best protection for the T-shirt size of height 160 one centimeter\\nand wait 60 1 kg is M. Or you can say that a new customer Fittin to size M. Well this was all about Body theoretical session,\\nbut before we drill down to the coding part, let me just tell you why people call KN as a lazy learner.\\nWell Cannon for classification is a very simple algorithm, but that's not why they are called lazy KN is a lazy learner\\nbecause it doesn't have a discriminative function from the training data. But what it does it memorizes the training data,\\nthere is no learning phase of the model and all of the work happens at the time. Your prediction is requested.\\nSo as such there's the reason why KN is often referred to us lazy learning algorithm. So this was all about Or detail reticle session now,\\nlet's move on the coding part. So for the Practical implementation of the Hands-On part, I'll be using the IRS data set.\\nSo this data set consists of 150 observation. We have four features and one class label the four features include\\nthe sepal length sepal width petal length and the petrol head whereas the class label decides which flower belongs\\nto which category. So this was the description of the data set, which we are using now, let's move on and see what are the step\\nby step solution to perform a KNN algorithm. So first we'll start by handling the The data what we have to do we have to open the data set\\nfrom the CSV format and split the data set into train and test part next we'll take the similarity\\nwhere we have to calculate the distance between two data instances. Once we calculate the distance next.\\nWe'll look for the neighbor and select K Neighbors which are having the least distance from a new point. Now once we get our neighbor,\\nthen we'll generate a response from a set of data instances. So this will decide whether the new Point belongs to class A or Class B.\\nFinally, we'll create the accuracy function and in the end. We'll tie it all together in the main function.\\nSo let's start with our code for implementing KNN algorithm using python. I'll be using jupyter notebook python 3.0 installed on it.\\nNow, let's move on and see how can an algorithm can be implemented using python. So there's my jupyter notebook,\\nwhich is a web-based interactive Computing notebook environment with python 3.0 installed on it\\nso that the launched its launching so there's our jupyter notebook and we'll be riding our python codes on it.\\nSo the first thing that we need to do is load our file, our data is in CSV format without a header line\\nor any code we can open the file the open function and read the data line using the reader function\\nin the CSV module. So let's write a code to load our data file. Let's execute the Run button.\\nSo once you execute the Run button, you can see the entire training data set as the output next.\\nWe need to split the data into a training data set that KN can use to make prediction and a test data set\\nthat we can use to evaluate the accuracy of The module so we first need to convert the flower measure\\nthat were loaded as string into numbers that we can work. Next. We need to split the data set randomly to train and test ratio\\nof 67 is 233 for test is to train as a standard ratio, which is used for this purpose.\\nSo let's define a function as load data set that loads a CSV with the provided file named and split it randomly into training\\nand test data set using the provided split ratio. So this is our function load data set which is using filenames\\nthat ratio training data set and testing data set. As its input. All right. So let's execute the Run button and check for any errors.\\nSo it's executed with zero errors. Let's test this function. So there's our training set testing set load data set.\\nSo this is our function load data set on inside that we are passing. Our file is data with a split ratio of 0.66\\nand training data set and test data set. Let's see what our training data set and test data set its dividing into so it's giving a count\\nof training data set and testing data set. The total number of training data set as split into is 97 and total number\\nof Test data set we have is 53. So total number of training data set we have here is 97 and total\\nnumber of test data set we have here is 53. All right. Okay. So our function load data set is performing.\\nWell, so let's move on to step two which is similarity. So in order to make prediction, we need to calculate the similarity between\\nany two given data instances. This is needed so that we can locate the kamo similar data instances\\nin the training data set are in turn make a prediction given that all for flour measurement are numeric and have same unit.\\nWe can directly use the euclidean distance measure. This is nothing but the square root of the sum of squared differences between two eras\\nof the number given that all the for flower measurements are numeric and have same unit. We can directly use the euclidean distance measure\\nwhich is nothing but the square root of the sum of squared difference between two arrays or the number additionally we want to control\\nwhich field to include in the distance calculation. So specifically we only want to include first for attribute.\\nSo our approach will be to limit the euclidean distance to a fixed length. All right. So let's define our euclidean function.\\nSo these are euclidean distance function which takes instance one instance to and length as parameters instance one and instance two are the two points\\nof which you want to calculate the euclidean distance, whereas this length and denote that how many attributes you want to include.\\nOkay. So there's our euclidean function. Let's execute it. It's executing fine without any errors.\\nLet's test the function suppose the data one or the first instance consists of the data point us to to to and it belongs to class A. A\\nand data to consist of four for four and it belongs to class P. So when we calculate the euclidean distance\\nof data one to data to and what we have to do we have to consider only first three features of them.\\nAll right. So let's print the distance as you can see here. The distance comes out to be three point four six four.\\nAll right. So this is nothing but the square root of 4 minus 2 whole Square. So this distance is nothing but the euclidean distance\\nand it is calculated as square root of 4 minus 2 whole square plus 4 minus 2 whole square that is nothing but 3 times or 4 minus 2 whole That is 12 +\\nsquare root of 12 is nothing but 3.46 for all right. So now that we have calculated the distance now,\\nwe need to look for K nearest neighbors. Now that we have a similarity measure we can use it to collect\\nthe kamo similar instances for a given unseen instance. Well, this is a straightforward process of calculating the distance for all the instances\\nand selecting a subset with the smallest distance value. And now what we have to do we have to select the smallest distance values.\\nSo for that will be defining a function as get neighbors. So for that what we will be doing will be defining a function\\nas get neighbors what it will do it will return the K most similar Neighbors From the training set for a given test instance.\\nAll right. So this is how our get nabal function look like it takes training data set and test instance and K as its input here.\\nThe K is nothing but the number of nearest neighbor you want to check for. All right. So basically what you'll be getting\\nfrom this get Mabel's function is K different points having least euclidean distance from the test instance. All right, let's execute it.\\nSo the function executed without any errors. So let's test our function. Suppose the training data set includes the data like 2 to 2\\nand it belongs to class A and other data includes four four four and it belongs to class P and our testing instances five five five or now.\\nWe have to predict whether this test instance belongs to class A or it belongs to class be. All right for k equal 1 we have to predict\\nits nearest neighbor and predict whether this test instance it will belong to class A or will it belong to class be?\\nAll right. So let's execute the Run button aligned. So an executing the Run button you can see that we have output is 4 4 4\\nand B. Be a new instance 5 5 5 is closest to point 4 4 4 which belongs to class be?\\nAll right. Now once you have located the most similar neighbor for a test instance next task is to predict a response based\\non those neighbors. So how we can do that. Well, we can do this by allowing each neighbor to vote for the class attribute\\nand take the majority vote as a prediction. Let's see how we can do that. So we have a function as getresponse with takes neighbors as the input.\\nWell, this neighbor was nothing but the output of this get me / function. The output of get neighbor function will be fed\\nto get response. All right, let's execute the Run button. It's executed. Let's move ahead and test our function get response.\\nSo we have a neighbor as one one one. It belongs to class A to to to it belongs to class a33. It belongs to class B.\\nSo this response what it will do it will store the value of get response by passing this neighbor value.\\nAll right. So what we want to check is we want to predict whether that test instance five five five.\\nIt belongs to class A or Class B. Be when the neighbors are 1 1 1 a 2 2 A + 3 3 p.\\nSo let's check our response now that we have created all the different function which are required for a KNN algorithm.\\nSo important main concern is how do you evaluate the accuracy of the prediction and easy way to evaluate the accuracy of the model\\nis to calculate a ratio of the total correct prediction to all the prediction made. So for this I will be defining function\\nas get accuracy and inside that I'll be passing my test data set and the predictions get accuracy function.\\nCheck it. Executed without any error. Let's check it for a sample data set.\\nSo we have our test data set as 1 1 1 which belongs to class A 2/2 which again belongs to class 3 3 3 which belongs to class B\\nand my predictions is for first test data. It predicted latter belongs to class A which is true\\nfor next it predicted that belongs to class E, which is again to and for the next again it predictive that it belongs to class A which is false in this case\\ncause the test data belongs to class be. All right. So in total we have to correct prediction out of three.\\nAll right. Right. So the ratio will be 2 by 3, which is nothing but 66.6. So our accuracy rate is 66.6.\\nSo now that you have created all the function that are required for KNN algorithm. Let's compile them into one single main function.\\nAlright, so this is our main function and we are using Iris data set with a split of 0.67 and the value of K is 3 Let's see.\\nWhat is the accuracy score of this check how accurate are modulus so in training data set,\\nwe have 113 values and then the test data set we have Seven values. These are the predicted and the actual values\\nof the output. Okay. So in total, we got an accuracy of ninety seven point two nine percent, which is really very good.\\nAlright, so I hope the concept of this KNN algorithm is here devised in a world full of machine learning\\nand artificial intelligence surrounding almost everything around us classification and prediction is one\\nof the most important aspects of machine learning. So before moving forward, let's have a Look at the agenda.\\nI'll start of this video by explaining you guys. What exactly is Nave biased then we'll understand\\nwhat is space theorem which serves as a logic behind the name pass algorithm moving forward.\\nI'll explain the steps involved in the neighb as algorithm one by one and finally, I'll finish off this video with a demo on the Nave bass\\nusing the sklearn package noun a bass is a simple but surprisingly powerful algorithm\\nfrom predictive analysis. It is a classification technique based on base. him with an assumption\\nof Independence among predictors it comprises of two parts, which is knave\\nand bias in simple terms neighbors classifier assumes that the presence of a particular feature\\nin a class is unrelated to the presence of any other feature, even if this features depend on each other\\nor upon the existence of the other features, all of these properties independently contribute\\nto the probability whether a fruit is an apple or an orange or a banana, so That is why it\\nis known as naive now naive based model is easy to build and particularly useful for very large data sets\\nin probability Theory and statistics based theorem, which is already known as the base law\\nor the base rule describes the probability of an event based on prior knowledge of the conditions\\nthat might be related to the event now pasted here m is a way to figure out conditional probability.\\nThe conditional probability is the probability of an event happening given that it has some relationship.\\nOne or more other events, for example, your probability of getting a parking space is connected\\nto the time of the day. You park where you park and what conventions are you going on at that time\\nBayes theorem is slightly more nuanced in a nutshell. It gives you an actual probability of an event given\\ninformation about the tests. Now, if you look at the definition of Bayes theorem, we can see that given a hypothesis H\\nand the evidence e-base term states that the relationship between the E of the hypothesis before getting the evidence,\\nwhich is the P of H and the probability of the hypothesis after getting the evidence that is p of H given e is defined as probability\\nof e given H into probability of H divided by probability of e it's rather confusing, right?\\nSo let's take an example to understand this theorem. So suppose I have a deck of cards and\\nif a single card is drawn from the deck of playing cards, the probability that the card is a king is for by 52\\nsince there are four Kings in a standard deck of 52 cards. Now if King is an event, this card is a king.\\nThe probability of King is given as 4 by 52 that is equal to 1 by 13.\\nNow if the evidence is provided for instance someone looks Such as the card that the single card is a face card the probability\\nof King given that it's a face can be calculated using the base theorem by this formula.\\nNow since every King is also a face card the probability of face given that it's a king is equal to 1\\nand since there are three phase cards in each suit. That is the chat king and queen.\\nThe probability of the face card is equal to 12 by 52. That is 3 by 30.\\nNo using base certain we can find out the probability of King given that it's a face.\\nSo our final answer comes to 1 by 3, which is also true. So if you have a deck of cards,\\nwhich has having only faces now, there are three types of phases which are the chat king and queen.\\nSo the probability that it's the king is 1 by 3. Now. This is the simple example of how based on works now\\nif we look at the proof as in how this paste Serum evolved. So here we have probability of a given B\\nand probability of B given a now for a joint probability distribution over the sets A and B,\\nthe probability of a intersection B, the conditional probability of a given B is defined as the probability\\nof a intersection B divided by probability of B, and similarly probability of B, given a is defined as probability of B intersection\\na divided by probability of a now we Equate probability of a intersection p\\nand probability of B intersection a as both are the same thing now from this method\\nas you can see, we get our final base theorem proof, which is the probability of a given b equals probability of B,\\ngiven a into probability of P divided by the probability of a now while this is the equation\\nthat applies to any probability distribution over the events A and B. It has a particular nice interpretation in case\\nwhere a is represented as the hypothesis h H and B is represented\\nas some observed evidence e in that case the formula is p\\nof H given e is equal to P of e given H into probability of H divided by probability of e now this relates\\nthe probability of hypothesis before cutting the evidence, which is p of H to the probability of the hypothesis after getting the evidence\\nwhich is p of H given e for this reason P of H is known as the prior probability\\nwhile P of It's given e is known as the posterior probability and the factor that relates the two is known as the likelihood ratio Now using\\nthis term space theorem can be rephrased as the procedure probability equals. The prior probability times the likelihood ratio.\\nSo now that we know the maths which is involved behind the Bayes theorem. Let's see how we can implement this in real life scenario.\\nSo suppose we have a data set. Set in which we have the Outlook the humidity and we need to find out\\nwhether we should play or not on that day. So the Outlook can be sunny overcast rain\\nand the humidity are high normal and the wind are categorized into two phases which are the weak and the strong winds.\\nThe first of all will create a frequency table using each attribute of the data set.\\nSo the frequency table for the Outlook looks like this we have Sunny overcast and rainy the frequency table\\nof humidity looks like this. And a frequency table of when looks like this we have strong\\nand weak for wind and high and normal ranges for humidity. So for each frequency table,\\nwe will generate a likelihood table now now the likelihood table contains the probability\\nof a particular day suppose we take the sunny and we take the play as yes\\nand no so the probability of Sunny given that we play yes is 3 by 10, which is 0.3 the probability of X,\\nwhich is the probability of Sunny He is equal to 5 by 14. Now. These are all the terms\\nwhich are just generated from the data which we have here. And finally the probability of yes is 10 out of 14.\\nSo if we have a look at the likelihood of yes given that it's a sunny we can see using Bayes theorem.\\nIt's the probability of Sunny given yes into probability of yes divided by the probability of Sunny.\\nSo we have all the values here calculated. So if you put that in our base serum equation,\\nwe get the likelihood of Is a 0.59 similarly the likelihood\\nof no can also be calculated here is 0.40 now similarly. We are going to create the likelihood table\\nfor both the humidity and the win there's a for humidity the likelihood for yes given the humidity\\nis high is equal to 0.4 to and the probability of playing know\\ngiven the Venice High is 0.58 the similarly for table wind.\\nThe probability of e is given that the wind is week is 0.75 and the probability of no given\\nthat the win is week is 0.25 now suppose we have of day\\nwhich has high rain which has high humidity and the wind is weak. So should we play or not?\\nThat's all for that. We use the base theorem here again the likelihood of yes on that day is equal\\nto the probability of Outlook rain given that it's a yes into probability. Of humidity given that say yes,\\nand the probability of when that is we given that it's we are playing yes into the probability of yes,\\nwhich equals to zero point zero one nine and similarly the likelihood of know on that day is equal\\nto zero point zero one six. Now if we look at the probability of yes for that day of playing we just need to divide it\\nwith the likelihood some of both the yes and no so the probability of playing tomorrow,\\nwhich is yes is .5. Whereas the probability of not playing is equal to 0.45.\\nNow. This is based upon the data which we already have with us.\\nSo now that you have an idea of what exactly is named by as how it works and we have seen\\nhow it can be implemented on a particular data set. Let's see where it is used in the industry.\\nThe started with our first industrial use case, which is news categorized. It's move on to them or we can use\\nthe term text classification to broaden the spectrum of this algorithm news in the web are rapidly growing in the era of Information Age\\nwhere each new site has its own different layout and categorization for grouping news.\\nNow these heterogeneity of layout and categorization cannot always satisfy individual users need to remove these heterogeneity\\nand classifying the news articles. Owing to the user preference is a formidable task companies\\nuse web crawler to extract useful text from HTML Pages the news articles\\nand each of these news articles is then tokenized now these tokens are nothing but the categories of the news now\\nin order to achieve better classification result. We remove the less significant Words, which are the stop was from the documents\\nor the Articles and then we apply the Nave base classifier for classifying the news contents based on the news.\\nNow this is by far one of the best examples of Neighbors classifier,\\nwhich is Spam filtering. Now. It's the Nave Bayes classifier are a popular statistical technique for email filtering.\\nThey typically use bag-of-words features to identify at the spam email\\nand approach commonly used in text classification as well. Now it works by correlating the use of tokens,\\nbut the spam and non-spam emails and then the Bayes theorem, which I explained\\nearlier is used to calculate the probability that an email is or not a Spam so named by a Spam filtering is\\na baseline technique for dealing with Spam that container itself to the emails need of an individual user\\nand give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering\\nwith its roots in the 1990s particular words have particular probabilities of occurring in spam.\\nAnd in legitimate email as well for instance most emails users will frequently encounter the world lottery\\nor the lucky draw a spam email, but we'll sell them see it in other emails. The filter doesn't know these probabilities in advance\\nand must be friends. So it can build them up to train the filter. The user must manually indicate\\nwhether a new email is Spam or not for all the words in each straining email. The filter will adjust the probability\\nthat each word will appear in a Spam or legitimate. All in the database now after training the word probabilities also known\\nas the likelihood functions are used to compute the probability that an email with a particular set of words as in belongs\\nto either category each word in the email contributes the email spam probability. This contribution is called the posterior probability\\nand is computed again using the base 0 then the email spam probability is computed over all the verse in the email\\nand if the total exceeds a certain threshold say Or 95% the filter will Mark the email as spam.\\nNow object detection is the process of finding instances of real-world objects such as faces bicycles\\nand buildings in images or video now object detection algorithm typically use extracted features\\nand learning algorithm to recognize instance of an object category here again,\\na bias plays an important role of categorization and classification of object now medical area.\\nThis is increasingly voluminous amount of electronic data, which are becoming more and more complicated.\\nThe produced medical data has certain characteristics that make the analysis very challenging and attractive\\nas well among all the different approaches. The knave bias is used. It is the most effective and efficient classification\\nalgorithm and has been successfully applied to many medical problems empirical comparison\\nof knave bias versus five popular classifiers on Medical data sets shows\\nthat may bias is well suited for medical application and has high performance in most of the examine medical problems.\\nNow in the past various statistical methods have been used for modeling in the area of disease diagnosis.\\nThese methods require prior assumptions and are less capable of dealing with massive and complicated nonlinear and dependent data one\\nof the main advantages of neighbor as approach which is appealing to Physicians is that all the available information is used?\\nTo explain the decision this explanation seems to be natural for medical diagnosis and prognosis.\\nThat is it is very close to the way how physician diagnosed patients now weather is one\\nof the most influential factor in our daily life to an extent that it may affect the economy of a country\\nthat depends on occupation like agriculture. Therefore as a countermeasure to reduce the damage\\ncaused by uncertainty in whether Behavior, there should be an efficient way to print the weather now\\nwhether projecting has Challenging problem in the meteorological department since ears even after the technology skill\\nand scientific advancement the accuracy and production of weather has never been sufficient even\\nin current day this domain remains as a research topic in which scientists and mathematicians are working to produce a model\\nor an algorithm that will accurately predict the weather now a bias in approach based model is created by\\nwhere procedure probabilities are used to calculate the likelihood of each class label for input.\\nData instance and the one with the maximum likelihood is considered as the resulting output now earlier.\\nWe saw a small implementation of this algorithm as well where we predicted whether we should play or not based on the data,\\nwhich we have collected earlier. Now, this is a python Library which is known as scikit-learn it helps to build in a bias\\nand model in Python. Now, there are three types of named by ass model under scikit-learn Library.\\nThe first one is the caution. It is used in classification and it Assumes that the feature follow a normal distribution.\\nThe next we have is multinomial. It is used for discrete counts. For example, let's say we have a text classification problem\\nand here we consider bernouli trials, which is one step further and instead of word occurring in the document.\\nWe have count how often word occurs in the document you can think of it as a number of times outcomes number is observed\\nin the given number of Trials. And finally we have the bernouli type.\\nOf Naples, the binomial model is useful if your feature vectors are binary bag of words model\\nwhere the once and the zeros are words occur in the document and the verse which do not occur\\nin the document respectively based on their data set. You can choose any of the given discussed model here,\\nwhich is the gaussian the multinomial or the bernouli.\\nSo let's understand how this algorithm works. And what are the different steps? One can take to create a bison model and use knave bias\\nto predict the output so here to understand better. We are going to predict the onset of diabetes Now\\nthis problem comprises of 768 observations of medical details\\nfor Pima Indian patients. The record describes instantaneous measurement taken\\nfrom the patient such as the age the number of times pregnant and the blood work group\\nnow all the patients are women aged 21 and Old and all the attributes are numeric and the unit's vary\\nfrom attribute to attribute. Each record has a class value that indicate whether the patient suffered on onset of diabetes\\nwithin five years or the measurements. Now, these are classified as zero.\\nNow, I've broken the whole process down into the following steps. The first step is handling the data in which we load\\nthe data from the CSV file and split it into training and test data sets. The second step is summarizing the data.\\nIn which we summarize the properties in the training data sets so that we can calculate the probabilities and make predictions.\\nNow the third step comes is making a particular prediction. We use the summaries of the data set to generate a single prediction.\\nAnd after that we generate predictions given a test data set and a summarize training data sets.\\nAnd finally we evaluate the accuracy of the predictions made for a test data set as the percentage correct out of all the predictions made\\nand finally We tied together and form. Our own model of nape is classifier.\\nNow. The first thing we need to do is load our data the data is in the CSV format without a header line\\nor any codes. We can open the file with the open function and read the data lines using the read functions\\nin the CSV module. Now, we also need to convert the attributes that were loaded as strings into numbers\\nso that we can work with them. So let me show you how this can be implemented now for that you need to Tall python\\non a system and use the jupyter notebook or the python shell.\\nHey, I'm using the Anaconda Navigator which has all the things required to do the programming in Python.\\nWe have the Jupiter lab. We have the notebook. We have the QT console. Even we have a studio as well.\\nSo what you need to do is just install the Anaconda Navigator it comes with the pre installed python also,\\nso the moment you click launch on The jupyter Notebook. It will take you to the Jupiter homepage\\nin a local system and here you can do programming in Python. So let me just rename it as by my India diabetes.\\nSo first, we need to load the data set. So I'm creating here a function load CSV now before that.\\nWe need to import certain CSV the math and the random method. So as you can see,\\nI've created a load CSV function which will take the pie my Indian diabetes data dot CSV file using the CSV dot reader method\\nand then we are converting every element of that data set into float originally all the Ants are in string,\\nbut we need to convert them into floor for our calculation purposes. Now next we need to split the data into training data sets\\nthat nay bias can use to make the prediction and this data set that we can use to evaluate the accuracy of the model.\\nWe need to split the data set randomly into training and testing data set in the ratio of usually\\nwhich is 70 to 30, but for this example, I am going to use 67 and 33 now 70 and 30 is a Ratio for testing algorithms\\nso you can play around with this number. So this is our split data set function.\\nNow the Navy base model is comprised of summary of the data in the training data set. Now this summary is then used while making predictions.\\nNow the summary of the training data collected involves the mean the standard deviation of each attribute by class value now, for example,\\nif there are two class values and seven numerical attributes, then we need a mean\\nand the standard deviation for each of these seven attributes and the class value which makes The 14 attribute summaries\\nso we can break the preparation of this summary down into the following sub tasks which are the separating data by class calculating mean\\ncalculating standard deviation summarizing the data sets and summarizing attributes by class.\\nSo the first task is to separate the training data set instances by class value so that we can calculate statistics for each class.\\nWe can do that by creating a map of each class value to a list of instances that belong to the class.\\nClass and sort the entire dataset of instances into the appropriate list. Now the separate by class function just the same.\\nSo as you can see the function assumes that the last attribute is the class value the function returns a map of class value to the list\\nof data instances next. We need to calculate the mean of each attribute for a class value.\\nNow, the mean is the central middle or the central tendency of the data and we use it as a middle\\nof our gaussian distribution when Calculating the probabilities. So this is our function for mean now.\\nWe also need to calculate the standard deviation of each attribute for a class value. The standard deviation is calculated as a square root\\nof the variance and the variance is calculated as the average of the squared differences\\nfor each attribute value from the mean now one thing to note that here is that we are using n minus one method\\nwhich subtracts one from the number of attributes values when calculating the variance.\\nThe now that we have the tools to summarize the data for a given list of instances, we can calculate the mean and standard deviation\\nfor each attribute. Now that's if function groups the values for each attribute across our data instances into their own lists\\nso that we can compute the mean and standard deviation values for each attribute. The next comes the summarizing attributes by class.\\nWe can pull it all together by first separating our training data sets into instances growth by class then calculating the summaries\\nfor each a To be with now. We are ready to make predictions using the summaries prepared from our training data making predictions involves\\ncalculating the probability that a given data instance belong to each class then selecting the class\\nwith the largest probability as a prediction. Now we can divide this whole method into four tasks\\nwhich are the calculating gaussian probability density function calculating class probability making a prediction\\nand then estimating the accuracy now to calculate the gaussian probability density function.\\nWe use the gaussian function to estimate the probability of a given attribute value given the node mean\\nand the standard deviation of the attribute estimated from the training data. As you can see the parameters are x\\nmean and the standard deviation now in the calculate probability function, we calculate the exponent first then calculate the main division\\nthis lets us fit the equation nicely into two lines. Now, the next task\\nis calculating the class properties now that we had can calculate the probability of an attribute\\nbelonging to a class. We can combine the probabilities of all the attributes values for a data instance\\nand come up with a probability of the entire. Our data instance belonging to the class. So now that we have calculated the class properties.\\nIt's time to finally make our first prediction now, we can calculate the probability of the data instance belong\\nto each class value and we can look for the largest probability and return the associated class\\nand for that we are going to use this function to predict which uses the summaries and the input Vector which is basically all the probabilities\\nwhich are being input for a particular label now finally we can An estimate the accuracy\\nof the model by making predictions for each data instances in our test data for that.\\nWe use the cat predictions method. Now this method is used to calculate the predictions based upon the test data sets\\nand the summary of the training data set. Now, the predictions can be compared to the class values in our test data set\\nand classification accuracy can be calculated as an accuracy ratio between the zeros and the hundred percent.\\nNow the get accuracy method will calculate this accuracy ratio. Now finally to sum it all up.\\nWe Define our main function we call all these methods which we have defined earlier one by one to get\\nthe Courtesy of the model which we have created. So as you can see, this is our main function in which we have the file name.\\nWe have defined the split ratio. We have the data set. We have the training and test data set.\\nWe are using the split data set method next. We are using the summarized by class function using\\nthe get prediction and the get accuracy method as well. So guys as you can see the output of this one gives us\\nthat we are splitting the seven sixty eight rows into 514 which is the training and 254\\nwhich is the test data set rows and the accuracy of this model is 68% Now we can play with the amount of training\\nand test data sets which are to be used so we can change the split ratio to seventies.\\n238 is 220 to get different sort of accuracy. So suppose I change the split ratio from 0.67 20.8.\\nSo as you can see, we get the accuracy of 62 percent. So splitting it into 0.67 gave us a better result\\nwhich was 68 percent. So this is how you can Implement Navy bias caution classifier.\\nThese are the step by step methods which you need to do in case of using the Nave Bayes classifier,\\nbut don't worry. We do not need to write all this many lines of code to make a model this with The Sacketts.\\nAnd I really comes into picture the scikit-learn library has a predefined method\\nor as say a predefined function of neighbor bias, which converts all of these lines,\\nof course into merely just two or three lines of codes. So, let me just open another jupyter notebook.\\nSo let me name it as sklearn a pass. Now here we are going to use the most famous data set\\nwhich is the iris dataset. Now, the iris flower data set is a multivariate\\ndata set introduced by the British statistician and biologists Roland Fisher and based on this fish is linear discriminant model this data set\\nbecame a typical test case for many statistical classification techniques in machine learning.\\nSo here we are going to use the caution NB model, which is already available in the sklearn.\\nAs I mentioned earlier, there were three types of Neighbors which are the question multinomial and the bernouli.\\nSo here we are going to use the caution and be model which is already present in the sklearn library,\\nwhich is the cycle learn Library. So first of all, what we need to do is import the sklearn data sets\\nand the metrics and we also need to import the caution and be Now\\nonce all these libraries are lowered we need to load the data set which is the iris dataset.\\nThe next what we need to do is fit a Nave by a small to this data set. So as you can see we have so easily defined the model\\nwhich is the gaussian NB which contains all the programming which I just showed you earlier all the methods\\nwhich are taking the input calculating the mean the standard deviation separating it bike last\\nand finally making predictions. Calculating the prediction accuracy. All of this comes under the caution and be method\\nwhich is inside already present in the sklearn library. We just need to fit it according to the data set\\nwhich we have so next if we print the model we see which is the gaussian NB model.\\nThe next what we need to do is make the predictions. So the expected output is data set dot Target\\nand the projected is using the pretend model and the model we are using is the cause in NB here.\\nHere now to summarize the model which created we calculate the confusion Matrix\\nand the classification report. So guys, as you can see the classification to provide\\nwe have the Precision of Point Ninety Six, we have the recall of 0.96.\\nWe have the F1 score and the support and finally if we print our confusion Matrix,\\nas you can see it gives us this output. So as you can see using the gaussian and we method just putting it in the model\\nand using any of the data. Fitting the model which you created into a particular data set\\nand getting the desired output is so easy with the scikit-learn library.\\nSo guys, this is it. I hope you understood a lot about the nape Bayes classifier how it is used\\nwhere it is used and what are the different steps involved in the classification technique\\nand how the scikit-learn makes all of those techniques very easy to implement in any data set which we have.\\nAs we M or support Vector machine is one of the most effective machine learning classifier\\nand it has been used in various Fields such as face recognition cancer classification and so on today's session\\nis dedicated to how svm works the various features of svm and how it is used in the real world.\\nSo without any further due let's take a look at the agenda for today. We're going to begin the session\\nwith an introduction to machine learning and the different types of machine learning. Next we'll discuss\\nwhat exactly support Vector machines are and then we'll move on and see how svm works\\nand how it can be used to classify linearly separable data will also briefly discuss about\\nhow nonlinear svm's work and then we'll move on and look at the use case of svm in colon cancer classification\\nand finally we'll end the session by running a demo where we'll use svm to predict whether a patient is suffering from a heart disease or not.\\nOkay, so that was the agenda. Let's get stood with our first topic. So what is machine learning machine learning is a science\\nof getting computers to act by feeding them data and letting them learn a few tricks on their own.\\nOkay, we're not going to explicitly program the machine instead. We're going to feed it data and let it learn\\nthe key to machine learning is the data machines learn just like us humans. We humans need to collect information\\nand data to learn similarly machines must also be fed data in order to learn and make decisions.\\nLet's say that you want a machine to predict the value of a stock. All right in such situations.\\nYou just feed the machine with relevant data after which you develop a model which is used to predict the value of the stock.\\nNOW one thing to keep in mind is the more data you feed the machine the better it will learn and make more accurate predictions obviously machine\\nlearning is not so simple in order for a machine to analyze and get useful insights from data.\\nIt must process and study the data by running different. Algorithms on it. All right. And today we'll be discussing about one of the most widely\\nused algorithm called the support Vector machine. Okay. Now that you have a brief idea about what machine learning is,\\nlet's look at the different ways in which machines Lon first. We have supervised learning in this type\\nof learning the machine learns under guidance. All right, that's why it's called supervised learning\\nnow at school. Our teachers guided us and taught us similarly in supervised learning machines\\nlearn by feeding them labeled data. Explicitly telling them. Hey, this is the input and this is\\nhow the output must look. Okay. So guys the teacher in this case is the training data.\\nNext we have unsupervised learning here. The data is not labeled and there is no guide of any sort.\\nOkay, the machine must figure out the data set given and must find hidden patterns in order to make predictions\\nabout the output an example of unsupervised learning is an adult's like you and me.\\nWe don't need a guide to help us with our daily activities. They figured things out on our own without any supervision.\\nAll right, that's exactly how I'm supervised learning work. Finally. We have reinforcement learning.\\nLet's say you were dropped off at an isolated island. What would you do now initially you would panic\\nand you'll be unsure of what to do where to get food from How To Live and all of that but after a while you will have to adapt you must learn\\nhow to live in the island adapt to the changing climate learn what to eat and what not to eat.\\nYou're basically following the hit and trial. Because you're new to the surrounding and the only way to learn is experience and then learn\\nfrom your experience. This is exactly what reinforcement learning is. It is a learning method wherein an agent interacts\\nwith its environment by producing actions and discovers errors or words. Alright, and once it gets trained it gets ready to predict\\nthe new data presented to it. Now in our case the agent was you basically stuck\\non the island and the environment was the island. All right? Okay now now let's move on and see\\nwhat svm algorithm is all about. So guys svm or support Vector machine is a supervised learning algorithm,\\nwhich is mainly used to classify data into different classes now unlike most algorithms svm makes use of a hyperplane\\nwhich acts like a decision boundary between the various classes in general svm can be used to generate\\nmultiple separating hyperplanes so that the data is divided into segments.\\nOkay and each These segments will contain only one kind of data. It's mainly used for classification purpose wearing you want to classify\\nor data into two different segments depending on the features of the data. Now before moving any further,\\nlet's discuss a few features of svm. Like I mentioned earlier svm is a supervised learning algorithm.\\nThis means that svm trains on a set of labeled data svm studies the label training data\\nand then classifies any new input data depending on what it learned in the training.\\nIn Phase a main advantage of support Vector machine is that it can be used for both classification\\nand regression problems. All right. Now even though svm is mainly known for classification the svr\\nwhich is the support Vector regressor is used for regression problems. All right, so svm can be used both for classification.\\nAnd for regression. Now, this is one of the reasons why a lot of people prefer svm because it's a very good classifier and along with that.\\nIt is also used for regression. Another feature is the svm kernel functions svm can be used\\nfor classifying nonlinear data by using the kernel trick the kernel trick basically means\\nto transform your data into another dimension so that you can easily draw a hyperplane\\nbetween the different classes of the data. Alright, nonlinear data is basically data\\nwhich cannot be separated with a straight line. Alright, so svm can even be used on nonlinear data sets.\\nYou just have to use a kernel functions to do this. All right, so Guys, I hope you all are clear with the basic concepts of svm.\\nNow. Let's move on and look at how svm works so guys an order to understand how svm Works let's consider a small scenario now\\nfor a second pretend that you own a firm. Okay, and let's say that you have a problem and you want to set up a fence to protect your rabbits\\nfrom the pack of wolves. Okay, but where do you build your fence one way to get around?\\nThe problem is to build a classifier based on the position of the rabbits and words in your Faster.\\nSo what I'm telling you is you can classify the group of rabbits as one group and draw a decision boundary between the rabbits\\nand the world. All right. So if I do that and if I try to draw a decision boundary\\nbetween the rabbits and the Wolves, it looks something like this. Okay. Now you can clearly build a fence along this line\\nin simple terms. This is exactly how SPM work it draws a decision boundary,\\nwhich is a hyperplane between any two classes in order to separate them or class. Asif I them now,\\nI know you're thinking how do you know where to draw a hyperplane the basic principle behind svm is to draw a hyperplane\\nthat best separates the two classes in our case the two glasses of the rabbits and the Wolves.\\nSo you start off by drawing a random hyperplane and then you check the distance between the hyperplane\\nand the closest data points from each glove these closes on your is data points to the hyperplane are known as support vectors and that's\\nwhere the name comes from support. Active machine. So basically the hyperplane is drawn\\nbased on these support vectors. So guys an Optimum hyperplane will have a maximum distance from each of these support vectors.\\nAll right. So basically the hyper plane which has the maximum distance from the support vectors is the most optimal hyperplane\\nand this distance between the hyperplane and the support vectors is known as the margin.\\nAll right. So to sum it up svm is used to classify data by using a hyper plane such\\nthat the distance distance between the hyperplane and the support vectors is maximum. So basically your margin has to be maximum.\\nAll right, that way, you know that you're actually separating your classes or add because the distance between the two classes is maximum.\\nOkay. Now, let's try to solve a problem. Okay. So let's say that I input a new data point.\\nOkay. This is a new data point and now I want to draw a hyper plane such that it best separates the two classes.\\nOkay, so I start off by drawing a hyperplane like this and then I check the distance between Hyper plane\\nand the support vectors. Okay, so I'm trying to check if the margin is maximum for this hyperplane,\\nbut what if I draw a hyper plane which is like this? All right. Now I'm going to check the support vectors over here.\\nThen I'm going to check the distance from the support vectors and with this hyperplane, it's clear that the margin is more right\\nwhen you compare the margin of the previous one to this hyperplane. It is more. So the reason why I'm choosing this hyperplane is\\nbecause the distance between the support vectors and the hi Hyperplane is maximum in this scenario.\\nOkay, so guys this is how you choose a hyperplane. You basically have to make sure that the hyper plane has a maximum.\\nMargin. All right, it has two best separate the two classes. All right. Okay so far it was quite easy.\\nOur data was linearly separable which means that you could draw a straight line to separate the two classes.\\nAll right, but what will you do? If the data set is like this you possibly can't draw a hyper plane like this.\\nAll right. It doesn't separate the two. At all, so what do you do in such situations now earlier in the session I mentioned\\nhow a kernel can be used to transform data into another dimension that has a clear dividing margin between the classes of data.\\nAlright, so kernel functions offer the user this option of transforming nonlinear spaces into linear ones.\\nNonlinear data set is the one that you can't separate using a straight line. All right, in order to deal with such data sets you're going\\nto Ants form them into linear data sets and then use svm on them. Okay. So simple trick would be to transform the two variables\\nX and Y into a new feature space involving a new variable called Z. All right, so guys so far we were plotting our data\\non two dimensional space. Correct? We will only using the X and the y axis so we had only those two variables X and Y now\\nin order to deal with this kind of data a simple trick would be to transform the two variables X\\nand I into a new feature space involving a new variable called Z. Ok, so we're basically visualizing the data\\non a three-dimensional space. Now when you transform the 2D space into a 3D space,\\nyou can clearly see a dividing margin between the two classes of data right now. You can go ahead and separate the two classes\\nby drawing the best hyperplane between them. Okay, that's exactly what we discussed in the previous slides.\\nSo guys, why don't you try this yourself dry drawing a hyperplane, which is the most Optimum. For these two classes.\\nAll right, so guys, I hope you have a good understanding about nonlinear svm's now. Let's look at a real world use case of support Vector machines.\\nSo guys s VM as a classifier has been used in cancer classification\\nsince the early 2000s. So there was an experiment held by a group of professionals\\nwho applied svm in a colon cancer tissue classification. So the data set consisted\\nof about 2,000 transmembrane protein samples and Only about 50 to 200 genes samples were input\\nInto the svm classifier Now this sample which was input into the svm classifier had both colon cancer tissue samples\\nand normal colon tissue samples right now. The main objective of this study was to classify Gene samples\\nbased on whether they are cancerous or not. Okay, so svm was trained using the 50 to 200 samples\\nin order to discriminate between non-tumor from tumor specimens. So the performance of The svm classifier\\nwas very accurate for even a small data set. All right, we had only 50 to 200 samples.\\nAnd even for the small data set svm was pretty accurate with its results. Not only that its performance was compared\\nto other classification algorithm like naive Bayes and in each case svm outperform naive Bayes.\\nSo after this experiment it was clear that svm classify the data more effectively\\nand it worked exceptionally good with small data sets.\\nLet's go ahead and understand what exactly is unsupervised learning. So sometimes the given data is unstructured and unlabeled\\nso it becomes difficult to classify the data into different categories. So unsupervised learning helps to solve this problem.\\nThis learning is used to Cluster the input data in classes on the basis of their statistical properties.\\nSo example, we can cluster Different Bikes based upon the speed limit their acceleration\\nor the average. Average that they are giving so and suppose learning is a type of machine learning algorithm\\nused to draw inferences from data sets consisting of input data without labels responses.\\nSo if you have a look at the workflow or the process flow of unsupervised learning, so the training data is collection of information\\nwithout any label. We have the machine learning algorithm and then we have the clustering malls.\\nSo what it does is that distributes the data into different clusters and again if you provide any Lebanon new data,\\nit will make a prediction and find out to which cluster that particular data or the data set belongs\\nto or the particular data point belongs to so one of the most important algorithms in unsupervised learning is clustering.\\nSo let's understand exactly what is clustering. So a clustering basically is the process of dividing the data sets\\ninto groups consisting of similar data points. It means grouping of objects based\\non the information found in the data describing the objects or their relationships,\\nso So clustering malls focus on and defying groups of similar records and labeling records according to the group\\nto which they belong now. This is done without the benefit of prior knowledge about the groups\\nand their creator districts. So and in fact, we may not even know exactly how many groups are\\nthere to look for. Now. These models are often referred to as unsupervised learning models,\\nsince there's no external standard by which to judge the malls classification performance.\\nThere are no right or wrong answers to these model and if we talk about why clustering is used\\nso the goal of clustering is to determine the intrinsic growth in a set of unlabeled data sometime.\\nThe partitioning is the goal or the purpose of clustering algorithm is to make sense\\nof and exact value from the last set of structured and unstructured data.\\nSo that is why clustering is used in the industry. And if you have a look at the various use cases\\nof clustering in Industry so first of all, it's being used in marketing. So discovering distinct groups\\nin customer databases such as customers who make a lot of long distance calls customers\\nwho use internet more than cause they're also using insurance companies for like identifying groups\\nof Corporation insurance policy holders with high average claim rate Farmers crash cops,\\nwhich is profitable. They are using C Smith studies and Define probability areas of oil or gas exploration based.\\nDon't cease make data and they're also used in the recommendation of movies.\\nIf you'd say they are also used in Flickr photos. They also used by Amazon for recommending the product which category it lies in.\\nSo basically if we talk about clustering there are three types of clustering. So first of all, we have the exclusive clustering\\nwhich is the hard clustering so here and item belongs exclusively to one cluster not several clusters\\nand the datapoint belong exclusively to one cluster. ER so an example of this is the k-means clustering so\\nclaiming clustering does this exclusive kind of clustering so secondly, we have overlapping clustering\\nso it is also known as soft clusters in this and item can belong to multiple clusters as its degree of association\\nwith each cluster is shown and for example, we have fuzzy or the c means clustering\\nwhich has been used for overlapping clustering and finally we have the hierarchical clustering\\nso When two clusters have a parent-child relationship or a tree-like structure,\\nthen it is known as hierarchical cluster. So as you can see here from the example, we have a parent-child kind\\nof relationship in the cluster given here. So let's understand what exactly is K means clustering.\\nSo today means clustering is an Enquirer them whose main goal is to group similar elements of data points into a cluster\\nand it is a process by which objects are classified into a predefined number of groups\\nso that they They are as much just similar as possible from one group to another group\\nbut as much as similar or possible within each group now if you have a look at the algorithm working here,\\nyou're right. So first of all, it starts with and defying the number of clusters,\\nwhich is K that I can we find the centroid we find that distance objects to the distance object\\nto the centroid distance of object to the centroid. Then we find the grouping based on the minimum distance.\\nPast the centroid Converse if true then we make a cluster false. We then I can't find the centroid repeat\\nall of the steps again and again, so let me show you how exactly clustering was with an example here.\\nSo first we need to decide the number of clusters to be made now another important task here is\\nhow to decide the important number of clusters or how to decide the number of classes will get into that later.\\nSo first, let's assume that the number of clusters we have decided. It is three. So after that then we provide the centroids\\nfor all the Clusters which is guessing and the algorithm calculates the euclidean distance\\nof the point from each centroid and assize the data point to the closest cluster now euclidean distance.\\nAll of you know is the square root of the distance the square root of the square of the distance.\\nSo next when the centroids are calculated again, we have our new clusters for each data point then again the distance from the points.\\nTo the new classes are calculated and then again the points are assigned to the closest cluster.\\nAnd then again, we have the new centroid scattered and now these steps are repeated\\nuntil we have a repetition the centroids or the new centralized are very close to the very previous ones.\\nSo until unless our output gets repeated or the outputs are very very close enough.\\nWe do not stop this process. We keep on calculating the euclidean distance of all the points to the centroid.\\nIt's then we calculate the new centroids and that is how K means clustering Works basically,\\nso an important part here is to understand how to decide the value of K or the number of clusters\\nbecause it does not make any sense. If you do not know how many classes are you going to make?\\nSo to decide the number of clusters? We have the elbow method. So let's assume first\\nof all compute the sum squared error, which is sse4 some value of a for example.\\nTake two four six and eight now the SSE which is the sum squared error is defined as a sum\\nof the squared distance between each number member of the cluster and its centroid mathematically and\\nif you mathematically it is given by the equation which is provided here. And if you brought the key against the SSE,\\nyou will see that the error decreases as K gets large not this is because the number of cluster increases\\nthey should be smaller. So the Distortion is also smaller know. The idea of the elbow method is to choose the K at which\\nthe SSE decreases abruptly. So for example here if we have a look at the figure given here.\\nWe see that the best number of cluster is at the elbow as you can see here the graph here changes abruptly\\nafter the number four. So for this particular example, we're going to use for as a number of cluster.\\nSo first of all while working with k-means clustering there are two key points to know first of all,\\nBe careful about where you start so choosing the first center at random during the second center.\\nThat is far away from the first center similarly choosing the NIH Center as far away as possible from the closest\\nof the of the other centers and the second idea is to do as many runs of k-means each with different random starting points\\nso that you get an idea of where exactly and how many clusters you need to make and where exactly the centroid lies\\nand how the data is getting converted. Divorced now k-means is not exactly a very good method.\\nSo let's understand the pros and cons of k-means clustering. We know that k-means is simple and understandable.\\nEveryone learns to the first go the items automatically assigned to the Clusters.\\nNow if we have a look at the cons, so first of all one needs to define the number of clusters,\\nthere's a very heavy task asks us if we have three four or if we have 10 categories, and if you do not know\\nwhat the number of clusters are going to be. It's very difficult for anyone. You know to guess the number of clusters not all the items\\nare forced into clusters whether they are actually belong to any other cluster or any other category.\\nThey are forced to rely in that other category in which they are closest to this against happens because of the number\\nof clusters with not defining the correct number of clusters or not being able to guess the correct number of clusters.\\nSo and for most of all, it's unable to handle the noisy data and the outliners because anyways machine learning engineers and date.\\nOur scientists have to clean the data. But then again it comes down to the analysis\\nwhat they're doing and the method that they are using so typically people do not clean the data\\nfor k-means clustering or even if the clean there's sometimes a now see noisy\\nand outliners data which affect the whole model so that was all for k-means clustering.\\nSo what we're going to do is now use k-means clustering for the movie datasets,\\nso, Have to find out the number of clusters and divide it accordingly.\\nSo the use case is that first of all, we have a data set of five thousand movies.\\nAnd what we want to do is grip them if the movies into clusters based on the Facebook likes,\\nso guys, let's have a look at the demo here. So first of all, what we're going to do is import deep copy numpy pandas\\nSeaborn the various libraries, which we're going to use now and from my proclivities in the use ply plot.\\nAnd we're going to use this ggplot and next what we're going to do is import the data set and look\\nat the shape of the data set. So if you have a look at the shape of the data set we can see that it has 5043 rose with 28 columns.\\nAnd if you have a look at the head of the data set we can see it just 5043 data points,\\nso George we going to do is place the data points in the plot we take the director Facebook likes\\nand we have a look at the data columns face number in post cars\\ntotal Facebook likes director Facebook likes. So what we have done here\\nnow is taking the director Facebook likes and the actor three Facebook likes, right.\\nSo we have five thousand forty three rows and two columns Now using the k-means from sklearn\\nwhat we're going to do is import it. First we're going to import k-means from scale and Dot cluster.\\nRemember guys eschaton is a very important library in Python for machine learning.\\nSo and the number of cluster what we're going to do is provide as five now this again,\\nthe number of cluster depends upon the SSE, which is the sum of squared errors all the we're going to use the elbow method.\\nSo I'm not going to go into the details of that again. So we're going to fit the data into the k-means to fit and\\nif you find the cluster, Us than for the k-means and printed. So what we find is is an array of five clusters\\nand Fa print the label of the k-means cluster. Now next what we're going to do is plot the data\\nwhich we have with the Clusters with the new data clusters, which we have found and for this we're going to use the CC Bond\\nand as you can see here, we have plotted that car. We have plotted the data\\ninto the grid and you can see here we have five clusters. So probably what I would say is\\nthat the cluster 3 and the cluster zero are very very close.\\nSo it might depend see that's exactly what I was going to say. Is that initially the main Challenge\\nand k-means clustering is to define the number of centers which are the K. So as you can see here\\nthat the third Center and the zeroth cluster the third cluster and the zeroth cluster up very very close to each other.\\nSo guys It probably could have been in one another cluster and the another disadvantage was that we do not exactly know\\nhow the points are to be arranged. So it's very difficult to force the data into any other cluster\\nwhich makes our analysis a little different works fine. But sometimes it might be difficult to code\\nin the k-means clustering now, let's understand what exactly is c means clustering.\\nSo the fuzzy see means is an extension of the k-means clustering the popular simple.\\nClustering technique so fuzzy clustering also referred as soft clustering is a form\\nof clustering in which each data point can belong to more than one cluster.\\nSo k-means tries to find the heart clusters where each point belongs to one cluster.\\nWhereas the fuzzy c means discovers the soft clusters in a soft cluster any point can belong\\nto more than one cluster at a time with a certain Affinity value towards each 4zc means assigns the degree of membership,\\nwhich Just from 0 to 1 to an object to a given cluster. So there is a stipulation that the sum of Z membership\\nof an object to all the cluster. It belongs to must be equal to 1 so the degree of membership\\nof this particular point to pull of these clusters as 0.6 0.4. And if you add up we get 1\\nso that is one of the logic behind the fuzzy c means so and and this Affinity is proportional to the distance\\nfrom the point to the center of a cluster now then again We have the pros and cons of fuzzy see means.\\nSo first of all, it allows a data point to be in multiple cluster. That's a pro. It's a more neutral representation of the behavior\\nof jeans jeans usually are involved in multiple functions. So it is a very good type of clustering\\nwhen we're talking about genes First of and again, if we talk about the cons again,\\nwe have to Define c which is the number of clusters same as K next. We need to determine the membership cutoff value also,\\nso that takes a lot of I'm and it's time-consuming and the Clusters are sensitive to initial assignment of centroid.\\nSo a slight change or deviation from the center's it's going to result in a very different kind of, you know,\\na funny kind of output with that from the fuzzy see means and one of the major disadvantage of c means clustering is\\nthat it's this a non-deterministic algorithm. So it does not give you a particular output as\\nin such that's that now let's have a look at At the throat type of clustering which is the hierarchical clustering.\\nSo hierarchical clustering is an alternative approach which builds a hierarchy from the bottom up\\nor the top to bottom and does not require to specify the number of clusters beforehand.\\nNow, the algorithm works as in first of all, we put each data point in its own cluster and if I the closest to Cluster\\nand combine them into one more cluster repeat the above step till the data points are in a single cluster.\\nNow, there are two types of hierarchical clustering one is I've number 80 plus string and the other one is division clustering.\\nSo a cumulative clustering bills the dendogram from bottom level while the division clustering it starts all the data points\\nin one cluster the fruit cluster now again hierarchical clustering also has some sort of pros and cons.\\nSo in the pros don't know Assumption of a particular number of cluster is required and it may correspond to meaningful tax anomalies.\\nWhereas if we talk about the cons once a decision is made to combine two clusters. It cannot be undone and one\\nof the major disadvantage of these hierarchical clustering is that it becomes very slow. If we talked about very very large data sets and nowadays.\\nI think every industry are using last year as it's and collecting large amounts of data.\\nSo hierarchical clustering is not the act or the best method someone might need to go for so there's\\nthat Hello everyone\\nand welcome to this interesting session on a prairie algorithm. Now many of us have visited retails shops such as\\nWalmart or Target for our household needs. Well, let's say that we are planning to buy a new iPhone from Target.\\nWhat we would typically do is search for the model by visiting the mobile section of the stove and then select the product\\nand head towards the billing counter. But in today's world the goal of the organization is to increase the revenue.\\nCan this be done by just pitching one? I worked at a time to the customer. Now. The answer to Is is clearly no hence organization began\\nmining data relating to frequently bought items. So a Market Basket analysis is one of the key techniques\\nused by large retailers to uncover associations between items now examples could be the customers\\nwho purchase Bread have a 60 percent likelihood to also purchase Jam customers\\nwho purchase laptops are more likely to purchase laptop bags as well. They try to find out\\nassociations between different items and products that can be sold together which gives assisting in the right product placement.\\nTypically, it figures out what products are being bought together and organizations can place products in a similar manner,\\nfor example, people who buy bread also tend to buy butter, right and the marketing team\\nat retail stores should Target customers who buy bread and butter and provide an offer to them\\nso that they buy a But item suppose X so if a customer buys bread\\nand butter and sees a discount offer on X, he will be encouraged to spend more and buy the eggs\\nand this is what Market Basket analysis is all about. This is what we are going to talk about in this session,\\nwhich is Association rule Mining and the a prayer real Corinth now Association rule can be thought of as\\nan if-then relationship just to elaborate on that. We have come up with a rule suppose\\nif an item a is Been bought by the customer. Then the chances of Item B being picked by the customer to under\\nthe same transaction ID is found out you need to understand here that it's not a cash reality rather.\\nIt's a co-occurrence pattern that comes to the force. Now, there are two elements to this rule first if and second is the then now\\nif is also known as antecedent. This is an item or a group of items\\nthat are typically found in the item set and the later one. Is called the consequent this comes along as an item\\nwith an antecedent group or the group of antecedents a purchase. Now if we look at the image here a arrow B,\\nit means that if a person buys an item a then he will also buy an item b or he will most probably by an item B.\\nNow the simple example that I gave you about the bread-and-butter and the x is just a small example,\\nbut what if you have thousands and thousands of items if you go to any proof additional data scientist\\nwith that data, you can just imagine how much of profit you can make if the data scientist provides you with the right examples\\nand the right placement of the items, which you can do and you can get a lot of insights.\\nThat is why Association rule mining is a very good algorithm which helps the business make profit.\\nSo, let's see how this algorithm works. So Association rule mining is all about building the rules\\nand we have just seen one rule that If you buy a then there's a slight possibility\\nor there is a chance that you might buy be also this type of a relationship in which we can find the relationship\\nbetween these two items is known as single cardinality, but what if the customer\\nwho bought a and b also wants to buy C or if a customer who bought a b and c also wants to buy D. Then\\nin these cases the cardinality usually increases and we can have a lot of combination around.\\nThese data and if you have around 10,000 or more than 10,000 data\\nor items just imagine how many rules you're going to create for each product.\\nThat is why Association rule mining has such measures so that we do not end up creating tens of thousands of rules.\\nNow that is where the a priori algorithm comes in. But before we get into the a priori algorithm,\\nlet's understand. What's the maths behind it. Now there are three types of matrices.\\nWhich help to measure the association? We have support confidence and lift.\\nSo support is the frequency of item a or the combination of item ARB.\\nIt's basically the frequency of the items, which we have bought and what are the combination of the frequency of the item.\\nWe have bought. So with this what we can do is filter out the items, which have been bought less frequently.\\nThis is one of the measures which is support now what confidence tells us so conference.\\nGives us how often the items NB occur together given the number of times a occur.\\nNow this also helps us solve a lot of other problems because if somebody is buying a\\nand b together and not buying see we can just rule out see at that point of time. So this solves another problem is\\nthat we obviously do not need to analyze the process which people just by barely.\\nSo what we can do is according to the sages we can Define our minimum support and confidence and when you\\nhave set Values we can put this values in the algorithm and we can filter out the data and we\\ncan create different rules and suppose even after filtering you have like five thousand rules.\\nAnd for every item we create these 5,000 rules. So that's practically impossible.\\nSo for that we need the third calculation, which is the lift so lift is basically the strength of any Rule now,\\nlet's have a look at the denominator of the formula given here and if you see Here,\\nwe have the independent support values of A and B. So this gives us the independent occurrence probability of A and B.\\nAnd obviously there's a lot of difference between the random occurrence and Association and\\nif the denominator of the lift is more what it means is that the occurrence\\nof Randomness is more rather than the occurs because of any association.\\nSo left is the final verdict where we know whether we have to spend time. On this particular rule what we have got here or not.\\nNow, let's have a look at a simple example of Association rule mining. So suppose.\\nWe have a set of items a b c d and e and a set of transactions T1 T2 T3 T4\\nand T5 and as you can see here, we have the transactions T1 in which we have ABC T to a CD t3b CDT for a d e and T5 BCE.\\nNow what we generally do is create. At some rules or Association rules such as a gives T\\nor C gives a a gift C B and C gives a what this basically means is\\nthat if a person buys a then he's most likely to buy D. And if a person by C, then he's most likely to buy a and\\nif you have a look at the last one, if a person buys B and C is most likely to buy the item\\na as well now if we calculate the support confidence and lift using these rules\\nas you can see here in the table, we have the rule. And the support confidence handle lift values.\\nLet's discuss about a prairie. So a priori algorithm uses the frequent itemsets\\nto generate the association Rule and it is based on the concept that subset of a frequent itemsets must also be\\na frequent item set itself. Now this raises the question what exactly is a frequent item set.\\nSo a frequent item set is an item set whose support value is greater than the threshold value just now we discussed\\nthat the marketing team according to the says have a minimum threshold value for the confidence as well as the support.\\nSo frequent itemsets is that animset who support value is greater than the threshold value already specified example,\\nif A and B is a freaker item set Than A and B should also be frequent itemsets individually.\\nNow, let's consider the following transaction to make the things such as easier suppose.\\nWe have transactions 1 2 3 4 5 and these Items out there. So T 1 has 1 3 & 4 T 2 has 2 3 and 5 T3 has\\n1 2 3 5 T 4 to 5 and T 5 1 3 & 5 now the first step is to build a list\\nof items sets of size 1 by using this transactional data. And one thing to note here is that the minimum support count\\nwhich is given here is to Let's suppose it's too so the first step is to create item sets\\nof size 1 and calculate their support values. So as you can see here. We have the table see one\\nin which we have the item sets 1 2 3 4 5 and the support values if you remember the formula of support,it was frequency divided by the total number of occurrence. So as you can see here for the items\\nthat one the support is 3 as you can see here that item set one up here s and t 1 T 3 and T 5.\\nSo as you can see, it's frequency is 1 2 & 3 now as you can see the item set\\nfor has a support of one as it occurs only once in Transaction one but the minimum support value is 2\\nthat's why it's going to be eliminated. So we have the final table which is the table F1,\\nwhich we have the item sets 1 2 3 and 5 and we have the support values 3 3 4 & 4 now the next step is\\nto create Adam sets of size 2 and calculate their support values now all the combination of the item sets in the F1,\\nwhich is the final table in which it is carded the for are going to be used for this iteration.\\nSo So we get the table c 2. So as you can see here, we have 1 2 1 3 1 5 2 3 2 5 & 3 5 now\\nif you calculate the support here again, we can see that the item set 1 comma 2 has a support of one\\nwhich is again less than the specified threshold. So we're going to discard that so if we have a look\\nat the table f 2 we have 1 comma 3 1 5 2 3 2 5 & 3 5 again,\\nwe're going to move forward and create the atoms. That of size 3 and calculate this support values.\\nNow all the combinations are going to be used from the item set F to for this particular iterations.\\nNow before calculating support values, let's perform proning on the data set. Now what is pruning now\\nafter the combinations are being made we device c 3 item sets to check if there is another subset whose support is less\\nthan the minimum support value. That is what frequent items that means. So if you have a look here the item sets.\\nWe have is 1 2 3 1 2 1 3 2 3 4 the first one\\nbecause as you can see here if we have a look at the subsets of one two, three, we have 1 comma 2 as well,\\nso we are going to discard this whole item set same goes for the second one.\\nWe have one to five. We have 1/2 in that which was discarded in the previous set or the previous step.\\nThat's why we're going to discard that also which leaves us with only two factors,\\nwhich is 1 3 5 8. I'm set and the two three five and the support for this is 2\\nand 2 as well. Now if we create the table C for using four elements,\\nwe going to have only one item set, which is 1 2 3 and 5 and if you have a look at the table here the transaction table one,\\ntwo, three and five appears only one. So the support is one and since C for the support of the whole table C\\n4 is less than 2 so we're going to stop here and return to the previous item set that It is 3 3\\nso the frequent itemsets have 1 3 5 and 2 3 5 now let's assume our minimum confidence value is 60 percent for that.\\nWe're going to generate all the non-empty subsets for each frequent itemsets. Now for I equals 1 comma 3 comma 5 which is the item set.\\nWe get the subset one three one five three five one three and five similarly\\nfor 2 3 5 we get to three to five three five two three. and five now this rule states\\nthat for every subset s of I the output of the rule gives something like s gives i2s\\nthat implies s recommends I of s and this is only possible if the support of I divided by the support of s is greater\\nthan equal to the minimum confidence value now applying these rules to the item set of F3 we get rule 1 which is 1 3\\ngives 1 comma 3 comma 5 and 1/3 3 it means 1 and 3 gives 5\\nso the confidence is equal to the support of 1 comma 3 comma fire driver support\\nof 1 comma 3 that equals 2 by 3 which is 66% and which is greater than the 60 percent.\\nSo the rule 1 is selected now if we come to rule 2 which is 1 comma 5 it gives 1 comma 3 comma 5 and 1 5\\nit means if we have 1 & 5 it implies. We also going to have three know.\\nCalculate the confidence of this one. We're going to have support 1 3 5 whereby support 1/5\\nwhich gives us a hundred percent which means rule 2 is selected as well. But again if you have a look at rule 506 over here similarly,\\nif it's select 3 gives 1 3 5 & 3 it means if you have three, we also get one and five.\\nSo the confidence for this comes at 50% Which is less than the given 60 percent Target.\\nSo we're going to reject this Rule and same. Goes for the rule number six. Now one thing to keep in mind here is\\nthat all those are rule 1 and Rule 5 look a lot similar they are not so it really depends\\nwhat's on the left hand side of the arrow. And what's on the right-hand sides of the arrow. It's the if-then possibility.\\nI'm sure you guys can understand what exactly these rows are and how to proceed with this rules.\\nSo, let's see how we can implement the same in Python, right? So for that what I'm going to do is create a new python.\\nand I'm going to use the chapter notebook. You're free to use any sort of ID.\\nI'm going to name it as a priority. So the first thing what we're going to do is we will be using\\nthe online transactional data of retail store for generating Association rules. So firstly what we need to do is get the pandas and ml x\\n10 libraries imported and read the file. So as you can see here,\\nwe are using the online retail dot xlsx format file and from ml extant.\\nWe're going to import a prairie and Association rules at all comes under MX 10.\\nSo as you can see here, we have the invoice the stock quote the description the quantity\\nthe invoice data unit price customer ID and the country now next in this step. What we're going to do is do data cleanup\\nwhich includes removing the spaces from some of the descriptions. And drop the rules that do not have invoice\\nnumbers and remove the great grab transactions because that is of no use to us.\\nSo as you can see here at the output in which we have like five hundred and thirty two thousand rows\\nwith eight columns. So after the cleanup, we need to consolidate the items into one transaction per row\\nwith each product for the sake of keeping the data set small. We are only looking at the sales for France.\\nSo as you can see here, we have excluded all the other says we're just looking at the sales for France.\\nNow. There are a lot of zeros in the data. But we also need to make sure any positive values are converted to 1\\nand anything less than zero is set to 0 so as you can see here, we are still 392 Rose.\\nWe're going to encode it and see. Check again. Now that you have structured the data properly in this step.\\nWhat we're going to do is generate frequent itemsets that have support at least seven percent,\\nbut this number is chosen so that you can get close enough and generated rules with the corresponding support confidence and lift.\\nSo go ahead you can see here. The minimum support is 0.71 of what if we add another constraint\\non the rules such as the lift is greater than 6 and the conference is greater than 0.8.\\nSo as you can see here, we have the left-hand side and the right-hand side of the association rule, which is the antecedent and the consequence.\\nWe have the support. We have the confidence to lift the leverage and the conviction. So guys, that's it for this session.\\nThat is how you create Association rules using the API. Real gold tone which helps a lot in the marketing business.\\nIt runs on the principle of Market Basket analysis, which is exactly what big companies like Walmart.\\nYou have Reliance and Target to even Ikea does it and I hope you got\\nto know what exactly is Association rule mining what is lift confidence and support and how to create Association rules.\\nSo guys reinforcement learning. Dying is a part of machine learning where an agent is put in an environment\\nand he learns to behave in this environment by performing certain actions. Okay, so it basically performs actions and it either gets\\na rewards on the actions or it gets a punishment and observing the reward which it gets from those actions reinforcement learning is all\\nabout taking an appropriate action in order to maximize the reward in a particular situation.\\nSo guys in supervised learning the training data comprises of the input and the expected output\\nAnd so the model is trained with the expected output itself, but when it comes to reinforcement learning,\\nthere is no expected output here. The reinforcement agent decides what actions to take in order to perform a given task in the absence\\nof a training data set. It is bound to learn from its experience itself. Alright. So reinforcement learning is all about an agent\\nwho's put in an unknown environment and he's going to use a hit and trial method in order to figure out the environment and then come up\\nwith an outcome. Okay. Now, let's look at it. Reinforcement learning within an analogy. So consider a scenario where in a baby is learning\\nhow to walk the scenario can go about in two ways. Now in the first case the baby starts walking\\nand makes it to the candy here. The candy is basically the reward it's going to get so since the candy is the end goal the baby is happy.\\nIt's positive. Okay, so the baby is happy and it gets rewarded a set of candies now another way in which this could go is\\nthat the baby starts walking but Falls due to some hurdle in between The baby gets hot and it doesn't get any candy and obviously the baby is sad.\\nSo this is a negative reward. Okay, or you can say this is a setback. So just like how we humans learn from our mistakes by trial\\nand error reinforcement learning is also similar. Okay, so we have an agent which is basically the baby and a reward\\nwhich is the candy over here. Okay, and with many hurdles in between the agent is supposed\\nto find the best possible path to read through the reward. So guys. I hope you all are clear with the reinforcement learning now,\\nlet's look at At the reinforcement learning process. So generally a reinforcement learning system has\\ntwo main components, right? The first is an agent and the second one is an environment. Now in the previous case,\\nwe saw that the agent was the baby and the environment was the living room where in the baby was crawling.\\nOkay. The environment is the setting that the agent is acting on and the agent over here\\nrepresents the reinforcement learning algorithm. So guys the reinforcement learning process starts\\nwhen the environment sends a state to the And then the agent will take some actions based\\non the observations in turn the environment will send the next state and the respective reward back to the agent.\\nThe agent will update its knowledge with the reward returned by the environment and it uses\\nthat to evaluate its previous action. So guys this Loop keeps continuing until the environment sends a terminal state which means\\nthat the agent has accomplished all his tasks and he finally gets the reward. Okay. This is exactly\\nwhat was depicted in this scenario. So the agent keeps climbing up ladders until he reaches his reward to understand this better.\\nLet's suppose that our agent is learning to play Counter Strike. Okay. So let's break it down now initially the RL agent\\nwhich is basically the player player 1. Let's say it's a player one who is trying to learn how to play the game.\\nOkay. He collects some state from the environment. Okay. This could be the first date of Counter-Strike now based\\non the state the agent will take some action. Okay, and this action can be anything that causes a result.\\nSo if the Almost left or right it's also considered as an action. Okay, so initially the action is going to be random\\nbecause obviously the first time you pick up Counter-Strike, you're not going to be a master at it. So you're going to try with different actions\\nand you just want to pick up a random action in the beginning. Now the environment is going to give a new state.\\nSo after clearing that the environment is now going to give a new state to the agent or to the player.\\nSo maybe he's across th one now. He's in stage 2. So now the player will get a reward\\nour one from the environment. Because it cleared stage 1. So this reward can be anything. It can be additional points or coins or anything like that.\\nOkay. So basically this Loop keeps going on until the player is dead or reaches the destination.\\nOkay, and it continuously outputs a sequence of States actions and rewards. So guys, this was a small example to show you\\nhow reinforcement learning process works. So you start with an initial State and once a player clothes that state he gets a reward\\nafter that the environment will give another stage to the player. And after it clears that state it's going to get another award\\nand it's going to keep happening until the player reaches his destination. All right, so guys, I hope this is clear now,\\nlet's move on and look at the reinforcement learning definitions. So there are a few Concepts that you should be aware\\nof while studying reinforcement learning. Let's look at those definitions over here. So first we have the agent now an agent is basically\\nthe reinforcement learning algorithm that learns from trial and error. Okay, so an agent takes actions like For example a soldier\\nin Counter-Strike navigating through the game. That's also an action. Okay, if he moves left right or if he shoots at somebody\\nthat's also an action. Okay. So the agent is responsible for taking actions in the environment.\\nNow the environment is the whole Counter-Strike game. Okay. It's basically the world through which the agent\\nmoves the environment takes the agents current state and action as input and it Returns the agency reward and its next state as output.\\nAlright next we have action now all the possible. Steps that an agent can take are called actions.\\nSo like I said, it can be moving right left or shooting or any of that. Alright, then we have state now state is\\nbasically the current condition returned by the environment. So whichever State you are in if you are in state 1 or if you're in state\\nto that represents your current condition. All right. Next we have reward a reward is basically an instant return\\nfrom the environment to appraise Your Last Action. Okay, so it can be anything like coins\\nor it can be audition. Two points. So basically a reward is given to an agent\\nafter it clears the specific stages. Next we have policy policies basically the strategy\\nthat the agent uses to find out his next action based on his current state policy is just the strategy with which you\\napproach the game. Then we have value. Now while you is the expected long-term return\\nwith discount so value in action value can be a little bit confusing for you right now, but as we move further,\\nyou'll understand what I'm talking. Kima okay. So value is basically the long-term return that you get with discount.\\nOkay discount. I'll explain in the furthest lines. Then we have action value now action value is also known as Q value.\\nOkay. It's very similar to Value except that it takes an extra parameter, which is the current action.\\nSo basically here you'll find out the Q value depending on the particular action that you took.\\nAll right. So guys don't get confused with value and action value. We look at examples in the further slides and you will understand this better.\\nOkay. So guys make sure that you're familiar with these terms because you'll be seeing a lot of these terms in the further slides.\\nAll right. Now before we move any further, I'd like to discuss a few more Concepts. Okay. So first we will discuss the reward maximization.\\nSo if you haven't already realized it the basic aim of the RL agent is to maximize the reward now,\\nhow does that happen? Let's try to understand this in depth. So the agent must be trained in such a way\\nthat he takes the best action so that the reward is Because the end goal of reinforcement learning\\nis to maximize your reward based on a set of actions. So let me explain this with a small game now\\nin the figure you can see there is a fox there's some meat and there's a tiger so our agent is basically the fox and his end goal\\nis to eat the maximum amount of meat before being eaten by the tiger now since the fox is a clever fellow he eats the meat\\nthat is closer to him rather than the meat which is closer to the tiger. Now this is because the closer he is to the tiger the\\nhigher our his chances of getting killed. So because of this the rewards which are near the tiger,\\neven if they are bigger meat chunks, they will be discounted. So this is exactly what discounting means\\nso our agent is not going to eat the meat chunks which are closer to the tiger because of the risk. All right now,\\neven though the meat chunks might be larger. He does not want to take the chances of getting killed. Okay. This is called discounting.\\nOkay. This is where you discount because it improvise and you just eat the meat which are closer to you instead of taking risks\\nand eating the meat which are The to your opponent. All right. Now the discounting of reward Works based\\non a value called gamma will be discussing gamma in our further slides but in short the value of gamma is between 0 and 1.\\nOkay. So the smaller the gamma the larger is the discount value. Okay. So if the gamma value is lesser,\\nit means that the agent is not going to explore and he's not going to try and eat the meat chunks\\nwhich are closer to the tiger. Okay, but if the gamma value is closer to 1 it means that our agent is actually We're going to explore\\nand it's going to dry and eat the meat chunks which are closer to the tiger. All right, now, I'll be explaining this in depth in the further slides.\\nSo don't worry if you haven't got a clear concept yet, but just understand that reward maximization is a very important step\\nwhen it comes to reinforcement learning because the agent has to collect maximum rewards by the end of the game.\\nAll right. Now, let's look at another concept which is called exploration and exploitation. So exploration like the name suggests is\\nabout exploring and capturing. More information about an environment on the other hand exploitation is\\nabout using the already known exploited information to heighten the rewards. So guys consider the fox and tiger example\\nthat we discussed now here the fox eats only the meat chunks which are close to him, but he does not eat the meat chunks\\nwhich are closer to the tiger. Okay, even though they might give him more Awards. He does not eat them\\nif the fox only focuses on the closest rewards, he will never reach the big chunks of meat.\\nOkay, this is what exploitation is the about you just going to use the currently known information\\nand you're going to try and get rewards based on that information. But if the fox decides to explore a bit,\\nit can find the bigger award which is the big chunks of meat. This is exactly what exploration is.\\nSo the agent is not going to stick to one corner instead. He's going to explore the entire environment and try\\nand collect bigger rewards. All right, so guys, I hope you all are clear with exploration and exploitation.\\nNow, let's look at the markers decision process. So guys this is basically a mathematical approach\\nfor mapping a solution in reinforcement learning in a way. The purpose of reinforcement learning is to solve\\na Markov decision process. Okay. So there are a few parameters that are used to get to the solution.\\nSo the parameters include the set of actions the set of states the rewards the policy\\nthat you're taking to approach the problem and the value that you get. Okay, so to sum it up the agent must take\\nan action a to transition from a start state. The end State s while doing\\nso the agent will receive a reward are for each action that he takes. So guys a series\\nof actions taken by the agent Define the policy or it defines the approach and the rewards\\nthat are collected Define the value. So the main goal here is to maximize the rewards\\nby choosing the optimum policy. All right. Now, let's try to understand this with the help of the shortest path problem.\\nI'm sure a lot of you might have gone through this problem when you are in college. So guys look at the graph over here.\\nSo our aim here is to find the shortest path between a and d with minimum possible cost.\\nSo the value that you see on each of these edges basically denotes the cost. So if I want to go from a to c it's going to cost me 15 points.\\nOkay. So let's look at how this is done. Now before we move and look at the problem in this problem the set of states are denoted by the nodes,\\nwhich is ABCD and the action is to Traverse from one node to the other. So if I'm going from a Be\\nthat's an action similarly a to see that's an action. Okay, the reward is basically the cost\\nwhich is represented by each Edge over here. All right. Now the policy is basically the path that I choose to reach the destination.\\nSo let's say I choose a seed be okay that's one policy in order to get to D and choosing a CD\\nwhich is a policy. Okay. It's basically how I'm approaching the problem. So guys here you can start off at node a\\nand you can take baby steps to your destination now initially you're Clueless. So you can just take the next possible node,\\nwhich is visible to you. So guys if you're smart enough, you're going to choose a to see instead of ABCD or ABD.\\nAll right. So now if you are at nodes see you want to Traverse to note D. You must again choose a wise path\\nor red you just have to calculate which path has the highest cost or which path will give you the maximum rewards.\\nSo guys, this is a simple problem. We just drank to calculate the shortest path between a\\nand d by traversing through these nodes. So if I travels from a CD it gives me the maximum reward.\\nOkay, it gives me 65 which is more than any other policy would give me okay. So if I go from ABD,\\nit would be 40 when you compare this to a CD. It gives me more reward. So obviously I'm going to go with a CB.\\nOkay, so guys was a simple problem in order to understand how Markov decision process works.\\nAll right, so guys, I want to ask you a question. What do you think? I did hear did I perform exploration\\nor did I perform exploitation? Now the policy for the above example is of exploitation\\nbecause we didn't explore the other nodes. Okay. We just selected three notes and we Traverse through them. So that's why this is called exploitation.\\nWe must always explore the different notes so that we can find a more optimal policy. But in this case, obviously a CD has the highest reward\\nand we're going with a CD, but generally it's not so simple. There are a lot of nodes there hundreds of notes to Traverse\\nand they're like 50 60 policies. Okay, 50 60 different policies. So you make sure you explore.\\nAll the policies and then decide on an Optimum policy which will give you a maximum reward.\\nSo guys before we perform the Hands-On part. Let's try to understand the math behind our demo.\\nOkay. So in our demo will be using the Q learning algorithm which is a type of reinforcement learning algorithm.\\nOkay, it's simple, it just means that if you take the best possible actions to reach your goal or to get the most rewards.\\nAll right, let's try to understand this with an example. So guys, this is exactly what be running in In our demo,\\nso make sure you understand this properly. Okay. So our goal here is we're going to place an agent\\nin any one of the rooms. Okay. So basically these squares you see here our rooms. OK 0 is a room\\nfor is a room three is a room one is a room and 2:05 is also a room. It's basically a way outside the building.\\nAll right. So what we're going to do is we're going to place an agent in any one of these rooms and the goal is to reach outside the building.\\nOkay outside. The building is room number five. Okay, so these are These spaces are basically doors,\\nwhich means that you can go from zero to four. You can go from 4 to 3 3 to 1 1 to 5\\nand similarly 3 to 2, but you can't go from 5 to 2 directly. All right, so there are certain set of rooms\\nthat don't get connected directly. Okay. So like of mentioned here each room is numbered from 0 to 4,\\nand the outside of the building is numbered as five and one thing to note here is Room 1 and room\\nfor directly lead to room number five. All right. So room number one and four will directly lead out\\nto room number five. So basically our goal over here is to get to room number five.\\nOkay to set this room as a goal will associate a reward value to each door. Okay.\\nDon't worry. I'll explain what I'm saying. So if you re present these rooms in a graph this is\\nhow the graph is going to look. Okay. So for example from true, you can go to three and then three two,\\none one two five which will lead us to our goal these arrows represent the link between the dose.\\nNo, this is quite understandable now. Our next step is to associate a reward value\\nto each of these doors. Okay, so the rooms that are directly connected to our end room,\\nwhich is room number five will get a reward of hundred. Okay. So basically our room number one will have a reward five now.\\nThis is obviously because it's directly connected to 5 similarly for will also be associated with a reward of hundred\\nbecause it's directly connected to 5. Okay. So if you go out from for it will lead to five now the other know.\\nRoads are not directly connected to 5. So you can't directly go from 0 to 5. Okay. So for this will be assigning a reward of zero.\\nSo basically other doors not directly connected to the Target room have a zero reward.\\nOkay now because the doors are to weigh the two arrows are assigned to each room. Okay, you can see two arrows assigned to each room.\\nSo basically zero leads to four and four leads back to 0 now. We have assigned 0 0 over here\\nbecause 0 does not directly lead to five but one directly leads to Five and that's why you can see a hundred over here similarly\\nfor directly leads to our goal State and that's why we were signed a hundred over here and obviously five two five is hundred as well.\\nSo here all the direct connections to room number five are rewarded hundred and all the indirect connections\\nare awarded zero. So guys in q-learning the end goal is to reach the state with the highest reward\\nso that the agent arrives at the goal. Okay. So let me just explain this graph to you\\nin detail now these These rooms over here labeled one, two, three to five they represent the state an agent is in so\\nif I stay to one It means that the agent is in room number one similarly the agents movement\\nfrom one room to the other represents the action. Okay. So if I say one two, three, it represents an action.\\nAll right. So basically the state is represented as node and the action is represented by these arrows.\\nOkay. So this is what this graph is about these nodes represent the rooms and these Arrows represent the actions.\\nOkay. Let's look at a small example. Let's set the initial state to 0. So my agent is placed in room number two,\\nand he has to travel all the way to room number five. So if I set the initial stage to to he can travel to State 3.\\nOkay from three he can either go to one or you can go back to to or you can go to for if he chooses to go to\\nfor it will directly take him to room number 5, okay, which is our end goal and even if he goes from room number\\n3 2 1 it will take him to room number. High five, so this is how our algorithm works is going to drivers different rooms.\\nIn order to reach the Gold Room, which is room number 5. Now, let's try and depict these rewards\\nin the form of a matrix. Okay, because we'll be using this our Matrix or the reward Matrix to calculate the Q value\\nor the Q Matrix. Okay. We'll see what the Q value is in the next step. But for now, let's see how this reward Matrix is calculated.\\nNow the - ones that you see in the table, they represent the null values. Now these -1 basically means\\nthat Wherever there is no link between nodes. It's represented as minus 1 so 0\\n2 0 is minus 1 0 to 1 there is no link. Okay, there's no direct link from 0 to 1.\\nSo it's represented as minus 1 similarly 0 to 2 or 2. There is no link. You can see there's no line over here.\\nSo this is also minus 1, but when it comes to 0 to 4, there is a connection and we have numbered 0\\nbecause the reward for a state which is not directly connected to the goal is zero, but if you look\\nat this 1 comma 5 which is is basically traversing from Node 1 to node 5, you can see the reward is hundred.\\nOkay, that's basically because one and five are directly connected and five is our end goal.\\nSo any node which will directly connected to our goal state will get a reward of hundred. Okay.\\nThat's why I've put hundred over here similarly. If you look at the fourth row over here. I've assigned hundred over here.\\nThis is because from 4 to 5 that is a direct connection. There's a direct connection which gives them a hundred reward.\\nOkay, you can see from 4 to 5. There is a direct link. Okay, so from room number for to room number five you can go directly.\\nThat's why there's a hundred reward over here. So guys, this is how the reward Matrix is made. Alright, I hope this is clear to you all.\\nOkay. Now that we have the reward Matrix. We need to create another Matrix called The Q Matrix.\\nOK here, you'll store or the Q values that will calculate now this Q Matrix basically\\nrepresents the memory of what the agent has learned through experience. Okay. So once he traverses from one room to the final room,\\nwhatever he's learned. It is stored in this Q Matrix. Okay, in order for him to remember that the next time he travels this we use this Matrix.\\nOkay. It's basically like a memory. So guys the rows of the Q Matrix will represent the current state\\nof the agent The Columns will represent the possible actions and to calculate the Q value use this formula.\\nAll right, I'll show you what the Q Matrix looks like, but first, let's understand this formula. Now this Q value\\nwill calculating because we want to fill in the Q Matrix. Okay. So this is basically a Matrix over here initially,\\nit's all 0 but as the agent Traverse is from different nodes to the destination node.\\nThis Matrix will get filled up. Okay. So basically it will be like a memory to the agent. He'll know that okay,\\nwhen he traversed using a particular path, he found out that his value was maximum or as a reward was maximum of year.\\nSo next time he'll choose that path. This is exactly what the Q Matrix is. Okay. Let's go back now guys,\\ndon't worry about this formula for now because we'll be implementing this formula in an example.\\nIn the next slide. Okay, so don't worry about this formula for now, but here just remember that this Q basically represents the Q Matrix the r represents\\nthe reward Matrix and the gamma is the gamma value which I'll talk about shortly and here you just finding out the maximum from the Q Matrix.\\nSo basically the gamma parameter has a range from 0 to 1 so you can have a value of 0.1 0.3 0.5 0.8 and all of that.\\nSo if the gamma is closer to zero it means That the agent will consider only the immediate rewards which means\\nthat the agent will not explore the surrounding. Basically, it won't explore different rooms. It will just choose a particular room\\nand then we'll try sticking to it. But if the value of gamma is high meaning that if it's closer to one the agent will consider future Awards\\nwith greater weight. This means that the agent will explore all the possible approaches\\nor all the possible policies in order to get to the end goal. So guys, this is what I was talking about when I\\nmention ation and exploration. All right. So if the gamma value is closer to 1 it basically means\\nthat you're actually exploring the entire environment and then choosing an Optimum policy.\\nBut if your gamma value is closer to zero, it means that the agent will only stick to a certain set of policies\\nand it will calculate the maximum reward based on those policies. Now next. We have the Q learning algorithm\\nthat we're going to use to solve this problem. So guys now this is going to look very confusing to y'all.\\nSo let me just explain In this with an example. Okay. We'll see what we're actually going to run in our demo.\\nWe will do the math behind it. And then I'll tell you what this Q learning algorithm is. Okay, you'll understand it as I'm showing you the example.\\nSo guys in the Q learning algorithm the agent learns from his experience. Okay, so each episode,\\nwhich is basically when the agents are traversing from an initial room to the end goal is equivalent to one training session\\nand in every training session the agent will explore the environment it will Receive some reward\\nuntil it reaches the goal state which is five. So there's a purpose of training is to enhance the brain of our agent.\\nOkay only if he knows the environment very well, will he know which action to take and this is why we calculate the Q Matrix okay in Q Matrix,\\nwhich is going to calculate the value of traversing from every state to the end state from every initial room\\nto the end room. Okay, so when we calculate all the values or how much reward we're getting from each policy\\nthat we We know the optimum policy that will give us the maximum reward. Okay, that's why we have the Q Matrix.\\nThis is very important because the more you train the agent and the more Optimum your output will be so basically here\\nthe agent will not perform exploitation instead. He'll explore around and go back and forth through the different rooms\\nand find the fastest route to the goal. All right. Now, let's look at an example. Okay. Let's see how the algorithm works.\\nOkay. Let's go back to the previous slide and Here it says that the first step is to set the gamma parameter.\\nOkay. So let's do that. Now the first step is to set the value of the learning parameter, which is gamma and we have randomly set it\\nto zero point eight. Okay. The next step is to initialize the Matrix Q 2 0 Okay.\\nSo we've set Matrix Q 2 0 over here and then we will select the initial stage Okay, the third step is select a random initial State and here\\nwe've selected the initial State as room number one. Okay. So after you initialize the matter Q as a zero Matrix\\nfrom room number one, you can either go to room number three or number five. So if you look at the reward Matrix can see\\nthat from room number one, you can only go to room number three or room number five. The other values are minus 1 here,\\nwhich means that there is no link from 1 to 0 1 2 1 1 2 2 and 1 to 4.\\nSo the only possible actions from room number one is to go to room number 3 and to go to room number five.\\nAll right. Okay. So let's select room number five, okay. So from room number one, you can go to 3 and 5 and we have randomly selected five.\\nYou can also select three but for example, let's select five over here. Now from Rome five, you're going to calculate the maximum Q value\\nfor the next state based on all possible actions. So from number five, the next state can be room number one four or five.\\nSo you're going to calculate the Q value for traversing 5 to 1 5 2 4 5 2 5 and you're going to find out\\nwhich has the maximum Q value and that's how you're going. Compute the Q value. So let's Implement our formula.\\nOkay, this is the q-learning formula. So right now we're traversing from room number one to room number 5.\\nOkay. This is our state. So here I've written Q 1 comma 5. Okay one represents our current state\\nwhich is room number one. Okay. Our initial state was room number one and we are traversing to room number five.\\nOkay. It's shown in this figure room number 5 now for this we need to calculate the Q value next in our formula.\\nIt says the reward Matrix State and action. So the reward Matrix for 1 comma 5 let's look at 1 comma\\n5 1 comma 5 corresponds to a hundred. Okay, so I reward over here will be hundred so\\nr 1 comma 5 is basically hundred then you're going to add the gamma value. Now the gamma value will be initialized it\\nto zero point eight. So that's what we have written over here. And we're going to multiply it with the maximum value\\nthat we're going to get for the next date based on all possible actions. Okay. So from 5, the next state is 1 4 and 5.\\nSo if Travis from five to one that's what I've written over here 5 to 4. You're going to calculate the Q value of Fire 2 4 & 5 to 5.\\nOkay. That's what I mentioned over here. So Q 5 comma 1 5 comma 4 and 5 comma 5 are the next possible actions\\nthat you can take from State V. So r 1 comma 5 is hundred. Okay, because from the reward Matrix,\\nyou can see that 1 comma 5 is hundred 0.8 is the value of gamma after that.\\nWe will calculate Q of 5 comma 1 5 comma 4 and 5 comma 5 Like I mentioned earlier\\nthat we're going to initialize Matrix Q as zero Matrix So based setting the value of 0\\nbecause initially obviously the agent doesn't have any memory of what is happening. Okay, so he just starting from scratch.\\nThat's why all these values are 0 so Q of 5 comma 1 will obviously be 0 5 comma 4 would be 0\\nand 5 comma 5 will also be zero and to find out the maximum between these it's obviously 0.\\nSo when you compute this equation, you will get hundred so the Q value of 1 comma 5 is\\nSo if I agent goes from room number one to room number five, he's going to have a maximum reward\\nor Q value of hundred. All right. Now in the next slide you can see that I've updated the value of Q of 1 comma 5.\\nOkay, it said 200. All right now similarly, let's look at another example so that you understand this better.\\nSo guys, this is exactly what we're going to do in our demo. It's only going to be coded. Okay. I'm just explaining our code right now.\\nI'm just telling you the math behind it. Alright now, let's look at another example. Example OK this time.\\nWe'll start with a randomly chosen initial State. Let's say that we've chosen State 3.\\nOkay. So from room 3, you can either go to room number one two, or four randomly will select room number\\none and from room number one, you're going to calculate the maximum Q value for the next state based on all possible actions.\\nSo the possible actions from one is to go to 3 and to go to 5 now if you calculate the Q value using this formula,\\nso let me explain this to you once again now, 3 comma 1 basically represents that we're in room number three and we are going\\nto room number one. Okay. So this represents our action? Okay. So we're going from 3 to 1\\nwhich is our action and three is our current state next we will look at the reward\\nof going from 3 to 1. Okay, if you go to the reward Matrix 3 comma 1 is 0 okay. Now this is\\nbecause there's no direct link between three and five. Okay, so that's why the reward here is zero. So the value here will be 0\\nafter that we have the gamma value, which is zero point. Eight and then we're going to calculate the Q Max\\nof 1 comma 3 and 1 comma 5 out of these whichever has the maximum value we're going to use that.\\nOkay, so Q of 1 comma 3 is 0. All right 0 you can see here 1 comma 3 is 0\\nand 1 comma 5 if you remember we just calculated 1 comma 5 in the previous slide.\\nOkay 1 comma 5 is hundred. So here I'm going to put a hundred. So the maximum here is hundred.\\nSo 0.8 in 200 will give us c t so that's the Q value. Going to get if you Traverse from three two one.\\nOkay. I hope that was clear. So now we have Travers from room number three to room number one with the reward of 80.\\nOkay, but we still haven't reached the end goal which is room number five. So for our next episode the state will be room.\\nNumber one. So guys, like I said, we'll repeat this in a loop because room number one is not our end goal.\\nOkay, our end goal is room number 5. So now we need to figure out how to get from room number one to room number 5.\\nSo from room number one, you can either either go to three or five. That's what I've drawn over here. So if we select five we know that it's our end goal.\\nOkay. So from room number 5, then you have to calculate the maximum Q value for the next possible actions.\\nSo the next possible actions from five is to go to room number one room number four or room number five.\\nSo you're going to calculate the Q value of 5 to 1 5 2 4 & 5 2 5 and find out which is the maximum Q value\\nhere and you're going to use that value. All right. So let's look at the formula now now again, we're in room number one and Want to go\\nto room number 5. Okay, so that's exactly what I've written here Q 1 comma 5 next is the reward Matrix.\\nSo reward of 1 comma 5 which is hundred. All right, then we have added the gamma value which is 0.8.\\nAnd then we're going to find the maximum Q value from 5 to 1 5 2 4 & 5 to 5.\\nSo this is what we're performing over here. So 5 comma 1 5 comma 4 and 5 comma 5 are all 0 this is\\nbecause we initially set all the values of the Q Matrix as 0 so you get Hundred over here and the Matrix Remains the Same\\nbecause we already had calculated Q 1 comma 5 so the value of 1 comma 5 is already fed to the agent.\\nSo when he comes back here, he knows our okay. He's already done this before now. He's going to try and Implement another method.\\nOkay is going to try and take another route or another policy. So he's going to try to go from different rooms\\nand finally land up in room number 5, so guys, this is exactly how our code runs. We're going to Traverse through each and every node\\nbecause we want an Optimum ball. See, okay. An Optimum policy is attained only when you Traverse through all possible actions.\\nOkay. So if you go through all possible actions that you can perform only then will you understand which is the best action\\nwhich will lead us to the reward. I hope this is clear now, let's move on and look at our code.\\nSo guys, this is our code and this is executed in Python and I'm assuming that all of you have a good background in Python.\\nOkay, if you don't understand python very well. I'm going to leave a link in the description. You can check out that video on Python\\nand then maybe come back to this later. Okay, but I'll be explaining the code to you anyway, but I'm not going to spend a lot of time explaining each\\nand every line of code because I'm assuming that you know python. Okay. So let's look at the first line of code over here.\\nSo what we're going to do is we're going to import numpy. Okay numpy is basically a python library\\nfor adding support for large multi-dimensional arrays and matrices and it's basically for computing\\nmathematical functions. Okay so first Want to import that after that we're going to create the our Matrix.\\nOkay. So this is the our Matrix next we're going to create a q Matrix and it's a 6 into 6 Matrix\\nbecause obviously we have six states starting from 0 to 5. Okay, and we are going to initialize the value to zero.\\nSo basically the Q Matrix is going to be initialized to zero over here. All right, after that we're setting the gamma parameter to 0.8.\\nSo guys you can play with this parameter and you know move it to 0.9 or movement logo to 0.8.\\nOkay, you can see see what happens then then we'll set an initial stage. Okay initial stage is set as 1 after that.\\nWe're defining a function called available actions. Okay. So basically what we're doing here is\\nsince our initial state is one. We're going to check our row number one. Okay, this is our own number one.\\nOkay. This is wrong number zero. This is zero number one and so on. So we're going to check the row number one and we're going\\nto find the values which are greater than or equal to 0 because these values basically The nodes that we can travel to now\\nif you select minus 1 you can Traverse 2-1. Okay, I explained this earlier the - one represents all the nodes that we can travel to but we\\ncan travel to these nodes. Okay. So basically over here a checking all the values which are equal to 0\\nor greater than 0 these will be our available actions. So if our initial state is one we can travel to other states\\nwhose value is equal to 0 or greater than 0 and this is stored in this variable called.\\nAll available act right now. This will basically get the available actions in the current state.\\nOkay. So we're just storing the possible actions in this available act variable over here. So basically over here\\nsince our initial state is one we're going to find out the next possible States we can go to okay\\nthat is stored in the available act variable. Now next is this function chooses at random which action\\nto be performed within the range. So if you remember over here, so guys initially we are in stage number.\\nOkay are available actions is to go to stage number 3 or stage number five. Sorry room number 3 or room number 5.\\nOkay. Now randomly, we need to choose one room. So for that using this line of code, okay.\\nSo here we are randomly going to choose one of the actions from the available act this available act.\\nLike I said earlier stores all our possible actions. Okay from the initial State. Okay.\\nSo once it chooses an action is going to store it in next action, so guys this action will Present\\nthe next available action to take now next is our Q Matrix. Remember this formula that we used.\\nSo guys this formula that we use is what we are going to calculate in the next few lines of code.\\nSo in this block of code, which is executing and Computing the value of Q. Okay, this is our formula for computing the value\\nof Q current state Karma action. Our current state Karma action gamma into the maximum value.\\nSo here basically we're going to calculate the maximum index meaning that To be going to check\\nwhich of the possible actions will give us the maximum Q value read if you remember\\nin our explanation over here this value over here Max Q or five comma 1 5 comma 4 and 5 comma 5 we had\\nto choose a maximum Q value that we get from these three. So basically that's exactly what we're doing in this line of code,\\nthe calculating the index which gives us the maximum value after we finish Computing the value of Q will just\\nhave to update our Matrix. After that, we'll be updating the Q value and will be choosing a new initial State.\\nOkay. So this is the update function that is defined over here. Okay. So I've just called the function over here.\\nSo guys this whole set of code will just calculate the Q value. Okay. This is exactly what we did in our examples after that.\\nWe have the training phase. So guys remember the more you train an algorithm the better it's going to learn.\\nOkay so over here I have provided around 10,000 titrations. Okay. So my range is 10 thousand iterations meaning\\nthat my age It will take 10,000 possible scenarios and in go to 10,000 titrations to find out the best policy.\\nSo you're exactly what I'm doing is I'm choosing the current state randomly after that. I'm choosing the available action from the current state.\\nSo either I can go to stage 3 or straight five then I'm calculating the next action and then I'm finally updating the value\\nin the Q Matrix and next. We just normalize the Q Matrix. So sometimes in our Q Matrix the value might exceed.\\nOkay, let's say it. Heated to 500 600 so that time you want to normalize The Matrix. Okay, we want to bring it down a little bit.\\nOkay, because larger numbers we won't be able to understand and computation would be very hard on larger numbers.\\nThat's why we perform normalization. You're taking your calculated value and you're dividing it with the maximum Q value in 200.\\nAll right, so you are normalizing it over here. So guys, this is the testing phase. Okay here you will just randomly set a current state and you\\nwant given any other data because you've already trained our model. Okay, you're To give a Garden State then\\nyou're going to tell your agent that listen you're in room. Number one. Now. You need to go to room number five.\\nOkay, so he has to figure out how to go to room number 5 because we have trained him now. All right. So here we have set the current state to one\\nand we need to make sure that it's not equal to 5 because 5 is the end goal. So guys this is the same Loop that we executed earlier.\\nSo we're going to do the same I trations again now if I run this entire code, let's look at the result.\\nSo our current state here we've chosen as one. Okay and And if we go back to our Matrix,\\nyou can see that there is a direct link from 1 to 5, which means that the route that the agent should take is one to five.\\nOkay directly. You should go from 1 to 5 because it will get the maximum reward like that. Okay.\\nLet's see if that's happening. So if I run this it should give me a direct path from 1 to 5.\\nOkay, that's exactly what happened. So this is the selected path so directly from one to five\\nit went and it calculated the entire Q Matrix. Works for me. So guys this is exactly how it works.\\nNow. Let's try to set the initial stage as that's a to so if I set the initial stage as to and if I try to run the code,\\nlet's see the path that it gives so the selected path is 2 3 4 5 now chose this path\\nbecause it's giving us the maximum reward from this path. Okay. This is the Q Matrix that are calculated\\nand this is the selected path. All right, so guys with this we come to the end of this demo.\\nSo basically what we did was we just placed an agent in a room random room and we ask it to Traverse\\nthrough and reach to the end room, which is room number five. So basically we trained our agent and we made sure\\nthat it went through all the possible paths. to calculate the best path the for a robot\\nand environment is a place where it has been put to use. Now. Remember this reward is itself the agent for example\\nan automobile Factory where a robot is used to move materials from one place to another now the task we discussed just now\\nhave a property in common. Now, these tasks involve and environment and expect the agent to learn from the environment.\\nNow, this is where traditional machine learning phase and hence the need for reinforcement learning now,\\nit is good to have Establish overview of the problem that is to be solved using the Q learning\\nor the reinforcement learning. So it helps to define the main components of a reinforcement learning solution.\\nThat is the agent environment action rewards and States. So let's suppose we are to build\\na few autonomous robots for an automobile building Factory. Now, these robots will help the factory personal\\nby conveying them the necessary parts that they would need in order to pull the car. Now these different parts are located\\nat Nine different positions within the factory warehouse the car part include the chassis\\nWheels dashboard the engine and so on and the factory workers have prioritized the location\\nthat contains the body or the chassis to be the topmost but they have provided the priorities for other locations as well,\\nwhich will look into the moment. Now these locations within the factory look somewhat like this.\\nSo as you can see here, we have L1 L2 L3 all of these stations. Now one thing you might notice here\\nthat there are little obstacle prison in between the locations. So L6 is the top priority location\\nthat contains the chassis for preparing the car bodies. Now the task is to enable the robots\\nso that they can find the shortest route from any given location to another location on their own.\\nNow the agents in this case are the robots the environment is the automobile factory warehouse the let's talk\\nabout the state's the states. Are the location in which a particular robot is present\\nin the particular instance of time which will denote it states the machines understand numbers\\nrather than let us so let's map the location codes to number. So as you can see here, we have map location l 1 to this t 0 L 2 and 1\\nand so on we have L8 as state 7 + L line at state.\\nSo next what we're going to talk about are the actions. So in our example, the action will be the direct location that a robot can.\\nCall from a particular location, right consider a robot that is a tel to location and the Direct locations\\nto which it can move our L5 L1 and L3. Now the figure here may come in handy to visualize this now\\nas you might have already guessed the set of actions here is nothing but the set of all possible states\\nof the robot for each location the set of actions that a robot can take will be different.\\nFor example, the set of actions will change if the robot is. An L1 rather than L2. So if the robot is in L1,\\nit can only go to L 4 and L 2 directly now that we are done with the states and the actions.\\nLet's talk about the rewards. So the states are basically zero one two, three four and the actions are also 0 1\\n2 3 4 up till 8:00. Now, the rewards now will be given to a robot. If a location\\nwhich is the state is directly reachable from a particular location. So let's take an example suppose l Lane is\\ndirectly reachable from L8. Right? So if a robot goes from LA to align and vice versa,\\nit will be rewarded by one and if a location is not directly reachable from a particular equation.\\nWe do not give any reward a reward of 0 now the reward is just a number and nothing else it enables the robots to make sense\\nof the movements helping them in deciding what locations are directly reachable and what are not now with this Q. We\\ncan construct a reward table which contains all the required. Use mapping between all possible States.\\nSo as you can see here in the table the positions which are marked green have a positive reward.\\nAnd as you can see here, we have all the possible rewards that a robot can get by moving in between the different states.\\nNow comes an interesting decision. Now remember that the factory administrator prioritized L6\\nto be the topmost. So how do we incorporate this fact in the above table now, this is done by associating the topmost priority location\\nwith a very high reward. The usual ones so let's put 999 in the cell L 6 comma and six now the table\\nof rewards with a higher reward for the topmost location looks something like this. We have not formally defined all the vital components\\nfor the solution. We are aiming for the problem discussed now, we will shift gears a bit and study some\\nof the fundamental concepts that Prevail in the world of reinforcement learning and q-learning the first of all we'll start\\nwith the Bellman equation now consider the following Square. Rooms, which is analogous\\nto the actual environment from our original problem. But without the barriers now suppose a robot needs to go\\nto the room marked in the green from its current position a using the specified Direction.\\nNow, how can we enable the robot to do this programmatically one idea would be introduced some kind of a footprint\\nwhich the robot will be able to follow now here a constant value is specified in each of the rooms,\\nwhich will come along the robots way if it follows the directions by Fight about now in this way\\nif it starts at location a it will be able to scan through this constant value and will move accordingly\\nbut this will only work if the direction is prefix and the robot always starts at the location a now consider the robot starts\\nat this location rather than its previous one. Now the robot now sees Footprints\\nin two different directions. It is therefore unable to decide which way to go in order to get the destination which is the Green Room.\\nIt happens. Primarily because the robot does not have a way to remember the directions to proceed.\\nSo our job now is to enable the robot with a memory. Now, this is where the Bellman equation comes into play.\\nSo as you can see here, the main reason of the Bellman equation is to enable the reward with the memory.\\nThat's the thing we're going to use. So the equation goes something like this V of s gives maximum a r of s comma a plus gamma of vs -\\nwhere s is a particular state Which is a room is the Action Moving between the rooms as -\\nis the state to which the robot goes from s and gamma is the discount Factor now we'll get into it in a moment\\nand obviously R of s comma a is a reward function which takes a state as an action a and outputs the reward now V\\nof s is the value of being in a particular state which is the footprint now we consider all the possible actions\\nand take the one that yields the maximum value. Now there is one constraint.\\nHowever regarding the value footprint that is the room marked in the yellow just below the Green Room.\\nIt will always have the value of 1 to denote that is one of the nearest room adjacent to the green room.\\nNow. This is also to ensure that a robot gets a reward when it goes from a yellow room to The Green Room.\\nLet's see how to make sense of the equation which we have here. So let's assume a discount factor of 0.9\\nas remember gamma is the discount value or the discount Factor. So let's Take a 0.9.\\nNow for the room, which is Mark just below the one or the yellow room, which is the Aztec Mark for this room.\\nWhat will be the V of s that is the value of being in a particular state? So for this V of s would be something\\nlike maximum of a will take 0 which is the initial of our s comma.\\nHey plus 0.9 which is gamma into 1 that gives us zero point nine now here the robot\\nwill not get any reward for Owing to a state marked in yellow hence the IR s comma a is 0 here\\nbut the robot knows the value of being in the yellow room. Hence V of s Dash is one following this\\nfor the other states. We should get 0.9 then again, if we put 0.9 in this equation,\\nwe get 0.81 then zero point seven to nine and then we again reached the starting point.\\nSo this is how the table looks with some value Footprints computer. From the Bellman equation now\\na couple of things to notice here is that the max function has the robot to always choose the state\\nthat gives it the maximum value of being in that state now the discount Factor gamma notifies the robot\\nabout how far it is from the destination. This is typically specified by the developer of the algorithm.\\nThat would be installed in the robot. Now, the other states can also be given their respective values\\nin a similar way. So as you can see here the boxes Into the green one have one and\\nif we move away from one we get 0.9 0.8 1 0 1 7 to 9.\\nAnd finally we reach 0.66 now the robot now can precede its way through the Green Room utilizing these value Footprints event\\nif it's dropped at any arbitrary room in the given location now, if a robot Lance up in the highlighted Sky Blue Area,\\nit will still find two options to choose from but eventually either of the parties.\\nIt's will be good enough for the robot to take because Auto V the value Footprints are not only that out.\\nNow one thing to note is that the Bellman equation is one of the key equations in the world of reinforcement learning and Q learning.\\nSo if we think realistically our surroundings do not always work in the way we expect there is always a bit\\nof stochastic City involved in it. So this applies to robot as well. Sometimes it might so happen\\nthat the robots Machinery got corrupted. Sometimes the robot makes come across some hindrance on its way\\nwhich may not be known to it beforehand. Right and sometimes even if the robot knows\\nthat it needs to take the right turn it will not so how do we introduce this to cast a city\\nin our case now here comes the Markov decision process now consider the robot is currently in the Red Room\\nand it needs to go to the green room. Now. Let's now consider the robot has a slight chance\\nof dysfunctioning and might take the left or the right or the bottom. On instead updating the upper turn in order to get\\nto The Green Room from where it is now, which is the Red Room. Now the question is, how do we enable the robot to handle this when it is out\\nin the given environment right. Now, this is a situation where the decision making regarding which turn is to be taken is partly random\\nand partly another control of the robot now partly random because we are not sure\\nwhen exactly the robot mind dysfunctional and partly under the control of the robot because it is still Making a decision\\nof taking a turn right on its own and with the help of the program embedded into it. So a Markov decision process\\nis a discrete time stochastic Control process. It provides a mathematical framework for modeling\\ndecision-making in situations where the outcomes are partly random and partly under control of the decision maker.\\nNow we need to give this concept a mathematical shape most likely an equation\\nwhich then can be taken further now you might be Price that we can do this with the help\\nof the Bellman equation with a few minor tweaks. So if we have a look at the original Bellman equation V of X is equal to maximum\\nof our s comma a plus gamma V of s stash what needs to be changed in the above equation\\nso that we can introduce some amount of Randomness here as long as we are not sure\\nwhen the robot might not take the expected turn. We are then also not sure in which room it might end up\\nin which is nothing but the room it. Moves from its current room at this point\\naccording to the equation. We are not sure of the S stash which is the next state or the room,\\nbut we do know all the probable turns the reward might take now in order to incorporate each\\nof this probabilities into the above equation. We need to associate a probability with each\\nof the turns to quantify the robot if it has got any experts it is chance of taking this turn now\\nif we do, so We get PS is equal to maximum of our s comma a plus gamma\\ninto summation of s - PS comma a comma s stash into V of his stash now the PS a--\\nand a stash is the probability of moving from room s to establish with the action a\\nand the submission here is the expectation of the situation that the robot in curse,\\nwhich is the randomness now, let's take a look at this example here. So when We associate the probabilities to each\\nof these Stones. We essentially mean that there is an 80% chance that the robot will take the upper turn.\\nNow, if you put all the required values in our equation, we get V of s is equal to maximum of our of s comma a +\\ncomma of 0.8 into V of room up plus 0.1\\ninto V of room down 0.03 into a room of V of from left\\nplus 0.03 into Vo Right now note that the value Footprints will not change due to the fact\\nthat we are incorporating stochastic Ali here. But this time we will not calculate those values Footprints instead.\\nWe will let the robot to figure it out. Now up until this point. We have not considered about rewarding the robot\\nfor its action of going into a particular room. We are only watering the robot when it gets to the destination now,\\nideally there should be a reward for each action the robot takes to help it better as Assess the quality of the actions,\\nbut there was need not to be always be the same but it is much better than having some amount\\nof reward for the actions than having no rewards at all. Right and this idea is known as the living penalty in reality.\\nThe reward system can be very complex and particularly modeling sparse rewards is an active area\\nof research in the domain of reinforcement learning. So by now we have got the equation\\nwhich we have a so what? To do is now transition to Q learning. So this equation gives us the value of going\\nto a particular State taking the stochastic city of the environment into account. Now, we have also learned very briefly about the idea\\nof living penalty which deals with associating each move of the robot with a reward so Q learning processes\\nand idea of assessing the quality of an action that is taken to move to a state rather than\\ndetermining the possible value of the state which is being moved to So earlier we had 0.8\\ninto V of s 1 0.03 into V of S 2 0 point 1 into V\\nof S 3 and so on now if you incorporate the idea of assessing the quality\\nof the action for moving to a certain state so the environment with the agent and the quality of the action will look something like this.\\nSo instead of 0.8 V of s 1 will have q of s 1 comma a one will have q\\nof S 2 comma 2 You of S3 not the robot now has four different states to choose\\nfrom and along with that. There are four different actions also for the current state it is in so\\nhow do we calculate Q of s comma a that is the cumulative quality of the possible actions\\nthe robot might take so let's break it down. Now from the equation V of s equals maximum a RS comma a +\\ncomma summation s - PSAs stash - into V of s -\\nif we discard the maximum function we have is of a plus gamma into summation p\\nand v now essentially in the equation that produces V of s we are considering all possible actions\\nand all possible States from the current state that the robot is in and then we are taking the maximum value caused\\nby taking a certain action and the equation produces a value footprint,\\nwhich is for just one possible action. In fact if we can think of it as the quality\\nof the action so Q of s comma a is equal to RS comma a plus gamma of summation p and v now\\nthat we have got an equation to quantify the quality of a particular action. We are going to make a little adjustment\\nin the equation we can now say that we of s is the maximum of all the possible values\\nof Q of s comma a right. So let's utilize this fact and replace V of s Stash as a function\\nof Q so q s comma a becomes R of s comma a + comma of summation PSAs -\\nand maximum of the que es - a - so the equation\\nof V is now turned into an equation of Q, which is the quality.\\nBut why would we do that now? This is done to ease our calculations because now we have only one function Q,\\nwhich is also the core of the Programming language. We have only one function Q to calculate an R of s comma\\na is a Quantified metric which produces reward of moving to a certain State.\\nNow, the qualities of the actions are called The Q values and from now on we will refer to the value Footprints\\nas the Q values an important piece of the puzzle is the temporal difference. Now temporal difference is the component\\nthat will help the robot calculate the Q values which respect to the change. Changes in the environment over time.\\nSo consider our robot is currently in the mark State and it wants to move to the Upper State.\\nOne thing to note that here is that the robot already knows the Q value of making the action\\nthat is moving through the Upper State and we know that the environment is stochastic in nature and the reward\\nthat the robot will get after moving to the Upper State might be different from an earlier observation.\\nSo how do we capture this change the real difference? We calculate the new Q as My a with the same formula\\nand subtract the previous you known qsa from it. So this will in turn give us the new QA now the equation\\nthat we just derived gifts the temporal difference in the Q values which further helps to capture the random changes\\nin the environment which may impose now the new q s comma a is updated as the following\\nso Q T of s comma is equal to QT minus 1 s comma a plus Alpha TD.\\nET of a comma s now here Alpha is the learning rate which controls\\nhow quickly the robot adapts to the random changes imposed by the environment the qts comma is the current state q value\\nand a QT minus 1 s comma is the previously recorded Q value. So if we replace the TDS comma a with its full form equation,\\nwe should get Q T of s comma is equal to QT - 1 of s comma y plus Alpha\\ninto our of S comma a plus gamma maximum of q s Dash a dash minus QT\\nminus 1 s comma a now that we have all the little pieces of q line together.\\nLet's move forward to its implementation part. Now, this is the final equation of q-learning, right?\\nSo, let's see how we can implement this and obtain the best path for any robot to take now to implement the algorithm.\\nWe need to understand the warehouse. Ian and how that can be mapped to different states.\\nSo let's start by reconnecting the sample environment. So as you can see here, we have L1 L2 L3 to align and as you can see here,\\nwe have certain borders also. So first of all, let's map each of the above locations in the warehouse\\ntwo numbers or the states so that it will ease our calculations, right? So what I'm going to do is create a new Python 3 file\\nin the jupyter notebook and I'll name it as learning Numb, but\\nokay, so let's define the states. But before that what we need to do is import numpy\\nbecause we're going to use numpy for this purpose and let's initialize the parameters.\\nThat is the gamma and Alpha parameters. So gamma is 0.75, which is the discount Factor whereas Alpha is 0.9,\\nwhich is the learning rate. Now next what we're going to do is Define the states and map\\nit to numbers. So as I mentioned earlier l 1 is Zero and online. We have defined the states in the numerical form.\\nNow. The next step is to define the actions which is as mentioned above represents the transition\\nto the next state. So as you can see here, we have an array of actions from 0 to 8.\\nNow, what we're going to do is Define the reward table. So as you can see here is the same Matrix\\nthat we created just now that I showed you just now now if you understood it correctly,\\nthere isn't any real Barrel limitation as depicted in the image, for example, the transitional for tell one is allowed\\nbut the reward will be 0 to discourage that path or in tough situation. What we do is add a minus 1 there\\nso that it gets a negative reward. So in the above code snippet as you can see here,\\nwe took each of the It's and put once in the respective state that are directly reachable from the certain State.\\nNow. If you refer to that reward table, once again, which we created the above or reconstruction will be easy to understand\\nbut one thing to note here is that we did not consider the top priority location L6 yet.\\nWe would also need an inverse mapping from the state's back to its original location\\nand it will be cleaner when we reach to the other depths of the algorithms. So for that what we're going to do is Have the inverse\\nmap location state to location. We will take the distinct State and location and convert it back.\\nNow. What will do is will not Define a function get optimal which is the get optimal route,\\nwhich will have a start location and an N location.\\nDon't worry the code is back. But I'll explain you each and every bit of the code. It's not the get optimal root function will take two arguments\\nthe starting location in the warehouse and the end location in the warehouse recipe lovely and it will return the optimal route\\nfor reaching the end location from the starting location in the form of an ordered list containing the letters.\\nSo we'll start by defining the function by initializing the Q values to be all zeros.\\nSo as you can see here we have Even the Q value has to be 0 but before that what we need to do is copy the reward Matrix to a new one.\\nSo this the rewards new and next again, what we need to do is get the ending State corresponding\\nto the ending location. And with this information automatically will set the priority of the given ending stay to the highest one\\nthat we are not defining it now, but will automatically set the priority of the given ending State as nine nine nine.\\nSo what we're going to do is initialize the Q values to be 0 and in the Learning process what you can see here.\\nWe are taking I in range 1000 and we're going to pick up a state randomly.\\nSo we're going to use the MP dot random randint and for traversing through the neighbor location\\nin the same maze we're going to iterate through the new reward Matrix and get the actions which are greater than 0 and after that\\nwhat we're going to do is pick an action randomly from the list of the playable actions in years to the next state\\nwill going to compute the temporal difference, which is TD, which is the rewards plus gamma into the queue of next state\\nand will take n p dot ARG Max of Q of next 8 minus Q of the current state.\\nWe going to then update the Q values using the Bellman equation as you can see here. We have the Bellman equation\\nand we're going to update the Q values and after that we're going to initialize the optimal route\\nwith a starting location now here we do not know what the next location yet.\\nSo initialize it with a value of the starting location, which Again is the random location.\\nSo we do not know about the exact number of iteration needed to reach to the final location.\\nHence while loop will be a good choice for the iteration. So when you're going to fetch the starting State fetch\\nthe highest Q value penetrating to the starting State we go to the index or the next state,\\nbut we need the corresponding letter. So we're going to use that state to location function.\\nWe just mentioned there and after that we're going to update the starting location for the The next iteration\\nand finally we'll return the root. So let's take the starting location of n line\\nand and location of L while and see what part do we actually get?\\nSo as you can see here we get Airline l8l 5 L2 and L1. And if you have a look at the image here,\\nwe have if we start from L9 to L1. We got L8 L5 L 2 l 1 l 8l v L2 L1\\nthat would He does the maximum value of the maximum reward for the robot.\\nSo now we have come to the end of this Q learning session and I hope you got to know\\nwhat exactly is Q learning with the analogy all the way starting from the number of rooms and I hope the example which I took the analogy\\nwhich I took was good enough for you to understand q-learning understand the Bellman equation\\nhow to make quick changes to the Bellman equation and how to create the reward table the cue.\\nWill and how to update the Q values using the Bellman equation, what does alpha do what does karma do?\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_4.txt'}, 'embedding': None, 'id': '6d0bf47ab17722c700e1f0b0f0a29e6f'}>, <Document: {'content': \"Hello everyone and welcome to this interesting session on data science full course. So before we begin let's have a quick look at the agenda of this\\nsession so first of all I'll be starting off by explaining you guys about the evolution of data how it led to the growth of data science, machine learning,\\nAI and all the different aspects of data. Then we'll have a quick introduction to data science, understand what exactly it is then we'll move forward to the data\\nscience careers and the salary and understand what are the different job profiles in the data science career path how to become a data scientist data\\nanalyst or a machine learning engineer. Then we'll move on to the first and the foremost part of data science which is statistics and after completing statistics\\nwe'll move on to machine learning where we'll understand what exactly is machine learning what are the different types of machine learning and how are they used\\nand where are they used the different algorithms and next we'll understand\\nwhat is deep learning and how deep learning is different from machine learning, what is the relationship between AI, machine learning and deep\\nlearning in terms of data science and understand how exactly neural network works, how to create a neural network and much more, So let's begin our session now.\\nData is increasingly shaping the systems that we interact with every day, whether you are searching something on Google using Siri or browsing your Facebook\\nfeed you are consuming the result of data analysis. It is increasing at a very alarming rate where we are generating 2.5 quintillion bytes of it every day.\\nNow that's a lot of data and considering there are more than 3 billion Internet users in the world a quantity that has tripled in the last 12 years\\nand 4.3 billion cell phone users that's a heck lot of data and this rapid growth\\nhas generated opportunity for new professionals who can make sense out of this data. Now given its transformation ability it's no wonder that so many data\\narrays with jobs have been created in the past few years like data analysts, data scientists, machine learning engineers, artificial intelligence\\nengineers and much more. And before we dwell into the details of all of these different professionals, let's understand exactly\\nwhat data science is. So data science also known as the relevant science is an\\ninterdisciplinary field about scientific methods, processes and systems to extract\\nknowledge or insights from data in various forms. It's structured or unstructured. It is the study of where information comes from what it\\nrepresents and how it can be turned into a valuable resource in the creation of business and IT strategies. So data science employs many techniques and\\ntheories from fees like mathematics, statistics, information science as well as computer science, and can be applied to small data sets also yet most people\\nthink data science is when you are dealing with big data or large amounts of data. So this brings the question which job profile is suitable for you, is\\nit the data analysts, the data scientist or the machine learning engineer. Now\\ndata scientist has been called the sexiest java 21st century nonetheless data science is a hot and growing field so before we drill into the data science\\nlet's discuss all of these profiles one by one and see what this roles are and how\\nthey work in the industries so read a science career usually starts with mathematics and stats as the base which brings up the force profile in our data\\nscience career path which is a data analyst so Idina analyst delivers value to the companies by taking information about specific topics and then\\ninterpreting analyzing and presenting the finding in comprehensive reports now many different types of businesses use data analysts to help as experts data\\nanalysts are often called on to use the skills and tools provide competitive analysis and identify trends within the industry's most entry-level professional\\ninterested in going into Data related jobs start off as data analyst qualifying for this role is as simple as it gets all you need is a bachelor's\\ndegree in computer science mathematics and a good statistical knowledge strong technical skills would be a plus and can give you an edge over most other\\napplicants so next we have data scientists there are several definitions available on data scientists but in simple words\\nscientist is one who practices the art of data science the highly popular term data scientist was coined by DJ Patton and Jeff hammer backer data scientists\\nare those who crack complex data problems with a strong expertise in certain scientific disciplines they work with several elements related to\\nmathematics statistics computer science and much more now data scientists are usually business analysts or data analysts with a difference it is a\\nposition for specialists and you can specialize in different types of skills like speech analytics text analytics which is the natural language processing\\nimage processing video processing medicine simulation material simulation\\nnow each of these specialists roles are very limited in number and hence the\\nvalue of such a specialist is immense now if we talk about AI or machine learning ingenious so machine learning engineers are sophisticated programmers\\nwho develop machines and systems that can learn and apply knowledge without specific direction artificial intelligence is the goal of a machine\\nlearning engineer they are computer programmers but their focus goes beyond specifically programming machines to perform specific tasks now they create\\nprograms that will enable machines to take actions without being specifically directed to perform those tasks so now if we have a look at the salary trends\\nof all of these professionals so starting with a data analyst the average salary in the u.s. is around 83,000 dollars or it's almost close to eighty\\nfour thousand dollars whereas in India it's around four lakh and four thousand\\nrupees per annum. Now coming to data scientist the average salary is ninety one thousand dollars nine eleven point five thousand dollars and in India it is\\nalmost seven lakh rupees and finally four ml in ten years the average salary\\nin the u.s. is around one hundred and eleven thousand dollars whereas in India is around seven lakh and twenty thousand dollars so as you can see the radius\\nscientist an ml ingenious position are a certain higher position which requires certain degree of expertise in that field so that's the reason why there is\\na difference in the salary of all the three professionals so if you have a look at the road map of becoming any one of these profession\\nso what first one needs to do is own a bachelor's degree now this bachelor's degree can be in either computer science mathematics\\ninformation technology statistics finance or even economics now after\\ncompleting a bachelor's degree the next comes is fine-tuning the technical skills during the technical skills is one of the most important parts in the\\nroadmap where you learn all the statistical methods and packages you either learn are Python essays languages which are very important you learn about\\ndata warehousing business intelligence data cleaning visualization reporting techniques walking knowledge of Hadoop and MapReduce is very very important and\\nif you talk about machine learning techniques it is one of the most important parts of the data science career now apart from these technical\\nskills there are also some business skills which are very much required so this involves analytical problem-solving effective communication creative\\nthinking as well as industry knowledge now after fine-tuning your technical skills and developing all the business skills you have the options of either\\ngoing for a job or either going for a master's degree or certification programs now I might suggest as you go for a master's degree as just coming out\\nof the BTech world and having the technical skills is not enough so you need to have a certain level of expertise in the field so it's better to\\ngo for any masters or PhD programs which are in computer science statistics or\\nmachine learning you can also go for big data certifications and you can also go for industry certifications regarding the data analysis machine learning or\\nthe data science it so happens that arica also provides a machine learning data analysis as well as a data science certification training they have\\nmaster's program which are equivalent to a master's degree which you get from a certain University so do check it out guys I'll leave the link to all of these\\nin the description box below and after you have completed the master's degree what comes is working on the projects which are related to this field so it's\\nbetter if you work on machine learning deep learning or data ethics projects that will give you an edge over other competitors while applying for a job\\nscenario so a certain level of expertise in the field is also required and this\\nis how you will succeed in the rate of science career path there are certain skills which are required which I was talking about earlier the technical\\nskills and the non technical skills now if you talk about the skills which are required to become all of these professions so they are mostly the same\\nso for any data analyst first of all you need to have analytical skills which involves Maths having good knowledge of matrix multiplications the Fourier\\ntransformations and all next we have communication skills so come looking for a data analyst require someone who has the good communication skills who can\\nexplain all of their technical terms to non-technical teams such as marketing or the sales team another important skill required is critical thinking you need\\nto think in certain directions and gain insights from the data so that's one of the most important part of a data analysts job obviously you need to pay\\nattention on the details so as a minor shift or the deviation in the result or in the calculation what you say the analysis might result in some sort of\\nloss of the company it's not necessarily to create a loss but it's better to avoid any kind of deviation from the results so paying attention to the\\ndetail is very very important and then again we talk about the mathematical skills knowing about all the types of differentiations and integrations is\\ngoing to help a lot because you know a lot of machine learning algorithms as I would say are mostly mathematical terms or mathematical functions so having good\\nknowledge of mathematics is also required apart from this the usage of technical tools such as Python are we have essays you need to know about the\\nbig data ecosystem how it works the HDFS how to extract data create a pipeline you know about JavaScript a little and if you talk about the skills of data\\nscientist it's almost the same having analytical and statistics knowledge now another important part here is to know the machine learning algorithms as it\\nplays an important role in the data science career from solving skills obviously now another important aspect if you talk about the skill which\\ndiffers from that of a data analyst is only deep learning so deep learning I'll\\ntalk about deep learning later in the second half or the later part of the video so having a good knowledge of deep learning and the various frameworks such\\nas tensorflow PI torch you have piano all of this is very required for data scientists and again business communication as I\\nmentioned earlier is very much required because as you know these are one of the technical roles most technical roles in the industries and the output of these\\nroles or what I would say the output of what these professions - is not that much technical is more business oriented so they have to explain all of these\\nfindings to either the non-technical teams the sales the marketing and again\\nyou need the technical tools and the skills now for machine learning engineer obviously programming languages having good knowledge of our Python C++ or Java\\nit's very much required you need to know about calculus and statistics as I mentioned earlier learning about mattresses integration now another\\nimportant skill here is signal processing so a lot of times machine learning engineers have to work on robots and signal processing they work\\non human-like robots they work on robotics which mimic human behavior so a\\nlot of signal processing techniques are also required in this field applied mathematics as I mentioned earlier and again neural networks it is one of the\\nbase of artificial intelligence which is being used and again we have natural language processing so as you know we have personal assistants like Siri and\\nCortana and they work on language processing and not just language processing you have audio processing as well as video processing so that they\\ncan interact with a real environment and provide a certain answer to a particular question so these were the skills I would say for all of these three roles\\nnext if we have a look at the peripherals of data science so first of\\nall we have statistics needless to say there are programming languages we have short read integrations then we have machine learning which is a big part of\\ndata science and then again we have big data so let's start with statistics\\nwhich is the first area of data science or I should say the first milestone which we should cover so for statistics let's understand first\\nwhat exactly is data so data in general terms refers to facts and statistics collected together for reference or analysis when working with\\nstatistic it's important to recognize the different types of data so data can be broadly classified into numerical categorical and ordinal now data with no\\ninherent order or ranking such as a gender or race is called nominal data so\\nas you can see in the type 1 we have male female male female that is nominal data now data with an ordered series is\\ncalled ordinal data so as you can see here we have an ordered series where we have the customer IDs and the rating scale no data with only two options\\nseries is called binary data now in this type of data there are only two options like either yes or no or true or false or 1 or 0 so as you can see here we have\\ncustomer ID and in the owner or car column we have either yes or no now the\\ntypes of data we just discussed under law describe the quality of something in size appearance value or something such kind of data is broadly classified into\\nqualitative data now data which can be categorized into a classification data\\nwhich is based upon counts there is only a finite number of values possible and the values cannot be subdivided meaningfully is called discrete data so\\nas you can see here in our example we have organization and the number of products so this cannot be subdivided into number of sub products right and if\\nyou talk about data which can be measured on a continuum or a scale no data which can have almost any numeric value and can be subdivided into finer\\nand finer increments is called continuous data so as you can see here in patient ID we have weight of the patient it is 6.5 kgs now kgs can be\\nsubdivided into grams and milligrams and final refinement is also possible now\\nthis type of data that can be measured by the quantity of something rather than its quality is called quantitative data now that we have honest with the\\ndifferent types of data qualitative and quantitative it's time to understand the types of variables we have now there are majorly two types of variables dependent\\nand independent variables so if you want to know whether caffeine affects your appetite the presence or the absence of the amount of\\ncaffeine would be the independent variable and how hungry you are would be the dependent variables so in statistics dependent variable is the outcome of an\\nexperiment as you change the independent variable you watched what happens to the dependent variable whereas if you talk about independent variable a variable\\nthat is not affected by anything that you or the researcher does usually plotted on the x-axis now the next step after knowing about\\nthe datatypes and the variables is to know about population and sampling and that comes into experimental research now in experimental research the aim is\\nto manipulate an independent variable and then examine the effect that this change has on a dependent variable now since it is possible to manipulate the\\nindependent variable experimental research has the advantage of enabling a researcher to identify a cause and effect between the variables well\\nsuppose there are 100 volunteers at the hospital and a doctor needs to check the working of a particular medicine which has been cleared by the government so\\nthe doctor divides those hundred patients into two groups of 50 and then asked one group to take one type of medicine and the other group to not take\\nany medicine at all and then after of me then compare the results and in non experimental research the researcher does not manipulate the independent\\nvariable this is not to say that it is impossible to do so but it will either be impractical or it will be unethical to do so so for example a researcher may\\nbe interested in the effect of illegal recreational drug views which is the independent variable on certain types of behavior which is the dependent variable\\nhowever why is possible it would be unethical to ask an individual to take\\nillegal drugs in order to study what effects this hat on certain behaviors it\\nis always good to go for experimental research rather than non experimental research so next in our session we have population and sampling those are two of\\nthe most important terms in statistics so let's understand these terms so in\\nstatistic the term population is the entire pool from which a sample is drawn statistician also speak of a population of objects or events or procedures or\\nobservation including such things as the quantity of the number of vehicle owned by a penny person now population is thus an\\naggregate of creatures things cases and so on and a population commonly contains\\ntoo many individuals to study conveniently an investigation is often restricted to one or most samples drawn from it now a world chosen sample will\\ncontain most of the information about a particular population parameter but the relationship between the sample and the population must be such as to allow true\\ninferences to be made about a population from that sample for that we have different types of sampling techniques so in probabilities there are sampling\\nmethods which are classified either as probability or non probability so in probability sampling each member of the population has a known nonzero\\nprobability of being selected probably the methods include random sampling systematic sampling and stratified sampling whereas in nonprobability\\nsampling members are selected from a population in some non-random manner but\\nthese includes convenience sampling judgement sampling quota sampling and snowball sampling while sampling is important there is another term which is\\nknown as sampling error so sampling error is a degree to which a sample might differ from the population when inferring to a population results are\\nreported plus or minus the sampling error now in probability sampling there are three terms which are random sampling systematic sampling and\\nstratified sampling so talking about random sampling probability of each member of the population to be chosen has equal chance of being selected such\\ntype of sampling is random sampling never talk about systematic sampling it is often used instead of random sampling and it is also called the NEP name\\nselection technique now pay attention to the name called Anette name so after the\\nrequired sample size has been calculated every NS record is selected from the list of the population member now it's only advantage over Anna's having\\ntechnique is its simplicity now the final type of sampling is a stratified sampling so a stratum is a subset of the population that shares at least one\\ncommon characteristics the researcher first hand you fires irrelevant stratums and there actual representations in the population\\nbefore analysis so now that we know how our data is and what kind of sampling is done let's have a look at the measure of center which helps describe to what\\nextent this pattern holds for a specific numerical value so as you can see in measure of center we have three terms which are the mean median and mode and\\nI'm sure everyone must be aware of all of these terms I'll not get into the details of these terms what's more important is to know\\nabout the measure of spreads now a measure of spread sometime called a measure of dispersion is used to describe the variability in the sample\\nor population it is usually used in conjunction with a measure of Center tendency such as the mean or median provide an overall description of a set\\nof data now if you talk about deviation it is the difference between each X I\\nand the mean for a sample population which is known as the deviation about the mean whereas variance is based on deviation and entails computing squares\\nof deviation so as you can see here we have the formula for the variance which\\nis the difference between the mean and the particular data point squared and divided by the total number of data points and it's summation standard\\ndeviation is basically the under root of variance so as you can see the formula is the same just we have the under root over the variance so that was stand\\nevasion and variance another topic in probability and statistics is kunis so\\nskewness is a measure of symmetry or more precisely the lack of symmetry so\\nas you can see here we have left skewed symmetric non symmetric left skewed we have right skewed so normally distributed curves are the most\\nsymmetric curves we'll talk about normal distribution later so after skewness what we need to know about is the confusion matrix now\\nconfusion matrix represent a tabular representation of actual versus the predicted values now this help us find the accuracy of the model when we are\\ncreating any machine learning or the team learning model to find the accuracy what we do is plot a confusion matrix so what you need to do is you can calculate\\nthe accuracy of your model with adding the true positives and the true negative and dividing it with the true positives plus true negatives plus false positive\\nplus false negatives that will give you the accuracy of the model so as you can see in the image we have good bad for predicted as well as actual and as you\\ncan see here the true positive D and the true negative a are the two areas where\\nwe have created it it was good and the actual value was good in true negative a\\nwe have the predicted it was bad and the actually it's bad so model which gets\\nthe higher true positive and true negatives are the ones which have the higher accuracy so that's what confusion matrix are for now the next term and a\\nvery important term in statistics is probability so probability is the measure of how likely something will occur it is the ratio of desired\\noutcomes to the total outcomes now if I roll a dice there are six total possibilities one two three four five and six\\nnow each possibility has one outcome so each has a probability of one out of six\\nnow for instance the probability of getting a number two is one out of six since there is only a single two on the dice now when talking about the\\nprobability distribution techniques or the terminologies there are three possible terms which are the probability density function normal distribution and\\nthe central limit theorem so probability density function it is the equation describing a continuous probability distribution so it is usually referred\\nas PDF now if we talk about normal distribution so the normal distribution is a probability distribution that associates the normal random variable X\\nwith a cumulative probability the normal distribution is defined by the following equation so as you can see here Y is 1 by Sigma into the square root of 2 pi 2\\nwhole multiplied by E raised to power minus X minus mu whole square divided by 2 Sigma square where X is a random normal variable mu is the mean and Sigma\\nis the standard deviation now the central limit theorem states that the sampling distribution of the mean of any independent random variable will be\\nnormal or nearly normal if the sample size is large enough now accuracy or the\\nresemblance to normal distribution depends on however two factors the first one is a number of sample points taken and second is the shape of the\\nunderlying population now enough about statistics if you want to know more about statistics and if you want to get in-depth knowledge over statistics you\\ncan refer to our statistics for data science video I'll leave the link to that video in the description box so that video talks about statistics and\\nprobability in a more depth movie then I explained here so I will talk about the\\np-value is the hypotheses what all are required or any data science project so\\nlet's move on to our next part of data science learning which is learning paths which is the machine learning so let's understand what exactly is machine\\nlearning so machine learning is an application of artificial intelligence that provides systems the ability to automatically learn and improve from\\nexperience without being explicitly programmed now getting computers to\\nprogram themselves and also teaching them to make decisions using data where\\nwriting software is a bottleneck let the data do the work instead now machine learning is a class of algorithms which is data driven that is unlike normal\\nalgorithms it is the data that does what the good answer is so if we have a look\\nat the various features of machine learning so first of all it uses the data to detect patterns in a data set and adjust the program actions\\naccordingly it focuses on the development of computer programs that can teach themselves to grow and change when exposed to new data so it's not\\njust the old data on which it has been trained so whenever a new data is entered the program changes accordingly it enables computers to find hidden\\ninsights using iterative algorithms without being explicitly programmed either so machine learning is a method of data analysis that automates\\nanalytical model building now let's understand how exactly it Wells so if we have a look at the diagram which is given here we have traditional\\nprogramming on one side we have machine learning on the other so first of all in traditional program what we used to do was provide the data provide the program\\nand the computer used to generate the output so things have changed now so in machine learning what we do is provide the data and we provide a predicted\\noutput to the machine now what the machine does is learns from the data find hidden insights and creates a model now it takes the output data also again\\nand it reiterates and trains and grows accordingly so that the model gets\\nbetter every time it's a strain with the new data or the new output so the first and the foremost application of machine learning in the industry I would like to\\nget your attention towards is the navigation or the Google Maps so Google\\nMaps is probably the app we use whenever we go out and require assistant in directions and traffic right the other day I was traveling to another city and\\ntook the expressway and the math suggested despite the havoc traffic you are on the fastest route no but how does it know that well it's a combination of\\npeople currently using the services the historic data of that fruit collected over time and a few tricks acquired from the other companies everyone using maps\\nis providing their location their average speed the route in which they are traveling which in turn helps Google collect massive data about the traffic\\nwhich may extemporary the upcoming traffic and it adjust your route according to it which is pretty amazing right now coming to the second\\napplication which is the social media if we talk about Facebook so one of the most common application is automatic friend tanks suggestion in Facebook and\\nI'm sure you might have gotten this so it's present in all the other social media platform as well so Facebook uses face detection and image recognition to\\nautomatically find the face of the person which matches its database and hence it suggests us to tag that person based on deep face\\nnow if the face is Facebook's machine learning project which is responsible for recognition of faces and define which person is in the picture and it\\nalso provides alternative tags to the images already uploading on Facebook so for example if we have a look at this image and we introspect the following\\nimage on Facebook we get the alt tag which has a particular description so in\\nour case what we get here is the image may contain sky grass outdoor and nature\\nnow transportation and commuting is another industry where machine learning is used heavily so if you have used an app to book a cab recently then you are\\nalready using machine learning to an extent and what happens is that it provides a personalized application which is unique to you it automatically\\ndetects your location and provides option to either go home or office or any other frequent basis based on your history and patterns it uses machine\\nlearning algorithm layered on top of historic trip date had to make more accurate ETA predictions now uber with the implementation of machine learning\\non their app and their website saw a 26 percent accuracy in delivery and pick up\\nthat's a huge a point now coming to the virtual person assistant as a name\\nsuggests virtual person assistant assist in finding useful information when asked why a voice or text if you have the major applications of machine learning\\nhere a speech recognition speech to text conversion natural language processing and text-to-speech conversion all you need to do is ask a simple question like\\nwhat is my schedule for tomorrow or show my upcoming flights now for answering\\nyour personal assistant searches for information or recalls your related queries to collect the information recently personal assistants are being\\nused in chat pods which are being implemented in various food ordering apps online training web sites and also in commuting apps as well again product\\nrecommendation now this is one of the area where machine learning is absolutely necessary and it was one of the few areas which emerged the need for\\nmachine learning now suppose you check an item on Amazon but you do not buy it then and there but the next day you are watching videos on YouTube and suddenly\\nyou see an ad for the same item you switch to Facebook there also you see the same ad and again you go back to any other side and you see the ad for the\\nsame sort of items so how does this happen well this happens because Google tracks your search history and recommends asked based on your search\\nhistory this is one of the coolest application of machine learning and in fact 35% of Amazon's revenue is generated by the products recommendation\\nnow coming to the cool and highly technological side of machine learning we have self-driving cars if we talk about self-driving car it's here\\nand people are already using it now machine learning plays a very important role in self-driving cars as I'm sure you guys might have heard about Tesla\\nthe leader in this business and the excurrent artificial intelligence is driven by the hardware manufacturer Nvidia which is based on unsupervised\\nlearning algorithm which is a type of machine learning algorithm now in media state that they did not train their model to detect people or any of the\\nobjects as such the model works on deep learning and Traut sources it's data\\nfrom the other vehicles and drivers it uses a lot of sensors which are a part\\nof IOT and according to the data gathered by McKenzie the automotive data will hold a tremendous value of 750 billion dollars but that's a lot of\\ndollars we are talking about it now next again we have Google Translate now remember the time when you travel to the new place and you find it difficult to\\ncommunicate with the locals or finding local spots where everything is written in a different languages well those days are gone\\nGoogle's G and M T which is the Google neural machine translation is a neural\\nmachine learning that works on thousands of languages and dictionary it uses natural language processing to provide the most accurate translation of any\\nsentence of words since the tone of the word also matters it uses other techniques like POS tagging named entity recognition and chunking and it is one\\nof the most used applications of machine learning now if we talk about dynamic pricing setting the rice price for a good or a service is an old problem in\\neconomic theory there are a vast amount of pricing strategies that depend on the objective sort be it a movie ticket a plane ticket or a cafe everything is\\ndynamically priced now in recent year machine learning has enabled pricing solution to track buying trends and determine more competitive product\\nprices now if we talk about uber how does Oberer determine the price of your right who was biggest use of machine learning\\ncomes in the form of surge pricing a machine learning model named as geosearch if you are getting late for a meeting and you need to book an uber in\\na crowded area get ready to pay twice the normal fear even for flats if you're traveling in the festive season the chances are that\\nprices will be twice as much as the original price now coming to the final application of machine learning we have is the online video streaming we have\\nNetflix Hulu and Amazon Prime video now here I'm going to explain the\\napplication using the Netflix example so with over 100 million subscribers there\\nis no doubt that Netflix is the daddy of the online streaming world when Netflix\\nPD dries has all the movie industrialists taken aback forcing them\\nto us how on earth could one single website take on Hollywood now the answer\\nis machine learning the Netflix algorithm constantly gathers massive amounts of data about user activities like when you pause rewind fast-forward\\nwhat do you want the content TV shows on weekdays movies on weekend the date you\\nwatch the time you watch whenever you pause and leave a content so that if you ever come back they would such as the same video the rating events which are\\nabout four million per day the searches which are about three million per day the browsing and the scrolling behavior and a lot more now they collect this\\ndata for each subscriber they have and use the recommender system and a lot of\\nmachine learning applications and that is why they have such a huge customer retention rate so I hope these applications are enough for you to\\nunderstand how exactly machine learning is changing the way we are interacting with the society and how fast it is affecting the world in which we live in\\nso if you have a look at the market trend of the machine learning here so as you can see initially it wasn't much in the market but if you have a look at the\\n2016 side there was an enormous growth in machine learning and this happened\\nmostly because you know earlier we had the idea of machine learning but then again we did not had the amount of big data so as you can see the red line we\\nhave here in the histogram and the power plot is that of the Big Data so Big Data also increased during the years and which led to the increase in the amount\\nof data generated and recently we had that power or I should say the\\nunderlying technology and the hardware to support that power that makes us\\ncreate machine learning programs that will work on the spectator so that is\\nwhy you see very high inclination during the 2016 period time as compared to 2012\\nso because during 2016 we got new hardware and we were able to find insights using those hardware and program and create models which would\\nwork on heavy data now let's have a look at the life cycle of machine learning\\nso a typical machine learning life cycle has six steps so the first step is\\ncollecting data second is video wrangling then we have the third step\\nper be analyzed the data fourth step where we train the algorithm the fifth step is when we test the algorithm and the sixth step is when we deploy that\\nparticular algorithm for industrial uses so when we talk about the fourth step\\nwhich is collecting data so here data is being collected from various sources and this stage involves the collection of all the relevant data from various\\nsources now if we talk about data wrangling so data wrangling is the process of cleaning and converting raw data into a format that allows\\nconvenient consumption now this is a very important part in the machine learning lifecycle as it's not every time that we receive a data which is\\nclean and is in a proper format sometimes their value is missing sometimes there are wrong values sometimes data format is different so a\\nmajor part in a machinery lifecycle goes in data wrangling and data cleaning so\\nif we talk about the next step which is data analysis so data is analyzed to\\nselect and filter the data required to prepare the model so in this step we\\ntake the data use machine learning algorithms to create a particular model\\nnow next again when we have a model what we do is strain the model now here we\\nuse the data sets and the algorithm is trained on between data set through which algorithm understand the pattern and the rules which govern the\\nparticular data once we have trained the algorithm next comes testing so the testing data set determines the accuracy of our\\nmodels so what we do is provide the test dataset to the model and which tells us the accuracy of the particular model\\nwhether it's 60% 70% 80% depending upon the requirement of the company and\\nfinally we have the operation and optimization so if the speed and\\naccuracy of the model is acceptable then that moral should be deployed in the real system the model that is used in the production should be made with all\\nthe available data models improve with the amount of available data used to create them all the result of the moral needs to be incorporated in the business\\nstrategy now after the model is deployed based upon its performance the model is updated and improved if there is a dip in the performance the moral is\\nretrained so all of these happen in the operation and optimization stage now\\nbefore we move forward since machine learning is mostly done in Python and us so and if we have a look at the difference between Python and our I'm\\npretty sure most of the people would go for Python and the major reason why\\npeople go for python is because python has more number of libraries and python is being used in just more than data analysis and machine learning so some of\\nthe important Python libraries here which I want to discuss here so first of all I'll talk about matplotlib now what Matt brought lib does is that it enables\\nyou to make bar charts scatter plots the line charts histogram basically what it\\ndoes is helps in the visualization aspect as data analyst and machine learning ingenious what one needs to represent the data in such a format that\\nit is used that it can be understood by non-technical people such as people from\\nmarketing people from sales and other departments as well so another important\\nPython library here we have a seaborne which is focused on the visuals of statistical models which includes heat maps and depict the overall\\ndistributions sometimes people work on data which are more geographically aligned and I would say in those cases he traps are very\\nmuch required now next we come to scikit-learn and scikit-learn is the one of the most famous libraries of python i would say\\nit's simple and efficient or data mining and for data analysis it is built on\\nnumpy and my rock lab and it is open-source next on our list we have pandas it is the perfect tool for data wrangling which is designed for quick\\nand easy data manipulation aggregation and visualization and finally we have\\nnumpy now numpy stands for a numerical Python provides an abundance of useful\\nfeatures for operation on n arrays which has an umpire's and matrices in spite\\nand mostly it is used for mathematical purposes so which gives a plus point to any machine learning algorithm so as these were the important part in larry's\\nwhich one must know in order to do any price and programming for machine\\nlearning or as such if you are doing Python programming you need to know about all of these libraries so guys next what we are going to discuss other\\ntypes of machine learning so then again we have three types of machine learning which are supervised reinforcement and unsupervised machine learning so if we\\ntalk about supervised machine learning so supervised learning is where you have the input variable X and the output variable Y and you use an algo I know to\\nlearn the mapping function from the input to the output so if we take the case of object detection here so or face detection I rather say so first of all\\nwhat we do is input the raw data in the form of labelled faces and again it's\\nnot necessary that we just input faces to train the model what we do is input a\\nmixture of faces and non-faces images so as you can see here we have labeled face\\nand labeled on faces what we do is provide the data to the algorithm the algorithm creates a model it uses the training dataset to understand what\\nexactly is in a face what exactly is in a picture which is not a face and after\\nthe model is done with the training and processing so to test it what we do is provide particular input of a face or an on face what we know see the major part\\nof supervised learning here is that we exactly know the output so when we are\\nproviding a face we our selves know that it's a phase so to test that particular model and get the accuracy we use the labeled input raw\\ndata so next when we talk about unsupervised learning unsupervised learning is the training of a model using information that is neither\\nclassified nor labeled now this model can be used to cluster the input data in\\nclasses or the basis of the statistical properties for example for a basket full\\nof vegetables we can cluster different vegetables based upon their color or sizes so if I have a look at this particular example here we have what we\\nare doing is we are inputting the raw data which can be either apple banana or mango what we don't have here which was previously there in supervised learning\\nare the labels so what the algorithm does is that it visually gets the features of a particular set of data it makes clusters so what will happen is\\nthat it will make a cluster of red looking fruits which are Apple yellow local fruits which are banana and based upon the shape also it determines what\\nexactly the fruit is and categorizes it as mango banana or apple so this is\\nunsupervised learning now the third type of learning which we have here is reinforcement learning so reinforcement learning is the learning by interacting\\nwith a space or an environment it selects the action on the basis of its past experience the exploration and also by new choices a reinforcement learning\\nagent learns from the consequences of its action rather than from being taught\\nexplicitly so if we have a look at the example here the input data we have what\\nit does is goes to the training goes to the agent where the agent selects the\\nalgorithm it takes the best action from the environment gets the reward and the\\nmodel is strange so if you provide a picture of a green apple although the Apple which it particularly nose is red what it will do is it will\\ntry to get an answer and with the past experience what it has and it will\\nrecreate the algorithm and then finally provide an output which is according to our requirements so now these were the major types of machine learning\\nalgorithms next what we never do is dig deep into all of these types of machine\\nlearning one by one so let's get started with supervised learning first and understand what exactly is supervised learning and what are the different\\nalgorithms inside it how it works the algorithms the working and we'll have a\\nlook at the various algorithm demos now which will make you understand it in a much better way so let's go ahead and understand what exactly is supervised\\nlearning so supervised learning is where you have the input variable X and the\\noutput variable Y and using algorithm to learn the mapping function from the input to the output as I mentioned earlier with the example of face\\ndetection so it is cos subbu is learning because the process of an algorithm learning from the training data set can be thought of as a teacher supervising\\nthe learning process so if we have a look at the supervised learning steps or\\nwhat will rather say the workflow so the model is used as you can see here we\\nhave the historic data then we again we have the random sampling we split the data enter training error set and the testing data set using the training data\\nset we with the help of machine learning which is supervised machine learning we create statistical model now after we have a model which is being generated\\nwith the help of the training data set what we do is use the testing data set for prediction and testing what we do is get the output and finally if we have\\nthe model validation outcome that was third training and testing so if we have a look at the prediction part of any particular supervised learning algorithm\\nso the model is used for operating outcome of a new data set so whenever performance of the model degraded the model is retrained or if there are any\\nperformance issues the model is retrained with the help of the new data now when we talk about supervisor in there are not just one but\\nquite a few algorithms here so we have linear regression logistic regression this is entry we have random forest we have made biased classifiers so linear\\nregression is used to estimate real values for the cost of houses the number of cars the total sales based on the continuous\\nvariable so that is what Rainier generation is now when we talk about logistic regression it is used to estimate discrete values for example\\nwhich are binary values like zero and one yes or no true and false based on the given set of independent way so for example when you are talking about\\nsomething like the chance of winning or if we talk about winning which can be the true or false if will it rain today which it can be the yes or no so it\\ncannot be like when the output of a particular algorithm or the particular question is either yes/no or binary then only we use a logic regression now next\\nwe have decision trees so so these are used for classification problems it works for both categorical and continuous dependent variables and if we\\ntalk over random forest so random forest is an N symbol of a decision tree it\\ngives better prediction and accuracy that decision tree so that is another type of supervised learning algorithm and finally we have the Nate Byars\\nclassifier so it is a classification technique based on the based theorem with an assumption of independence between predictors so we'll get more\\ninto the details of all of these algorithms one by one so let's get started with linear regression so first of all let us understand what exactly\\nlinear regression is so linear regression analysis is a powerful technique you operating the unknown value of a variable which is the\\ndependent variable from the known value of another variable which is the independent variable so a dependent variable is the variable to be predicted\\nor explained in a regression model whereas an independent variable is a variable related to the dependent variable in a regression equation so if\\nyou have a look here as a simple linear regression so it's basically equivalent to a simple line which is with a slope which is y equals a plus B X where Y is\\nthe dependent variable a is the y-intercept we have P which is the slope\\nof the line and X which is the independent variable so intercept is the value of the dependent variable Y when the value of\\nthe independent variable X is 0 it is the the line cuts the y-axis whereas slope is the change in the dependent variable\\nfor a unit increase in the independent variable it is the tangent of the angle made by the line with the x-axis now when we talk about the relation between\\nthe variables we have a particular term which is known as correlation so correlation is an important factor to check the dependencies when there are\\nmultiple variables what it does is it gives us an insight of the mutual relationship among variables and it is used for creating a correlation plot\\nwith the help of the Seabourn library which I mentioned earlier which is one of the most important libraries in Python so correlation is very important\\nterm to know about now if we talk about regression lines so linear regression analysis is a powerful technique used for predicting the unknown value of a\\nvariable which is the dependent variable from the regression line which is simply a single line that best fits the data in terms of having the smallest overall\\ndistance from the line to the points so as you can see in the plot here we have\\nthe different points or the data points so these are known as the fitted points then again we have the regression line which has the smallest overall distance\\nfrom the line to the points so you have a look at the distance between the point\\nto the regression line so what this line shows is the deviation from the\\nregression line so exactly how far the point is from the regression line so\\nlet's understand a simple use case of linear regression with the help of a demo so first of all there is a real state company use case which I'm going\\nto talk about so first of all here we have John he has some baseline for pricing the villa's and the independent houses he has in Boston so here we have\\nthe data set description which we're going to use so this data set has different columns such as the crime rate per capita which is CRI M it has\\nproportional residential residential land zone for the Lots proportion of non\\nretail business the river the United Rock side concentration average number of rooms and the proportion of the owner occupying the built prior to 1940 the\\ndistance of the five Boston employment centers in excess of accessibility to Riedl highways and much more so first of all\\nlet's have a look at the data set we have here so one number I don't thing here guys is that I'm gonna be using Jupiter notebook\\nto execute all my practicals you are free to use the spider notebook or the\\nconsole either so it basically comes down to your preference so for my\\npreference I'm going to use the Jupiter notebook so for this use case we're gonna use the Boston housing data set so as you can see here we have the data set\\nwhich has the CRI mzn in desc CAS NO x the different variables and we have the\\ndata set of form almost I would say like 500 houses so what John needs to do is\\nplan the pricing of the housing depending upon all of these different\\nvariables so that it's profitable for him to sell the house and it's easier for the customers also to buy the house so first of all let me open the code\\nhere for you so first of all what we're gonna do is import the library is necessary for this project so we're going to use the numpy we're going to\\nimport numpy as NP import pandas at PD then we're gonna also import the\\nmatplotlib and then we are going to do is read the Boston housing data set into\\nthe BOS one variable so now what we are going to do is create two variables x\\nand y so what we're gonna do is take 0 to 13 I'll say is from CR I am two LS\\ndat in 1x because that's the independent variable and Y here is dependent\\nvariable which is the MA TV which is the final price so first of all what we need to do is plot a correlation so what we're gonna do is import the Seabourn\\nlibrary as SN s we're going to use the correlations to plot the correlations\\nbetween the different 0 to 13 variables what we gonna do is also use ma DV here\\nalso so what we're going to do is SN s dot heatmap correlations to be going to use the square to differentiate usually it comes up in\\nsquare only or circles so you don't know so we're gonna use square you want to\\nsee you see map with the Y as GNP you this is the color so there's no rotation\\nin the y axis and we're gonna rotate the excesses to the 90 degree and let's we\\ngonna plot it now so this is what the plot looks like so as you can see here the more thicker or the more darker the color gets the more is the correlation\\nbetween the variables so for example if you have a look at CRI M and M a DV\\nright so as you can see here the color is very less where the correlation is very low so one thing important what we can see here is the tax and our ad which\\nis the full value of the property and RIT is the index of accessibility to the\\nradial highways now these things are highly correlated and that is natural\\nbecause the more it is connected to the highway and more closer it is to the highway the more easier it is for people to travel and hence the tax on it is\\nmore as it is closer to the highways now what we're going to do is from SQL and dot cross-validation we're going to import the Train test split and we're\\ngonna split the data set now so what we are going to do is create four variables which are the extreme X test Y train white tests and we're going to use a\\ntrain test split function to split the x and y and here we're going to use the test size 0.3 tree which will split the data set into the test size will be 33%\\nwell as the training size will be 67% now this is dependent on you usually it\\nis either 60/40 70/30 this depends on your use case your data you have the\\nkind of output you are getting the model you are creating and much more then again from SQL learn dot linear model we're going to import linear regression\\nnow this is the major functions we're gonna use just linear regression function which is present in SQL which is a scikit-learn so we going to create\\nour linear regression model into LM and the model which are going to create and we're going to fit the training videos which has the X train and the why train\\nthen we're gonna create a prediction underscore 5 which is the LM dot credit and I take the X test variables which will provide the predicted Y variables\\nso now finally if we plot the scatter plot of the Y test and the y predicted what we can see is that and we give the X label as white test and the Y label\\nhas y predicted we can see the regression line which we have plotted in at the scatter plot and if you want to draw a regression line it's usually it\\nwill go through all of these points excluding the extremities which are here present at the endpoints so this is how a normal linear regression works in\\nPython what you do is create a correlation you find out you split the dataset into training and testing variables then again you define what is\\ngoing to be your test size import the reintegration moral use the training data set into the model fitted use the test data set to create the predictions\\nand then use the wireless code test and the predicted Y and plot the scatter\\nplot and see how close your model is doing with the original data it had and\\ncheck the accuracy of that model now typically you use these steps which was\\ncollecting data what we did data wrangling analyze the data we trained the algorithm we use the test algorithm and then we deployed so fitting a model\\nmeans that you are making your algorithm learn the relationship between predictors and the outcomes so that you can predict the future values of the\\noutcome so the best fitted model has a specific set of parameters which best\\ndefines the problem at hand since this is a linear model with the equation y equals MX plus C so in this case the parameters of the model learns from the\\ndata that are M and C so this is what more fitting now if it have a look at\\nthe types of fitting which are available so first of all machine learning algorithm first attempt to solve the problem of underfitting\\nthat is of taking a line that does not approximate the data well and making it\\napproximate to the data better so machine does not know where to stop in order to solve the problem and it can go ahead from appropriate to overfit more\\nsometimes when we say a model overfits we mean that it may have a low error\\nrate for training data but it may not generalize well to the overall population of the data we are interested in so we have under fact appropriate and\\nover fit these are the types of fitting now guys this was linear regression which is a type of supervised learning algorithm in machine learning so next\\nwhat we're going to do is understand the need for logistic regression so let's\\nconsider a use case as in political elections are being contested in our country and suppose that we are interested to know which candidate will\\nprobably win now the outcome variables result in binary either win or lose the\\npredictor variables are the amount of money spent the age the popularity rank and etc etcetera now here the best fit line in the regression war is going\\nbelow 0 and above what and since the value of y will be discrete that is between 0 & 1 the linear rain has to be clipped at 0 & 1 now linear regression\\ngives us only a single line to classify the output with linear regression our\\nresulting curve cannot be formulated into a single formula as you obtain three different straight lines what we need is a new way to solve this problem\\nso hence people came up with logistic regression so let's understand what\\nexactly is logic regression so logistic regression is a statistical method for\\nanalyzing a data set in which there are 1 or more independent variables that determine an outcome and the outcome is a binary class type so example a patient\\ngoes a followed a teen checkup in the hospital and his interest is to know whether the cancer is benign or malignant now a patient's data such as\\nsugar level blood pressure eight skin width and the previous medical history are recorded and a daughter checks the patient data and it reminds the outcome\\nof his illness and severity of illness the outcome will result in binary that\\nis zero if the cancer is malignant and one if it's been I know no strict\\nregression is a statistical method used for analyzing a dataset there were say one or more dependent variables like we discuss like the sugar level blood\\npressure skin with the previous medical history and the output is binary class type so now let's have a look at the lowest ik\\nregression curve now the law disintegration code is also called a sigmoid curve or the S curve the sigmoid function converts any value from minus\\ninfinity to infinity to the discrete value 0 or 1 now how to decide whether the value is 0 or 1 from this curve so let's take an example what we do is\\nprovide a threshold value we set it we decide the output from that function so let's take an example with the threshold value of 0.4 so any value above 0.4 will\\nbe rounded off to 1 and anyone below 0.4 we really reduce to 0 so similarly we\\nhave polynomial regression also so when we have nonlinear data which cannot be predicted with a linear model we switch to the polynomial regression now such a\\nscenario is shown in the below graph so as you can see here we have the equation y equals 3x cubed plus 4x squared minus 5x plus 2 now here we cannot perform\\nthis linearly so we need polynomial regression to solve these kind of problems now when we talk about logistic regression there is an important term\\nwhich is decision tree and this is one of the most used algorithms in\\nsupervised learning now let's understand what exactly is a decision tree so our decision tree is a tree like structure in which internal load represent tests\\non an attribute now each attribute represents outcome of test and each leaf\\nnode represents the class label which is a decision taken after computing all\\nattributes apart from root to the leaf represents classification rules and a decision tree is made from our data by analyzing the\\nvariables from the decision tree now from the tree we can easily find out whether there will be came tomorrow if the conditions are rainy and less windy\\nnow let's see how we can implement the same so suppose here we have a data set in which we have the outlook so what we can do is from each of the Outlawz we\\ncan divide the data as sunny overcast and rainy so as you can see in the sunny\\nside we get two yeses and three noes because the outlook is sunny the humidity is now and oven is weak and strong so it's a\\nfully sunny day what we have is that it's not a pure subset so what we're\\ngonna do is split it further so if you have a look at the overcast we have humidity high normal week so yes during overcast weekend play and if you\\nhave a look at the Raney's area we have three SS and - no so again what we're going to do is split it further so when we talk of a sunny then we have humidity\\nin humidity we have high and normal so when the humidity is normal we're going\\nto play which is the pure subset and if the humidity is high we are not going to play which is also a pure subset now so let's do the same for the rainy day so\\nduring rainy day we have the vent classifier so if the wind is to be it\\nbecomes a pure subset we're going to play and if the vent is strong it's a pure substance we not gonna play so the final decision tree looks like this so\\nfirst of all we check if the outlook is sunny overcast or rain if it's overcast\\nwe will play if it's sunny we then again check the humidity if the humidity is\\nhigh we will not play if the humidity is normal real play then again in the case of rainy if we check the vent if the wind is weak the play will go on and\\nsimilarly if the wind is strong the play must stop so this is how exactly a\\ndecision tree works so let's go ahead and see how we can implement logisitics relation in decision trees now for logistic regression we're going to use\\nthe Casa data set so this is how the data set looks like so here we have the\\neye diagnosis radius mean - I mean parameter mean these are the stats of particular cancer cells or the cyst which are present in the body so we have\\nlike total 33 columns all the way starting from IDE - unnamed 32 so our\\nmain goal here is to define whether or I'll say predict whether the cancer is\\npinang on mannequin so first of all what vinegar - is from scikit-learn dot small\\nselection we're gonna import cross-validation score and again we're going to use numpy for linear algebra we're gonna use\\npandas as PD because for data processing the CSV file input for data manipulation\\nin sequel and most of the stuff then we're going to import the matplotlib it\\nis used for plotting the graph we're going to import Seabourn which is used to plot interactive graph like in the last example we saw we plotted a heatmap\\ncorrelation so from SK learn we're going to import the logistic regression which\\nis the major model or the algorithm behind the whole logic regression we're\\ngonna import the train dressed split so as to split the raita into two paths training and testing data set we're going to import metrics to check the\\nerror and the accuracy of the model and we're gonna import decision tree\\nclassifier so first of all what we're gonna do is create a variable data and\\nuse the pandas PD to read the data from the data set so here the header 0 means\\nthat the zeroth row is our column name and if we have a look at the data or the top six part of the data we're going to use the friend data dot head and get the\\ndata dot info so as you can see here we have so many data columns such as highly\\ndiagnosis radius being in text remain parameter main area means smoothness mean we have texture worst symmetry worst we have fractal dimension worse\\nand lastly we have the unnamed so first of all we can see we have six rows and 33 columns and if you have a look at all of these columns here right we get the\\ntotal number which is the 569 which is the total number of observation we have and we check whether it's non null and then again we check the type of the\\nparticular column so it's integer it's object float mostly most of them are float some are integer so now again we're going to drop the unnamed column\\nwhich is the column 30 second 0 to 33 which is the 30 second column so in this\\nprocess we will change it in our data itself so if you want to save the old data you can also see if that but then again that's of no use so theta dot\\ncolumns will give us all of these columns when we remove that so you can see here in the output we do not have the final one which was the unnamed\\nthe last one we have is the type which is float so latex we also don't want the\\nID column for our analysis so what we're gonna do is we're gonna drop the ID again so as I said above the data can be divided into three paths so let's divide\\nthe features according to their category now as you know our diagnosis column is object type so we can map it to the integer value so we what we wanna do is\\nuse the data diagnosis and we're gonna map it to M 1 and B 0 so that the output\\nis either M or B now if we use a rated or described so you can see here we have\\n8 rows and 1 columns because we dropped two of the columns and in the diagonals we have the values here let's get the frequency of the cancer stages so here\\nwe're going to use the Seabourn SNS not count plot data with diagnosis and Lee\\nwill come and if we use the PLT dot show so here you can see the diagnosis for 0 is more and for 1 is less if you plot the correlation among this data so we're\\ngoing to use the PLT dot figure SNS start heat map we're gonna use a heat map we're going to plot the correlation c by true we're going to use square true\\nand we're gonna use the cold warm technique so as you can see here the correlation of the radius worst with the area worst and the parameter worst is\\nmore whereas the radius worst has high correlation to the parameter mean and\\nthe area mean because if the radius is more the parameter is more area is more so based on the core plot let's select some features from the model now the\\ndecision is made in order to remove the : era t so we will have a prediction variable in which we have the texture mean the parameter mean the smoothness\\nmean the compactors mean and the symmetry mean but these are the variables which we'll use for the prediction now we'll gonna split the\\ndata into the training and testing data set now in this our main data is splitted into training a test data set with the 0.3 test size that is 30 to 70\\nratio next what we're going to do is check the dimension of that training and the testing data says so what we're going to do is use the print command and\\npass the parameter train dot shape and test our shape so what we can see here is that we have almost like 400 398 observations were 31 columns in the\\ntraining dataset whereas 171 rows and 31 columns in the testing dataset so then again what we're going to do is take the training data\\ninput what we're going to do is create a Train underscore X with the prediction underscore rad and train is for y is for the diagnosis now this is the output of\\nour training data same as we did for the test so we're going to use test underscore X for the test prediction variable and test underscore Y for the\\ntest diagnosis which is the output of the test data now we're going to create a logistic regression method and create a model logistic dot fit in which you're\\ngoing to fit the training data set which is strain X entering Y and then we're going to use a TEM P which is a temporary variable in which you can\\noperate X and then what we're going to do is we're going to compare to EMP which is a test X with the test Y to check the accuracy so the accuracy here\\nwe get is 0.9 1 then again what we need to do this was like location normal\\nroads retribution are we going to use classifier so we're going to create a decision tree classifier with random state given as 0 now what next we're\\ngoing to do is create the cross-validation school which is the CLF we take the moral we take the train X 3 and Y and C V equals 10 the\\ncross-validation score now if we fit the training test and the sample weight we\\nhave not defined here check the input of his true and XID x sorted is none so if\\nwe get the parameters true we predict using the test X and then predict the\\nlong probability of test X and if we compare the score of the test X to test\\nY with the sample weight none we get the same result as a decision tree so this\\nis how you implement a decision tree classifier and check the accuracy of the particular model so that was it so next on our list is random forest so\\nlet's understand what exactly is a random forest so random forest is an\\nsymbol classifier made using many decision tree models so so what exactly\\nare in symbol malls so n symbol malls combines the results from different models the result from an N simple mall is usually better than the result of the\\none of the individual model because every tree votes for one class the final decision is based upon the majority of votes and it is better than decision\\ntree because compared to decision tree it can be much more accurate it rests if efficiently on the last data set it can handle thousands of input variables\\nwithout variable deletion and what it does is it gives an estimate of what variables are important in the classification so let's take the example\\nof weather data so let's understand I know for us with the help of the\\nhurricanes and typhoons data set so we have the data about hurricanes and typhoons from 1851 to 2014 and the data comprises off location when the pressure\\nof tropical cyclones in the Pacific Ocean the based on the data we have to classify the storms into hurricanes typhoons and the sub categories as\\nfurther to predefined classes mentioned so the predefined classes are TD tropical cyclone of tropical depression intensity which is less than 34 knots if\\nit's between thirty four to six to 18 oz it's D s greater than 64 knots it's a cheer which is a hurricane intensity e^x is esta tropical cyclone s T is less\\nthan 34 it's a subtropical cyclone or subtropical depression s s is greater\\nthan 34 which is a subtropical cyclone of subtropical storm intensity and then again we have L o which is a low that is neither a tropical cyclone a tropical\\nsubtropical cyclone or non and extraterrestrial cyclone and then again finally we have DB which is disturbance of any intensity now these were the\\npredefined classes description so as you can see this is the data in which we have the ID name date event say this line it's your longitude maximum when\\nminimum when there are so many variables so let's start with imp\\nthe pandas then again we import the matplotlib then we gonna use the aggregate method in matplotlib we're going to use the matplotlib in line\\nwhich is used for plotting interactive graph and I like it most for plots so next what we're going to do is import Seabourn as SNS now this is used to plot\\nthe graph again and we're going to import the model selection which is the Train test split so we're gonna import it from a scaler and the scikit-learn\\nwe have to import metrics watching the accuracy then we have to import sq learn\\nand then again from SQL and we have to import tree from SQL or dot + symbol we're gonna import the random forest classifier from SQL and Road metrics\\nwe're going to import confusion matrix so as to check the accuracy and from SQL\\nand on message we're gonna also import the accuracy score so let's import random and let's read the dataset and print the first six rows\\nof the data sets you can see here we have the ID we have the name date time it will stay this latitude longitude so in total we have 22 columns here so as\\nyou can see here we have a column name status which is TS TS TS for the four\\nsix so what we're gonna do is data at our state as visible P dot categorical\\ndata the state so what we can do is make it a categorical data with quotes so\\nthat it's easier for the machine to understand it rather than having certain categories as means we're gonna use the categories as numbers so it's easier for\\nthe computer to do the analysis so let's get the frequency of different typhoons so what we're going to do is random dot seed then again what are we gonna do is\\nif we have to drop the status we have to drop the event because these are unnecessary we're gonna drop latitude longitude we're gonna drop ID then name\\nthe date and the time it occurred so if we print the prediction list so ignore\\nthe error here so that's not necessary so we have the maximum and minimum and pressure low went any low when deci low when s top blue and these are the\\nparameters on which we're going to do the predictions so now we'll split that into training and testing data sets so then again we have the trained comet\\ntest and we're gonna use a trained test split especially in the 70s of 30 industrial standard ratio now important thing here to note is that you can split\\nit in any form you want can be either 60/40 70/30 80/20 it all depends upon\\nthe model which you have our the industrial requirement which you have so then again if after printing let's check the dimensions so the training dataset\\ncomprises of eighteen thousand two hundred and ninety five rows were twenty two columns whereas the testing dataset comprised of eight thousand rows with\\ntwenty two columns we have the training data input train x we had a train y so\\nstatus is the final output of the training data which will tell us the status whether it's a TS d d which it's an hu which kind of a hurricane or\\ntyphoon or any kind of subcategories which are defined which were like subtropical cyclone the subtropical typhoon and much more so our prediction\\nor the output variable will be status so so this is these are the list of the training columns which we have here now same we have to do for the test variable\\nso we have the test x with the prediction underscore rat with a test y with the status so now what we're going to do is build a random foils classifier\\nso in the model we have the random forest classifier with estimators as 100\\na simple random for small and then we fit the training data set which is a training X and train by then we again make the prediction which is the world\\nor predict that with the test underscore X then that and this will predict for\\nthe test data and prediction will contain the rated value by our model predicted values of the diagnosis column for the test inputs so if you print the\\nmetrics of the accuracy score between the prediction and the test and a score why to check the accuracy we get 95% accuracy now the same if we're going to\\ndo with decision tree so again we're gonna use the model tree dot decision\\ntree classifier we're going to use the Train X and tree in Y which other training data sets new prediction is smaller for a task or\\ntext we're going to create a data frame which is the Parador data frame and if\\nwe have a look at the prediction and the test underscore Y you can see the state has 10 10 3 3 10 10 11 and 5 5 3 11 and 3 3 so it goes on and on so it has 7840\\n2 rows and 1 column and if you print the accuracy we get a ninety-five point five\\nseven percent of accuracy and if you have a look at the accuracy of the random for us we get 95 point six six percent which is more than 95 point five\\nseven so as I mentioned earlier usually random forest gives a better output or\\ncreates a better more than the decision tree classifier because as I mentioned\\nearlier it combines the result from different models you know so the final decision is based upon the majority of votes and is usually higher than the\\ndecision tree models so let's move ahead with our knee by selca rhythm and let's\\nsee what exactly is neat bias so nave bias is a simple but surprisingly powerful algorithm for predictive modeling now it is a classification\\ntechnique based on the base theorem with an assumption of independence among\\npredictors it comprises of two parts which are the nave and the bias so in simple terms an a bias classifier assumes that the presence of a\\nparticular feature in a class is unrelated to the presence of any other feature even of these features depend on each other or upon the existence of the\\nother features all of these properties independently contribute to the probability that a fruit it's an apple or an orange and that is why it is known\\nas a noun a base model is easy to build and particularly useful for very large data sets in probability theory and statistics Bayes theorem which is\\nalternatively known as the base law or the Bayes rule also emitted as Bayes theorem describes the probability of an event based on the prior knowledge of\\nconditions that might be related to the event so Bayes theorem is a way to figure out the conditional probability now conditional probability is the\\nprobability of an event happening given that it has some to one or more other events for example your probability of getting a parking\\nspace is connected to the town today you park where you park and what conventions\\nare going on at the same time so base Hyrum is slightly more nuanced and a\\nnutshell it gives us the actual probability of an event given information about tests so let's talk about the base Hyrum now so now given\\nany I policies edge and evidence II Bayes theorem states that the relationship between the probability of the hypothesis before getting the\\nevidence pH and the probability of the hypothesis after getting the evidence which is P H bar e is PE bar H into probability of H there are a probability\\nof e which means it's the probability of even after in the hypothesis inter\\npriority of the hypothesis divided by the probability of the evidence so let's understand it with a simple example here so now for example if a single card is\\ndrawn from standard deck of playing cards the probability of that card being a king is 4 out of 52 now since there are 4 kings in a standard deck of 52\\ncards the rewarding this if the king is the event this card is a king the\\npriority of the king that is the probability of king equals 4 by 52 which\\nin turn is 1 by 30 now if the event is is varieties or instance someone looks\\nat the card that the single card is a face card then the posterior probability which is the P of King given it's a face can be calculated using the Bayes\\ntheorem given the probability of King given its face is equal to probability of the face given its a king there is a probability of face into the probability\\nof King since every King is also a face card so the probability of face given\\nits a king is equal to 1 and since there are 3 face cards in each suit that are jacking and Queen the probability of face card is 3 out of 30\\ncombining these given likelihood ratios are we get the value using the paste\\ntheorem of probability of King events of face is equal to 1 out of 3 so foreign\\njoint probability distribution with events a and B the probability of a intersection B which is the conditional probability of a given B is now defined\\nas property of intersection B divided by the probability of B now this is how we get the base theorem now that we know the different basic proof of how we got\\nthe base theorem so let's have a look at the working of the base your answer with the help of an examples here so let's take the same example of the radius set\\nof the these forecasts in which we had the sunny rainy overcast so first of all what we're gonna do is first we will create a\\nfrequency table using each attribute of the data set so as you can see here we have the frequency table here for the outlook humidity and the wind so for\\nOutlook we have the frequency table here we have the frequency table for humidity and the wind so next what we're gonna do is create the probability of sunny given\\nsay s that is three out of ten find the probability of sunny which is five out of 14 and this 14 comes from the total number of observations there and from\\nyes and no so similarly we're gonna find the probability of yes also which is 10\\nout of 14 which is 0.7 one for each frequency table will generate these kind\\nof likelihood tables so the likelihood of yes given it's a sunny is equal to 0.51 similarly the likelihood of no given\\nsunny is equal to 0.40 so here you can look that using Bayes theorem we have\\nfound out the likelihood of yes given it's a sunny and no given it's a sunny similarly we're gonna do the save all likelihood table for humidity and the\\nsame for wind so for humidity we're gonna check the probability of yes given its high humidity is high probability of plane no given the humidity is high is\\nyour going to calculate it using the same base theorem so suppose we have a day with the following values in which we have the outlook as rain humidity as\\nhigh and wind as we since we discussed the same example earlier with the decision tree we know the answer so let's not get ahead of ourselves and\\nlet's try to find out the answer using the Bayes theorem let's understand how neat bass works actually so first of all we gonna use\\nthe likelihood of yes on that day so that equals to probability of Outlook of rain given it's a yes into probability of humidity high given SAS interpretive\\nNVQ NCS into probability of yes okay so that gives us zero point zero one nine\\nsimilarly they're probably likelihood of no on that day is the outlook is rain in units and no humidity is high given its and no and win this week given so know\\nthat equals to zero point zero one six now what we're going to do is find the probability of V s and no and for that what we're going to do is take the\\nprobability the likelihood and divide it with the sum of the likelihoods obvious and known so and that really gonna get the probability of yes overall so you\\nthink that formula we get the probability of years as zero point five five and the probability of no as zero point four five and our model predicts\\nthat there is a fifty five percent chance that there will be game tomorrow if it's rainy the humidity is high and the wind is\\nweak now if you have a look at the industrial use cases of any bias we have new scatterings use categorization as what happens is that the news are comes\\nin a lot of tags and it has to be categorized so that the user gets information he needs in a particular format then again we have spam filtering\\nwhich is one of the major use cases of Nate Byars classifier as it classifies the email as spam or ham then finally we have with a prediction also as we saw\\njust with the example that we predict whether we're going to play or not that sort of prediction is always there so guys this was all about supervised\\nlearning we discussed linear regression logistic regression we discussed named\\npies we've discussed random forests decision tree and we understood how the\\nrandom forest is better than decision tree in some cases it might be equal to\\ndecision tree but nonetheless it's always gonna provide us a better result so guys that was all about the supervised learning so but before that\\nlet's go ahead and see how exactly we're gonna implement nay bias so guys here we have another data set run or walk it's the kinematic data sets\\nand it has been measured using the mobile sensor so let the target were able to be Y assign all the columns after it to X using scikit-learn a by a\\nsmall we're going to observe the accuracy generate a classification report using scikit-learn now we're going to repeat the model once\\nonly the acceleration values as predictors and then using only the gyro value aspirators and we're going to comment on the difference in accuracy\\nbetween the two moles so here we have a data set which is run or walk so let me\\nopen that for you so here I was data sets run or walk so as you can see we\\nhave the date time user name risk activity acceleration XY assertions see Cairo ex Cairo y Cairo Z so based on it let's see how we can implement the name\\nby is classifier and so first of all what we're gonna do is import pandas at speedy then we gonna import matplotlib for plotting we're gonna read the run or\\nwalk data file with pandas period or tree and a CSV let's have a look at the\\ninfo so first of all we see that we have 88 thousand five hundred eighty eight\\nrows with 11 columns so we have the date/time username rest activity\\nassertion XYZ Cairo XYZ and the memory uses is send point 4 MB data so this is\\nhow you look at the columns D F dot columns now again we're gonna split the dataset into training and testing data sets so we're going to use the Train\\ntest flight model so that's what we're gonna do is split it into X train X test\\ny train by test and we're gonna split it into the size of 0.2 here so again I am\\nsaying it depends on you what is the test size so let's print the shape of\\nthe training and see it's 70,000 observation has six columns now what\\nwe're going to do is from the scikit-learn dot knee pius we're going to import the caution NB which is the question a bias and we're going to put\\nthe classifier as caution NB then we'll pass on the extreme and white rain\\nvariables to the classifier and again we have the wireless co-credit which is the\\nclassifier predict X text and we gonna compare the Y underscore predict with the y underscore test to see the accuracies for that so for that we're\\ngoing to import sq learn dot matrix we're going to import the accuracy score now let's compare both of these so the accuracy what we get is ninety five\\npoint five four percent now another way is to get a confusion matrix bill so\\nfrom scikit-learn dot matrix we're going to import the confusion matrix and we're gonna plot the matrix of five predict and white test so as you can see here we\\nhave 90 and 699 that's a very good number so now what we're gonna do is\\ncreate a classification report so from metrics we're gonna import the classification because reports we're going to put the target names as walk\\ncomma run and friends the report using white s and by predict within target means we have so for walking we get the precision of 0.92 and the recall of 0.99\\nf1 score is zero point nine six the support is eight thousand six hundred seventy three and for runway appreciation of ninety ninety percent\\nwith the recoil of 0.92 and f1 score of zero point 95 so guys this is how you\\nexactly use the Gaussian in me or the new pie's classifier on it and all of\\nthese types of algorithms which are present in the supervisor or unsurprised or reinforcement learning are all present in the cyclotron library so one\\nsecond assist SQL learn is a very important library when you are dealing with machine learning because you do not have to code any algorithm hard coding\\nalgorithm every algorithm is present there all you have to do is just passed it either split the dataset into training and testing dataset and then\\nagain you have to find the predictions and then compare the predicted Y with\\nthe test case Y so that is exactly what we do every time we work on a machine\\nlearning algorithm now guys that was all about supervised learning let's go ahead and understand what exactly is unsupervised learning so sometimes the\\ngiven data is unstructured and unlabeled so it becomes difficult to classify the data into different categories so answer learning helps to solve this problem\\nthis learning is used to cluster the input data in classes on the basis of their statistical properties so example we can cluster different bikes based\\nupon the speed limit their acceleration or the average that they are giving so\\nI'm supporting is a type of machine learning algorithm used to draw inferences from Veda sets consisting of input data without labeled responses so\\nif you have a look at the workflow or the process flow of unsupervised learning so the training data is collection of information without any\\nlabel we have the machine learning algorithm and then began the clustering models so what it does is that distributes the data into different\\nclusters and again if you provide any unlabeled new data it will make a prediction and find out to which cluster that particular data or the data set\\nbelongs to or the particular data point belongs to so one of the most important algorithms in unsupervised learning is clustering so let's understand exactly\\nwhat is clustering so a clustering basically is the process of dividing the datasets into groups consisting of similar data points\\nit means grouping of objects based on the information found in the data describing the objects or their relationships so clustering models\\nfocused on identifying groups of similar records and labeling records according\\nto the group to which they belong now this is done without the benefit of prior knowledge about the groups and their characteristics so and in fact we\\nmay not even know exactly how many groups are there to look for now these models are often referred to as unsupervised learning models since there\\nis no external standard by which to judge the models classification performance there are no right or wrong answers to these model and if we talk\\nabout why clustering is used so the goal of clustering is to determine the intrinsic group in a set of unlabeled data sometime the partitioning is the\\ngoal or the of clustering algorithm is to make sense of and exact value from the last set of structured and unstructured data so that\\nis why clustering is used in the industry and if you have a look at the various use cases of clustering in the industry so first of all it's being used\\nin marketing so discovering distinct groups in customer databases such as customers who make a lot of long-distance calls\\ncustomers who use internet more than cause they also using insurance companies for like identifying groups of cooperation insurance policyholders with\\nhigh average game rate farmers crash cops which is profitable they are using\\ncease mix studies and define probable areas of oil or gas exploration based on Seesmic data and they're also used in the recommendation of movies if you\\nwould say they are also used in Flickr photos they also use by Amazon for\\nrecommending the product which category it lies in so basically if we talk about clustering there are three types of clustering so first of all we have the\\nexclusive clustering which is the hard clustering so here an item belongs exclusively to one cluster not several clusters and the data point belong\\nexclusively to one cluster so an example of this is the k-means clustering so claiming clustering does this exclusive kind of clustering so secondly we have\\noverlapping clustering so it is also known as soft clusters in this an item\\ncan belong to multiple clusters as its degree of association with each cluster\\nis shown and for example we have fuzzy or the C means clustering which means\\nbeing used for overlapping clustering and finally we have the hierarchical clustering so when two clusters have a painting change relationship or a\\ntree-like structure then it is known as hierarchical cluster so as you can see here from the example we have a pain child kind of relationship in the\\ncluster given here so let's understand what exactly is k-means clustering so today means clustering is an inquiry um whose main goal is to group similar\\nelements of data points into a cluster and it is the process by which objects\\nare classified into a predefined number of groups so that they are as much it is similar as possible from one group to another\\ngroup but as much as similar or possible within each group now if you have a look\\nat the algorithm working here you're right so first of all it starts with an defying the number of clusters which is key then again we find the centroid we\\nfind the distance objects to the distance object to the centroid distance\\nof objects to the centroid then we find the grouping based on the minimum distance has the centroid converge if true then we make a cluster false we\\nthen I can find the centroid repeat all of the steps again and again so let me\\nshow you how exactly clustering was with an example here so first we need to decide the number of clusters to be made now another important task here is how\\nto decide the important number of clusters or how to decide the number of clusters we'll get into that later so force let's assume that the number of\\nclusters we have decided is three so after that then we provide the centroids\\nfor all the creditors which is guessing the algorithm calculates the Euclidean\\ndistance of the point from each centroid and assigns the data point to the closest cluster now Euclidean distance all of you know is the square root of\\nthe distance the square root of the square of the distance so next when the\\ncentroids are calculated again we have our new clusters for each data point then again the distance from the points to the new clusters are calculated and\\nthen again the points are assigned to the closest cluster and then again we have the new centroid scatter it and now these steps are repeated until we have a\\nrepetition in the centroids or the new centers are very close to the very previous ones so until unless our output gets repeated or the outputs are very\\nvery close enough we do not stop this process we keep on calculating the Euclidean distance of all the points to the centroids then we calculate the new\\ncentroids and that is how claiming is clustering works basically so an important part here is to understand how to decide then value of K or the number\\nof clusters it does not make any sense if you do not know how many class are you going to make so to decide the number of clusters\\nwe have the elbow method so let's assume first of all compute the sum squared\\nerror which is the SS e for some value of K for example let's take two four six\\nand eight now the SS e which is the sum squared error is defined as a sum of the squared distance between each number member of the cluster and its centroid\\nmathematically and if you mathematically it is given by the equation which is provided here and if you brought the key against the SS II you will see that the\\nerror decreases as K gets large now this is because the number of cluster increases they should be smaller so this distortion is also smaller now the idea\\nof the elbow method is to choose the key at which the SSE decreases abruptly so\\nfor example here if we have a look at the figure given here we see that the\\nbest number of cluster is at the elbow so as you can see here the graph here genius abruptly after number four so for this particular example we're going to\\nuse for as a number of cluster so first of all while working with k-means\\nclustering there are two key points to know first of all be careful about where\\nyou start so choosing the first Center at random choosing the second Center that is far away from the first Center some of it choosing the NH Center as far\\naway possible from the closest of the all the other centers and the second idea is to do as many runs of k-means each with different random standing\\npoints so that you get an idea where exactly and how many clusters you need to make and where exactly the centroid lies and how the data is getting\\nconverged now he means he's not exactly a very good method so let's understand the pros and cons of k-means clustering z' we know that k-means is simple and\\nunderstandable everyone don't see that the first go the items automatically assigned to the clusters now if we have a look at the\\ncorns so first of all one needs to define the number of clusters this is a very heavy task as us if we have 3/4 or if we have 10 categories and if you do\\nnot know but number of clusters are gonna be it's very difficult for anyone to you know to guess the number of clusters now all the\\nitems are forced into clusters whether they are actually belong to any other cluster or any other category they are forced to to lie in that other category\\nin which they are closest to and this against happens because of the number of clusters with not defining the correct number of clusters or not being able to\\nguess the correct number of clusters so and most of all it's unable to handle the noisy data and the outliners because anyways and machine learning engineers\\nand data scientists have to clean the data but then again it comes down to the\\nanalysis what they are doing and the method that they are using so typically\\npeople do not clean the data for k-means clustering or even if the clean there are sometimes are now see noisy and outliners data which affect the whole\\nmodel so that was all for k-means clustering so what we're gonna do is now a use k-means clustering for the movie data sets so we have to find out the\\nnumber of clusters and divide it accordingly so the use case is that first of all we have at the air set of five thousand movies and what we want to\\ndo is group them look the movies into clusters based on the facebook lights so\\nguys let's have a look at the demo here so first of all what we're gonna do is import deep copy numpy pandas Seabourn the various libraries which we're going\\nto use now and from map rat levels when you use ply PI plot and we're gonna use\\nthis GD plot and next what we're gonna do is import the data set and look at the shape of the data set so if you have a look at the shape of the data set we\\ncan see that it has five thousand and forty three rows with 28 columns and if you have a look at the head of the data set we can see it has five thousand\\nforty three data points so what we're gonna do is place the data\\npoints in the plot we take the director Facebook Likes and we have a look at the\\ndata columns yeah face number in poster cast total Facebook Likes director\\nFacebook Likes so what we have done here now is taking the director Facebook\\nLikes and the actor 3 Facebook Likes right so we have five thousand forty three rows and two columns now using the key means from s key alone what we're\\ngoing to do is import it first when import key means from SQL or cluster remember guys sq done is a very important library in Python for machine\\nlearning so and the number of cluster what we're gonna do is provide as five note this again the number of cluster depends upon the SSE which is the sum\\nsquared errors or the we're going to use the elbow method so I'm not going to go into the details of that again so we're gonna fit the data into the k-means dot\\nfit and if you find the cluster centers then for the k-means and print it so\\nwhat we find is is an array of five clusters and if you print the label of\\nthe k-means cluster now next what we're gonna do is plot the data which we have\\nwith the clusters with the new data clusters which we have found and for this we're going to use the Seabourn and as you can see here we have plotted the\\ncard we have plotted the data into the grid and you can see here we have five\\nclusters so probably what I would say is that the cluster three and the cluster\\nzero are very very close so it might depend see that's exactly what I was\\ngoing to say is that initially the main challenge and k-means clustering is to define the number of centers which are the key so as you can see here that the\\nthird center and the zeroth cluster the third cluster and is your cluster are\\nvery very close to each other so guys it probably could have been in one another cluster and the another disadvantage was that we do not exactly know how the\\npoints are to be arranged so it's very difficult to force the data into any other cluster which makes our analysis a little different\\nworks fine but sometimes it might be difficult to code in the k-means clustering now let's understand what exactly is\\nsiemens clustering so the fuzzy c means is an extension of a key means\\nclustering and the popular simple clustering technique so fuzzy clustering\\nalso referred as soft clustering is a form of clustering in which each data point can belong to more than one cluster so he means tries to find the\\nhard clusters where each point belongs to one cluster whereas the fuzzy c means discovers the soft clusters in a soft cluster any point can belong to more\\nthan one cluster at a time with a certain affinity value towards each fuzzy c means assigns the degree of membership which ranges from 0 to 1 to\\nan object to a given cluster so there is a stipulation that the sum of fuzzy\\nmembership of an object to all the cluster it belongs to must be equal to 1 so the degree of membership of this particular point to pool of these\\nclusters 0.6 and 0.4 and if you add a peak at 1 so that is one of the logic\\nbehind the fuzzy c means so on and this affinity is proportional to the distance from the point to the center of the cluster now then again we have the pros\\nand cons of fuzzy c means so first of all it allows a data point to be in multiple clusters that's a pro it's a more neutral representation of the\\nbehavior of genes genes usually are involved in multiple functions so it is\\na very good type of clustering when we are talking about genes first of and again if we talk about the cons again we have to define C which is the number of\\nclusters same as K next we need to determine the membership cutoff value also so that takes a lot of time and it's time-consuming and the clusters are\\nsensitive to initial assignment of centroid so a slight change or deviation\\nfrom the center's is going to result in a very different kind of you know a funny kind of output we get from the fuzzy see means and one of the major\\ndisadvantage of a C means clustering is that it's this are non-deterministic algorithm so it does not give you a particular output as in such\\nthat's that now let's have a look at the third type of clustering which is the hierarchical clustering so uh hierarchical clustering is an\\nalternative approach which builds a hierarchy from the bottom up or the top\\nto bottom and does not require to specify the number of clusters beforehand another algorithm works as in first of\\nall we put each dita point in its own cluster and if I that closes to cluster and combine them into one more cluster repeat the above step till the data\\npoints are in a single cluster now there are two types of hierarchical clustering one is elaborated clustering and the other one is division clustering so a\\ncumulative clustering builds the dendogram from bottom level while the division clustering it starts all the data points in one cluster from cluster\\nnow again her archaic clustering also has some sort of pros and cons so in the\\npros though no assumption of a particular number of cluster is required and it may correspond to meaningful taxonomies whereas if we talk about the\\ncourse once a decision is made to combine two clusters it cannot be undone and one of the major disadvantage of these hierarchical clustering is that it\\nbecomes very slow if we talk about very very large datasets and nowadays I think every industry are using last year as its and collecting large amounts of data\\nso hierarchical clustering is not the app or the best method someone might\\nneed to go for so there's that now when we talk about unsupervised learning so\\nwe have k-means clustering and again and there's another important term which\\npeople usually miss while talking about us was learning and there's one very important concept of market basket analysis now it is one of the key\\ntechniques used by large retailers to uncover association between items now it\\nworks by looking for combination of items that occurred together frequently in the transactions to put it it another way it allows retailers to analyze the\\nrelationships between the items that the people buy for example people who buy bread also tend to buy butter the marketing team at the retail store\\nshould target customers who buy bread and butter and provide them an offer so that they buy a third eye like an egg so if a customer buys bread\\nand butter and sees a discount or an offer on eggs he will be encouraged to spend more money and buy the eggs but this is what market basket analysis is\\nall about now to find the association between the two items and make predictions about what the customers will buy there are two algorithms which\\nare the Association rule mining and the ebrary algorithms so let's discuss each of these algorithm with an example first of all if we have a look at the\\nAssociation rule mining now it's a technique that shows how items are associated to each other for example customers who purchase bread have a 60%\\nlikelihood of also purchasing Jam and customers who purchase laptop are more likely to purchase laptop bags now if you take an example of an association\\nrule if you have a look at the example here a aro B it means that if a person\\nbuys an Adam 8 then he will also buy an item P now there are three common ways to measure a particular Association because we have to find these rules on\\nthe basis of some statistics right so what we do is use support confidence and lift now these three common ways and the measures to have a look at the\\nAssociation rule mining and know exactly how good is that rule so first of all we have support so support gives the fraction of the transaction which\\ncontains an item a and B so it's basically the frequency of the item in the whole item set whereas confidence gives how often the item a and B\\noccurred together given the number of item given the number of times a occur so it's frequency a comma B divided by the frequency of a now lift what\\nindicates is the strength of the rule over the random co-occurrence of a and B if you have a close look at the denominator of the lift formula here we\\nhave support a into support B now a major thing which can be noted from this is that the support of a and B are independent here so if the value of lift\\nor the denominator value of the lift is more it means that the items are\\nindependently selling more not together so that in turn will decrease the value of lift so what happens is that suppose the value of lift is more that implies\\nthat which we get it implies that the rule is strong and it can be used for later purposes because in that case the\\nsupport in to support p-value which is the denominator of lift will be low which in turn means that there's a relationship between the items a and B\\nso let's take an example of Association rule mining and understand how exactly it works so let's suppose we have a set of items a B C D and E and we have the\\nset of transactions which are t1 t2 t3 t4 and t5 and what we need to do is\\ncreate some sort of rules for example you can see a D which means that if a\\nperson buys a he buys D if a person buys C he buys a if it wasn't by his a he by\\nC and for the fourth one is if a person buy a B and C he is in turn by a now\\nwhat we need to do is calculate the support confidence and left of these rules now head again we talk about a priori algorithm so a priori algorithm\\nand the associated rule mining go hand-in-hand so what a predators is algorithm it uses the frequent itemsets to generate the Association rules and it\\nis based on the concept that a subset of a frequent item set must also be a frequent Isum set so let's understand what is a frequent item set and how all\\nof these work together so if we take the following transactions of items we have transaction T 1 T 2 T 5 and the items are 1 3 4 2 3 5 1 2 3 5 to 5 and 1 3 5\\nnow another more important thing about support which I forgot to mention was\\nthat when talking about Association rule mining there is a minimum support count\\nwhat we need to do now the first step is to build a list of items set of size 1\\nusing this transaction data set and use the minimum support count 2 now let's\\nsee how we do that if we create the tables see when if you have a close look at the table C 1 we have the item set 1 which has a support 3 because it appears\\nin the transaction 1 3 & 5 similarly if you have a look at the item set the\\nsingle item 3 so it has a supporter of 4 it appears in t 1 D 2 D 3 and T 5 but if we have a look at the items at 4 it only appears in the\\ntransaction once so it's support value is 1 now the item set with the support rally which is less than the minimum support value that is to have to be\\neliminated so the final David which is a table F 1 has 1 2 3 and 5 it does not\\ncontain the 4 now what we're going to do is create the item list of the size 2\\nand all the combination of the item sets in f1 are used in this iteration so we've left four behind we just have 1 2 3 and 5 so the possible item sets of 1 2\\n1 3 1 5 2 3 2 5 & 3 5 then again we'll calculate these support so in this case\\nif we have a closer look at the table c2 we see that the items at 1 comma 2 is having a support value 1 which has to be eliminated so the final table F 2 does\\nnot contain 1 comma 2 similarly if we create the item sets of size 3 and\\ncalculate these support values but before calculating the support let's perform the peirong on the data set now what Spearing so after all the\\ncombinations are made we divide the table see three items to check if there\\nare another subset whose support is less than the minimum support value this is a priori algorithm so in the item sets 1 2 3 what we can see that we have 1 2 and\\nin the 1 to 5 again we have 1 2 so we'll discard poor of these item sets and\\nwe'll be left with 1 3 5 & 2 3 5 so with 135 we have three subsets 1 5 1 3 3 5\\nwhich are present in table F 2 then again we have 2 3 2 5 & 3 5 which are also present in tea we'll have to so we have to remove 1 comma 2 from the table\\nC 3 and create the table F 3 now if we're using the items of C 3 to\\ncreate the adults of c4 so what we find is that we have the item set 1 2 3 5 the\\nsupport value is 1 which is less than the minimum support value of 2 so what\\nwe're going to do is stop and we're gonna return to the previous item set that is the table c3 so the final table f3 was one three five with\\nthe support value of two and two three five with the support value of two now what waiting a Jew is generate all the subsets of each frequent itemsets so\\nlet's assume that our minimum confidence value is 60% so for every subset s of AI the output rule is that s gives I two s is that s\\nrecommends i ns if the support of I divided by the support of s is greater\\nthan or equal to the minimum confidence value then only we'll proceed further so keep in mind that we have not used lift till now we are only working with\\nsupport and confidence so applying rules with Adam sets of f3 we get rule 1 which\\nis 1 comma 3 which gives 1 3 5 & 1 3 it means if you buy 1 & 3\\nthere's a 66% chance that you'll buy item 5 also similarly the rule 1 comma 5\\nit means that if you buy 1 & 5 there's 100% chance that you will buy 3\\nalso similarly if we have a look at rule 5 & 6 here the confidence value is less\\nthan 60% which was the assumed confidence value so what we're going to do is we'll reject these files now an important thing to note here is that\\nhave a closer look to the rule 5 and rule 3 you see it's it has 1 5 3 1 5 3 3 1 5 it's very confusing so one thing to keep\\nin mind is that the order of the item sets is also very important that will help us allow create good rules and avoid any kind of confusion so that's\\ndone so now let's learn how Association rule I used in market basket analysis\\nproblem so what we'll do is we will be using the online transactional data of a\\nretail store for generating Association rules so first of all what you need to do is import pandas MLT ml X T and D libraries from the imported and read the\\ndata so first of all what we're going to do is read the data what we're gonna do is from ml X T and e dot frequent patterns we're going to\\nimprove the a priori and Association rules as you can see here we have the head of the data you can see we have inverse number of stock code the\\ndescription quantity the inverse TTL unit price customer ID and the country so in the next step what we will do is we will do the data cleanup which\\nincludes reviewing spaces from some of the descriptions given and what we're\\ngoing to do is drop the rules that do not have the inverse numbers and remove the Freight transaction so hey what what you're gonna do is remove which do not\\nhave an invoice number if the string contains type seen was a number then\\nwe're going to remove that those are the credits remove any kind of spaces from the descriptions so as you can see here we have like five iron and 32,000 rows\\nwith eight columns so next what we wanted to do is after the clean up we need to consolidate the items into one transaction per row with each product\\nfor the sake of keeping the data assets small we gonna only look at the sales for France so we're gonna use the only France and group by invoice number\\ndescription with the quantity sum up and C so which leaves us with 392 rows and\\n1563 columns now there are a lot of zeros in the data but we also need to\\nmake sure any positive values are converted to a 1 and anything less than 0 is set to 0 so for that we're going to use this code defining end code units if\\nX is less than 0 it owns 0 if X is greater than 1 returns 1 so what we're going to do is map and apply it to the whole data set we have here so now that\\nwe have structured the data properly so the next step is to generate the frequent item set that has support of at least 7%\\nnow this lumber is chosen so that you can you get close enough now what we're gonna do is generate the ruse with the corresponding support confidence and\\nlift so we had given the minimum support at 0.7 the metric is lift frequent item\\nset and threshold is one so these are the following rules now a few rules with\\na high lift value which means that it occurs more frequently than would be expected given the number of transaction the product combinations most of the\\nplaces the confidence is high as well so these are few of the observations what\\nwe get here if we filter the data frame using the standard pandas code for large\\nlift six and high confidence 0.8 this is what the output is going to look like\\nthese are 1 2 3 4 5 6 7 8 so as you can see here we have the eh rules which are\\nthe final rules which are given by the Association rule mining and that is how all the industries or any of these we've talked about large retailers they tend\\nto know how their products are used and how exactly they should rearrange and provide the offers on the products so that people spend more and more money\\nand time in the shop so that was all about Association rule mining so so guys\\nthat's all for unsupervised learning I hope you got to know about the different formulas how unsupervised learning works because you know we did not provide any\\nlabel to the data all we did was create some rules and not knowing what the data\\nis and we did clusterings different types of clusterings k-means simi's hierarchical clustering so now coming to the third and last type of\\nlearning is the reinforcement learning so what reinforcement learning is it's a type of machine learning where an agent is put in an environment and it learns\\nto behave in this environment by performing certain actions and observing the rewards which it gets from those actions so a reinforcement learning is\\nall about taking an appropriate action in order to maximize a reward in the particular situation and in supervised learning the training theater comprises\\nof input and expected output so the model is strained with the expected output itself but when it comes to reinforcement learning\\nthere is no expected output the reinforcement agent decides what actions\\nto take in order to perform a given task in the absence of a training dataset it\\nis bound to learn from its expertise so let's understand reinforcement learning with an analogy so consider a scenario wherein a baby is learning how to walk\\nnow this scenario can go in two ways first the baby starts walking in and makes it to the candy now since the candy is the end goal the\\nbaby is happy it's positive the baby is happy positive reward now coming to the\\nsecond scenario the baby starts walking but falls due to some hurdle in between now the baby gets hurt and does not get to the candy it's negative the baby is\\nsad negative reward just like we humans learn from our mistakes by a trial and an earth reinforcement learning is also similar and we have an agent which is\\nbaby a reward which is candy and many hurdles in between the agent is supposed to find the best possible path to reach the reward so guys if you have a look at\\nsome of the important reinforcement learning definitions first of all we have the agent so the reinforcement learning algorithm that learns from\\ntrial in err that's the agent now if we talk about environment the world through which the agent moves or the obstacles which the agent has to conquer or the\\nenvironment now actions a are all the possible steps that the agent can take the state s is the current conditions returned by the\\nenvironment then again we have reward R and instant return for the environment to appraise the last action then again we have policy which is PI it is the\\napproach that the agent uses to remind the next action based on the current state we have value V which is the expected long-term return with discount\\nas open to the short-term what are then again we have the action value Q this is\\nsimilar to value except it takes an extra parameter which is the current state action which is a now let's talk about reward maximization for a moment\\nnow reinforcement learning agent works based on the theory of reward maximization this is exactly why the RL must be trained in such a way that he\\ntakes the best action so that the reward is maximum now the collective rewards at a particular time and the respective\\naction is written as G T equals RT plus one RT plus two and so on\\nnow the equation is an ideal representation of rewards generally things do not work out like this while summing up the cumulative rewards now\\nlet me explain this with a small gape in the figure you see a fox right some meat and a Tyler our reinforcement learning agent is the Fox and his end goal is to\\neat the massive Otto meat before being eaten by the tiger since this fox is clever fellow he eats the meat that is closer to him rather\\nthan the meat which is close to the tiger because the closer he goes to the Tiger the tiger the higher are his chances of getting killed as a result\\nthe reward near the tiger in if they are bigger meat chunks will be discounted this is done because of the uncertainty factor that the tiger might kill the Fox\\nnow the next thing to understand is how discounting of reward works now to do\\nthis we define a discount called the gamma the value of gamma is between 0 &\\n1 the smaller the gamma the larger the discount and vice versa so our cumulative discounted reward is GT\\nsummation of K 0 to infinity gamma to the power P as DK\\nt plus k plus 1 where gamma belongs to 0 to 1 but if the Fox decides to explore a\\nbit it can find bigger rewards that is this big chunk of meats this is called exploration so the reinforcement learning basically works on the basis of\\nexploration and exploitation so exploitation is about using the\\nalready known expert information to heighten the rewards whereas exploration is all about exploring and capturing more information about the environment\\nthere is another problem which is known as the K armed bandit problem the K\\narmed bandit it is a metaphor representing a casino slot machine with K pull levers or arms the users or the customer pulls any one of the levers to\\nwin a projected reward the objective is to select the leeward that will provide the user with the highest reward now here comes the\\nepsilon greedy algorithm it tries to be fair to do opposite cause of exploration\\nexploitation by using a mechanism of flipping a coin which is like if you flip a coin and comes up head you should explore for memory butter comes up days\\nyou should exploit it takes whatever action seems best at the present moment so with probability while epsilon the epsilon greedy algorithm exploits the\\nbest known option with probability epsilon by 2 epsilon 0 it explores the\\nbest known option and with the probability epsilon by 2 with probability epsilon by 2 the algorithm explores the best known option and with\\nthe probability epsilon by 2 the epsilon greedy algorithm explores the worst known option now let's talk about Markov decision process the mathematical\\napproach for mapping a solution in reinforcement learning is called Markov decision process which is MDP in a way the purpose of reinforcement learning is\\nto solve a Markov decision process now the following parameters are used to attain a solution set of actions a set of states s we have the reward our\\npolicy PI and the value V and we have translational function T probability\\nthat our forum leads to s now to briefly sum it up the agent must take up an\\naction to transition from the start state to end state s while doing so the\\nagent receives the reward R for each action he takes the series of actions taken by the agent define the policy PI and the rewards collected by collected\\nto find the value of V the main goal here is to maximize the rewards by choosing the optimum policy now let's take an example of choosing the shortest\\npath now consider the given example here so what we have is given the above representation our goal here is to find the shortest path between a and D each\\nedge has a number linked to it and this denotes the cost to traverse that edge now the task at hand is to traverse from point A to D with the minimum possible\\ncost in this problem the set of states are denoted by the nodes ABCD a\\nd the action is to traverse from one node to another are given by a arrow B\\nor C our OD reward is the cost represented by each edge and the policy\\nis the path taken to reach each destination a to C to D so you start off\\nat node a and take baby steps to your destination initially only the next\\npossible node is visible to you if you follow the greedy approach and take the most optimal step that is choosing a to see instead of a to B or C now you are\\nat node C and want to traverse to node T you must again choose the path wisely choose the path with the lowest cost we can see that a CD has the lowest cost\\nand hence we take that path to conclude the policy is a to C to D and the value\\nis 120 so let's understand Q learning algorithm\\nwhich is one of the most use reinforcement learning algorithm with the help of examples so we have five rooms in a building\\nconnected by toast and each room is numbered from 0 through 4 the outside of\\nthe building can be thought of as one big room which is tea room number five now dose 1 & 4 lead into the building from the room 5\\noutside now let's represent the rooms on a graph and each node each room has a\\nnode and each door as link so as you can see here we have represented it as a\\ngraph and our goal is to reach the node 5 which is the outer space so what we're\\ngonna do is and the next step is to associate a reward value to each toe so\\nthe dose that directed read to the you will have a reward of 100 whereas the\\ndoors that do not directly connect to the target have a reward and because the dose had to weigh two arrows are assigned to each room and each row\\ncontains an instant about valley so after that the terminology in the q-learning includes the term states and action so the room 5 represents a state\\nagents movement from one room to another room represents in action and in this\\nfigure a state is depicted as a node while an action is represented by the arrows so for example let's say can eat in that Traverse from room to to the\\nroof I so the initial state is gonna be the state to it then the next step is from stage 2 to stage 3 next is to moves from stage 3 to stage either 2 1 or 4 so\\nif it goes to the 4 it reaches stage 5 so that's how you represent the hole traversing of any particular agent in all of these rooms a represents their\\nactions via notes so we can put this state diagram and instant reward values\\ninto a reward table which is the matrix R so as you can see the minus 1 here in\\nthe table represents the null values because you cannot go from 1 to 1 right and since there is no way from to go from 1 to 0 so that is also minus 1 so\\nminus 1 represents the null values whereas the 0 represents zero reward and 100 represents the reward going to the room\\nfive so one more important thing to know here is that if you're enrolled fireman you could go to room five the reward is hundred so what we need to do is add\\nanother matrix Q representing the memory of what the agent has learned to experience the rows of matrix Q represent the current state of the agent\\nwhereas the columns represent the possible action leading to the next state now if the formula to calculate the Q matrix is if a particular Q at a\\nparticular state and the given action is equal to the R of that state in action plus gamma which we discussed earlier the Kurama parameter which we discussed\\nearlier which ranges from 0 to 1 into the maximum of the Q or the next state\\ncomma all actions so let's understand this with an example so here are the\\nnine steps which any Q learning algorithm particularly has so first of\\nall is to set the gamma parameter and the environment rewards in the matrix R then we need to do is initialize the matrix Q to 0 select the random initial\\nstate set the initial state to current state select one among all the possible actions for the current state using this possible action consider going to the\\nnext state when you get the next state get the maximum Q value for this next state based upon all the actions compute the Q value using the formula repeat the\\nabove steps until the current state equals your code so the first step is to set the values of the learning parameters gamma which is 0.8 and\\ninitial state as room number one so the next initialize the Q matrix a zero\\nmatrix so on the left hand side as you can see here we have the Q matrix which has all the values as 0 now from room 1 you can either go to room 3 or room 5 so\\nlet's select room 5 because that's our end goal so from room 5 calculate the maximum cube value for this next state based on all possible actions so Q 1\\ncomma 5 equals R 1 comma 5 which is hundred plus zero point eight which is\\nthe gamma into the maximum of Q 5 comma 1 5 comma 4 and 5 comma 5 so\\nmaximum or five comma one five comma four five comma five is hundred so the Q\\nvalues from initially as you can see here the Q values are initialized to zero so it does not matter as of now so the maximum is zero so the final Q value\\nfor Q 1 comma 5 is 100 so so that's how we're gonna update our Q matrix so Q\\nmatrix the position has 1 comma 5 in the second row gets updated to 100 so the first step we have turned right now that for the next episode we start with a\\nrandomly chosen initial state so let's assume that the stage is 3 so from rule number 3 you can either go to room number 1 2 or 4 so let's select the\\noption of room number 1 because from our previous experience what we've seen is that one has directly connected to room 5 so from room / 1 calculate the maximum\\nQ value for this next state based on all possible action so 3 comma 1 if we take\\nwe get our 3 4 1 plus 0 point 8 comma into maximum of T's we get the value as\\n80 so the matrix Q gets updated now for the next episode the next state 1 now\\nbecomes the current state we repeat the inner loop of the Q learning algorithm because tip 1 is not the goal state from 1 you can either go to 3 of 5 so let's\\nselect 105 as that's our goal so from room row 5 again we can go from all of these so the Q matrix remains the same since Q 1 5 is already fed to the agent\\nand that is how you select the random starting points and fill up the Q Q\\nmatrix and see where which path will lead us there with the maximum provide\\npoints now what we gonna do is do the same coding using the Python in machine\\nlearning so what we're going to do is improve an umpire's NP we're gonna take the R matrix as we defined earlier so that the minus 1 are the nerve values\\nzeros are the values which provides a 0 and hundreds is the value so what we're\\ngoing to do is initialize the Q matrix now to 0 we're going to put gamma as 0.8\\nand set the initial state as 1 now here returns all the available actions in the state given as an argument so if we define the of\\naction with the given state we get the available action in the current state so\\nwe have the another function here which is known as a sample next action what this function does is that chooses at random which action to be performed\\nwithin the range of all the available actions and finally we have action which is the sample next action with the available act now again we have another\\nfunction which is update now what it does is that it updates the Q matrix according to the path selected and a Q learning algorithm so so initially our Q\\nmatrix is all 0 so what we're gonna do is we're gonna train it over 10,000\\niterations and let's see what exactly gives the output of the Q value so if\\nthen the agent learns more through for the iterations it will finally breach converges value in Q matrix so the Q matrix can then be normalized at is\\nconverted to percentage by dividing all the non-zeros entities by the highest number which is 500 in this case so once the matrix Q gets close enough to the\\nstate of convergence agent has learned the most optimal path to the goal State so what we're gonna do next is divide it by 5 which is the maximum here so Q R\\nand P Q max in 200 so that we get a normalized now once the Q matrix gets close enough to the state of convergence the agent\\nhas learned or the paths so the optimal path given by the Q learning employer\\nThomas if it starts from 2 it will go to 3 then go to 1 and then go to 5 if it\\nstarts at 2 it can go to 3 then 4 then 5 that will give us the same results so as\\nyou can see here is the output given by the Q learning algorithm is the selected path is 2 3 1 and Feinstein from the Q State - so this is how exactly a\\nreinforcement learning algorithm works it finds the optimal solution using the path and given the action and rewards and the various other definitions or the\\nvarious other challenges I would say actually the main goal is to get the master reward and get the maximum value through the environment and that's how\\nan agent learns through its own path and going millions and millions of\\niterations learning how each part will give us what reward so that's how the Q\\nlearning algorithm works and that's how it works in Python as well as I showed you so now that you have a clear idea of the different machine learning\\nalgorithms how it works the different phases of machine learning the different applications of machine learning how supervised learning works how\\nunsupervised learning works our reinforcement learning works and what to choose in what scenario what are the different algorithms under all of these\\ntypes of machine learning next move forward to the next part our session\\nRich's understanding about artificial intelligence deep learning and machine learning well data science is something that has\\nbeen there for ages nonetheless and data science is the extraction of knowledge from data by using scientific techniques and algorithms people usually have a\\ncertain level of dilemma or I would say a certain level of confusion when it comes to differentiating between the terms artificial intelligence machine\\nlearning and deep learning so don't worry I'll clear all of these doubts for you artificial intelligence is a technique which enables machine to mimic\\nhuman behavior now the idea behind artificial intelligence is fairly simple yet fascinating which is to make intelligent machines that can take\\ndecisions on their own now for years it was thought that computers would never match the power of the human brain well back then we did not have enough data\\nand computational power but now with big data coming into existence and with the\\nadvent of GPUs artificial intelligence is possible now machine learning is a\\nsubset of artificial intelligence technique which uses statistical method to enable machines to improve with experience whereas deep learning is a\\nsubset of machine learning which makes the computation of multi-layer neural network feasible it uses the neural networks to stimulate human-like\\ndecision-making so as you can see if we talk about the data science ecosystem we have artificial intelligence machine learning and deep learning deep learning\\nbeing the innermost circle is very much required for machine learning as well as artificial in but why was deep learning required so\\nfor that less understand the need for deep lolly so a step towards artificial\\nintelligence was machine learning and machine learning was a subset of ei play it deals with the extraction of patterns from the last dataset haslam la dataset\\nwas not a problem what was a problem was machine learning algorithms could not handle the hight dimensional data where we have a large number of inputs and\\noutputs which rounds thousands of dimensions handling and processing such\\ntype of data becomes very complex and resource exhaustion now this is also termed as the curse of dimensionality now another challenge faced by machine\\nlearning was to specify the features to be extracted so as we saw earlier in all\\nthe algorithms which are discussed now we had to specify the features to be extracted now this plays an important role in protecting the outcome as well\\nas in achieving better actress therefore without feature extraction the challenge for the programmer increases as the effectiveness of the algorithm very much\\ndepends on how insightful the programmer is now this is where deep learning comes\\ninto picture and comes to the rescue but deep learning is capable of handling the high dimensional data and is also efficient in focusing on the right\\nfeatures on its own so what exactly is deeper so deep learning is a subset of machine learning as I mentioned earlier where similar machine learning\\nalgorithms are used to Train deep neural networks so as to achieve better accuracy in those cases where the former was not performing up to the MA\\nbasically deep learning mimics the way our brain functions and learns from experience so as you know our brain is made up of billions of neurons that\\nallows us to do amazing things when the brain of a small kid is capable of solving complex problems which are very difficult to solve even using the\\nsupercomputers so how can we achieve the same functionality in programs now this\\nis where we understand artificial neuron and artificial neural networks so first\\nof all let's have a look at the different applications of deep learning we have automatic machine translation object classification before\\nautomatic handwriting generation character text generation we have image caption generation colorization of black and white images we have\\nautomatic game playing and much more now google lens is a set of vision based\\ncomputing capabilities that allows your smartphone to understand what's going on\\nin a photo video or any live feed for instance point your phone at a flower and google lens will tell you on the screen which type of flower it is\\nyou can in that camera at any restaurant sign to see the reviews and other recommendations now if we talk word mushroom transition this is a task where\\nyou are given words in some language and you have to translate the words to a desired language see English but this kind of translation is classic example\\nof image recognition and final application of deep learning which we have here is image polarization so automatic colorization of black and\\nwhite images as you know earlier we did not had color photographs back there in\\n40s and 50s we did not have any color photographs so through deep learning analyzing water shadows is present in the image how the light is bouncing off\\nthe skin tone of the people automatic colorization is now possible and this is\\nall possible because of deep learning now deep learning studies the basic unit\\nof a brain cell called a neuron now let us understand the functionality of a biological neuron and how we mimic this functionality in the perceptron or what\\nwe call is an artificial neuron so as you can see here we have the image of a\\nbiological neuron so it has a cell body it has mitochondrion nucleus we have\\ndendrites there we have the axon we have the node of the ran of ear you have the\\nscavenge cell and the synapse so we need not know about all of these so what we\\nneed to know mostly about is dendrite which receives signals from other neurons we have a cell body which sums up all the inputs and we have axon which\\nis used to transmit the signals to the other cells now an artificial neuron or\\nperceptron is a linear model which is based upon the same principle and is used for binary classification it models a neuron which has a set of\\ninputs each of which is given a specific weight and the neuron computes some\\nfunctions on these weighted inputs and gives the outputs it receives n inputs\\ncorresponding to each feature it then sums up those inputs applies the transformation and produces an output it has generally two functions which are\\nthe summation and the transformation but the transformation is also known as activation functions so as you can see here we have certain inputs we have\\ncertain weights we have the transfer function and then we have the activation function now the transfer function is nothing but the summation function here\\nand it is the schematic for a neuron in a neural network so this is how we mimic\\na biological neuron in terms of programming now the way it shows the\\neffectiveness of a particular input move the weight of input more it will have an\\nimpact on the neural network on the other hand bias is an additional parameter in the perceptron which is used to address the output along with\\nthe weighted sum of the inputs to the neuron which helps the model in a way that it can best fit for the given data activation functions translate the\\ninputs into outputs and it uses a threshold to produce an output there are\\nmany functions that are use has activation functions such as linear or identity we have unit or binary step we have sigmoid logistic tan edge ray Lu\\nand soft Max now if we talk about the linear transformation or the activation function so a linear transform is basically the identity function where\\nthe dependent variable has a direct proportional relationship with the independent variable now in practical terms it means that a function passes\\nthe signal through unchanged now the question arises when to use linear transform function simple answer is when we want to solve a linear regression\\nproblem we apply a linear transformation function and next in our list of activated functions we have your next step the output of a unit step function\\nis either 1 or 0 now it depends on the threshold value we define a\\nstep function with the threshold value five is shown here so let's consider X is five so if the value is less than five the output will be zero whereas if\\nthe value is equal to or greater than five then the valuable one this equal to\\nis very much important to consider here because sometimes people put up the equal two in the lower end of the side so that's not it how it is used but\\nrather it's used on the upper hand side where if the value is greater than particular X greater than or equal to X then only the value will be one now a\\nsigmoid function is a machine that converts an independent variable of near\\ninfinite range into simple probabilities between 0 & 1 now most of its output\\nwill be very close to either 0 or 1 and if you have a look at the function here\\nwe have 1 divided by n plus y raise to power minus beta X so I'm not going to\\nthe details or the mathematical function of a particular sigmoid but it's very much used to convert the independent variables of very large infinite range\\nto the values between 0 & 1 now the question arises when to use a sigmoid transformation function so when we want to map the input values to a\\nvalue in the range of 0 to 1 where we know the output should lie only between these two numbers we apply the sigmoid transformation function note an H is a\\nhyperbolic trigonometric function now unlike the sigmoid function the normalized range of tan H is minus 1 to 1 it's very much similar to the sigmoid\\nfunction but the advantage of tan H is that it can deal more easily with negative numbers now next on our list we have Ray Lu now rail you or the rectify\\nlinear unit transform function only activates our node if the input is above\\na certain quantity while the input is below 0 the output is 0 but when the\\ninput Rises about a certain threshold or if we take in this case at 0 but if you\\nhave a certain value X if it crosses that certain threshold it has a linear relationship with the dependent variable now this is very much different from a\\nnormal linear transformation so has certain threshold now the question\\narises here again when to use a railroad transformation function so when we want\\nto map the input values to a value in the range so as input X to maximum 0\\ncomma X that is it Maps the negative inputs to 0 and the positive inputs are output without any change we apply a rectified linear unit or the railroad\\ntransformation function now the final one which we have is sort max so when we have four or five classes of outputs the softmax function will give the\\nprobability distribution of each it is useful for finding out the class which\\nhas the maximum probability so soft mass is a function you will often find at the\\noutput layer of a classifier now suppose we have an input of say the letters of\\nEnglish words and we want to classify which letter it is so for that case we're going to use the sort max function because in the output we have certain\\nclasses but I would say in English if we take English we had 26 classes from A to\\nZ so in that case softmax activation function is very much important now\\nartificial neuron can be used to implement logic gates now I'm sure you guys must be familiar with the working of all K that is the output is one if\\nany of the input is also one therefore a perceptron can be used as a separator or\\na decision line that divides the input set of or gate into two classes the\\nfirst class being the inputs having output as 0 that lies below the decision line and the second class would be inputs having output as 1 that lie above\\nthe decision line or the separator so mathematically a perceptron can be thought of like an equation of weights inputs and bias as you can see here we\\nhave f of X is equal to weight into the input vector plus the bias so let's go\\nahead with our demo understand how we can implement this perceptron example which is of an or gate using neural networks using artificial neuron or the\\nperceptron and here we're going to use tensor flow along with Python so let's\\nunderstand what exactly is tensor flow first before going it to the demo so basically tensor flow is a deep learning framework by Google\\nto understand it in a very easy way let's understand the two terms of tensorflow which are the tensors and the flow so starting with tensors tensors\\nare standard way of representing theater in deep learning and they are just multi-dimensional arrays it is an extension of two-dimensional table\\nmatrices through the data with higher dimension so as you can see have first of all we have a tensor of dimension 6 then we have a tensor of dimension 6\\ncomma 4 which is 2d and again we have a tensor of dimension 6 4 and 2 which is\\nreading now this dimension is not restricted to 3 we can have four dimensions five dimensions it depends upon the number of inputs or the number\\nof classes or the parameters which we provide to a particular neural net or a\\nparticular perceptron so which brings us tensorflow intensive flow the\\ncomputation is approached as a data flow graph so we have a tensor and then again\\nwe have a flow in which we suppose for taking the example here we have the data we do addition then we do matrix multiplication then we check the result\\nif it's good then it's fine and if the result is not good then we again do some sort of matrix multiplication or addition it depends upon the function\\nwhat we are using and then finally we have the output so if you want to know about it as a flow we have an entire playlist on tensor flow and deep\\nlearning which you should see i'll give the link to all of these videos in the description box so let's go ahead with our demo and understand how we can\\nimplement the or gates using perceptron so first of all what we're going to do is import all the required libraries and Here I am going to import only one\\nlibrary which is the tensor flow library so what we're going to do is import tensorflow a steal now the next step what we're going to do\\nis define vector variables for input and output so for that we need to create\\nvariables for storing the input output and the bias for the perceptron so as\\nyou can see here we have the training input and again we have the training output now what we're going to do next is define the weight variable and here\\nwe are we will define the tensor variable of the shape 3 comma 1\\nand for our weights and we will assign some random values to it initially so we're going to use T AF dot variable and we're going to use TF run random normal\\nto assign random variables to the 3 cross 1 tensor next what we do is define\\nplaceholders for input and output and so that they can accept external inputs on\\nthe run so this will be T F dot float32 so for X we are going to use a dimension\\nfor 3 and for y it's dimension of 1 now as discussed earlier the input received by a positron is force multiplied by the respective weights and then all of these\\nweights input our sum together now this sum value is then fed to the activation\\nfor obtaining the final result of the or gate perceptron so this is the output\\nhere what we are defining so it's TF dot neural networks dot relu using the relu\\nactivation function here and we are doing the matrix multiplication of the weights and biases in this case I have used the rayleigh function but you are\\nfree to use any of the activation functions according to your needs the next what we're going to do is calculate the cost or ere so we need to calculate\\nthe cost which is the mean squared error which is nothing but the square of the differences or the perceptron output and the desired output so the equation will\\nbe loss equals D F dot reduce some and we'll use the TF dot Square output minus\\ny now the cool of a perceptron is to minimize the loss or the cost or the error so here we are going to use the gradient descent optimizer which will\\nreduce the loss and it is a very important part of any neural network to\\nuse any sort of optimizer so here we are using the gradient descent optimizer you can know more about the gradient descent optimizer in other a Drake of videos or\\ndeep learning and neural networks now the next step comes is to initialize the variables so variables are only defined with TF dot variables the\\ninitially what weighted so we need to initialize this variable define so for that we're going to use the T F dot global variable initializer and we're\\ngoing to create the F dot session and we will not run with the initialization variables so as all the variables are initialized not\\ncoming to the last step what we're going to do is we need to train our perceptron that is update away our values of the weights and the biases in the successive\\niteration to minimize the error or the Ross so here I will be training our perceptron in hundred epochs\\nso as you can see here for I in range hundred we are going to run the session with training data in and why as a trainee at the output and we're going to\\ncalculate the loss and feed it directly to the X train and why train and again and print the epoch so as you can see here for the first iteration the loss\\nwas two point zero seven and coming down if as soon as the iterations increase\\nthe loss is decreasing because of the gradient optimizer it's learning how the data is and coming down to the hundredths or the final epoch here we\\nhave the loss of zero point two seven start with two point zero seven here initially and we ended up with zero point two seven loss which is very good\\nthis was how perceptron works on a particular given data set it learns\\nabout it and as you saw earlier we gave a set of input the input variables we\\nprovided weights we had a summation function and then we use the rail u\\nactivation function in the code to get the final output and then we trained the\\nparticular model for hundred iterations with the training data so as to minimize\\nthe loss and the loss came down all the way from two point seven to zero point two seven well if you think perceptron solves all the problem of making a human\\nbrain then you were wrong there are two major problems first problem being that the single layer perceptron cannot classify non linearly separable data\\npoints and which other complex problems that involve a lot of parameters cannot\\nbe solved by a single layer perceptron now consider the example here and the\\ncomplexity with the parameters involved to take a decision by the marketing team so as you can see here for every email direct paid referral program or organic\\nwe have certain number of social media subcategories Google Facebook LinkedIn we have twitter and then we have the type such as the search ad remarketing\\nas interest as ad look like ads and again the parameters to be considered are the customer acquisition cost money span leads generated customers generated\\ntime taken to become a customer and all of these problems cannot be solved by a single layer of perceptron our one neuron cannot take in so many\\ninputs and that is why more than one neuron would be used to solve these kind of problems so neural network is really just a\\ncomposition of perceptrons connected in different ways and operating on activation functions so for that we have three different terminologies in a\\nparticular neural network we have the input layers we have the hidden layers and we have the output layers so in hidden layer we have hidden nodes which\\nprovide information from the outside world to the network and heart together referred to as the input layer now the hidden nodes perform computations and\\ntransfer information from the input nodes to the output nodes now a collection of hidden nodes forms idle layer in our image we have one two three\\nfour hidden layers and finally the output nodes are collectively referred to as output layers and are responsible for computation and transferring\\ninformation from the network to the outside world now that you have an idea of how a perceptron behaves the different\\nparameters involved and the different layers of neural networks let's continue this session and see how we can create our own neural network from scratch in\\nthis image as you can see here we have given a list of faces first of all the\\npatterns of local contrast is being computed in the input layer then in the hidden layer 1 we get the face features and in the hidden layer 2 we get the\\ndifferent features of the face and finally we have the output layer now if we talk about training networks and weights in a particular neural networks\\nwe can estimate the weight values for our training data using stochastic gradient descent optimizer as I mentioned earlier now it requires\\ntwo parameters which is the learning rate and as I mentioned earlier learning rate is used to limit the amount of each weight is corrected each time it is\\nupdated and epoch is a number of times to run through the training data while updating the way so in the previous example we had 100 ebox so we trained\\nthe whole model hundred times and these along with the training data will be the arguments to the function as data scientists or data analysts\\nor machine learning engineers working on the hyper parameters is the most important part because anyone can do the coding it's your experience and your way\\nof thinking about the learning rate and the epochs the model which you are working the input data you are taking how much time it will require to train\\nbecause time is limited and as you know these hyper parameters are the only things which are successful data centers will be guessing when creating a\\nparticular model and these play a huge role in the model such as even a slight difference in learning create of the e box might result in the model training\\ntime so as it will take longer time to Train having a large amount of data using the particular data set that these all things are what data scientist or\\nmachine learning engineer keeps in mind while creating them all let's create our own new network and here we are going to use the MN is DDS a so the MN IC data\\nset consists of 60,000 training samples and 10,000 testing samples of\\nhandwritten digit images not the images are of the size 28 into 28 pixels and\\nthe output can lie anywhere between 0 to 9 now the task here is to train a model\\nwhich can accurately identify the digit present on the image so let's see how we\\ncan do this using tensor fro and Python so firstly we are going to use the\\nimport function here to bring all the print function from Python 3 into python\\n2.6 or the future statements let's continue with our cone\\nso next what we are going to do is from pencil for examples tutorials we can take the mi nasty data which is already provided by tensorflow in their example\\ntutorials data but this is only for the learning part and later on you can use this particular data for more purposes for your learning now next what we are\\ngoing to do is create MN ist and we're going to use the input data tour tree data set and one hot is given us through here so we're going to import tensorflow\\nand whack plot lib next what we are going to do is define the hyper parameters here so as I mentioned earlier we have few hyper parameters\\nlike learning rate equals batch size display step is not a very big hyper parameter to consider here but so the learning rate we have given here is\\n0.001 training epochs is 15 that is up to you because more than number of\\nepochs the more time it will take for the model to Train and here you have to take a decision between the amount of time it takes for the model to train and\\ngive the output versus the speed again we have the batch size of 100 now this\\nis one of the most important have a parameter to be considered because you cannot take all of the images at once and create the radius so you need to do\\nit in a bath size manner and for that we define a bad size of 100 so out of 60,000 we're going to take 100 as a bath size 100 images which will go through 15\\niterations and the training set has 60,000 images so you do the math how many batch we will require and how many epochs for each batch we'll have 15 a\\nbox the next step is defining the hidden layers and the input and the classes so\\nfor input layers have taken 256 numbers these are the number of perceptron I\\nneed or the number of features to be extracted in the first layer so this number is arbitrary you can use it according to your requirements and your\\nneeds so for simplicity I am using two bits X here and the same I'm going to use for the hidden layer 2 now for the number of inputs I'm going to use 784\\nand that is why because as I discussed earlier the MST data has an image or the\\nshape 28 cross 28 which is 784 so in short we have 784 pixels to be\\nconsidered in a particular image and each pixel will provide immense amount of data so I am taking a 784 input and number of output classes Here I am\\ndefining ten because the output can either range from zero one two three four five six seven eight and nine so the total number of classes or the\\noutput classes here I'm going to use are ten and again we are going to create x and y variables X for the input and Y for the output classes now as you can\\nsee here we have the multi-layer perceptron in which we have defined all the hidden layers and the output layers so the layer one will do the addition\\nand first I will do the matrix multiplication of the weights and the input with the biases and then it will provide a summation and then again the\\noutward for this one will be given to layer two by using the activation function of rail you here so as you can see here we have rail you activation\\nfunction for layer 1 layer 2 will take the input of layer 1 with the weights\\nprovided in h2 hidden to layer with the biases of b2 layer it will do the multiplication of layer 1 into weights it will add the biases and then again\\nwe'll have a rail lu activation function and the output of this layer 2 will be given to the output layer so as you can see here in the final output layer we\\nhave matrix multiplication of layer 2 into weights of the output layer plus the biases of the output layer and what we're going to do is return the output\\nso let's mention the weights and the biases so here we are taking random\\npoints for that and next what we're going to do is use the prediction of the multi-layer perceptron using the input weights and biases and one thing more\\nimportant what we're going to do here is define the cost so we're going to use the TF naught reduce mean and we are using the short max cross entropy with\\nlogits this is a function and here we are using the atom optimizer rather than the gradient descent optimizer with learning\\nrate provided initially and what we're going to do is minimize the cost\\nso again we're going to initialize all the global variables and we have two\\narrays for cos history and accuracy history so as to store all the values and train our model so we're going to create a session and the training cycle\\nfor epoch in the range of 15 we first initialize the average cost at zero and\\nthe total patch is the MN asset in number of examples divided by bass has which is 100 and we loop it over all the patches run the optimization or the back\\npropagation and the cost operation to get the loss value and then we have to\\ndisplay the logs per each Ipoh for that will show the epochs and the cost at\\neach step we're going to calculate the accuracy add the last to the correct prediction and will append the accuracy to the list after every epoch we will\\nappend the cost after every epoch because that is what and we have created cos history and the accuracy history for that purpose and finally we will plot\\nthe cost history using the matplotlib and we'll plot the accuracy history also\\nand what we're going to do is we're going to see how accurate is our model so so let's train it now and as you can see at first epoch we have cost 188 and\\naddress is 0.85 so if you see just have the second epoch the cost has reduced\\nfrom 188 to 42 now it's 26 as you can see the accuracy is increasing from 0.85\\nto 0.909 one you have reached five epochs you see the cost is diminishing\\nat a huge rate which is very good and you can use different types of optimizers or gradient descent or be it atom optimizer and not go to the details\\nof the optimization because that is another half an hour or one hour to explain you guys what exactly it is and how exactly it works so as you can see\\ntill the tenth epoch or 11th epoch we have cost 2.4 and the accuracy is 0.94\\nlet's wait a little further till the 50th epoch is turn so as you can see in the 15th eat walk we have cost 0.83 and actress is 0.94 we\\nstart with cost 188 and accuracy 0.85 have you ever east the accuracies of 0.94 so as you can see this is the graph of the cost\\nit started from 188 ending at 0.8 3 we have the crop of the accuracy which\\nstarted from 0 point 8 4 or 8 5 2 all the way to zero point nine four so as\\nyou can see the 14th epoch reached an accuracy of 0.9 4/7 as you can see here\\nin the graph again and in the 15th epoch we came to the accuracy of 0.9 for now\\none might ask the question the accuracy was higher in that particular epoch why\\nhas the accuracy decreased another important aspect or have a parameter to consider here is the cost the more lower the cost the more accurate will be your\\nmod so the goal is to minimize the cost which will in turn increase the accuracy\\nand finally accuracy here we have a 0.9 for tonight which is very good now this\\nwas all about deep learning neural networks and tensorflow how would create a perceptron or deep neural network what are the different hyper parameters\\ninvolved how does a neuron work so let's have a look at the companies hiring these professionals these data professionals in the data science\\nenvironment we have companies all the way from startups to big giants so the\\nmajor companies here we can see as our Dropbox Adobe IBM we have Walmart who\\nwere chase LinkedIn Red Hat and there are so many companies and as I mentioned\\nearlier the required for these professions are high but the people applying are too low because you need a certain level of experience to\\nunderstand how things are working you need to understand machine learning to understand deep learning you need to understand all the statistics and\\nproperty and that is not an easy task so you require at least 3 to 6 months of\\nrigorous training with minimum one to two years of practical implementation\\nand project work I would say to go into data science career if you think that's the career you want to go so Yurika as you know provides data science\\nmaster program we have a machine learning master program but as you can see in the data master program we have Python statistics we have our statistics\\nwe have data size using our Python for data science we have Apache spark and Scylla we have PA and deep learning with tensorflow we have tableau so guys as\\nyou can see here we have 12 courses in this master program with 250 hours of interactive learning via capstone projects and as you can see here we have\\na certain discount going on the hike in salary you get is much more if you go\\nfor data science rather than any other program so you can see we have Python statistics a statistics data science using Python we have Python for data\\nscience Apache spark and Scala which is a very important part in data science you need to know what the Hadoop ecosystem we have deep learning with\\ntensorflow you have tableau and this is a 31 feet course as I mentioned earlier it's not an easy task and you do not become a D\\nassigned all in one month or in two months you cry a lot of training and a\\nlot of practice to become a data scientist or machine learning engineer or even a data analyst because you see a lot of topics on a vast list of areas is\\nwhat you need to cover and once you cover all of these topics what you need to do is select an either which you wanna work the kind of data\\nwhich you're going to be handling whether it be text data it would be medical records if it's video audio or images for processing it is not an easy\\ntask to become a data scientist so you need a very good and a very correct path\\nof learning to become a real scientist so so guys that's it for this session I\\nhope you enjoyed the session and got to know about data science the different aspects of data science how it works all the ways to either from statistics\\nprobability machine learning deep learning and finally coming to AI so\\nthis was the path of data science and I hope you enjoyed this session and if you\\nhave any queries regarding session or any other session please feel free to mention it in the comment section below and we'll happily answer all of your\\nqueries till then thank you and happy learning. I hope you have\\nenjoyed listening to this video, please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest.\\nDo look out for more videos in our playlist and subscribe to edureka! channel to learn more. Happy learning!\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_4.txt'}, 'embedding': None, 'id': '31be27fb9c26a0af74f6b7927d3295a3'}>, <Document: {'content': \"\\nhello everyone so welcome to this data science roadmap and in this session i'm going to talk about how to do a\\npreparation for a data science uh data science is one of the hottest skill you can say uh nowadays\\nand most of us are trying to prepare for a data science industry but some of us are able to make a transition\\nor we are able to get into a data science industry easily and some of us\\nare facing uh some sort of issue and yes that is that is very very common and it happens with all of us nothing new right\\nso not even a data science so let's suppose if i'm trying to uh get into any other industry so for sure i'll have to do a\\npreparation and according to my industry i have to adopt a couple of things i\\nhave to learn couple of things and i have to do a couple of projects so here in this roadmap i'm going to talk\\nabout that how to do that preparation what and all things are required what and all things are not required even if\\nthings are required but till what extent we are not supposed to get a mastery\\ninto each and everything this is not our industry expectations right for sure you all or we all will be able to work if i\\nknow about a couple of things and this is something which i'm going to discuss in my data science roadmap i have\\ndivided this entire things into a four segment so first of all i will be\\ntalking about uh fresher so zero year experience to a two year experienced person how they will be able\\nto get into a data science industry and what in all kind of preparations they are supposed to do and then two to five\\nyear five to eight year and eight plus year this is how i have divided this entire road map in a very clear way so\\nthat you all will be able to get an advantage of these things and if you are\\ngoing to do a preparation according to these things which i'm going to discuss believe me\\nanyone will be able to get into this industry it's not tough the only thing is we have to structure ourselves we\\nshould know that what we are supposed to study and what we are supposed to leave otherwise if you will just keep on\\nstudying something uh there is no limitation right there is no limitation after some time you will feel\\nfrustrated and you will say that no uh it's it's not it's not something which i can do but no that is not a fact\\neveryone can do it right either you you are from a tech background or maybe from you are from a dante background but\\nyes it is possible to do all of these things so fine guys let's talk about a\\nvery first segment a junior data scientist let's suppose if i'm a fresher right\\nand if i'm willing to get into a data science industry then what in all preparations i'm supposed to do as a\\njunior data scientist here as you can see so in my slide i have mentioned that you are supposed to aware about a\\ncomputer science fundamentals right when i say computer science fundamental means about an operating system which is very\\nvery important and i believe nowadays most of us are aware about it 90 plus people are aware about it a computer\\nscience fundamental uh linux i have mentioned over here a basic terminal uses i have mentioned over here what is\\napi again not in depth just as an overview i'm supposed to aware about all\\nof these terms right so this is the only requirement that we have over here now\\ncoming to this part now this part is very very very very important for all of\\nus as a fresher so if i'm a fresher i'll try to focus more and more on\\nprogramming and coding side even if i don't know about a programming or a\\ncoding even if i have no interest in programming and coding but still that is\\na requirement and here we have python one of the best\\nlanguage one of the best library available so far on this planet to learn\\nit's very very easy as well as compared to a c c plus plus java scala or any\\nother languages you will be able to find out that python is one of the best and\\none of the easiest possible language exists on this planet so it's not tough\\nto learn even if you are coming from a non-tech background let's suppose a civil background or mechanical\\nbackground i don't say that as a non-tech but yeah it's a people's perspective i'm just talking about it\\nright uh we we all have done let's suppose uh engineering or bca mca that's\\ncompletely fine right so at least we have studied some sort of a tech so i i just like uh keep every one\\nof them into uh tech itself but still there are some people who say that i'm coming from a non-tech background civil\\nmechanical electrical according to me it's tech so yeah\\nmoving ahead so programming is important for sure so you are supposed to be aware about the programming now again if i'll\\ntalk about a python programming so there are couple of library like python basic is important oops concept in python is\\nimportant logging exception handling modular programming all of these things\\nare very very important as we have mentioned over here object oriented programming numpy is important pandas is\\nimportant so all of these libraries are very very very very important again\\nwhenever you are trying to do a coding whenever you are trying to learn coding always try to take a piece of data and\\nthen try to do some sort of implementation some sort of exploration on top of those data this is the way to\\nlearn programming because see again if i'll talk about libraries and of\\nfunctions or apis which is available inside a particular library so it's kind of ocean you will not be able to\\nmemorize you will not be able to remember each and everything so the best possible way to learn any kind of a code\\nor any kind of a programming is just try to solve some sort of use cases\\ninitially yes you are going to face a problem and yes we all face a kind of a problem every time right so whenever we\\nare trying to learn something new it's a human behavior it's a human nature right but still\\nif you are going to practice for a couple of weeks i'm not talking about uh years or maybe a six month or ten month\\ni'm just saying that if you are going to practice if you will keep on solving uh some sort of a problem on a regular\\nbasis right some sort of a real problem on a regular basis so for sure you will be able to learn each and everything in\\na best possible way and this is something that we have included inside your data science master's course which\\nis available inside your one neuron tech neuron segment so data science masters i\\nwill be talking about it right so in data science masters course this is designed in a way that you will be able\\nto see each and everything in a similar sequences so let me show you so how this data science masters\\ncurriculum looks like and where from where you will be able to learn this programming so here is a data science\\nmasters if i'm going to search a data science master so this is the course that we have designed for you and let's\\nsuppose if i'm going to click on any of these videos so here you will be able to find out a python basic you will be able\\nto find out oops concept you will be able to find out a concept of databases pandas numpy matplotli plotley c bond\\nedas so everything is already available and the way i'm going to discuss a\\nsequences so you can try to explore things in a similar way you can try to practice in a similar way and believe me\\nit's very very easy after doing a couple of week of like a practice so i believe\\nit's it's easy to do it now so here i have mentioned our programming language\\nas well but keep one thing in your mind either go for python or go for r\\nif i'll talk about industry so 95 plus people are using python programming\\nif i'll just talk about two segments r and python so if i'll try to do a comparison between i'm not talking about\\nall the languages so uh don't take me wrong over there so if i'll talk about a segment who is using either python or r\\nso it's 95 is to one so it's always better to go for a python and it's\\nactually very very easy right you will start taking an interest once you will start like solving some sort of a\\nproblem with the help of python and you will find out that yes now it's very easy i can do it easily right so either\\ngo for python or are not for a two programming languages at a\\ntime i'm not going to advise you to learn a multiple programming language in a first place if you are able to learn\\npython and then if you are going to explore any languages right you are not\\nsupposed to put up a similar kind of effort easily you all will be able to learn it it's very very easy because\\nsyntax is the only thing which is going to change if suppose if i have learned python and then if i'm going to explore\\njava so it is not going to give you like a lot of pain you will be able to adopt you will be able to even write a code\\nand again you are not supposed to reinvent a wheel from a very scratch right so always try to focus on single\\nlanguage that will be better now if i'll talk about uh query languages so sql\\nright so yes little bit of sql is required for sure because at the end of the day you\\nare going to deal with lots and lots of data set and that data set will be available in some sql system or maybe in\\nsome no sql system or maybe in some file system so yes sql is important and you\\nare supposed to know a basic of it if you are fresher it's better it's better\\nto have an idea about a sequel so that you will be able to perform a multiple things you will be able to build an\\napplication and you will be able to deploy something in a production server now coming to the next\\nsegment right so here database fundamental i was talking about sql so yes database\\nfundamental is very very important because without databases you will not be able\\nto build any industry grade application anywhere believe me so that is the\\nreason this databases are very very important so i have mentioned a multiple databases\\nover here but if i'm aware about one equal system and\\none no sql system that is more than enough if i'll talk about a sequel again\\nmy sequel postgres sequel maria db db so sorry so like uh there are many databases\\nwhich is available so again you're not supposed to explore each and every database that's completely fine again if\\ni'll talk about a nosql so initially it will be better if you are having an idea about a mongodb again easy to learn easy\\nto explore this one right so one from a sequel and one from a nosql db you are\\nsupposed to learn and again as and when it is required so let's suppose i have joined the organization and there is a\\nrequirement for an implementation of a cassandra over there or maybe an hbase\\nor maybe a graph db so yes you will be able to do an implementation if you are\\naware about at least one from this segment and one from other segment you will be able to explore and you will be\\nable to implement with no time so one sequel is very very important whenever\\nyou are trying to interact with the sequel so always try to interact with a python api python is a powerful language\\nthat provides you a driver that provides you an api to connect with any kind of a sql system and any kind of a nosql\\nsystem so always try to learn in that way and same thing we have discussed even inside your data science masters\\nagain mastery is not required keep these things in your mind if you will think that first of all i'll\\ntry to get a mastery into a sequel and no sequel and then i will move ahead my advice to you will be no it is not\\nrequired right if you have a basic understanding if you are able to consume\\na data set from a sql system to your like a current machine learning system that you are going to build on a\\nstatistical analysis system that you are trying to build that is more than enough and it's completely fine then coming to the next\\nsegment as we all know that basic of mathematics is required right as a data\\nscientist and many people try to focus just on mathematics\\nthat so again i'll give you one piece of advice over here that mathematics is required but not for all\\njob not for all job which exists inside a data science so\\nthere will be another video inside this uh roadmap you will be able to find out so\\nwhere i'm just talking about a data industry so how data industry is divided so you can try to watch that particular\\nvideo and i think you will be able to get a neat and clean understanding about it so mathematics is required but again\\na similar kind of a mathematics that we all have studied in our 9th and 11th or 12th right again not all kind of a\\nmathematics is required so basic of linear algebra algebraic equation right\\na basic of calculus a differential calculus is required a basic of statistics and basic of probability so\\nthese are the only thing which is required again don't think of doing a phds because math is again an ocean you\\nwill just uh like keep on solving a mathematical problem but you will not be\\nable to get an end of it so it's better to have a basic idea about a linear\\nalgebra a descriptive statistics in financial statistics probability right\\nor some sort of a calculus now these segments is going to help you out to\\nunderstand a machine learning algorithm theoretically and when you will try to\\ndo a practical implementation right so these things these mathematics will help\\nyou out to understand that whether i'm going into a right direction or i'm\\ngoing into a wrong direction when you will try to explore a data when you will try to build a relationship in between a\\ndata even at that point of a time mathematical concepts are going to help you out and it's it's more like a\\nkind of uh i would say uh like uh exploring uh things it's more\\nlike a kind of a building a perspective about the data so where this mathematical\\nthings is going to help you out right now after this there is an instruction\\nthat we have mentioned so once you are able to reach out to this till this label it simply means that that you have\\ncompleted python you have completed a db you have completed apis and you have completed a basic of mathematics part\\nnow if you have completed a basic of mathematics part now you can start building your network so even these\\npiece of advice is mentioned inside your roadmap you can start joining a data\\nscience community there are so many communities you will be able to find out over a linkedin uh over a facebook insta\\neverywhere you will be able to find out a multiple data science community now why i'm asking you to join a community\\nthere is a reason behind this one the reason is very very simple so see\\nyou are trying to get into our industry but you are not having any kind of idea that what i will be able to get when i'm\\ngoing to join this industry and that's true so when you will see a different people uh with a different different mindset a\\ndifferent different geographical location with a different different perspective at that point of a time so you can start\\nbuilding your own perspective that okay which one is required which one is not required uh you will start evaluating\\nyourself and this is where community is important you can start joining a data\\nscience community you can start exploring a perspective of a different different people persons\\nand yes accordingly you can start talking about or data science or maybe\\nyou can start exploring about a data science this is where this start joining a data science community comes into a\\npicture now here once you are done with a little bit of mathematical concept then you are\\nsupposed to do what so then try to focus on data analysis now inside this data analysis you are supposed to learn\\nfeature engineering data wrangling and then eda exploratory data analysis\\nand then you can start writing a article this part is again important so you can start writing article so that you will\\nbe able to build your public profile building a public profile showcasing a\\nwork to someone is very very important nowadays because let's suppose if i have a resume and if i'm not able to showcase\\nmy work no one will be able to believe or no one will give me a call because uh\\nmaybe like uh there is some alternate profile that they will be able to get where people\\nare showcasing all of their experiences all the expertise that they have so articles are very very important again\\nwhile you will write an article so when you will write an article so at that point of time uh you will be able\\nto see that yes i'm able to get some sort of a hold on top of a particular\\ntopic and that is the reason so we have mentioned start writing an article section over here you can try to focus\\non baby scrapping better scrapping is again important things because every time it's not like you will be able to\\nget a data in an excel seat or maybe in a csv or maybe in some other document it's like sometimes so we have to scrap\\na data from an internet from some website from some server or something right at that point of a time bib\\nscrapping is very very important and yes we all are going to use bevel scrapping\\neven in our professional life so i believe you are supposed to aware about this baby scrapping things and which is\\ngoing to help you out to build your next project and to do a multiple things at that point of a time so that is the\\nreason so we have mentioned that okay fine so try to do uh bev scrapping and uh it is it is going to hit you it is\\ngoing to give you some sort of advantages in some way and then you can start talking about machine learning\\nnow inside this masinali section so again we can try to divide these entire things into a multiple segment there are\\nhundreds of machine learning algorithm but you are not supposed to learn\\nmathematical concept and implementation for each and every algorithm there are some lists which i'll give you\\nso machine learning algorithm wise so yes there is something called a supervised machine learning there is\\nsomething called as unsupervised there is something called as like a semi-supervised machine learning algorithm which is a combination of\\nsupervised and unsupervised again inside supervised you will be able to find out regression you can find out a\\nclassification again inside a regression there are a multiple algorithm inside a\\nclassification again there are multiple algorithm so at least a basic algorithm\\nor a very very common algorithm we all are supposed to know again it's not like\\nthese other algorithm you are going to learn maybe 10 set of algorithm you are going to learn and\\nyou will be able to do each and every task for your entire life no that is not true but\\nwhy i'm advising you to learn only 10 there is a reason behind it so these\\nbasic 10 algorithm let's suppose i'm aware about a linear regression a multi-linear regression\\nand then i'm talking so i'm aware about a polynomial regression i'm aware about a ris i'm aware about lasso in\\nclassification segments so let's suppose i'm talking about uh like svm svr logistic decision tree\\nrandom forest so there are again multiple algorithm which you will be able to find out on this particular\\nsegment classification segment so if you are aware about these algorithm it's well and good it's more than enough\\nyou're not supposed to like uh think that okay fine so i'll not be able to explore i will not be able to understand\\nother algorithm no when time will come when it will be required you will be able to understand you will be able to\\nexplore now there are some algorithm from a clustering site like a cabin clustering hierarchy clustering or\\ndbs scan so these are the basic clustering algorithm that we are supposed to know if i'm talking about a\\ndimensional reduction so pca ldas so tsne so these are the things that we are\\nsupposed to know from this particular segment plus performance matrix is wise performs metrics for a classification\\nalgorithm regression algorithm as well as clustering algorithm so we are supposed to know at any point of a time\\nwhich is very very important plus hyper parameter tuning so if i'm aware about a\\ncouple of machine learning algorithm from this supervised unsupervised or semi supervised segment i'm well and\\ngood so when i'm saying that you are supposed to aware about a machine learning algorithm i simply mean that\\nthat theoretically mathematically as well as in a practical way\\nyou are supposed to aware about all of these algorithm it's not like if i just know about the theoretical things about\\na machine learning someone is going to give me a job no this that is not going to happen because people are going to\\nhire you companies are going to hire you so that you can productionize something you will be able to build or deliver\\nsomething to their client right so unless and until you are not going to explore these things still deployment\\nhow to do a deployment how to host a machine learning model how to like do a\\nretraining and all those things how to build a complete pipeline for a machine learning implementation how to dockerize\\nthis machine learning algorithm or how to build a complete ci cd pipeline for a\\nparticular project i believe your knowledge is going to be a partial or it's going to be half so\\nyou are supposed to focus on end to end deployment of a machine learning algorithm if you are just aware about 10\\nmuslin algorithm that's completely fine no one is going to complain but if you know just uh if you know hundreds of\\nmachine learning algorithm but just a theory of it it is not going to help you out right so 10 is better than 100 but\\nthose 10 till deployment this is the expectations of a market\\nnow after this for sure you will be able to get a confidence and you can start participating in a hackathon so again\\nthere are multiple companies even i don't keep on conducting a hackathons right so you can start participating in\\na hackathon and you will be able to win some of the hackathon i think that is going to boost your confidence for sure\\nright now there are like uh again machine learning algorithm which we have mentioned over there so you can try to\\nexplore maybe ensemble technique or machine learning advanced technique a boosting approach time series again you\\ncan try to explore with the help of machine learning you can try to build a machine learning recommendation system\\nyou can try to like uh develop some sort of a recommendation system for some of the website or some\\nof the businesses that is completely fine now after machine learning so you can start reading a research paper if\\nyou want again this is not mandatory but yeah try to explore some sort of a research paper and this is what we have\\nmentioned over there and then you can try to explore a little bit of deep learning\\nagain deep learning is not required in very depth but it's better it's better to have it right it's better to have it\\nat least a basic of it again inside a deep learning there are multiple libraries which is available like a pie\\ntorch keras or tensorflow so it's better to explore either of these libraries so\\nthat you will be able to extend yourself for a next one and this is something that we have mentioned plus a little bit\\nof deep neural network rna lstm cnn this is the variance just an overview about\\nagain i'm not saying that you are supposed to explore something in very very depth no if you're just aware about\\nit if you are just able to talk about it it's more than enough if you know something in depth if you if you have\\nthat kind of interest please go ahead no one is going to stop you right but\\nif you think that okay fine so this is like a too much for me and maybe uh\\nlike i will not be able to give this much of time it's completely fine you can stop yourself over here and then you\\ncan start working on your profile and resume a building and as we have given you\\na service right that you can come for a resume discussion so you can start\\nraising a request at this point of a time that okay i'm done with my study i'm done with my project right now i'm\\nable to deploy it i have done couple of the things now tell me how to build the resume you can come for a resume\\ndiscussions and we'll be happy to have you in our resume discussion one to one session at this point of a time and that\\nis our guidelines from i neuron now if i'll talk about a machine learning ops so yes tensorflow extend queue flow\\namazon sagemaker these are nothing but it's a ops so operational site and a\\nlittle bit of these things is required so let's suppose if i'm trying to build a product so without these things i will\\nnot be able to build a product i will not be able to host it on some server again so\\ndocker kubernetes teraforms ci cd github action jenkins all these things are\\nrequired now if i'll talk about a cloud right if i'll talk about a cloud so you're not supposed to explore each and\\nevery services which is available inside a cloud alone in aws itself there are more than more than 270 plus services\\nwhich is available now if you will uh like a start exploring all of these 270\\nservices which is available which is provided by alone aws right it is going to take a huge amount of\\ntime person is not going to hire you as an aws expert or cloud expert person is going to hire you companies are going to\\nhire you for a junior data scientist role so you are supposed to be aware\\nabout a services which is required for your machine learning algorithm deployment and creating and building a\\ncomplete end-to-end system and that is my advice for all of you and these are\\nthe only thing that you are supposed to explore now i'll show you your data\\nscience master's curriculum so where we have covered each and everything in detail so here as i was talking about uh\\npython so yes python basic is there oops concept is there database is there pandas is there right numpy is there\\nmatplotlib is there plotly is there c-bond eda biff framework python project\\nwith deployment i was talking about a scrapper right so yes that projects are available over here i was talking about\\na basic of mathematics yes those mathematics are available over here like i said many of us try to focus just on a\\nmathematics see uh study or learning is a never-ending process we will keep on\\nlearning but our first priority is to get into a job if i'll just think that\\nthat okay i'll get a mastery and then i will get into a job i think you will end up losing so much\\nof time so it's better to get into a job it's better to always focus on a things\\nwhich is going to give you a quick result right okay so here machine learning so again\\nall these algorithms has been discussed if i'll talk about a machine learning project there are so many machine learning pipeline based project which is\\navailable if i'll talk about a basic of deep learning so yes all of these things are already covered so whatever is\\nrequired every and everything is available inside your dashboard itself now if i'll talk about your raising a\\ndemand option so here you can raise a demand for a resume discussion so once you think that that okay i'm now i'm\\ngood so maybe i can i can go for a resume preparation and i should start applying for the job now if i'll talk\\nabout a mock interview right so yes uh you can come for a mock\\ninterview with us and we are going to do an assessment and we are going to tell you that okay so this is where you stand\\nas of now and maybe in this area or that area you are supposed to practice a lot\\ncoming back to this section so again to complete this section how much time is required on an average right so on an\\naverage how much time is required for a pressure to do a preparation like i said technology is not tough\\nand again some people say that yes it is tough but yeah it's just a matter of time i would say\\nso here a time which is required to do this preparation is max to max a four\\nmonth right max to max four month if religiously you are involving yourself\\nif on a daily basis you are trying to do a preparation for two hour to three hour not more than two to three hour so two\\nhour to three hour for four months right so you can try to keep a\\ntime for six months i would say six months so four month just go for the\\npreparation just try to do a preparation for four months right a complete hardcore preparation you can try to do\\nfor a four month and then for next two months just go for a revision plus\\nresume tuning plus just a project building and just a mock interview that\\nis and that should be the strategy and this is how you will be able to get it in a easiest possible way\\nright that should be your strategy at any point of a time two to three hour for four months regularly and then next\\ntwo months just on revision because multiple things many things i would say or maybe\\neverything that you will be able to find out over here is going to be new for you we completely agree right and as a human\\nbeing so it's a nature that we will not be able to retain each and everything and we will not be able to like uh tell\\neach and everything and everything will not be on your fingertips right so it's better it's better to do a revision for\\ntwo months parallel to this you can try to work on your resume parallel to this you can come for a mock interview and in\\nthis way it is going to help you out a lot so sixth month is maximum timing for\\ndoing this kind of a preparation for a fresher more than this is not required\\nand each and everything is there inside your data science master's curriculum which i have shown you inside your tech\\nneuron dashboard that's it and any kind of a help that you need in between let's\\nsuppose if you have a doubt or something you can raise a request over there and from our side so we are going to resolve\\nall of those doubt for all of you so with this uh thank you so much so hope you will try to spend at least a four\\nmonth of time plus two months for a revision and your final job preparation\\nas a pressure and hopefully it is going to help you out a lot and just let me know about\\nyour feedback if i have to add something over here or if you would like to discuss something over here so based on\\nyour feedback so i can try to modify and then uh maybe i'll come up with the latest version of it with that thank you\\nso much\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_6.txt'}, 'embedding': None, 'id': '7d83b3f3f1d2293cac465b5f45cdc835'}>, <Document: {'content': \"\\nData Science Roadmap For 2-5 Year | One Neuron\\nURL- https://www.youtube.com/watch?v=3cfVOEyKSro\\nTRANSCRIPT-\\nall neurons so in this session i'm going\\nto talk about how to do a data science\\npreparation uh for a people who is\\nhaving experience more than two year or\\nin between two to five years\\nso\\ni've just like uh released a previous\\nvideo in which i was talking about\\nroadmap for a pressure so now this is\\ngoing to be an extension of the same\\nthing so here when i'll talk about a\\nsenior data scientist because when you\\nhave an experience in between two to\\nfive year\\nthere is a very high chance that\\nposition wise they are going to offer\\nyou a position as a senior data\\nscientist now what in all things are\\nrequired so again computer science basic\\nis required that is very very important\\nprogramming as i have mentioned in my\\nprevious lecture\\nthat\\nprogramming wise\\nat least one single language is required\\nand my preferred language will be python\\nbecause\\nif i know one single language for sure i\\nwill be able to explore other languages\\nas well in the easiest possible way so\\ndon't try to focus on a multiple\\nprogramming languages python itself is a\\nvery very powerful language try to focus\\nalways on a python programming\\nand\\nyes so\\ndata structure algorithm now many people\\nask me a question that whether data\\nstitch algorithm is required or it's not\\nrequired\\nlet me tell you one thing very very\\nclearly over here\\nif you are going to start your career\\nwith a product-based companies\\nso yes it is required even as a fresher\\nit is required even if you are getting\\ninto a data science profile it is\\nrequired a basic of data stretch\\nalgorithm linked list queue tree tree\\ntraversal uh like a red factory like a\\nbinary tree all those things are\\nrequired\\nnow if you are going to apply for maybe\\na\\nservice based companies or some sort of\\na startup in that case data switch\\nalgorithm is not required so again make\\nsure of it it depends upon your\\nrequirements so you can try to adopt\\ndata algorithm or maybe you can try to\\ngo ahead with the simple programming\\nagain programming is very very important\\nside object-oriented programming modular\\ncoding again inside a python multiple\\nlibraries all those things are required\\nfor a data processing as i have already\\ndiscussed in my\\nprevious section of this video\\nnow if i'll talk about a programming\\ntechnique so here you are supposed to be\\naware about a design pattern so how we\\ncan try to design an entire solution for\\na particular problem that is something\\nwhich is very very important now if i'll\\ntalk about a test driven deployment\\nagain it is important so how you are\\ngoing to move your product or your\\nimplementation from a development system\\nto a testing environment or to a\\nproduction environment dependency\\ninjection so what in all dependencies\\nthat you are going to face may be\\nimprovised maybe a software device may\\nbe solution code wise so yes that is\\nrequired functional programming is very\\nvery important because without this no\\none is going to accept your code in a\\nproduction environment and then database\\nfundamentals are very very important\\nbecause\\nat the end of the day you are not going\\nto keep your data set inside your local\\nsystem your data must be available in\\nsome sql server in some file system or\\nmaybe in some nosql system and hence\\nthis database is required so you can\\nmaybe try to explore a sql and then you\\ncan try to explore one from nosql which\\nis a mongodb easy to implement easy to\\nunderstand and easy to consume these\\nthings inside your code again i have\\nmentioned a multiple nosql db again\\nthere is a huge list of sql and a nosql\\ndb each and everything is not required\\nat any point of a time if you are good\\nwith one i think you will be able to\\ncrack an interview in the easiest\\npossible way so please make sure of it\\nand programming design pattern as well\\nas database these things are very very\\nimportant and please make sure that you\\nhave and complete understanding about\\nthese things when you are going to apply\\nfor the job now coming to the next\\nsection mathematics\\nagain so don't try to like uh get a phd\\ninto it so yes math is required but till\\ncertain extent so not\\nlike everything is required inside a\\nmathematics whether it's a linear\\nalgebra whether it's a calculus whether\\nit's a probability whether it's a\\nstatistics so a couple of things is\\nrequired and more important thing is how\\nto do an implementation right how i will\\nbe able to use a particular things let's\\nsuppose if i'm talking about a jet test\\nif i'm talking about a p value if i'm\\ntalking about a b testing implementation\\nwise how i can utilize all of these\\nconcept and all of these knowledges in\\nmy development so this is how you are\\nsupposed to understand a mathematics not\\njust in terms of a mathematical way\\nright uh just by solving some sort of a\\nsimple problem no this time so you are\\nsupposed to understand this methodics in\\na way that you will be able to solve\\nsome of the data related problem and\\nthen you can start joining a data\\nscience community because again\\ncommunity is very very important here\\nyou will be able to see a perspective of\\nour mindset of a different people and an\\nexpectation of our companies and many\\nmore things you will be able to learn\\nand these things is going to help you\\nout even to get a job right so start\\nbuilding your com community but always\\nin a constructive direction not in a\\nnegative direction so when i say\\ncommunity i think we are supposed to uh\\nget an advantage of it maybe in a\\npositive way\\nnow coming to our next section so here\\ndata analysis again it's important it's\\nrequired right data analysis like a\\nfeature engineering data wrangling edas\\nand then you can start writing an\\narticle and then bib scrapping so like i\\nsaid in my previous section that some\\ntime so you will not be able to get all\\nthe data from your client and this is\\nwhere\\nscrapping is required sometime there\\nwill be a data dependency there is some\\nsort of a derived data you have to\\ncreate now at that point of a time\\nscrapping helps us a lot to prepare\\nthose kind of a data now you are\\nsupposed to justify that how i will be\\nable to use and scrapping if you are\\ngoing to mention a scrapping project\\ninside your resume but yes\\nthat is something that you are supposed\\nto understand now uh like coming to the\\nnext section so indexation so machine\\nlearning is important\\nobviously you are trying to prepare\\nyourself for a data science so\\nhow you can leave a machine learning\\nwithout machine learning so you can't\\nbecome a data scientist that is like a\\nabcd of data science so yes machine\\nlearning is required 10 algorithms i\\nbelieve not 100 but a way by which you\\nwill be able to productionize those 10\\nalgorithm you will be able to build on a\\nscalable system you will be able to do\\neverything that you want so in that way\\nyou are supposed to learn a machine\\nlearning algorithm\\nkeep always these things in our mind\\nknowing just a theoretical concept or\\nmaybe just a mathematical concept for a\\nmachine learning algorithms are not\\nsufficient nowadays it's not relevant in\\na current industry scenario because\\nlet's try to assume uh one one case over\\nhere so let's suppose there is a company\\nxyz and uh who is trying to hire you for\\ntheir like a senior data scientist\\nposition or this particular role\\nand you're just aware about a\\ntheoretical concept a mathematic concept\\nthose companies will not be able to\\ndeploy you they will not be able to\\nassign you any kind of a job any kind of\\na portfolio because you have never\\nworked into a deployment you have never\\nworked into a scalability you have never\\nwalked into a model retraining you're\\nnot aware about it right\\nand that is something which is required\\nyou have never you have never worked in\\na pipelining\\nof any kind of a production grid\\nsolution which is a requirement which is\\na requirement of a particular project so\\nwhy they are going to hire you\\njust for a reason that you know some\\nsort of a machine learning algorithm\\nin a theoretical way they are not going to hire you they are going to reject\\nyour profile they are going to reject\\nyour resume so it's always better\\nto learn all of these algorithm till\\ndeployment how i will be able to\\nproductionize these things how in a real\\nsense i will be able to use it and this\\nis how each and every algorithm whether\\nit's a machine learning or machine\\nlearning advanced algorithm so this is\\nhow in that way you are supposed to\\nlearn and then you can start reading a\\nresearch paper if you have an interest\\nbecause sometime in an interview so they\\ntry to ask you a question saying that\\nthat\\nwhat is the source of your knowledge\\nright so maybe you can give an argument\\nover there that okay fine so i keep on\\nreading a research paper i keep on\\nexploring multiple things by my side and\\nthis is how i just keep on like uh\\nupdating myself on a regular basis right\\nso this is where research paper comes\\ninto a picture now\\nproduct wise yes it is very very\\nimportant there are so many projects\\nthat we have given to you in your data\\nscience master's curriculum\\nnow\\nsomewhere\\ndata visualization is also important so\\nnot all maybe just a w maybe just a\\npower bi maybe just a looker maybe just\\na click view so at least one\\nyou must be having a basic idea and\\nunderstanding about this w power bi\\nlookup click view any of these things\\ni'm not telling you i'm not asking you\\nthat to get a mastery into each and\\neverything either of these is completely\\nfine to mention those things inside your\\nresume and that is going to increase our\\nweightage saying that that yes i'm a\\nperson who knows how to build how to\\ndeploy how to productionize a machine\\nlearning algorithm i know how to play\\nwith the data i know even a statistical\\nconcept so if you are going to give me\\nsome new challenges for sure i will be\\nable to solve it apart from that\\ni will be able to create a complete\\ndashboard right so where manager will be\\nable to see a stats they will be able to\\nvisualize each and everything they will\\nbe able to see a report and for a\\nreporting site these things as important\\nnow so as a senior data scientist yes\\nthere will be an expectations on a deep\\nlearning side you are supposed to\\navailable you can't say that no i don't\\nknow deep learning i just know machine\\nlearning i have already worked into a\\nmachine learning and i'm just looking\\nfor a machine learning profile because\\nnowadays almost every companies are\\ntrying to explore trying to build a\\nsolution by using this deep learning\\nwhich is again very very important and\\nyou are supposed to learn a deep\\nlearning concept this is important so\\nplease try to make sure that deep\\nlearning concept try to explore there\\nare a multiple vision algorithm you will\\nbe able to find out nlp based algorithm\\nyou will be able to find out or basic of\\ndeep learning basic of a and that's more\\nthan enough but yes please try to\\nexplore those things and then you can\\nstart building your resume this is\\nsomething that you should do\\nnow computer vision so yes a little bit\\nlike i have mentioned nlp a little bit\\nwhatever algorithm that we have\\nmentioned over there so you are supposed\\nto aware about it\\nbig data\\nso you are going to work as a senior\\ndata scientist and let's suppose if i\\nhave to ask you i have to assign you a\\ntask for a pipelining\\nthis is where this big data comes into a\\npicture a basic of big data is very very\\nimportant at least you must be aware\\nabout a component like a pig hive kafka\\nspark airflow nifi all those solutions\\nand why we are going to use the solution\\nso that in an interview if someone is\\ngoing to ask you a question that are you\\naware about this take a stack you can\\nsay yes i'm aware about it and maybe uh\\nlike uh\\ni can work if this task will be assigned\\nto me so yes that is important and\\nplease make sure that you must be having\\nsome sort of idea and we have already\\ngiven you a lecture inside your data\\nscience master's curriculum now if i'll\\ntalk about uh cloud computing again\\ncloud computing is required because at\\nthe end of the day everything you are\\ngoing to host inside a cloud it can be a\\naws azure it can be a gcp it can be some\\nother cloud but yes it is going to be a\\ncloud in majority of an environment in\\nmissouri of a companies so cloud\\ncomputing is important to build into a\\nsolution because you are going to work\\nas a\\nsenior data scientist and without this\\nhow you will be able to productionize a\\nsolution into it you will be able to\\nbuild a solution plus machine learning\\noperation or ai ops we used to say or ml\\nops we used to say somewhere so these\\nthings are very very important we are\\nsupposed to aware about an ops part so\\nthat i will be able to use a docker\\nkubernetes or jenkins or maybe i will be\\nable to use our tfx and many more things\\nright\\nso please make sure that try to explore\\nuh ai offside or maybe a mlob side which\\nwill give you a complete system design\\nperspective for any kind of ai system\\nthat we are going to build and this is\\nsomething that we have mentioned over\\nthere so like a model retraining\\nvalidations deployment scheduling\\neverything comes under the op section\\nagain inside the op section so docker\\nkubernetes teraform jenkins all those\\nthings comes into picture and cloud\\nservices so\\nthese this is the roadmap for a senior\\ndata scientist which is an extension of\\na junior data scientist itself not\\ntalking about a time so if i'll talk\\nabout a time how much time you are\\nsupposed to spend so far sure if you are\\ngoing to apply for a senior data\\nscientist position so in that sense you\\nare supposed to spend at least three\\nhour two four hour\\non an average on a daily basis for five\\nto six months\\nand apart from this so maybe you can try\\nto keep a buffer period of one to two\\nmonths\\nfor your projects right for your resume\\ndiscussion for your mock interview for\\nyour detailed project report creation\\nand a fine tuning of yourself you can\\ntry to keep a buffer of like one to two\\nmonths so in seven to eight months it is\\npossible if you are if as an average\\nperson so if you are trying to spend\\nthis quality hours three to four quality\\nhours i'm telling you i'm not saying\\nthat you're just sitting on a chair for\\nthree to four hour for five to six\\nmonths and then it's it can happen no\\nthe quality hour is very very important\\nif you are going to spend this much of\\ntime it's easy to get a job as a senior\\ndata scientist plus focus more and more\\non project side and if you are applying\\nfor a product based company focus on dsa\\nas well\\ndata structure algorithm which is again\\nmentioned inside your tech neuron each\\nand everything which i'm talking about\\nis already available inside your data\\nscience master's curriculum and you can\\ntry to explore each and everything from\\nthis tech neuron itself nothing will go\\nbeyond it\\nplus\\neven interview questions we are like uh\\nwe have already uploaded inside a 30\\ndays interview question series plus\\nthere is a github profile of i neuron\\ngithub like a link of iron where you\\nwill be able to find out interview\\nquestions and believe me no one is going\\nto surpass that particular label so you\\ncan try to plan yourself accordingly and\\nthen you can try to crack\\nthis one and these things can these\\nthings is possible so again i'm\\nconsidering a fact that we have started\\nfrom\\nzero right i'm not saying that you have\\nsome sort of experience and then this is\\ngoing to work for you no i'm talking\\nabout just a ground zero from where we\\nhave started and then if i'm going to\\nstart my preparation with this one\\ncalled as data science masters and\\neverything everything and\\nwhatever whatever i have discussed\\neverything you will be able to find out\\ninside this section itself there is a\\n1100 plus video which we have uploaded\\nso it's a huge curriculum that we have\\ndesigned\\njust say machine learning deep learning\\naiofs big data data analytics everything\\nprojects and everything will be\\navailable over it's already available\\nover here so please try to explore\\naccording to your requirements and if\\nyou are doing a preparation so from\\ntoday onwards you can start a\\npreparation create a chart create a\\ntable saying that okay fine so i'll try\\nto invest three to four hour of time for\\nnext five to six months and then last\\none to two months i will try to expand\\non a revision side on a mock interview\\nside on a resumer side and on a project\\nside because\\nlearning is completely fine but learning\\nis not going to give you a believe me\\nyour experience your expertise and a\\ncapability to build a solution is going\\nto give you a job the kind of packages\\nthat you're looking for the kind of a\\nprofile that you are looking for each\\nand everything will come out of your exp\\nnot out of your learning if you know\\nhundreds of machine learning algorithm\\nbut you don't know how to productionize\\neven a single one believe me\\none is going to help you\\nright so always try to focus on a real\\nthing\\njust try to maintain a habit\\nof like uh sitting on on a chair for\\nthree to four hour\\nand that is something which can change\\nagain\\nright so that is my advice to all of you\\nguys and hope you will do as it is as i\\nhave discussed and keep on writing your\\ncomments uh so if you have any kind of a\\ndoubt any kind of a concern so for sure\\ni will be happy to clarify those things\\nwith that thank you so much\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_6.txt'}, 'embedding': None, 'id': 'd38a8ffd43f3e5d5becd0e74e2b66e2e'}>, <Document: {'content': \"Data Science Roadmap 5-8 Year Experienced | One Neuron\\nURL- https://www.youtube.com/watch?v=guwOATf9U-A\\nTIMELAPSE-\\nhello everyone so welcome to oneiron and\\ntoday i'm going to talk about how to do\\na preparation so if i have an experience\\nindustry experience again i'm talking\\nabout so if i have an industry\\nexperience of in between maybe uh five\\nto eight years so in that case how i\\nwill be able to do a preparation so here\\nuh like uh yes computer science\\nfundamentals or maybe i will say like\\noperating system fundamentals is very\\nvery important so i believe i'm supposed\\nto be aware about uh basic of linux\\ncommand that is pretty much obvious what\\nis uh uses of terminal or what is a rest\\napi\\nor what in all different different\\nframeworks which is available to create\\napis plus a programming language again\\nprogramming language wise i will not\\nadvise you to go after a multiple\\nprogramming language i think one is more\\nthan enough\\nand the best one is python so it's a\\nvery very powerful language and easy to\\nlearn as i have mentioned in my\\nlike a roadmap of afresha in a very\\ndetail again inside a python programming\\nmodular coding is important object\\noriented programming is important a\\ncouple of libraries are very very\\nimportant like a numpy pandas\\nmatplotlib seaborn\\ncuff links all those things very very\\nimportant plus programming technique you\\nare supposed to know a design pattern\\nhow to create a test environment uh\\ndeployment dependencies and all those\\nthings right so programming technique\\nwise because you\\nwill be given a task to deploy to\\nproductionize and to build a complete\\nsystem\\nuh there is a chances that if you are\\ngoing to join an organization as a lead\\nin that case you will be able to get\\nyour team now you have to drive that\\nentire team so you should know all the\\nstandard procedures and protocols\\nfor example uh model coding right so\\nlogging monitoring debugging right\\neverything must be like there you are\\nsupposed to know about it you must be\\nlike a\\nlike a doing all of those things inside\\nyour project apart from that the\\nknowledge of databases because yes\\nwithout databases you will not be able\\nto build anything in like anywhere right\\nso knowledge of sql system and nosql\\nsystem is very very important if it is\\nrequired so try to learn a multiple\\nsequels and a nosql system again\\nafter some time you will be able to find\\nout that everything is same and it's\\nlike if i can do if i can work in one\\nthing i will be able to work in other\\nthing so just try to learn in that way\\nand just try to build your perspective\\nin this particular fashion that would be\\nmore than\\nnow coming to a mathematical side so yes\\nuh sometimes so you are going to face\\nyour client directly\\nsometimes so you will be taking a\\nbusiness requirements right uh business\\nis going to in fact with you on a\\nregular basis they are going to assign\\nyou a task they are going to check a\\nfeasibility\\nof a particular task with you so yes you\\nmust be aware about all the statistical\\nconcept uh which has been used to\\nevaluate a project to evaluate a data\\nset plus a multiple mathematical expects\\nfrom a linear algebra from a statistics\\nfrom a calculus and from everywhere\\nso that you will be able to utilize\\nthese things right you all will be able\\nto realize these things while evaluating\\na data while doing an assessment for a\\nparticular project for a particular\\nclient so you are supposed to know about\\na mathematics in a pure\\nlike a in a way you know in a way by\\nwhich you will be able to use all of\\nthese things in a real time that is this\\nis how you should learn a mathematics\\nlet's suppose i'm just talking about uh\\na b testing or if i'm talking about a\\nhypothesis testing if i'm talking about\\na probability right\\nso it's not like if you are able to\\nsolve a couple of problem in a\\nprobability your work is done no how to\\nuse these probabilities in a machine\\nlearning in a deep learning in a\\ncomputer vision inside a nlp this is how\\nyou are supposed to know about it this\\nis how you are supposed to adopt a\\nmathematics at this point of a time not\\nin a mathematical way but in a way\\nby which you will be able to explore\\nsomething out of the data something out\\nof the requirement given by your client\\nor maybe your team member so this is how\\nyou should learn a mathematics always go\\nin a practical way when i'm saying\\npractical doesn't mean that how to\\nconvert one particular like uh\\nlike equation into a programming no that\\nis not a practical way i'm talking about\\ni'm just talking about a way by which\\nyou can really implement it and then you\\nwill be able to\\nlike uh get some sort of insights out of\\nit that is a way by which you should\\nlearn mathematics and then you can start\\nbuilding your own data science community\\ni think that would be better you can\\nstart guiding someone you can start\\nlike interacting with the people who are\\nalready a part of a data science\\ncommunity uh because you will be able to\\nsee a perspective from our different\\ndifferent people and a person so this is\\nhow you should\\nmove ahead now coming to a data analysis\\nsection so yes programming wise you are\\nsupposed to do a data analysis but\\nalways keep one thing in mind\\nalways try to find out a logical\\ndependencies or a logical relationship\\nbetween a different different attributes\\neven inside your data this is how you\\nare supposed to do a data analysis\\nalways try to focus on a business and\\nthen try to understand a nature of the\\ndata a particular business is trying to\\ngenerate which will help you out to\\nbuild a case in a strongest possible way\\nand you can present it in front of your\\nmanager or in front of your client right\\nin a strongest possible way because\\non this label\\nyou will be uh you will experience some\\ntime that uh it's not possible to build\\na ai solution for every problem right\\nit's not possible\\nor sometimes so it's better to go ahead\\nwith a non-ai solution as compared to a\\nai solution because of maybe uh ease of\\ncreating it or ease of like or maybe\\nbecause of a less number of dependencies\\nyou will be able to get so it's not like\\nevery time if someone is giving you a\\nproblem statement we are supposed to go\\nin a ai way ai is just a tip right\\nmaybe we can look for even a simple\\nsolution right so instead of like going\\nahead with the hard way so maybe we can\\ntry to choose an easy one that is\\npossible so you are always supposed to\\npresent your case and that case you will\\nbe able to present if you are good with\\nan analysis of a data plus if you\\nunderstand a business\\nthe kind of attribute that business has\\nassigned to you the kind of a columns\\nthe kind of values that business has\\ngiven to you so based on that you have\\nto do a data analysis again it's a\\nbroader one it's like a you have to\\nunderstand a broader picture over here\\nand based on that so you are supposed to\\nrepresent these things in front of your\\nclient and then you can start writing\\nyour own article based on your own\\nthesis based on your own analysis on a\\ndaily basis that's completely fine which\\nwill build a social profile for you\\nbecause on your label these things are\\nvery very very very important right so\\nyou can start writing an article baby\\nscrapping and all those things it's just\\na part of it part of your job\\nso for sure you have to know anyhow\\nright now if i'll talk about machine\\nlearning algorithms so yes you are\\nsupposed to know about machine learning\\nalgorithm at least 10 to 20 algorithms\\nfrom a supervised machine learning from\\nunsupervised machine learning from a\\nsemi supervised machine learning site or\\nmaybe from a design machine inside so\\nyou are supposed to know about machine\\nlearning algorithm again if i'll talk\\nabout a sequence of algorithms so linear\\nregression and then like a polynomial\\nregression multi-linear regression risk\\nregression lasso regressions or maybe a\\ndecision tree regression with the help\\nof decision tree or maybe a support\\nvector regressions there are so many\\nalgorithms you will be able to find out\\nin a regulation site itself so yes you\\nare supposed to be aware about it now if\\ni'll talk about a classification\\nalgorithm so maybe a logistic relational\\nalgorithm support vector classifier\\nalgorithm decision tree or maybe uh xj\\nboost boost or maybe\\nlike a random forest so there are again\\na series of algorithm that you will be\\nable to find out now knowing an\\nalgorithm\\nand its mathematics and its theory is\\ngood\\nbut\\nit's not enough\\nit's not enough for a particular job if\\nyou are just working as a researcher\\nmaybe that is fine you can just keep on\\ndoing a research on a mathematical side\\non a stat side and it is completely fine\\nbut as a person who has joined a giant\\njoined an organization and who is having\\na task to deliver it\\nover here\\nso you are supposed to learn an\\nalgorithm plus its implementation now\\nwhen i'm talking about implementation it\\ndoesn't mean that that i'm just talking\\nabout like a calling in garth passing a\\ndata inside it like fitting up the data\\ninside the algorithm and then work is\\ndone no\\nthis is again not a way\\nright most of us have done that right\\nmost of us have like a created a model\\nbut it's of no use believe me\\nunless and until you are not going to\\ncreate a complete pipeline from your\\nextract transform and load plus building\\nan algorithm plus productionizing it\\nfirst of all into a development\\nenvironment then uat then into a\\nproduction unless and until you are not\\ngoing to build a complete pipeline for a\\nmodel retraining you are not going to\\ndefine a model retraining approach right\\nyou are not going to build a system\\nwhich can work for one and which can\\nwork for million right\\ni think your work is not done so you\\nhave to focus you have to have focus\\nmore and more on a system side\\non a architecture side on a design side\\nas well you will be the one who is going\\nto decide that or you are going to\\nadvise to your senior management saying\\nthat that this is the architecture this\\nis the databases which i have used\\nwhether it's a sequel or nosql\\nthis is the cloud infra which i have\\ntaken this is the services which i am\\nconsuming this is the derived data set\\nwhich i have created\\nright so complete detail project report\\nyou have to create by yourself and for\\nthat you have to learn all of those\\nthings you have to experiment and\\nexplore all of those things on this\\nlabel right and that is the reason so i\\nhave given you an advice that just\\nknowing a machine learning algorithm or\\nits mathematical term or theoretical\\nterm is not enough at all right so just\\ntry to explore these things beyond it\\nand everything is given to you in your\\ndata science master's curriculum for you\\nguys or you guys who falls in between\\nthis particular bracket\\nso the since your advice to all of you\\nis that focus more and more on\\nimplementation side instead of\\ntheoretical side\\nthat will give you experience and that\\nis something which is going to work in\\nyour interview if you are not focusing\\non your practical side things will go\\nsouth believe me things will go south if\\nyou think that that i just know about an\\nalgorithm and mathematical term i'm good\\nwith it and it is going to work no\\nalways and always people will try to\\nfocus on an implementation side and\\nagain a core end-to-end implementation\\ntill deployment till retraining approach\\nso just keep always these things in our\\nmind and then if you want you can start\\nreading a research paper if you want you\\ncan start exploring a different\\ndifferent kind of systems as well\\nnow coming to a data visualization even\\nif you have not worked into a data\\nvisualization but still you must be\\naware about a connectivity you must be\\naware about a basic of it right whether\\nit's a w or power bi or looker or click\\nview or any other dashboarding tool but\\nyes you are supposed to be aware about\\nit now if i'm talking about a deep\\nlearning\\ndeep learning is must for you guys\\ndeep learning a basic is at least\\nimportant if even if you are going to\\nswitch as a machine learning engineer\\npeople must be having an expectation\\nthat you are aware about a little bit of\\ndeep learning so please keep these\\nthings in mind that you have an idea\\nabout a deep learning and with the\\npreparation you should go right okay now\\ncoming to a computer vision so computer\\nvision again it's an ocean so you can\\nexplore the entire syllabus that we have\\ngiven to you with the\\nuse implementation almost every kind of\\nimplementation is mentioned inside your\\ncomputer vision which is a part of your\\ndata science master's curriculum if i'll\\ntalk about nlp yes everything is\\nmentioned if i'll talk about a big data\\nyes everything is mentioned why we have\\nmentioned all of these components there\\nis a reason behind this the reason is\\nthat\\nat the end of the day system is the only\\nthing which matters and you are going to\\nbuild a system\\nright so unless and until you are not\\nhaving an idea and understanding about\\nthe whole system and its integration how\\nyou will be able to build a system and\\nkeeping these things in your mind we\\nhave given you this entire huge\\ncurriculum yes it takes time but\\nit's going to pay you a lot\\nso it's better to go ahead with a\\npreparation in a war so that you will be\\nable to\\nwin it right so go for\\nwin here cloud computing infrastructure\\nmachine learning ops site again for you\\nguys it's very very important ci cd all\\nthose things are important\\nnow for\\nespecially for a person who is going to\\nwork as a lead i will try to focus on\\nthis side\\nyou will be engaged or you will be a\\npart of engaging a stakeholder your\\nclient or a person who is going to\\nassign you a work right so you will be\\nresponsible for deriving uh values\\nright identifying their need so maybe\\nyou can propose them that okay fine so\\nmaybe i can solve this problem in this\\nway maybe you can\\nbring a business for your uh like a\\nrespective organizations right so even\\nyou as a lead will be responsible for\\nexploring a new things creating a values\\ninside your company because your next\\nrole is going to be a managerial role or\\nmaybe\\nwhen whenever you will join a next\\ncompany so yes you are must be prepared\\nfor these roles right so how in a best\\npossible way you can engage your\\nstakeholder how in a best possible way\\nyou are going to introduce a new\\ncapacity inside your team infuse on new\\nthings inside your team how you are\\ndoing a pocs rfps how you are going to\\nbuild an entire team how you are going\\nto reskill your entire team along with\\nyourself matters a lot on this\\nparticular label so my sincere advice to\\nall of you will be to focus even on\\nthese side right generally as a techie\\nso\\nwe never try to focus on this side but\\nbelieve me if you are going to focus on\\nthis side it is going to change your\\nlife and it is going to right and it is\\ngoing to give you an open up\\nhell lot of opportunity in a market\\nright so please make sure that you do it\\nright you take a stick holding take a\\nownership take a leadership and deliver\\nit\\ntry to build a team build our\\ncapabilities right so let's suppose you\\nhave joined a team and over there so\\nthey're just working on machine learning\\ntry to set up a team who can even work\\ninto a deep learning even work into a\\ncomputer vision even work into a\\nreinforcement learning\\ntry to build a capability so that you\\nwill be able to do a integration of your\\nexisting system with any other system\\nand you will be able to do a maybe like\\na\\ndie to achieve a cost cutting try to\\nachieve a reskilling of the team and\\nthere are so many activities that we can\\ndo on a floor right on a daily basis so\\nalways try to work as a team leader over\\nthere\\nand\\njust try to build an awesome team over\\nthere okay\\nso educate educate your team right and\\nagain help assist each and everything\\nso that is a very very important and\\nthis is something which i have mentioned\\nso build a complete data science culture\\ntry to understand requirements of each\\nand every clients try to extract as much\\nbusiness as possible and that is\\nsomething which is going to give you a\\nsuccess now coming to now coming to our\\ndashboard so here inside this dashboard\\neach and everything we have given to you\\nright everything right from the\\nbeginning so whatever is required to\\nexplore to discuss to like uh experiment\\nright everything is available over here\\nyou are not supposed to go anywhere and\\nif something is missing yes you can\\nconnect with us and then we will try to\\nhelp you out but\\ntry to explore this data science masters\\nin detail just try to spend as much time\\nas possible now time wise if i'll say so\\ntime wise so maybe you can try to spend\\nso time wise you can try to spend maybe\\nright three to four hour which is\\naverage i think that's more than enough\\nso\\nthree to four\\nhours right maybe for six to seven\\nmonths and one to two months\\nyou can try to take for a revision uh\\nfor a resume preparation for your mock\\ninterview for a project for a detailed\\nproject report dpr and all those things\\nright so always always try to like a\\nwork as per the given instruction and\\neasy to make a transition again you can\\ncome for the resume discussions you can\\ncome for the mock interview with iron\\nteam so there is a razor demand option\\nwhich is given to you and just try to\\nutilize just try to take advantage of it\\nas much as possible\\nso hope this is going to help you out a\\nlot with this thank you so much and see\\nyou\\nsee all of you in my next lecture thank\\nyou\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_6.txt'}, 'embedding': None, 'id': '1bf2ec0ceed9d3e0d3859d3a8c1adbd5'}>, <Document: {'content': \"hello everyone welcome to oneiron and in\\nthis session i'm going to talk about uh\\nroadmap a data science roadmap for a\\nsenior people or a data scientist for a\\npeople who is having experience more\\nthan eight years so in this session i'm\\nnot going to follow a top-down approach\\ni'm going to follow a bottom-up approach\\nand there is a specific reason behind\\nthis approach which i am going to follow\\nto explain a road map for this data\\nscience for a senior people now just try\\nto understand in this way\\nthat if i'm a senior a person if i'm\\nworking as a manager or maybe if i'm\\nworking as a lead or architect in that\\ncase\\nright what will be my roles and\\nresponsibility my roles and\\nresponsibility will be to talk to a\\nstakeholder first of all\\nit is also a possibility that you will\\nbe a stakeholder in your respective\\norganization means you will be\\nresponsible for explaining you will be\\nresponsible for deriving a new business\\nopportunities for your organization and\\nin that case you have to interact with a\\nlot of people on a daily basis apart\\nfrom this so let's suppose if you are\\nworking on a technical side so for sure\\nyou are the one who will be responsible\\nfor designing the architecture system\\nrelease whatever you can say end-to-end\\nfor every application so here top-down\\napproach is not going to work if you are\\ngoing to start learning a python then if\\nyou will try to start like uh learning\\nmachine learning then deep learning i\\nthink that is not going to work and this\\nis not the approach that you all are\\nsupposed to follow so first of all try\\nto understand that what kind of a role\\nand responsibility you are going to play\\nwhen you are going to join your next\\norganization so for sure as a lead as a\\nmanager as a director as architect so\\nyou all are going to like build a team\\nso yes for a organization for a current\\norganization so maybe i'm going to join\\none company and\\nmanager of like or maybe our directors\\nare going to ask me to build a team\\nmaybe a company is trying to build a new\\ncapabilities for a machine learning for\\na deep learning for a computer vision\\nfor a hardware ai based research or\\nmaybe for a reinforcement learning so\\nyes that will be my responsibilities\\nfinding out a right person for your team\\nwill be an important task for you and\\nyou have to build a team let's suppose\\nif you have a existing team so in that\\ncase you have to think about a race\\nkilling your existing team because\\ntechnologies are changing on a daily\\nbasis again on a daily basis you will be\\nable to find out a new algorithm new\\nimplementation a new tech stack so again\\nthat will come on you for your company\\nso you have to bring a client because\\nunless and until you are not going to\\nbring a client inside your current\\norganization i believe your roles are\\nnot going to be a relevant one so again\\nyou have to work on a client acquisition\\nas well you have to bring a business\\nplus whatever business that you are or\\nwhatever the stakeholding that you have\\nas of now so you have to deliver those\\nthings on a daily basis maybe on a\\nweekly or monthly basis at a time so you\\nare not going to handle just a single\\nteam so at a time there is a chance that\\nyou will end up handling a multiple team\\nagain multiple team for a data science\\nor multiple team which is from a data\\nscience or maybe apart from data science\\nfor example so let's suppose if i'm\\ntrying to build a machine learning\\nsolution so if i'm trying to build a\\nmachine learning solution so it's not\\nlike a solution you will be able to\\nbuild just by having a capability of a\\nmachine learning engineer no big data\\nengineer is required someone is required\\nin on a dashboarding site someone is\\nrequired maybe on a website someone is\\nrequired on a platform site someone is\\nrequired on our off site someone is\\nrequired on an infrasight so there are\\nmultiple teams that you need and you\\nwill have to handle an entire team at a\\nat a time you have to build a flow you\\nhave to build a process and a pipeline\\nso that you will be able to deliver\\nsomething so you're not supposed to work\\njust on one single thing plus you have\\nto derive a new solution on a regular\\nbasis because that is a need that is the\\nneed of our industry this is how market\\nis going to work and so\\nbottom up approach you are supposed to\\nfollow so first of all you are supposed\\nto know let's suppose i'm trying to get\\ninto a healthcare domain or maybe into a\\nsupply chain domain so i'm supposed to\\nlike uh know first of all in a very\\nfirst phase that\\nwhat kind of a solution is possible in\\nai at least some because there are\\nmillions of there are infinite number of\\nsolution you will be able to find out in\\nor out of the organization but in a\\nbeginning itself you are supposed to\\nknow about it because people is going to\\nask you a question you know interview\\nthat how many projects you have\\ndelivered what is the kind of a business\\nthat you have bring inside the company\\nwhat is the total profit what is uh like\\na number of clients that you have\\nhandled so far so forth how many number\\nof people you have handled so far so\\nforth and what is a diversification of\\nthat particular team so these are the\\nquestion when comes in a first phase so\\nwhy we are not supposed to start from\\nthat particular area so if i have to\\napply for a job i'll start exploring\\nthese areas what is a solution now once\\ni will be able to understand that okay\\nso x y z kind of a solutions are\\npossible in this respective domain then\\ni will start exploring about a\\ntechnology and here if you can see\\nright so this is what i have mentioned\\nin this particular slide so i'm trying\\nto state over here that okay so let's\\nsuppose if i'm trying to like a build or\\nif i'm trying to do something so here\\narchitect role wise so i'm supposed to\\nfocus on architectural drivers so what\\nin all requirements what in all\\ntechnology from a different perspective\\ni'm supposed to learn then a technology\\nselection designing the entire solution\\nthen evaluation of the entire\\narchitecture quality assurance coaching\\nand mentorship again this is going to be\\nyour part so i'm supposed to focus on\\nall of these things building a right\\nteam with a right people right number of\\npeople in a like a timeline that is\\ngiven to you again that is a crucial\\ntask that we all used to play on a\\nsenior label\\nand you have to design a process process\\nfor a delivery process for reskilling\\nprocess for handling a team process for\\ngenerating a new business so everywhere\\nyou will be involved now the best part\\nor the best way to explore these things\\nis with the help of this particular\\ncourse which is available over here\\ncalled as dsar data science architecture\\nand resume so this is a session which i\\nhave delivered last year it's a bit\\nlengthy session you will be able to find\\nout but again at the same point of time\\nit is going to be an interesting one i\\nhave received many feedback many comment\\nfrom a multiple people saying that yes\\nit is very very effective and it has\\nhelped me a lot so first of all try to\\nunderstand a design first of all try to\\nunderstand architecture now if i'm going\\nto talk about this part two you will be\\nuh able to understand that here i have\\ndiscussed about one of the problem and a\\ndesign and the solution which\\nwe have created when i was working for\\nxyz company right so here you will be\\nable to see a conceptual architecture as\\nwell as you will be able to understand a\\ntechnical architecture which i have\\ndiscussed in a very very detailed way\\nyou will be able to understand a\\nbusiness problem and its respective\\nsolution and that is the reason so i had\\ncreated this dsar architecture classes\\nright through ysa data science\\narchitecture and resume discussion\\nclasses i have given for a community\\nlast year so for you it's a very first\\nthe very first thing that you are\\nsupposed to explore is this one dsar so\\nplease try to start from here please try\\nto understand that what you are supposed\\nto do and what you are not supposed to\\ndo\\nthis is fine so forget about technology\\nforget about everything right because ai\\nis nothing but it's a set of technology\\nby which you can solve a problem in\\nevery domain\\nwithout any exception without any\\noutlier you can solve we all can solve\\nit so please try to go through this one\\nnow coming back to this one\\nright so now let's suppose if i'm aware\\nabout right if i'm aware about the\\nentire things what i can do what i can't\\nright if i'm aware about the\\narchitecture i'm aware about the\\nsolution designing i'm aware about the\\nclient business problem and my team\\nright so each and every process is set\\nthen maybe we are supposed to look into\\na tech side now in some company if you\\nare going to join as a manager maybe\\nthese things are not required but yes in\\nsome company so it is required\\nespecially in product based company so\\nyes you must be having a business\\nunderstanding the understanding which i\\nwas talking about apart from this so you\\nare supposed to be aware about a\\ntechnology so it would be better if you\\ncan start again with the bottom up\\napproach so try to learn try to talk\\nabout first of all infrastructure how i\\nwill be able to utilize those\\ninfrastructure for like a ml solution\\nfor a python solution for a vision\\nsolution for a s device solution and all\\nsort of things so in this way if you are\\ngoing to learn\\noffside infrasight because that is going\\nto give you a broader view\\nand that is important for a senior\\npeople unless and until you are not\\ngoing to understand a broader view a\\nhigh label design you will not be able\\nto implement a low level design at all\\nbecause you are the person who will be\\nresponsible for designing this solution\\nyou will be the person who is going to\\nmake it or break it everything over here\\nin an organization you will be the one\\nbased on\\nbased on whose instruction entire team\\nis going to work so if you are the one\\nwho is not aware about these things how\\nyour team is going to work how your team\\nis going to deliver that is not possible\\nand that is the reason so i have asked\\nyou that try to follow a bottom up\\napproach try to look into this one and\\nthen you are supposed to explore this ai\\nops course\\nwhich is available over here ai ops\\ncourse is very very important for all of\\nyou especially for architect especially\\nfor seniors so after dsar you are\\nsupposed to get into these things these\\nentire things so that right so that you\\nwill be able to understand a complete\\ninfrasight in a detail and a depth way\\nbecause coding is fine so more or less\\nuh amount of coding is completely fine\\nif you are aware about something uh like\\na coding or maybe you're not that that\\nis a secondary thing but infra\\narchitecture design business clients\\nthese are the things which you should\\nfocus on in a very very first place and\\nthis is something that you are going to\\nmention inside your resume then\\nif i'll talk about a specialization\\nright so let's suppose if i'm aware\\nabout the offside if i'm aware about\\neach and everything then\\nas an overview again you are not\\nsupposed to go in depth i'll not suggest\\nyou so you should know almost everything\\nbut not\\nmastery into each and everything yeah\\nwith a timeline for sure you will be\\nable to get a mastery but as you are\\ntrying to make a transition and if you\\nwill invest your time\\nlike most of the time just to cover up a\\nbeginning maybe a python maybe a stats\\nmaybe ml algorithm in that case you will\\nfeel like a frustrated because as a\\nsenior people so we have a different\\ndifferent responsibility uh family\\nresponsibility or maybe in office so we\\nhave so many different different kind of\\nresponsibilities and it is going to\\ncreate a pain so just to remove that\\nparticular pain try to understand first\\na design architecture solution whether\\nit's possible whether it's not so once\\nyou will start talking about or you will\\nbe in a position to talk about a 10\\ndifferent different solutions right for\\nsure you can discuss about it in your\\ninterview and that is going to\\nlike uh that is going to be a point for\\nyou where you can crack an interview and\\nis in the easiest possible way right\\nthis is what i have seen from my\\nexperience right and i'm just trying to\\nshare my experience with all of you\\nthat's it\\nso here then you can come for the resume\\nbuilding and then slowly gradually so\\nyou can start exploring all of these\\nthings whether it's a language again not\\nin depth maybe as an overview maybe\\nmachine learning algorithms so maybe\\nlike a databases maybe like an analysis\\npart maybe a big data site so you can\\nstart exploring each and everything\\nafter that one so that is something\\nwhich is going to like give you a\\ncapability to make a transition again\\ni'm going to summarize each and\\neverything right so summary wise first\\nof all you are supposed to look into a\\nvery first thing called as dsar again\\nit's a lengthy session but i believe you\\nwill enjoy it for sure now the second\\nthing is you are supposed to look into\\nai ops the third part is you are\\nsupposed to look into a data science\\nmaster's program right data science\\nmaster's program so you can just look\\ninto it and just try to take an overview\\ni'm not saying that go in depth practice\\neach and everything solve all the\\nassignments solve all the problem and\\nthen go for the job no\\nit is going to create an headache for\\nyou so it's better move in this way so\\nthat you will be able to get a point to\\ndiscuss plus a broader overview and idea\\nand understanding you will be able to\\nget and then go for the 30 days\\ninterview preparation because there's 30\\ndays\\ninterview preparation so i have already\\ndiscussed about so many questions\\nhundreds and hundreds of questions\\nmulti-hundred questions is already\\ndiscussed even architecture and designs\\nare discussed over there so that is\\ngoing to help you out a lot\\nin a short term as well as in a long\\nterm i hope uh you are able to\\nunderstand that what you should do and\\nwhat you are not supposed to do right\\nand yes after that come for the resume\\ndiscussion come for a mock interview\\nuh come for the\\nassessments and we can we can tell you\\nover here that okay fine so here you are\\nlagging and here you are clear and you\\ncan you can move ahead this is my advice\\nand this is my data science roadmap for\\nsenior people with that thank you so\\nmuch see you again in my next lecture\\nthank you\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_6.txt'}, 'embedding': None, 'id': '8dbf45c8e5c635ff1d3773d0d4bc70ac'}>, <Document: {'content': \"hello everyone welcome back and today\\ni'm going to talk about a data\\nengineering roadmap let's suppose i'm a\\nperson and who is trying to get into a\\ndata engineering site or maybe i'm\\ntrying to join some organization i'm\\ntrying to make a transition into some of\\nthe organization as a big data engineer\\nso how i'm supposed to do a preparation\\nbecause there was a time when i was\\nlike uh going through a similar kind of\\na situation seven or eight years back so\\nwhen i was trying to do a transition\\nfrom sap to big data so because of this\\nlack of road map lack of resources i had\\nstruggled a lot so i thought of creating\\nthis particular video so that it is\\ngoing to help you out a lot\\nand for sure we all can do a transition\\ninto a data engineering site if and only\\nif we are going to work really really\\nhard for couple of months not for a year\\nnot for a decade just for a couple of\\nmonths so listen out entire stack\\ncarefully try to do one by one each and\\neverything that i'm going to discuss and\\nthen for sure you will be able to make a\\ntransition after your hard work so let's\\nsuppose if i'm trying to join some\\norganization as a data engineer or big\\ndata which is for sure a need you can go\\nto linkedin and you can try to check how\\nmany number of job opportunities are\\navailable and for sure it's very very\\nhigh it's in a lag so as a data engineer\\nor as a big data engineer there are so\\nmany opportunities which is available to\\nall of us so let's suppose if i'm trying\\nto get into a data engineering site so a\\nvery first thing which i will try to\\nfocus over here is i will try to\\nunderstand hadoop so if i'll talk about\\na number one so it's going to be\\nhadoop\\nhadoop is very very important it's a\\ndistributed file system which is called\\nas hdfs hadoop distributed file system\\nwhich has been derived from gdfs a\\ngoogle file system or google distributed\\nfile system gfs file system so hadoop\\nand\\nand a concept behind what is our meaning\\nof distributed computation what is the\\nmeaning of a parallel computation so all\\nof these things are very very important\\nso you are supposed to understand an\\narchitecture behind a hadoop system\\nunless and until you are not going to\\nunderstand this architecture behind a\\nhadoop system it is going to create an\\nissue in your interview so you are\\nsupposed to understand a complete\\ndistributed computation system\\narchitecture just an architecture try to\\nunderstand now so five or six years back\\nor seven years back when i was trying to\\ndo a preparation so at that point of a\\ntime even map reduce is import map\\nreduce was important but nowadays map\\nreduce is not important so you can just\\ntry to skip that particular part because\\nnowadays no one is using map reduce at\\nall so after hadoop so if i'll talk\\nabout seven year back so we used to\\nstudy a map reduce and we used to do a\\ncoding by using maybe a java maybe by\\nusing a scala but nowadays map reduce is\\nnot a relevant one because there are\\nsome powerful engine which is available\\nin a market which can work much much\\nbetter than a mapreduce and that too in\\na very optimized way so you are not\\nsupposed to touch map reduce again if\\ni'm talking about a hadoop distributed\\nsystem so maybe you can go ahead with a\\n2.x or 3 dot x so try to study these\\nsystem try to study a resource manager\\nlike a yarn or resource this kind of a\\nresource manager you are supposed to\\nunderstand try to understand how\\nreplication factor will be implemented\\ntry to understand again a multiple\\ncomponent which comes under hadoop\\nitself so what i'm trying to say over\\nhere is a complete hdfs file system you\\nare supposed to understand now when you\\nare trying to understand this hadoop\\nsystem a hadoop ecosystem\\nright at the same point of a time you\\ncan try to explore and you can try to do\\na practical implementations as well by\\nusing maybe a cloud data distributed\\nenvironment or maybe by using the hdp\\nhortonwork distributed platform so you\\ncan try to use cloud data or you can try\\nto use hdp hotend work nowadays now both\\nare same so you can try to use either of\\nthis and then you can try to each\\nimplement each and everything so in this\\nway for sure you will be able to get a\\npractical implementation of this entire\\nhdfs file system it's just a file system\\nso no need to worry about it but\\nat the same point of a time a second\\nthing which is important is a basic of\\nbasic of\\nlinux\\ncommand again this is important because\\nwhen you are going to deal with the sdfs\\nsystem at that point of a time so for\\nsure you are supposed to know so how to\\ncopy a file from one part to another\\npath so how to start one services how to\\nstop other services how to create a\\ndirectory and many more things is\\nimportant again i'm not asking you to\\nget into a depth of it if you are aware\\nabout uh like a detail of a linux system\\nand its command that's completely fine\\nbut at least a basic of a linux command\\nis very very important so just make sure\\nthat you know all of this basic command\\nat least 2025 number of commands and\\nthen it's okay okay to go ahead with a\\ndifferent different component which may\\ncome into our picture now a very third\\nthing which i would like to recommend\\nyou over here is a language\\nso language wise you can opt for\\nthree languages again you can opt for\\nmore but what i will advise you is to go\\nahead with either of these languages so\\nmaybe\\na java maybe\\nscala or maybe you can go ahead with\\npython many of us think that that java\\ncan be a complex one a complex to learn\\ncomplex to implement that's completely\\nfine scala may be a complex one complex\\nto learn complex to implement so in that\\ncase you can opt for a python because\\nnowadays whatever system that you are\\ngoing to use whatever services that you\\nare going to use you can and you will be\\nable to find out a python apis and\\npython library for each and everything\\nso just make sure language is a enabler\\nit is going to enable you to write a\\ncode it is going to enable you to design\\na system but nothing more than that so\\nyou are going to offer java scala or\\npython that's completely fine either of\\nthese things are going to work for you\\nand for sure you will be able to do an\\nimplementation but yes don't\\nmiss a language part because if you are\\ngoing to join any organization on a\\ntechnical side you can't miss the\\nlanguage part at all that is very very\\nimportant believe me so you are supposed\\nto understand and you should know a\\nlanguage path at least till a\\nintermediate label i'm not asking you to\\nget a mastery into any of the languages\\nbut at least till intermediate path you\\nare supposed to understand a language\\nside now a next thing which i would like\\nyou to understand\\nfor this data engineering part is\\na high\\nso again hive is a very important tool\\nfor a data warehousing which is an\\nintegrated part of your hadoop ecosystem\\nwhenever you are going to install\\nso like let's suppose if i'm using a\\ncloud database distributed environment\\nor maybe emr if i'm going to use or\\nmaybe i'm going to use hot and work so\\nin all of these environments you will be\\nable to find out hive which is installed\\nif you are going to do a custom\\ninstallation\\nso yes hive as a service you will be you\\nare supposed to install separately but\\nagain that's that's it's mentally and if\\na person is having an idea about a\\nsequel a basic of sql command so in that\\ncase\\nyou will be able to understand hql hive\\nquery language in an easiest possible\\nway again it is not going to give you\\nmuch pain because it's almost similar to\\nsql i'm not saying that it's exactly as\\nsequel but yes command wise syntax is\\nwise if you are unable to understand sql\\ni believe you can learn all of these\\nthings easily and again you will be able\\nto do a comparison as well in between\\nhive and a sequel so where i'm supposed\\nto use hive or where i'm supposed to use\\na sequel and easily you will be able to\\ndo an implementation so hype is very\\nvery important if i'm talking about a\\ndata engineering site because almost in\\nevery project we are using hive you\\nwon't be able to find out even a single\\nproject where we are not using hive or\\nits equivalence so everywhere it is been\\nused so just keep these things in our\\nmind that i should know hive now some\\npeople say that even pig latins are\\nrequired but i will not recommend you a\\nbig latin because in today's environment\\npig is not required so yes a seven year\\nback when i was trying to learn so pig\\nwas important believe me at that point\\nof a time so in every interview people\\nuse this to ask multiple commands from\\nthis side but nowadays don't worry no\\none is going to ask you any kind of a\\nquestion on a big site right even a\\ntheoretical or practical no one is going\\nto ask you at all so just chill out and\\nuh don't take attention about this big\\nthings now\\nuh if i know hype so for sure i will be\\nable to understand uh impala so again\\nimpala is almost equivalent to a hype\\nand it's not a big deal to learn impala\\nat all so it's equivalent to high and\\nhype is almost equivalent to a sequel\\nagain command-wise not architecture and\\ndesign-wise so it's been used for a\\ndifferent purposes for sure but yeah you\\nwill be able to understand in the\\neasiest possible way now the next thing\\nthat you are supposed to know is that\\nfifth number thing is uh maybe flume\\nscoop\\nor maybe\\nyou can try to learn knife\\nso plume scoop and nifi so all of these\\ntools are nothing but all of these\\nservices are nothing but which has been\\nis been used for doing a data transfer\\nfrom one system to another system so\\njust for transferring a data let's\\nsuppose i'm supposed to transfer data\\nfrom a sequel system to oop this hadoop\\nsystem so in that case maybe i can try\\nto go ahead with the scoop if i'm\\nlooking for a file transfer i can try to\\nlook for a flume if i'm looking for\\nevery kind of a transfer so i believe\\nnifi is a beautiful and powerful and\\nlight-weighted tool which is available\\nin a market and yes i will try to go\\nahead with a apache an i5 or maybe a\\nscoop or maybe a flume so all of these\\nthree tools are required these three\\ntechnologies are required on your\\nlike a data transfer site so data\\nwarehousing site hive is required\\nlanguage wise java scala or python\\neither of these\\nyou can pick and choose basic objects is\\nrequired hdfs system is required fluid\\nscoop and nifi is required on this side\\nagain if i'm talking about a database\\nsite\\nnosql database site so hbase is very\\nvery important database it's a very very\\nimportant believe me so you are supposed\\nto understand this edge base unless and\\nuntil you are not going to understand\\nhbase so it is going to create a lot of\\nproblem for you so make sure that you\\nshould know hbase it's been used on top\\nof hdfs file system and it is going to\\ngive you so much of advantages when you\\nwill try to draw or when you will try to\\nbuild a dashboard on top of your final\\ndata set or whenever you are dealing\\nwith a lot of financial data set and you\\nare trying to do a data search based on\\nthe indexes so hbase is again a very\\nvery important tool which is required or\\nit's a nosql based database which is\\nrequired for all of us\\nnow if i'll talk about maybe a seventh\\nstack over here which is a very very\\nimportant stack that you are supposed to\\nknow is called as s p\\na r k it's a in memory computation\\nenzyme a very powerful one\\nright a very powerful\\nin memory computation engine and that is\\na spark and a spark there are so many\\nadvantages you will be able to see you\\nwill be able to get with respect to a\\nspark so here you will be able to find\\nout maybe a spark and a spark sql spark\\nfor the streaming spark machine learning\\nlibraries spark even nlps are available\\nfrom a snowflake lab so again there are\\nmultiple variants of spark which you can\\ntry to find out for example so spark sql\\nor a spark ml lib you can try to find\\nout a spark\\nstreaming you can try to find out so you\\nare supposed to know each and everything\\nnow let's suppose if i'm aware about\\nspark and again a spark provides you a\\nflexibility to write a code by using a\\njava scala python or our programming\\nlanguages some of the apis are not\\navailable in our programming languages\\nbut yes languages are just a language\\nlike i said it's just an enabler it is\\ngoing to give you a capabilities to\\nmaybe replicate or build a system right\\nso it's completely fine doesn't matter\\nfor me which language i'm going to pick\\nand choose the concept is very very\\nimportant\\nand here a concept of a core spark\\nconcept of spark sql ml lib streaming\\nall of these things are very very\\nimportant now the eighth part which i\\nwould like to highlight over here is a\\nscheduler side so scenario wise you can\\ntry to use maybe a\\nug or you can try to use\\nairflow\\nso oozy is a xml based scheduler so by\\nwhich you will be able to schedule a job\\nat a particular time stamp and then it\\nis going to run a job in an automated\\nway and uh ooji is basically a xml based\\nxml based file system based scheduler\\nthat you can try to find out and any\\nkind of a job if whether it's a linux\\njob whether it's a spark related job\\nwhether it's a hype job whether it's a\\nflume job a scoop job\\nyou will be able to schedule with the\\nhelp of og and its alternative is an\\nairflow so airflow provides you a flavor\\nof python so here you can try to write\\nyour script not in an xml file but\\ninstead of writing a script in xml file\\nyou can write your like a complete\\nscheduler by using just a airflow by\\nusing a language python right so nothing\\ncan be more amazing than that so if i'm\\nworking with python i believe i will be\\nlike a choosing i will be going for like\\na airflow which is which is anyhow going\\nto my preferred choice so uj and airflow\\nthis is what you are supposed to know so\\nthese are the technology stack which is\\nvery very important it's a bare minimum\\ntechnology stack which is required now\\nif i'm going to talk about another stack\\nso yes uh for streaming purposes you\\nneed kafka kafka is again important\\nnowadays\\nkafka is very very important if you are\\nlooking for a streaming right and kafka\\nand spark integration and scheduling\\nthose things with the help of ooji and\\nairflow so nowadays we are almost using\\nthis kind of a stack everywhere in our\\nmarket in almost every project every big\\ndata engineering project so we are\\ntrying to use it so yes kafka is\\nimportant and again architecture of\\nkafka is very very important\\nimplementation creating a producer and\\nlike a consumer is very very important\\nand multiple consumers are very very\\nimportant and dealing with a streaming\\ndata set a batch data set or maybe a\\nmini batch data set is very very\\nimportant so all of these things you are\\nsupposed to know anyhow so this is a\\nbare minimum requirement guys\\nunless and until you are not aware about\\nall of this stack it is going to be\\ndifficult for you to get into a data\\nengineering site a bare minimum\\nrequirement believe me hadoop basic of\\nlinux any one language hive floom scoop\\nnifi either of this hbase spark inside\\nspark sequel mlm streaming uj airflow\\nfrom a scheduling site and kafka from a\\nstreaming site so all of these stacks\\nare important now let's suppose if i'm\\naware about all of these stacks then\\nwhat i will do is then i will go ahead\\nwith the resume a\\ndiscussion\\nso yes in your one year on platform\\nresume discussions is already available\\nso you can raise your request by using a\\none-year-old platform and you can come\\nfor the resume discussion but before\\ncoming for the resume discussion you are\\nsupposed to do our projects because\\ninside your resume what you are going to\\nwrite your experience and your projects\\nnow if i'll talk about a project so\\nalways try to build a pipeline based\\nproject so pipeline based project means\\nlet's suppose there is a data source so\\nfrom there how i am going to connect\\nfrom a multiple data sources how i am\\ngoing to filter out all of these\\nrequired informations how i am going to\\nperform a different different kind of\\ntransformation operation different\\ndifferent kind of a load operations and\\nthen finally how i'm going to dump my\\ndata set into my databasing solution or\\nmaybe into a databases so this kind of a\\npipelining project is required so where\\nin one single project you will end up\\nusing this entire things and project\\nalong with a deployment and scheduler is\\nvery very important right so i'm not\\ntalking about the op side but yes this\\nis the berman requirement that you are\\nsupposed to fulfill so you can come for\\nthe resume discussions you can do a\\nproject which is available inside your\\ndashboard and then yes you will be\\neligible for applying for a job in a\\nmarket which is available on a different\\ndifferent platforms so this is a basic\\nroadmap i would say\\nin my next data engineering roadmap so\\ni'm going to talk about a data\\nengineering roadmap\\nin\\ndepth so if you have like this video so\\nyou can give a comment and then you can\\nlet me know that\\nwhether this input is useful for all of\\nyou or not if not for sure so anyhow i\\ntold you that in my next video so again\\ni'm going to drill down each and\\neverything in on a micro label and i'm\\ngoing to explain you so why this\\ncomponent is required and why that\\ncomponent is not required so in this way\\ni'll try to give you explanation till\\nthen thank you so much and see you again\\nin my next session thank you so much\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data File_6.txt'}, 'embedding': None, 'id': '4b6fb9c853435449123616ca884019ef'}>, <Document: {'content': '\\nSUPPORT VECTOR (CONTINUE)\\nHello and welcome back to our discussion on Support Vector Machines.\\nSo, we were looking at the optimization problem corresponding to the optimal separating hyper\\nplane. So, to solve this problem, so with one of the techniques for solving these kinds\\nof constrained optimization problems is to set up a Lagrangian, which essentially looks\\nat the original objective function, which is half beta square and the second component\\ncorresponding to the constraints that we have. So, if you look at this quantity in the square\\nbrackets here, so you can see that this is the term on the left hand side of the inequality\\nand that is the term on the right hand side of the inequality and we really want to make\\nsure that, this difference is not negative. If this difference is negative, then that\\nwould mean that y i times x i transpose beta plus beta naught is actually less than 1.\\nSo, we do not want this to be negative, so what we essentially say is, this we added\\nhere as with a minus sign. So, this essentially means that, when I minimize\\nthis whole expression, so this term will become as large as possible, as largely positive\\nas possible. So, that essentially means that I will go and try and make this as larger\\nthan 1 as possible. So, this term here alpha i let us me control how much weight I want\\nto give to satisfying the constraints versus how much I really want to minimize the objective\\nfunction. So, we really need to satisfy the constraints as much as possible and since,\\nthere are solutions that will satisfy the constraint and give you a good optima.\\nSo, we should essentially be trying to derive this thing to as larger value as possible.\\nSo, this is called the primal of the problem and your goal is to minimize the primal. So,\\nI am going to do something fairly technical right now. So, if you do not understand all\\nof it in the first goal that is fine, you might have to do a little bit more reading\\non this side, but this is essentially give you an idea of how people go about solving\\nthese kinds of problems. So, we are going to try and create, what is\\ncalled the dual of this, the primal objective function. So, the dual is a way to create\\nsomething that create an optimization problem, that is simpler to solve in some sense than\\nthe primal and the dual at all points provides you some kind of a lower bound on the kind\\nof solutions that you can achieve with the primal and that the optima of the dual you\\nideally like the optima of the primal also to be achieved.\\nSo, we are going to create a problem called the dual, we are going to solve the dual and\\nwhen we reach the optima of the dual, you would like the optima of the primal to be\\nalso achieved. The same solution that gives you the optima in the dual problem should\\ngive you the optima and the primal problem and there are technical conditions under which\\nthis is satisfied and we are not going to go in to the technical conditions and this\\ngoing to be give you a flavor of kind of results that will be looking at.\\nSo, let us start by\\nsetting the derivative of L p to 0, derivative with respect to beta and beta naught. So,\\ntaking the derivative with respect to beta and setting it to 0 and solving for beta gives\\nme… So, you can figure there out by little bit of algebra here and likewise setting that\\nderivative with respect to beta naught to 0 and solving it gives me. So, you can substitute\\nthese back into the primal problem and do a lot of algebra, do a lot of algebra really\\nand then I can simplify this and I will get what is known as the dual, we write the dual\\nhere. So, this is just really obtained by substituting your beta into the expressions\\nhere and then, using the fact that alpha i y i will be 0 at the optimum.\\nSo, that is the thing, but then it is subject to be constrained. So, note that I said, so\\nyour dual is always going to give you a lower bound on the solution of the primal problem.\\nSo, really if you are minimizing the solution in your primal, it should be maximizing the\\nsolution in the dual, so that the two of them can coincide at some point. So, essentially\\nyou would be maximizing this subject to the constraint that, all your alpha i’s are\\ngreater than or equal to 0.\\nSo, if you think about it, this is the much easier constraint to wrap our heads around,\\nbecause it just says that you will only be doing it in the positive co ordinates and\\nwhile this had a more complex set of constraints. So, you kind of reduce the constraint to do\\nsomething easier and therefore, the dual problem is sometimes easier to solve. So, for the\\ndual and their primal to be at optima at the same time, so you really want them to satisfy\\na set of conditions, which are essentially to with the derivative of the primal problem.\\nSo, we required that this should hold, we required that this should hold, we write them\\nas 1, 2, 3. In addition, you required that this condition should also be required, that\\nthis condition should also be satisfied, these are called the KKT or the Karush Kuhn Tucker\\nconditions. And so far, the optimization problem to have the same solution, we require that\\nthe KKT conditions should be satisfied. So, once you have an optimal solution for\\nthe dual and the primal problem, because these KKT conditions have to be satisfied, you can\\nmake certain observations, especially we are working from condition 3 here. So, if alpha\\ni is greater than 0, so what does it mean. So, this has\\nto be equal to 0, then the term in the square bracket has to be equal to 0; that means,\\ny i into x i transpose beta plus beta naught should be 1. So, what does that mean?\\nIt means that, it is exactly on the edge of the margin when it is equal to 1, because\\nit is greater than equal to 1 is what we needed to satisfy, so when it is equal to 1; that\\nmeans, it is exactly on the margin. Likewise if so, if the quantity in the square bracket\\nis greater than 1, then alpha i has to be 0, but that essentially\\nmeans is, if your data point is something; that is far away from the hyper plane let\\nus just more than the margin away from the hyper plane, then the corresponding alpha\\nis will become 0. So, what does this mean for us, so if you\\nthink about it. So, the solution that we get, which is essentially beta that is the solution\\nthat we want to get is formed by taking the product of alpha i, y i and x i. So, if saying\\nthe alpha i is going to be 0, it essentially means that the corresponding x i has no role\\nto play in determining, what my beta should be if I say that it implies if x i is 0 that\\nimplies that x i has\\nno role in computing beta. So, which are the data points, which will\\nactually effect the solution beta here exactly those points for, which y i times x i transpose\\nbeta plus beta naught is 1; that means, these are exactly the points, which lie on the margin\\n. So, only these points will influence, how the solution beta looks like and all the other\\ndata points that we have, which are farther away from the separately high per plane, then\\nthese points do not matter in the solution. So, these points are called\\nsupport vectors. So, you don’t really have to solve this\\noptimization problem yourself there are enough tools that actually can do it for you the\\nwhole goal of this lecture is to get you to appreciate, what is said that you are doing\\nwhen you are using a support vector machine for solving a problem. So, at the end of the\\nday all we are going to do is fire up tool that is going to tell you, what is the separating\\nhyper plane given a bunch of data, But, it is good to have an appreciation of how the\\nclassifier is actually build. So, once you figure out the beta, then I can\\nsubstitute that I can substitute that into the KKT the third condition here and solve\\nfor beta naught. So, typically what you do is that you use every x i that is a support\\nvector and you substitute that here and then, try to solve for beta naught and typically\\nend of taking the average value of that. So, the couple of things, which I want to point\\nout about support vector machines. So, one thing is we should be very clear that\\nthe training data none of the training data will fall within the margin, but that it is\\nnot to say that the test data might fall might not fall within the margin the test data might\\nfall within the margin it might actually fall on the other side of the hyper plane. So,\\nfor all we know that the test data that could be errors on the test data it is just on the\\ntraining data it tries to fix something there is as far away as possible from the data points.\\nSo, the idea here is that, so if I give as much gap between the classes as possible,\\nthen the classifier would be more noise on either side. So, this is the assuming that\\nthe noise could be in this class or in this class if you know for sure that one class\\nis noisier than the other or if one class is more valuable than the other. So, you might\\nwant to actually modify your objective, so that the line does not go write in the middle,\\nbut it is goes to one side or the other. So, having said that under the assumptions\\nof the support vector machines if assumptions hold good, then is a very, very robust classifier.\\nSo, the reason is it pays attention only to the points that are closest to the class boundary.\\nSo, you know I can have as many data points here I say want I can have as many data points\\nhere I say want of the corresponding class it will be does not affect my classification,\\nbecause truly the once that are close the boundary are the once that need attention.\\nSo, that essentially makes support vector machines more robust and on the other hand\\nif you are going to have some kind of stochastic process that is generating the data right.\\nSo, if there are the few data points there are by chance or noise data points that actually\\nclose to the hyper plane that will affect the support vector machines tremendously.\\nAnd therefore, it will try to reduce the margin by a large extent while classifier that looks\\nat the entire data and tries to find the distribution for the entire data might be a little bit\\nmore robust to this kinds of noise. So, this is the, this is how you solve the basic optimization\\nproblem for support vector machines.\\n \\n \\n \\nSupport Vector Machines for Non Linearly Separable Data\\nNow, we look at the case where the data is not so-well behaved as you wanted to be. Specifically\\nthe data is not separable, right, is not linearly separable. So, you look at the non-separable\\ncase. So, I am going to introduce some additional data points whatever have looking at so far.\\nSo, this is a non-separable case, right, linearly non-separable case because some of my data\\npoints are really mixed up here, right. Now, still we will like to have large margin, but\\nnot only have I allowing data that is not separable, but I am also allowing data points\\nto fall within the margin. So, its essentially two sides of the same coin, right. So, if\\nI am allowing data to fall within the margin, so in some sense I am having the flexibility\\nto make some kind of errors as well.\\nSo, I essentially look at how far away am I from the margin, right in the long direction.\\nSo, I am going to denote these distances by which I am away from the margin by the symbol\\nzeta, right, but we still have same constraints here, but now going back to our optimization\\nproblem, but I am really looking at here is, I am going to modify my constraints such that,\\nso y i times x i times beta plus beta naught, right is really greater than or equal to 1\\nminus zeta i, but zeta i is some kind of slack variable that allows me to satisfy this constraint\\nwith some error in it. So, essentially if you look at this first data point that we\\ndrew here, if you look at the first data point I drew here, right. So, it has the slack of\\nzeta 1, right and the second data point as a slack of zeta 2, right and this one of the\\nreally fairly large slack, but I still it is possible under the circumstances. This\\nallows me to have a larger margin, right. So, if you think about it if I did not, even\\nif this data point was not there, if I did not allow these kinds of data point appear\\nin the margin, right. If I dint allow these things to appear in the margin, my classifier\\nwould actually have been here, right. My classifier would have been here trying to separate this\\nx n from this o, right and margin would have been very small. In all likelihood I am fitting\\na noise data point here which is not an optimal thing to do, right.\\nSo, now I am allowing some amount of data points to fall within the margin, right. I\\nam able to expand the size of the margin that I can have and I can also incorporate linearly\\nthe small number of errors that I have to make because the data is not linearly separable,\\nright, but then I donÕt want to this become arbitrarily large, right. I just cant say\\nthat hey it doesnÕt matter you can have slack variable for every data point that we have\\nand the slack variable can be as large as you wanted. I need to have a control on this\\nas well and therefore I try to minimize that, right. So, are these conditions sufficient\\nfor us to define a new problem now, but still one more condition that we need. So, we are\\nreally measuring zetas in one direction and not in the other direction, right. So, we\\nhave to be careful. So, I also need to add another set of constraints, let us say that\\nthe zeta have to be positive. So, that is the complete constraint optimization problem\\nfor us in the case where the data is not linearly separable.\\nSo, you have to minimize beta square, norm beta square plus the sum of the zetas that\\nwe are using subject to the condition that y i x i times for beta plus beta not is greater\\nthan 1 minus zeta i and zeta is greater than or equal to 0. So, what happens to our primal\\nobjective function now? So, what is that we need a component that corresponds to the actual\\nobjective function, and you need a component that corresponds to the constraints that we\\nare using, right. So, the first part of it remains as it is, right, but then we have\\nto add the newer components that we are bringing in, right. So, this is the second part of\\nthe objective function and this is the first constraint, right and this is the second constraint\\nand since these has to be applied to each and every data point, so you have the summation\\nover all the data point you have, likewise here.\\nSo, how do we go about deriving the dual in this case? So, just like we did earlier, so\\nstart differentiating the primal with respective of various parameters. So, you end up with\\nthe same condition that we had earlier, and here you end up with the same condition when\\nyou differentiate with respect to the zetas, you end up with so, one condition for each\\ni, right. So, alpha i equal to c minus mu i. So, I can put everything back into the\\nprimal, do some algebra and derive my dual which turns out look exactly like this, except\\nthat my alpha has to lie between 0 and c that you can actually see from that condition that\\nwe have there, right. alpha i y i have to be equal to 0. That we already have as a condition.\\nSo, the remainder of the KKT condition that we have, I am just going to go through this\\nvery quickly because I donÕt want to do the complete derivation here.\\nSo, the remainder of the KKT conditions will be, this is what we had last time except for\\nthe zeta i part, right, but you also have. It is essentially or initial constraint written\\nin a slightly different form, right so the third constraint here which is the original\\nconstraint with 1 minus zeta taking to the other side. So, what is that we notice from\\nhere? So, let us go back and let us do the same argument. If alpha i is greater than\\n0, then y i x i transpose beta, beta naught is actually less than 1 and only, then this\\nwill be zero because right. So, that would mean that for the particular choice of zeta\\ni. So, this goes to 0. That would mean that this is on the wrong side of the margin, right\\nor on the margin, right. If the zeta i is 0, then it will be on the margin and if the\\nzeta i is not 0, it will be within the margin. So, all the data points that are on one side\\nof the margin or all, right because again like last time, so beta depends only on those\\nvector for which alpha i is greater than 0, right.\\nSo, if the data happens to be on the right side of the margin, right then your alpha\\ni have to be 0 as we saw earlier. So, those data points have no role in computing. So,\\nthis is essentially kind of takes us over the entire optimal separating hyper plane\\npart, where it was linearly separable. So, we had a very easy solution, but when the\\ndata is not linearly separable, so we have to allow for the possibility of data points\\nto lie on the wrong side of the margin. So, when we do that, we can essentially take advantage\\nof the fact and try to push our margin away by allowing data point which are correctly\\nclassified, but are within the margin is small fraction of such data points are permitted\\nand therefore, we need to expand the margin a little bit. Therefore, we can come up with\\nthe solution. So, you donÕt have to, at this point here\\ndonÕt have to go in details of solving this optimization problem, but there are many powerful\\noptimization techniques it have been developed that allow you to solve these kinds of problems.\\nIn fact, SVM have let to the revival of popular class of optimization algorithms called interior\\npoint methods because these are very efficient in solving these kinds of optimization problems.\\nThey are pretty robust classifiers and are very widely used for wide variety of applications,\\nbut some of you will be probably thinking right now, but whenever I talk about a support\\nvector machines, people always tell me something about kernels, right. I thought to support\\nthat machine all about kernels and I have not talked about kernels at all at any point\\nhere, yes. So, the kernel idea is very crucial in support\\nvector machines, and I will be looking at that in more detail in the next module. What\\nyou should remember is the basic optimization problem that you are trying to solve with\\nsupport vector machine is the one of optimal separating hyper plane. So, in fact the kernel\\nidea is called the kernel trick because it allows you to solve really wide variety of\\nproblems which is not easily amenable to linear classification by using a very powerful idea,\\nbut then the under lying optimization problem that you are solving is still this, the same\\noptimization problem that on the boards so far in the last two modules.\\n ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw File_2.txt'}, 'embedding': None, 'id': '4409fb3213364bdb47e84e5e6896e94d'}>, <Document: {'content': 'Support Vector Machines and Kernel Transformations\\nSo far, we have been looking at the problem of the optimal separating hyper-plane in the\\nprevious two modules, but then the idea of Support Vector Machines is to be able to use\\nit in data which is not nearly linearly separable or only working in that space of linear hyper-planes,\\nright.\\nOne way of looking at extending this problem to complex settings is to think of taking\\nyour original data, right and transforming it into something else, and then trying to\\napply the same idea to the transformed function. This idea should not be new to you because\\nyou have always seen this in linear regression where you can look at transforming the input\\nvariables into some other kind of basis function, and then trying to do linear regression on\\nthat. So, the idea is similar to that, but we are going to make use of a very powerful\\ntechnique here, right. So, look at how predictor is going to work. So, we are going to have\\nf of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is\\nlesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will\\npredict it as being of class plus 1, right. As on this is, so this is separating hyper-plane\\nthat we have, fine. So, if you think about it, we said we have solution betas, right\\nor going to be of this form. Therefore, I can rewrite this, right. So, if you look at\\nit, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner\\nproduct of xi and similarly, if you look at how you are solving the optimization problem,\\nthis is what we wrote down last time. So, the duel is essentially going to have again\\nx i transpose x k, right. So, you can see that the xÕs appears in our problem always\\nas inner products, right and if somehow you are able to compute this inner products efficiently,\\nthen you should be able to solve the problem more efficiently. In particular, given that\\nyou are going to be looking at this kind of transformation, right. So, I can now write\\nthis as , where this denotes the inner products. This is what we can say inner product, right.\\nSo, likewise the dual also can be written as\\nSo, if you stop and think about it, it essentially tells you that I really donÕt need to know\\nh of x, right. If you have an efficient way of computing the inner product of the transformed\\nfunction, right, then you really donÕt need to know what the actual transformation itself\\nis, right. So, there is a class of function which I am going to call as kernel functions\\nhere. It is called as kernel function which allows you to compute this inner product efficiently.\\nSo, essentially it is going to say that right. So, the kernel corresponding to function h,\\nright when you give it as input x and x square, which is going to compute the inner product\\nof h of x and h of x square, right.\\nSo, one of the things that we require for a function to be a kernel function, right\\nis there it should be symmetric. So, k should be symmetric, positive-definite. So, if we\\ndo not really understand that, so k should be symmetric in the sense that if I give it\\nthe set of x and x prime x prime, so the kernel functions for x and x prime should be the\\nsame as x prime, x. The positive-definite essentially means that if I take any vector\\nx transpose K x, that should always be positive. So, there are technical reasons for why this\\ncondition should be satisfied. For one rough way to think about it to say that this essentially\\nyou would want this to be whole thing to be positive, so that your optimization problem\\nwill work as you wanted to. So, that is rough intuition behind why you\\nneed this condition. Some of the popular choices for the kernel functions are the polynomial\\nkernel. This is essentially 1 plus. I got the parameter d is something that we choose.\\nThe other one is Gaussian or the radial basis function. The other one is sometimes called\\nthe neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,\\nright? So, as I was mentioning earlier, you have a data there is given to in the original\\ndimensions, right. The data might be badly mixed up in that original dimension, it might\\nnot be easily separable at all in the original dimension, but then when you look at the transformed\\ndimension, then the data becomes linearly separable, right.\\nLet us take the example of the polynomial kernel and see what happens. So, I am going\\nto look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I\\nam assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these\\nare like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.\\nSo, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality\\nof the underlined space, but in this case I am assuming this has. So, if we take the\\nsquare of this, so I essentially end up with the expression that has six components to\\nit, right. So, if you think about it, this is somewhat\\nlike taking the inner product in a very higher dimensional space than the original space.\\nOriginally x and x prime were residing in a two-dimensional space, but if you look at\\nwhat is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,\\nh2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;\\nh4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine\\nthat I transformed my original two-dimensional representation into a six-dimensional representation\\nis essentially taking all the second order terms along with original terms.\\nNow, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with\\nthis expression, right. Inner product of h of x and h of x prime, right. So, the entire\\nsix-dimensional vector if I take the inner product, we are basically going to end up\\nwith this expression. So, what we have done here, we took the inner product in two-dimensional\\nspace, right and performed the squaring operation. So, this is going to give me number which\\nis equal to the number I will get by first transforming the data point from two-dimensions\\nis six-dimension and then, taking the inner product in the six-dimensional space. So,\\nessentially this allow to work in much higher dimensional space than originally intended,\\nbut by only looking at inner product computation in the original space, right.\\nSo, why this is a useful property to have in the case of support vector machine, is\\nthat all over operation here operate only within inner product whether we are finally\\ntrying to predict the output f of x, or when you are trying to solve the dual problem ld.\\nSo, all we really need to do is know what the inner product is, right. Now, I am able\\nto take the inner product in a higher dimensional space, but then do the computational only\\nin the lower dimensional space. This allows us to have a much greater advantage than simply\\noperating with the original dimension.\\nSo, to see how this polynomial transmission really helps us, let us look at a very simple\\nexample, right. I am going to assume that the single dimension, right and then I have\\ndata points, let us assume this is 0. I have data point that look like this, right. So,\\nthis is the dimension x1. So, obviously there is no single line that I can draw to separate\\nthese into two classes neatly, right. So, I can assume my slack variables and try to\\ndraw the line here that says ok I am not making too many errors bla bla, so on and so forth,\\nbut still that is not the satisfactory solution, but let us looks at what happens if I try\\nto plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square\\nas my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that\\nwill be x1 that is corresponding to this here and then looking at the square of that, but\\nthen looking at these data points, so this is going to go here, this will probably go\\nhere, right. Now, it is very clear that I can draw straight\\nline, right. I can draw if can find the linear decision boundary that separates these two\\nclasses once I have done the appropriate transformation. So, this essentially allows us to solve a\\nlarger class of problems. You see the kinds of basis transformation, then we could do\\njust by operating in the original space and trying to solve the linear optimal hyper-plane\\nproblem. So, this is essentially what all your choices of kernel functions or all about,\\nright. So, if you look at any SVM tool, they will tell you to pick one kernel function\\nwhich is either the polynomial kernel. So, in which case you have to pick in the appropriate\\nd or you have to pick radial basis function or Gaussian kernel, in which case you have\\nto pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or\\nwe can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa\\n2 which are the parameters that define how quickly this sigmoid function rises. We will\\nsee more about that when you look at neural networks.\\nSo apart from this, you still have one further parameter that we will have to worry about\\nwhich is this constant c, right. So, this tells you how much slack that you are willing\\nto tolerate, right. So, if you think about it, if c is very large, if c is infinite,\\nthen we have to be in the completely separable case because even a very small value of zeta\\nwill cost this objective function to become very large, right. Even for a small value\\nof zeta, right so you will find that this thing is actually very bad, right. c has to\\nbe if c is very large, then zeta has to be very small, right and if c is small, then\\nzeta can be large. So, what this is going to tell you is that in the higher dimensional\\nspace, where you are assuming that the higher the dimensionality that you are projecting\\ninto, more likely is the data will be separated, right because if c is large, right because\\nyou are going to try in fit more complex surface, right. c will look, the surface will look\\nvery vigil, right and if c is small, then you will really get much smoother surface,\\nbut the possibility of you making errors is also higher. So, thatÕs trade of that you\\nhave to figure out empirically by looking at how you are doing the, how you are performing\\nin the actual data. This brings us to the end of the module on\\nSupport Vector Machines and these are very powerful classifiers and quite often they\\nare the first classifiers of choice for people when they are trying to solve new problem\\nwhich they really donÕt know much. So, one thing I should point out that people have\\ncome up with different kinds of kernel functions. So, I have given you three choices of kernels\\nhere. These are essentially the most popularly used kernel choices, especially if you are\\noperating with text data, you would like to use linear kernel d is 1 and in most other\\nforms, you will be looking at RBF kernels, but then for special forms of data like graphs\\nand strings and so on and so forth, people have defined their own kernels and as long\\nas they are satisfying your properties of the kernel function, you can define your own\\nkernels and then have them help you solve the problem, right. That is topics for another\\nday or perhaps for another course. So, we will stop here. Just let me reframe that SVM\\nare one of the most popular and powerful classifier that are currently being used widely.', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw File_2.txt'}, 'embedding': None, 'id': 'dc83740895a08dc6bb7b36c25ba628ea'}>, <Document: {'content': ' \\n \\n \\n \\nEnsemble Methods and Random Forests\\n \\n \\nSo far, we have been looking at the problem of the optimal separating hyper-plane in the\\nprevious two modules, but then the idea of Support Vector Machines is to be able to use\\nit in data which is not nearly linearly separable or only working in that space of linear hyper-planes,\\nright.\\nOne way of looking at extending this problem to complex settings is to think of taking\\nyour original data, right and transforming it into something else, and then trying to\\napply the same idea to the transformed function. This idea should not be new to you because\\nyou have always seen this in linear regression where you can look at transforming the input\\nvariables into some other kind of basis function, and then trying to do linear regression on\\nthat. So, the idea is similar to that, but we are going to make use of a very powerful\\ntechnique here, right. So, look at how predictor is going to work. So, we are going to have\\nf of x equal x transpose beta plus beta naught. So, that is your predictor and if f of x is\\nlesser than 0, I will predict it as class minus 1. If f of x is greater than 0, I will\\npredict it as being of class plus 1, right. As on this is, so this is separating hyper-plane\\nthat we have, fine. So, if you think about it, we said we have solution betas, right\\nor going to be of this form. Therefore, I can rewrite this, right. So, if you look at\\nit, interestingly so x, all the xÕs here appear as x transpose x i. This is the inner\\nproduct of xi and similarly, if you look at how you are solving the optimization problem,\\nthis is what we wrote down last time. So, the duel is essentially going to have again\\nx i transpose x k, right. So, you can see that the xÕs appears in our problem always\\nas inner products, right and if somehow you are able to compute this inner products efficiently,\\nthen you should be able to solve the problem more efficiently. In particular, given that\\nyou are going to be looking at this kind of transformation, right. So, I can now write\\nthis as , where this denotes the inner products. This is what we can say inner product, right.\\nSo, likewise the dual also can be written as\\nSo, if you stop and think about it, it essentially tells you that I really donÕt need to know\\nh of x, right. If you have an efficient way of computing the inner product of the transformed\\nfunction, right, then you really donÕt need to know what the actual transformation itself\\nis, right. So, there is a class of function which I am going to call as kernel functions\\nhere. It is called as kernel function which allows you to compute this inner product efficiently.\\nSo, essentially it is going to say that right. So, the kernel corresponding to function h,\\nright when you give it as input x and x square, which is going to compute the inner product\\nof h of x and h of x square, right.\\nSo, one of the things that we require for a function to be a kernel function, right\\nis there it should be symmetric. So, k should be symmetric, positive-definite. So, if we\\ndo not really understand that, so k should be symmetric in the sense that if I give it\\nthe set of x and x prime x prime, so the kernel functions for x and x prime should be the\\nsame as x prime, x. The positive-definite essentially means that if I take any vector\\nx transpose K x, that should always be positive. So, there are technical reasons for why this\\ncondition should be satisfied. For one rough way to think about it to say that this essentially\\nyou would want this to be whole thing to be positive, so that your optimization problem\\nwill work as you wanted to. So, that is rough intuition behind why you\\nneed this condition. Some of the popular choices for the kernel functions are the polynomial\\nkernel. This is essentially 1 plus. I got the parameter d is something that we choose.\\nThe other one is Gaussian or the radial basis function. The other one is sometimes called\\nthe neural network kernel or the sigmoidal kernel. So, what do these kernels buy you,\\nright? So, as I was mentioning earlier, you have a data there is given to in the original\\ndimensions, right. The data might be badly mixed up in that original dimension, it might\\nnot be easily separable at all in the original dimension, but then when you look at the transformed\\ndimension, then the data becomes linearly separable, right.\\nLet us take the example of the polynomial kernel and see what happens. So, I am going\\nto look at the polynomial kernel of dimension two, right. It is essentially 1 plus So, I\\nam assuming that vector x consists of x 1, x 2 and x prime consists of, right. So, these\\nare like two-dimensional vectors and on which I am defining two-dimensional polynomial kernel.\\nSo, this is dimensionality of the kernel doesnÕt necessarily have to match the dimensionality\\nof the underlined space, but in this case I am assuming this has. So, if we take the\\nsquare of this, so I essentially end up with the expression that has six components to\\nit, right. So, if you think about it, this is somewhat\\nlike taking the inner product in a very higher dimensional space than the original space.\\nOriginally x and x prime were residing in a two-dimensional space, but if you look at\\nwhat is happening now, it is essentially something like this. So, h of x is 1, h 1 of x is 1,\\nh2 of x root 2 of x, the first coordinate; h3 of x is root of x 2, second coordinate;\\nh4 of x with the x1 square, h5 of x2 square, h 6 of x root 2 into x1. So, if you imagine\\nthat I transformed my original two-dimensional representation into a six-dimensional representation\\nis essentially taking all the second order terms along with original terms.\\nNow, if I take the inner product of h1 of x and h2 x prime, I will exactly end up with\\nthis expression, right. Inner product of h of x and h of x prime, right. So, the entire\\nsix-dimensional vector if I take the inner product, we are basically going to end up\\nwith this expression. So, what we have done here, we took the inner product in two-dimensional\\nspace, right and performed the squaring operation. So, this is going to give me number which\\nis equal to the number I will get by first transforming the data point from two-dimensions\\nis six-dimension and then, taking the inner product in the six-dimensional space. So,\\nessentially this allow to work in much higher dimensional space than originally intended,\\nbut by only looking at inner product computation in the original space, right.\\nSo, why this is a useful property to have in the case of support vector machine, is\\nthat all over operation here operate only within inner product whether we are finally\\ntrying to predict the output f of x, or when you are trying to solve the dual problem ld.\\nSo, all we really need to do is know what the inner product is, right. Now, I am able\\nto take the inner product in a higher dimensional space, but then do the computational only\\nin the lower dimensional space. This allows us to have a much greater advantage than simply\\noperating with the original dimension.\\nSo, to see how this polynomial transmission really helps us, let us look at a very simple\\nexample, right. I am going to assume that the single dimension, right and then I have\\ndata points, let us assume this is 0. I have data point that look like this, right. So,\\nthis is the dimension x1. So, obviously there is no single line that I can draw to separate\\nthese into two classes neatly, right. So, I can assume my slack variables and try to\\ndraw the line here that says ok I am not making too many errors bla bla, so on and so forth,\\nbut still that is not the satisfactory solution, but let us looks at what happens if I try\\nto plot this data in two-dimensional plane, where I have x1 as one of my axis and x1 square\\nas my other axis. So, two 0Õs will probably get mapped to somewhere here, right. So, that\\nwill be x1 that is corresponding to this here and then looking at the square of that, but\\nthen looking at these data points, so this is going to go here, this will probably go\\nhere, right. Now, it is very clear that I can draw straight\\nline, right. I can draw if can find the linear decision boundary that separates these two\\nclasses once I have done the appropriate transformation. So, this essentially allows us to solve a\\nlarger class of problems. You see the kinds of basis transformation, then we could do\\njust by operating in the original space and trying to solve the linear optimal hyper-plane\\nproblem. So, this is essentially what all your choices of kernel functions or all about,\\nright. So, if you look at any SVM tool, they will tell you to pick one kernel function\\nwhich is either the polynomial kernel. So, in which case you have to pick in the appropriate\\nd or you have to pick radial basis function or Gaussian kernel, in which case you have\\nto pick in appropriate gamma that tells you how fast the Gaussian is going to decay, or\\nwe can pick sigmoid or artificial neural network kernel where you have to pick kappa 1, kappa\\n2 which are the parameters that define how quickly this sigmoid function rises. We will\\nsee more about that when you look at neural networks.\\nSo apart from this, you still have one further parameter that we will have to worry about\\nwhich is this constant c, right. So, this tells you how much slack that you are willing\\nto tolerate, right. So, if you think about it, if c is very large, if c is infinite,\\nthen we have to be in the completely separable case because even a very small value of zeta\\nwill cost this objective function to become very large, right. Even for a small value\\nof zeta, right so you will find that this thing is actually very bad, right. c has to\\nbe if c is very large, then zeta has to be very small, right and if c is small, then\\nzeta can be large. So, what this is going to tell you is that in the higher dimensional\\nspace, where you are assuming that the higher the dimensionality that you are projecting\\ninto, more likely is the data will be separated, right because if c is large, right because\\nyou are going to try in fit more complex surface, right. c will look, the surface will look\\nvery vigil, right and if c is small, then you will really get much smoother surface,\\nbut the possibility of you making errors is also higher. So, thatÕs trade of that you\\nhave to figure out empirically by looking at how you are doing the, how you are performing\\nin the actual data. This brings us to the end of the module on\\nSupport Vector Machines and these are very powerful classifiers and quite often they\\nare the first classifiers of choice for people when they are trying to solve new problem\\nwhich they really donÕt know much. So, one thing I should point out that people have\\ncome up with different kinds of kernel functions. So, I have given you three choices of kernels\\nhere. These are essentially the most popularly used kernel choices, especially if you are\\noperating with text data, you would like to use linear kernel d is 1 and in most other\\nforms, you will be looking at RBF kernels, but then for special forms of data like graphs\\nand strings and so on and so forth, people have defined their own kernels and as long\\nas they are satisfying your properties of the kernel function, you can define your own\\nkernels and then have them help you solve the problem, right. That is topics for another\\nday or perhaps for another course. So, we will stop here. Just let me reframe that SVM\\nare one of the most popular and powerful classifier that are currently being used widely.\\nThank you.\\n ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw File_2.txt'}, 'embedding': None, 'id': 'e05d383f4d431b04ca8846c3f5aa9a77'}>, <Document: {'content': \"Artificial Neural Networks\\nHello and welcome to this module on Artificial Neural Networks.\\nSo, artificial neural networks, are computing models inspired by biology. So, we have neural\\nnetwork architectures that have been proposed for a variety of different analytics tasks\\nlike regression, classification, clustering, feature extraction, etc. So, these architectures\\nessentially are networks of simple computing entities and so this is like a very simple\\nthreshold entities that are connected together in the specific network architecture that\\ngive rise to complex computing functionality. Now, oscillate there has been significant\\nresurgence in interest in artificial neural networks, especially under the domain of T\\nnetworks about which we will see in one of the later modules. So, for this module and\\nthe discussion about artificial neural networks is concerned in this course, we will look\\nat only the classification task and many of the ideas we talk about here for classification\\nare generalizable to regression, like for while for the other kinds of analytics task\\nwe need different architectures and, but we are not going to cover that in this course.\\nSo, the inspiration comes from biological neuron. So, let us not worry about the complete\\ncomplex structure of a neuron, what we really have to focus here is on the input and the\\noutput. So, the neuron receive inputs from the dendrites or from the dendrite branches\\nfrom other neurons and when the input signals is above a certain threshold, it is going\\nto produce an output, that is going to be transmitted via the synopses to neurons that\\nare further down the line.\\nSo, these connections to the dendrites and synopses are going to be result in a very\\ncomplex network and even though, the computing done by each element is very, very simple\\nsummation and thresholding. The sum total of this taken across the entire network can\\ngive rise to daily complex computations, which we will see.\\nSo, the completing unit is something that is very simple. So, it is going to take a\\nset of inputs x 1 to x n and it is going to compute some functional arm, it is function\\nis very simply incredible and then it will produce an output. So, we will look at what\\nthis function is going to be in detail in the next few slides.\\nSo, the initial model for this for a biological neuron was proposed by McCulloch-Pitts in\\n1943 it is called the McCulloch-Pitts unit, it is only binary signals, so 0s and 1s. So,\\neither an input is active, then these cases are represented by 1 or if it is not active,\\nin this case it is represented by 0 and the nodes also produced only binary results. So,\\nthe outputs could be either 0s or 1. So, the edges between these different nodes were directed,\\nunweighted, they could be of two types that could be excitatory or inhibitory and again\\nI can mentioned earlier, the transfer binary signals.\\nSo, what is the computation that happens here? So, I let assume that the McCulloch-Pitts\\nunit gets inputs x 1 to x n through n excitatory edges. So, these are positive edges and inputs\\ny 1 to y m through inhibitory edges. So; that means, these are edges that could produce\\nthe depression in the function or could actually stop the functioning of the neuron. So, the\\nassumption that was made is, if m is greater than or equal to 1 that is at least one inhibitory\\nedge and if any one of the inhibitory edges is 1, so if there is a one inhibitory input\\nthen the unit as a whole does not produce any output, regardless of what the inputs\\nx 1 to x n are. If none of the inhibitory inputs are 1 or\\nif there are no inhibitory inputs at all, the unit computes the summation of x 1 to\\nx n, let us call it x and if x is greater than the threshold that is specified for each\\nunit, if it is greater than the threshold theta then the result of the computation is\\n1 as the result is 0. So, it is very simple, so essentially you can think of it as adding\\nup all the inputs that come to the neuron and if the summation is greater than our threshold\\ntheta, your output 1; otherwise, your output 0. So, the inhibitory edges in some sense\\nhere acting act as a gating signal. So, if it is 1, the output is always 0, if the inhibitory\\nis only 0 then the output is the result of the computation.\\nSo, it is essentially the McCulloch-Pitts unit, it is implementing just threshold function.\\nIf the input is below theta you are going to see a output of 0, if the input is above\\ntheta you are going to see a output of 1, that this is essentially a step function.\\nSo, what kind of computations can you do with this? So, you can actually do almost all your\\nfamiliar Boolean operations with the McCulloch-Pitts neuron. So, you can think of doing an AND\\noperation, you have two inputs x 1 and x 2 and the threshold is set a 2. So, if only\\nboth x 1 and x 2 are one, so it will be greater than or equal to the threshold and therefore,\\nthe output will be 1 and for implement in a OR you can set the threshold that one. So,\\nif either x 1 or x 2 is 1 to the output will be 1 after complimenting a NOT unit it can\\nimplement the NOT unit by having x 1 act as an inhibitory input. So, this circle here\\nindicates an inhibitory input. So, if x 1 is 1; that means, they neuron and\\ninhibitory output will be 0 on the other hand x 1 is 0 then the output will be whatever\\naccording to the result of the computation. But, we can see here that the threshold for\\nthis neuron set as 0 and that is for the output will be always 1 as long as there is no inhibitory\\ninput. So, if x 1 is 1 then the output will be 0, x 1 is 0 output will be 1, that how\\nwe have implemented NOT function. Now, once we unable to implement this kinds of AND,\\nOR and NOT then you know that you can connect neurons together and then implement any Boolean\\nfunction that we want and is this what really we are interested in.\\nSo, we are not really interested in that because we want to be able to do more complex classification\\nproblems, then we would like learn simple things like linear surfaces or more complex\\nsurfaces that is separate two classes. So, that is has been the goal of classification\\nwe have looked at so far. So, in 1957 rosenblatt proposed a very simple extension to the McCulloch-Pitts\\nmodel which we called the perceptron, the more crucial thing what the perceptron is\\nthat a it introduced weights at the inputs, crucial differences from the perceptron from\\nthe McCulloch-Pitts module is that the perceptron introduced weights at the input.\\nAnd then it the output could be either a one or a minus one depending on whether the weighted\\nsum of the inputs is greater than threshold that one that is the computing unit with a\\nthreshold theta So just repeating it. So, the output of the neuron is 1 if the weighted\\nsum of the inputs is greater than or equal to theta is equal to minus 1 other wise.\\nSo, what is the goal here in perceptron learning, when perceptron learning we are essentially\\ntrying to learn a hyper plane, trying to learn a separating surface as we have done in the\\npast in the other classification problems, we are trying to learn the separating surface\\nthat can separate one class from the other. So, what would the classes be in our case,\\nclasses in our case would be plus 1 and minus 1. So, this essentially means if w i x i is\\ngreater than equal to theta, it essentially defines the equation of a hyper plane as we\\nhave seen in the previous modules. So, if this you can take the theta to the\\nother side. So, we like w i x i minus theta is greater than or equal to 0. So, we have\\nseen that was greater than 0 to some one side of the hyper plane if it is lesser than 0\\nit is on other side of the hyper plane and we are going to say that data points to one\\nside of the hyper plane belong to class 1 data points other side of the hyper plane\\nbelongs to class minus 1. So, now, the question is given a set of training data that gives\\nyou the x x the vector x and the decided output y.\\nHow would we find these weights w i's such that the perceptron is actually implementing\\nthat hyper plane, implementing the right separating hyper plane. So, the weighted all this is\\nfollows, you start of the randomly initializing the weights to some value and then we look\\nat the prediction that is made by the way. So, the prediction that is made by the current\\nsetting of the weights, let us call it o and the target is the two class of the data point\\nx. So, with this, it will be plus 1 or minus 1 and likewise o is also plus 1 or minus 1.\\nSo, your goal is to make sure that here perceptron output matches the target value.\\nSo, the perceptron training algorithm has a very simple rule. So, at every presentation\\nof an input point, we change the weights by an amount that is proportional to difference\\nbetween the target value and the actual output produce times that the input on the particular\\nit. So, w i changes by an amount that is proportional to t minus o times x i. So, eta here is a\\nsmall constant may be 0.1 or 0.01 as called the learning rate.\\nSo, one thing to note here if for a particular input x I will produce the correct output.\\nSo, the class is minus 1 and I produce minus 1, the class is plus 1 and I produce plus\\n1. This expression evaluates to 0. You can see that this expression evaluates to 0 and\\ntherefore no changes in the weights will happen. So, essentially what happens here is you change\\nthe weights only whenever you make a mistake and that to you change the weight proportional\\nto the input variable. So, if x i is say a small value say 0.1 or 0.2 then will be changes\\nin the weight will be small and as for as the poster when x i is the large value let\\nus say 1 or 0.95 and things like that then the change it be next will be large.\\nSo, this essentially because the larger the input variable the more important it is going\\nto be in the production of the output at least the way we are set up this perceptron. So,\\nthat is essentially the simple training rule. So, whenever you make a mistake, you take\\nthe vector for which we have made a mistake add some small fraction of that vector to\\nthe weights.\\nSo, this looks like a very simple rule, but then back in 50's this perceptronÕs created\\na lot of human cried the people saw that the perceptronÕs by the able to learn from scratch\\ntrying to solve something which are considered hard learning problems and then they used\\nthe perceptronÕs they were able to solve that, so much so you can see here the hype\\nwas that they are going to build the computer that expects to be able to walk, talk, see,\\nwrite, weight reduce itself and be conscious of it is existence, such the significant amount\\nof hype and it is always hard to live up to any height that this proportionate and to\\nthe actual effect that was achieve that point.\\nSo, let us take a look let us just back and take look at what can of perceptron learning\\nis a news paper article really true or what are the limits to the perceptronÕs learning\\nability. So, here is a very simple perceptron here, so it has a two input variable x 1 and\\nx 2 and that is the threshold of 1 and the weight w 1 is 0.9 and w 2's 2. So, if you\\nlook at it essentially it implements this straight line here, so everything above the\\nstraight line this light color regions belong to one class and the dark color regions belong\\nto another class. So, we know that these are data which are\\nlinearly separable; you saw this in the case with SVM's. So, these are data that are separated\\nby a linear hyper plane or the linear separating surface. So, all data points for which the\\nw transpose x evaluate, so greater than x is 1 will get a class of plus 1 all those\\nthat evaluate to lesser than 1 and get a class of minus 1.\\nSo, again let us go back and look at the simple logic function that we saw earlier. So, it\\ncan implement that OR. So, essentially OR requires you to have a hyper plane and this\\npassing here. So, everything to this side this become plus 1 everything to this side\\nbecomes minus 1 and likewise you can implement and so you can draw a simple hyper plane.\\nSo, everything to this side become plus 1 and everything this side becomes minus 1 or\\n0, I mean depending on how you wanted to predict the output.\\nAnd let us look at another one, look at simple problem just like OR and AND the XOR problem.\\nSo, Minsky and Papert in 1969 in a famous monograph called the perceptrons showed that\\nwell a simple problem like XOR. So, where the truth table is given here is the inputs\\nof the same output is 0, if the inputs are differently output of 1, the simple problem\\nlike XOR is not linearly separable, you cannot draw a hyper plane that separates these two\\nclasses. So, forget about walking, forget about talking\\nand doing all those wonderful things that was claimed to news paper article perceptronÕs\\ncannot even solve this as simple problem as XOR is essentially says that two things are\\nsame, the output is 0, two things are different the output are 1 that we cannot recognize\\nthe similarity between this simple inputs like 0's and 1's what kind it do to complex\\ncomputations. So, once Minsky and Papert showed this, it is a kind of you dampened the research\\ninto neural networks for a long time until there was revival much later.\\nSo, perceptronÕs can learn only linear decision boundaries that is the take away message here.\\nSo, that is make that is whole idea of neural networks completely useless, because they\\ncan learn only linear decision boundaries in case of SVM's we saw that we could get\\nit to do all linear boundaries by going into Kernel expansion this has something similar\\nthat we can do here.\\nLet us look at how we can change the representations and try to do something more clever. So, if\\nyou look at the original problem the XOR problem, so I have my inputs x 1 and I have my input\\nx 2 and now we can see that in this space the problem is not separable. But, let us\\nlook to do a simple transformation on my data points, so instead of looking at x 1 I will\\ndefine my first variable as NOT x 1 and x 2 and similarly I will define my second variable\\nas x 1 and NOT x 2. So, if you think about it, so we can now plugging\\ndifferent values of x 1 and x 2 here and see what the outputs will be and then you can\\nsee that when x 1 is 0 and x 2 is 0, the output is going to be 0, when x 1 is 1 and x 2 is\\n0. So, the output here will be x 1 is 1 and x 2 is 0, the output here will again be 0\\nand x 1 is 1 and x 2 0 the out here will be 1. And we know that 0 1 the output has to\\nbe 1, so that we get it here and likewise for the symmetric case this will be the output\\nand so you can see that this is again going to be 1 and when x 2 x 1 x 2 both are 1 again\\nthe output will be 0 0 and therefore, this is the resulting point.\\nNow; obviously, this representation the data points are linearly separable. So, now, the\\ntask becomes one of finding the right representation, such that the data becomes linearly separable\\nfor the next level, next stage of computation. So, people realized this very quickly, so\\neven though a single perceptron cannot solve complex problems like XOR which are not linearly\\nseparable, he could actually stack layers of neural neurons and then have the first\\nlayer compute something that is simple. So, you can always compute NOT of x 1 we saw\\nthat earlier and also can be computed by a single neuron. So, you can this get have layers\\nof neuron that exactly compute your features and of NOT x 1 comma x 2 and then have another\\nneuron, which takes the output of this neurons combine same together and produces the output\\nthat you want. So, people very quickly realize that stacking this kinds of neurons into layers\\nallows you to do more complex computation. In fact, it is easy to show that stacking\\nthese neurons into layers actually builds a universal function representation that learns\\nto a represent any Boolean function, you see a combination of neurons. So, what is a problem,\\nnow we know how to solve this more complex problems, why did the research in neural networks\\npick up again.\\nSo, the question here is when I start connecting all of these neurons into layers. How do I\\nfind the weights? So, perceptron learning algorithm might no longer work in this case\\nactually does not work in this case and people were struggling to come up with the mechanism\\nfor training all these weights. So, you can see that the way of started putting these\\nthings into layer. So, that is one input layer and one output layer, so there is one input\\nlayer, there is one output layer and in between this you could have many layers of neurons,\\nthey are typically called hidden layers because you do not observe their outputs directly.\\nSo, now, we have this many, many hidden layers of weights and it is little hard to find out\\nwhat this weight should be and so in the mid 80's around 83 an algorithm was proposed called\\nback propagation which allow you to learn the weights of this and we solve this hidden\\nlayers.\\nSo, for the rest of the presentation, we will be looking at the standard three layer network.\\nSo, there is an input layer x 1 to x d and the output layer which will denote by f of\\nx and one hidden layer of neurons. So, these take the inputs from the input layer do the\\nweighted sum do your thresh holding function and then produce an output and then the neuron\\nand output layer will take all this outputs of the hidden layers take their weighted sum\\nand take the threshold or not and that produce the output, instead of using hard threshold\\nwe use a kind of a soft threshold like in order to do this competitions this is needed,\\nso that you can derive more efficient training algorithms later.\\nSo, the output of a hidden units and it given by g of the bias term, this is the theta that\\nwe had earlier. So, instead of theta so it is going to call it b 1 and plus w 1 times\\nx and the output of this will be note by h of x and the output of the final layer of\\nneurons is given by some function o of b 2 plus w 2 times h of x. And so now, the goal\\nhere is to figure out what this w 1 and w 2 are going to be. So, this is called the\\nthree layer network, even though there are only two sets of weights that we have to learn.\\nSo, the layers here talk about the neurons here, so we use for each input variable we\\nare same that there is separate neuron that is activating the hidden units. So, this is\\ncalled the standard three layer network structure.\\nSo, what are the different activation functions you can use? So, we already looked at one\\nwhich is the threshold function, we can also have just a linear activation function that\\nbasically takes the summation of weighted summation of all the inputs and outputs as\\nit is. We can also look at the sigmoidal function, sigmoid logistic function which takes the\\nsummation input and then squashes the input. So, that it remains between 0 and 1 and then\\nthere is a steep raised somewhere around the threshold. So, that it transitions rapidly\\nfrom 0 to 1. When if you are interested in having signed\\noutputs then you can think of using a hyperbolic tangent, where the outputs are going to taxation\\nbetween minus 1 and plus 1 and again around the threshold. So, there are parameters at\\ncontrol where the threshold would be and how steep the price would be. So, another transition\\nfunction some time gives is this squashing function, which is 0 before the threshold\\nand one at a certain distance higher than the threshold and in between you have a...\\nSo, linear approximation adds to the step function, this called the squashing function.\\nSo, typically in most of the neural network architectures that we look at will be looking\\nat either the hyperbolic tangent or this, the logistic sigmoid or the linear activation,\\nbecause these are different shape and this allows as to derive efficient training algorithms\\nfor the same. So, if you are doing a classification problem then the output layer could be the\\nhyperbolic or a logistic sigmoid and if your solving a regression problem, the output neuron\\ncould be a in linear neuron. So, that you can do appropriate regression fit.\\nThe hidden layer almost always has to be a non-linear function and where the little bit\\nof what you can show that if the hidden units have a linear activation, like you might as\\nfor not have them at all. And what is the function that is implemented is something\\nwhich can be as well implemented by a single layer of neurons. And the next module we look\\nat how you the exactly find out these weights given the assumption that they are working\\nwith the sigmoidal logistic function. So, the function for the sigmoidal logistic\\nthing is given by f of net is 1 by 1 minus e to the power of minus net. So, that is the\\nfunction and look at, given that this is the activation function how we are going to derive\\nthe weights of the two layer standard three layer neural network. So, that is in the next\\nclass.\\nArtificial Neural Networks(cont\\\\'d)\\nHello and welcome to the module on back propagation or how you are going to train artificial neural\\nnetworks and determine the weights.\\nSo, to keep things simple, let us start off with a single neuron and that is going to\\nhave a logistic sigmoid as the output function. So, you are going to write here f hat of x\\nis equal to b plus w transpose x and then, you pass that through your sigmoid function\\no. So, o in this case would be the logistic sigmoid, where we will say o of v is 1 by\\n1 plus e power minus v. So, we saw this in the last module, so the error measure that\\nwe will be using is the squared error. So, the excepted error of the parameters w\\ngoing to be half times, the excepted error w is going to be summation i equal 1 to n,\\nwhere n is the number of training data points that you have of the squared error for each\\ntraining data point. So, the way we are going to use this for changing the weights is essentially\\nto compute the gradient of the error. So, essentially that will be the error times the\\nderivative of the output function times minus x i.\\nIt is essentially taking the derivative of this with respect to the w function. Once,\\nyou have computed the gradient of the error with respect to the weights, then we essentially\\njust change the weights in the direction opposite to the eta times the gradient of this weights,\\nwhere eta is the essentially the step says parameter. So, this was fine when you have\\na single neuron. So, what about the case when you have layered networks like this?\\nSo, this is our standard three layer neural network. So, the first layer is essentially\\njust the inputs. So, the second layer of neurons takes in the inputs and computes the output,\\nthese are called the hidden neuron that is we discussed in the last module and then the\\noutput layers finally, take the output from the hidden units, again do the appropriate\\ntransformation and give you the final outputs. So, here we will denote the hidden layer outputs\\nby h and h is given us g of the weighted summation of the inputs and let me introduce the temporary\\nvariable here called t, which essentially is the summation of the outputs of the hidden\\nunits and f is finally, the transformation o of the outputs of the hidden units.\\nSo, one thing to note here is that, so I have used different functions g and o for the hidden\\nlayer and the output layer and typically for two class classification problems both g and\\no are logistic sigmoid as we saw earlier. So, g as well as o would be of the same form\\n1 by 1 plus e power v, but then if you are having a regression problem you can essentially\\nuse the same setup that we have here, except that o would be linear for the regression\\nproblems, so in which case o would be essentially just passing on the inputs that it is getting.\\nSo, suppose I have this multiple layers, how do I go about finding the weights of this\\nstandard three layer output? So, in some sense finding training rule for w 2 is not very\\nhard. So, w 2 if you think about it, it is just like a single neuron network. So, I can\\njust take all the weights that come to f 1 and then, essentially use the same rule that\\nI used earlier here for a single neuron. I can use the same rule for training f 1, except\\nthat instead of x i I will be using the output h, so that is clear. So, for finding w 2 I\\nreally can just stick with what I did earlier.\\nSo, I am going to write that here again just for clarity sake. So, I have rewritten this\\nin an appropriate fraction for working with this multi layer networks. So, I am going\\nto look at the gradient of the error with respect to a single weight that runs from\\nthe m'th neuron here to the k'th output neuron, this weight runs from the m'th hidden neuron\\nto the k'th output neuron. So, this is essentially the k'th output minus the prediction given\\nby the neural network. So, that is essentially our error times the derivative of the output\\nfunction of the k'th neuron with respect to the input that it receives times the input\\nthat is coming on the m'th line, which will be essentially h m i.\\nSo, if you think about it that is exactly the update that we had here except that I\\nhave change the notation to apply to the two net, the second layer weights in the three\\nlayer network. So, now, the interesting partÉ So, this is fine, so this we can just get\\nfrom the single neuron update. So, what we do about the first layer weights? So, let\\nus see how you will do this, this essentially uses the very simple idea of chain rule from\\ndifferentiation, I am going to take a very specific weight here. So, I would like to\\nfind the derivative of the error with respect to the first layer weight that runs from the\\nl'th input neuron to the m'th hidden neuron. So, it runs from some l'th neuron to the mth\\nneuron, so I am just looking at this one weight here, we are trying to find out what is the\\ngradient of the error at the output with respect to that one weight. So, I can rewrite this\\nas follows, so the derivative of the error with respect to the first layer weight is\\nessentially the derivative of the error with respect to the output of the m'th neuron times\\nthe derivative of the m'th neuron with respect to the weight.\\nSo, this makes sense, because the weight to the m'th neuron affects the output only via\\nthe output of the m'th neuron, so it affects the overall output of the network only via\\nthe output of the m'th neuron. So, I can essentially apply the chain rule here. So, let us take\\nthis bit by bit. So, if you look at this, so you can see that this is immediately obvious,\\nbecause this is the function we are talking about here. So, if you take the derivative\\nof this with respect to any specific w here, so we will essentially getÉ\\nSo, if we take the derivative of h m i with respect to w m l, then you essentially going\\nto get the derivative of g times the derivative of this expression with respect w m l which\\nwill just be x l i. So, this is the second term in the derivative. So, the first term\\nin this derivative is the one that requires a little bit more work, so let us see how\\nwe will do that. So, I am going to try and evaluate that expression here, so if you think\\nabout it, so the different ways in which the output of the m'th neuron can influence the\\noverall error is essentially through each one of the output neuron that m'th hidden\\nneuron connects to. So, it can influence the error through this\\noutput or it can influence error through this output. So, we are essentially summing over\\nall possible outputs k of the gradient of the error with respect to f k and the gradient\\nof f k with respect to the output of the hidden neuron. So, you can easily evaluate the derivative,\\nso the derivative of the error with respect to f k. So, if think about it, so this is\\nthe expression we have. So, the derivative of the error with respect\\nto f k, so going from this expression derivative of the error with respect f k going to be...\\nSo, negative of y i minus f hat k x i, because we are taking derivative with respect to f\\nk here. So, we do not have to worry about the further terms that constitute f k. The\\nsecond term is concerned that is essentially looking at this. So, derivative of f k with\\nrespect to h m is essentially the derivative of o with respect to h times the derivative\\nof the argument of o with respect to h which gives us just the weight w k m complex expression\\nfor this. So, before I put these things together to\\nmake things a little simple let me introduce the small notational thing. So, that it makes\\nlie for little easier for us, so I am going to say delta and I am going to say delta k\\nfor the input i this essentially the error term times the derivative of the last layers\\noutput function and also define this term row m for the input i as the essentially the\\nderivative of g with respect to w times the summation of w k m into delta k of i.\\nSo, putting these together now we can write our expressions more compactly essentially\\nwe can say that dou. So, the error of the derivative of the error with respect to the\\noutput layer weights, with respect the w to weights is essentially delta k i times h m\\ni, so likewise the derivative of the error with respect to the first layer weights, the\\nweights from the input layer to the hidden layer is given by row m i times x l i.\\nSo, if you think about it, so this part corresponds to this and that comes from here and x l i\\nis what is left out here. So, this expression look pretty compact and now we essentially\\nhave... So, you essentially update the parameters by w k m is just change by accept the gradient\\nhere and w m l is again change by the gradient that we are computed here. So, one thing I\\nwant to point out here that if you are function, the function g or your function o happens\\nto be a sigmoid, then g dash is\\nequal to essentially g of a into 1 minus g of e.\\nAnd suppose you are using a tanh function, because you want here outputs to run from\\nminus 1 to plus 1, suppose then g dash it is 1 minus g v squared essentially\\nthis gives you the... So, you can see that the sigmoid functions have a very convenient\\nform for the derivatives. So, one thing that we have to be careful about here is the fact\\nthat these are gradient following methods and therefore, and there is a good chance\\nthat will get stuck in local optima and there are many techniques for getting out of these\\nlocal optima, but we are not discuss too many of them there one of the simplest one is of\\ncourse, to try this with multiple starting stage, starting points for your weights and\\ntaking the other set of weights that gives you the best possible result at the end of\\nsome kind of experimentation. So, that brings us to the end of this module\\non training your neural network using back propagation. So, but this form of training\\nhas it is own draw backs will see what is that in the next module.\\nEnglish - NPTEL Official\\nDeep Learning\\nSo, in the previous module we saw how we can use back propagation in order to find the\\nweights of a neural network; upper three layered neural network. So, one of the earning success\\nstories of back propagation was learning to recognize hand written digits in an address;\\nright. So, this is work done by Yann LeCun back in 89, and essentially they trained a\\nslightly more complex network architecture call the convolutional neural network; in\\norder to recognize hand written digits. You can see the variation in the digits, that\\ntheir network manage to handle; the fairly well.\\nSo, another early success story of neural networks was the Backgammon Player built by\\nGammon Tesauro from IBM, T. J. Watson labs in 1992. So, he built 2 versions of this backgammon\\nplayer one called the Neurogammon which came in 89, which was essentially neural network\\ntrend using back propagation in supervise learning manner; in order to play a game of\\nbackgammon. So, it is like Ludo of people know about it. So, you throw a dice, you throw\\na dice and depending on that die roll; you move your coins around. So, that is a white\\nand black side. So, the idea is to move all your coins of the board by repeatedly rolling\\nthe dice. So, the TD-Gammon player essentially used the 3 layer, standard 3 layer neural\\narchitecture and was trying using a specific form of what is called reinforcement learning.\\nWe look at reinforcement learning in the later module, but he used reinforcement learning\\nin order to generate the error signals and, but then use back propagation to train the\\nweights of the hidden unit. And you manage to build Backgammon player which was able\\nto beat human Backgammon champions in game play.\\nSo, this was some of the early success and lot of interests was being spent on looking\\nat neural networks of solving variety of different problems. But then again people discovered\\na 2nd drawback with neural networks. So, the one thing was that It was incredibly hard\\nto train a neural network, because you had so many parameters you had to treat them appropriately.\\nSo, that they desired to deserved for obtained.\\nFurther there was this problem called the vanishing gradient problem. So, people quickly\\nfigure out that if we have many layers in the neural network; right. It was easier for\\nthe network to represent more complex functions; even though 2 layer network or a 3 layer network\\nand depending on the how we contact. The standard 3 layer network was the universal approximator,\\nhaving more layers allowed more complex representation to be build. But see, number of layers in\\nthe neural network increases, the gradient that we compute by doing back propagation\\nare becomes vanishingly small; right. And since there are not enough feedback for the\\nearly layers in the network. The rates in the lower layer remains random wherever universalize\\nthem ; right. And so, you are not able to learn any deep networks; right. So, the networks,\\nneural networks it will learn necessarily had to be shallow; because only a few 3 layers\\ncould be, 3 or 4 layers could be trained meaningfully using back propagation. So, people had come\\nup with different tricks for training deeper networks.\\nSo, and this discovery of these tricks for training deeper networks, is essentially what\\nis revive the interest in the field. And now we can build networks at have 8 layers, 10\\nlayers and so on so forth. And this produce some fantastic performances in problems that\\nwe are talk to be very hard to solve for AI and machine learning. And so, there has been\\nrevived interested looking at neural networks. So, I will talk about one specific mechanism\\nfor training multiple layers in the network, and then you can I mean if you are interested\\nin this follow it up with other material.\\nSo, this is proposed in mid 2000 by Hinton 2006 and independently by Ashwa, Bengio and\\nothers. In 2007 it is more like a greedy unsupervised layer wise pre training, so what we mean by\\nthis? So, you start off with very simple network architecture called an auto encoder. So, you\\nhave an input layer and you want to produce the same input at the output. So, the input\\nlayer and the output layer need to be identical, but in between the connections will go through\\na smaller hidden layer of neurons. So, the idea here is that the smaller layer is going\\nto learn some kind of a encoding or some kind of a reduce representation of the input; that\\nis sufficient for you to produce the output that you are looking for.\\nSo, you could train this using back prop or other slightly more advanced gradient techniques.\\nSo, if you think about it, there is no real supervision that is required here; because\\nas soon as have the input; right. The output is just the same input. So, I am going to\\nset it through a smaller hidden layer. So, that I am learning a reduce representation\\nof the input. So, once I have this 1st level of reduce representation;\\nright, I am going to iteratively deepen the network. So, what I do? So, once I have the\\n1st hidden layer of representation, the first hidden layer of representation I am going\\nto add another auto encoder network on top of it. So, this network takes the hidden layer\\nrepresentation for the input and tries to produce the same hidden layer representation\\nat the output, but in the middle layer is going to have fewer neurons. So, in the effect\\nI am taking this larger input and reducing it to a smaller hidden representation here.\\nAnd I can repeat this; right. So, once I have trained this, I remove the outer layer; and\\nthen take on another layer of auto encoders. So now, you can see that my training has gone\\nseveral layers deep. So, instead of just looking at one layer deep auto encoder which have\\nI was able to train using back prop efficiently. So, I am essentially using the same construction\\nagain and again. So, at any point the training happens only on a one layer auto encoder,\\nbut then because of this iterative deepening. Now I have actually taken this input representation\\nand progressively reduced it to a much smaller representation at the hidden layer. So, this\\nkind of an observation that you can use this layer wise pre training of the data, led to\\nresurgence in lot of deep architectures. So, why do we call this pre training? So, far\\nI am not talked about any kind of classification task that you have performing. we are just\\ntrying to find features in the input space. So, once I have found these features in the\\ninput space then I can take this, and I can tack on top of it.\\nOn top of this features that I have learned I can tack on other neural network. And now\\nI have my full fledged deep auto encoder and then this hidden layer representation can\\nnow be used as an input any learning task. So, so this part is call the fine tuning part\\nand the layer wise training part is called the pre training part; where I find the hidden\\nrepresentation. And after that I can add it to any complex neural network, that can do\\nmy classification task or my regression task whatever it is that I am looking at. So, this\\nkind of layer wise pre training allowed people to, drive more complex compressed representations.\\nThat very useful in a variety of problem solving and I use.\\nSo, this again revived a lot of hype in neural networks or in deep networks as they are called.\\nSo you can see very human similarities to the news items that I showed you from the\\n1959 thing. So, a stimulated brain and you can teach context to computers, machines at\\nlearn without humans, etcetera, etcetera. So, skate what you read in news paper with\\na pinch of solve, but again there is significant renewed interest in deep networks and neural\\nnetworks as there was in the 50s.\\nBut the overall feeling in the community is that, deep learning is come to stay; because\\nthere are lot of nice properties about this generation of neural networks as compare to\\nthe earlier generations. So, the things of more stable and things of reproducible; even\\nthough significant computing power is needed. So, most of the successful applications of\\ndeep learning that you see, would have had significant amounts of computational power.\\nBut then the computational power is also cheap; and you are able to solve really complex problems\\nusing neural networks.\\nAnd so, deep learning collects have yield at state of the art performance in the variety\\nof domains, like image classification, object detection, language modeling, machine translation,\\nspeech recognition, image description. These are problem at traditionally considered very\\nhard for machine learning algorithms and deep learning seems to have significant in pattern\\nthese areas.\\nSo, some of the images I used here are taken from the neural networks book by Raul Rojas.\\nAnd so, I like to show you few demos of deep networks and action law. So, here is one algorithm\\nfrom company called clarify. So, given an image with different kinds of textual labels\\nit says that, here given this image is say that is coffee, at there is croissant, there\\nis a beverage in it, and this is probably breakfast and it is had in the morning and\\noverall hey this looks like food. So, it is able to derive all of this tags just by looking\\nat this image. These are all similar images it does manage to retrieve images, similar\\nto these. You can see that all of these are images of breakfast or continental breakfast\\nand all of these have a cup of coffee in there. And so, even though the variety of this is\\nable to cover is truly expounding. So, like wise look at these picture here,\\nis able to figure out the here suspension bridge here and there is a river here; even\\nthough the river is not explicitly visible. And that is it night because it is lighted\\nup and it looks like bridge in the middle of the city. And likewise it has found similar\\nimages, which are all suspense bridges in cities on top of rivers.\\nAnd. So, likewise it is able to work on variety of outdoor seems, as well as indoor seems\\nand it’s able to perform really well. And this is state of the art in terms of image\\nunderstanding and labeling at correctly.\\nAnd likewise you can look at how well it works in machine translation. So, the both the image\\ntagging and the machine translation tasks use some much more complex neural network\\narchitecture that we have seen so far. But then so, here it’s a simple example from\\nhere. So, I typed in the sentence what a wonderful idea in English and then it gives me the equivalent\\nin French. And not only does it do that, it tells me which words correspond to which word\\nin re translated language. Which words in the original English language, correspond\\nto which word of the translated language. So, I do not know French. So, I am not sure\\nthat is a good translation, so let us not do this. So, let us stop with the previous…\\nSo, it is then this thing actually you can learn translate, can do translations between\\nmultiple language is not just in English and French. It first trained on the data from\\nUnited Nations and European parliament. So, it can do translation between the all the\\nEuropean languages. So, that brings us to the end of this module on deep learning.\\nThank you.\\nAssociative Rule Mining\\nHello and welcome to this module on Association Rule Mining or Frequent Pattern Mining, which\\nis essentially the basic problem underline association rule mining.\\nSo, we had a very brief look at association rule mining with very beginning of the machine\\nlearning section.\\nSo, the idea behind association rule mining is to first mine frequent patterns that occur\\nin the data and based on the frequent patterns that you have mined, derive association rules\\nwhich is to form at if A happens, then B is likely to happen.\\nSo, basically is like a conditional dependence relation that you are mining if A happens,\\nthat makes B more likely to happen.\\nYou could find such patterns in sequences looking at time series data, like financial\\ndata or looking at fault analysis, where one thing causes fault to occur; or you can look\\nat in the transactional data column context which is where it was originally proposed\\nand that is what we will look at in more detail in the rest of the module.\\nAnd more interestingly you could also look at mining frequent patterns and associations\\nin graphs, which is appropriately used in social network analysis.\\nSo, let us look at a mining transaction for the rest of the module.\\nSo, transaction is a collection of items that were bought together.\\nThat is the simple definition that we will use for the purposes of association rule mining.\\nAnd please note that the set or subset of items, is usually a denoted item set in the\\nassociation rule mining community.\\nSo, the goal here is to find first find frequent item sets, then you would say that an item\\nset A implies item set B; for example, if you could say that somebody buys milk, then\\nthey are likely to buy bread, if both A and the event A union B or frequent item sets.\\nThat would mean that both A, sorry which is milk in this case and A union B which is bread\\nand milk both should be frequent.\\nIn which case, I can say that, if you buy milk then, you buy bread as well.\\nLet us take a look at exists simple example here.\\nSo, here is a set of transactions, so I have 5 transactions and each color here denotes\\na different kind of item.\\nSo, the first 3 transactions are 4 item sets, the 4th one is the 3 item set, and the 5th\\none is a 2 item sets.\\nAnd let us assume that we have a frequency threshold of 3.\\nSo, we essentially have the following as frequent 1 item sets.\\nSo, we have blue, which occurs in all the 5 transactions and then purple which occurs\\nin 4 transactions.\\nAnd pink which occurs, big item which occurs in 3 transactions.\\nSo, none of the other items occur in 3 or more transactions.\\nSo, the frequent one item sets are just these.\\nSo, the next thing you have to look at is the frequent 2 item sets, in the frequent\\n2 item sets you can see or essentially purple and blue which occur in 4 transactions and\\npink and blue which occur in 3 transactions and so, none of the other combinations are\\nfrequent.\\nSo, the only are the things which we really have to look at this purple and pink; and\\npurple and pink occur only in 2 of the transactions together therefore, they are not frequent.\\nSo, the goal here is to first find such frequent item sets.\\nAnd from these frequent item sets, how do we determine which are interesting association\\nrules.\\nSo, the 2 measures of interestingness for association rules or essentially support.\\nSo, the support of a rule is the percentage of item sets that contain A union B; right.\\nSo, in this case and then the confidence of a rule is the other measure that we are interested\\nin.\\nSo, we will go back and look at the data set once we have understood what support and confidences.\\nSo, the confidence of a rule is a percentage of item sets containing A, that also contained\\nA union B. So, essentially this tells you how confident you are in making the association.\\nSo, typically we look for rules with both high support and confidence.\\nIf you think about it, if both there in the case of support and confidence we really need\\nto find the frequency of the item sets; right.\\nSo, once we determine what are the frequent item sets and what their frequencies are,\\nthen we can easily determine what are the relevant association rules.\\nSo, more effort needs to be focused on counting rather than the association rule itself.\\nSo, that is why I said there is frequent pattern mining part of this more interesting than\\nthe association rule mining part.\\nSo, let us go back and look at the pattern set we have mined so far.\\nSo, if you remember the one item sets are in really interesting except to establish\\nthe frequency part of it.\\nSo, let us look at a rule which says that purple implies blue.\\nIf you have bought purple then you likely to buy blue.\\nSo, if purple occurs in your transaction then blue is likely to occur.\\nSo, if you think about it, this is the valid rule to have because both purple as well as\\npurple and blue were frequent.\\nIn the earlier slide we saw that both purple and purple and blue are frequency, so it satisfies\\nthe A, A union B rule and, what about the support of this rule.\\nThis support is essentially all the transactions the number of transactions in which both A\\nand B occurs.\\nSo, that where A union B occurs divided by the total number of transactions.\\nSo, A union B occurs in 4 fifth of this data set; and therefore, the support of this rule is 4 \\n5.\\nWhat about the confidence of the rule?\\nThe confidence of the rule is, whenever A occurs; how many times A union B is occur.\\nAnd in this case it turns out that whenever A occurs, A union B occurs; therefore, its\\nconfidence is 1.\\nSo, likewise for the pink implies blue rule, you can see that the support is 3 by 5; because\\nthat is how many times a pink union blue occurs.\\nAnd the confidence is 1; because whenever pink occurs, pink union blue also occurs.\\nSo, what about this rule?\\nI mean is this the valid rule to look at?\\nBoth the blue is frequent and blue and purple is also frequent.\\nSo, that is a valid rule to look at, but is that the better rule then the one that we\\nsaw earlier.\\nThat is not necessarily in the case because the support here is 4 by 5 and the confidence\\nis 4 by 5 as well.\\nSo, the earlier rule which is purple implies blue had a higher confidence than this rule.\\nBut both of these are possible rules that you could consider, because both the rules\\nhave a high support and a high confidence.\\nSo, this is essentially the idea behind the association rule mining.\\nSo, it has been applied to variety of applications, Market Basket analysis is one.\\nSo, as I had mentioned earlier.\\nSo, market basket refers to the fact that when you go to super market, you are going\\nto buy a set of items together and put them together in your basket.\\nSo, essentially looking at what goes into the basket at the market.\\nSo, these baskets are considered as transactions, and you look at frequent pattern mining in\\nthese transactions.\\nYou could look at co-occurrence of words and that can be used to derive certain kinds of\\ntopic relationships.\\nAnd people are looked at plagiarism detection in terms of frequent pattern mining.\\nNow people have applied this to a biological data looking at bio markers and genes of proteins\\nversus diseases.\\nSo, try to find out associations between co-occurrence of a certain protein, abnormalities, and diseases.\\nYou also looked at in the contrast of time series, where co-occurrence of events can\\nbe used to model trigger events and this identifies trigger events.\\nSo, that brings us to the end of the first module on frequent pattern mining and the\\nsubsequent module will look at techniques for efficiently mining frequent patterns.\\nAssociation Rule Mining (contd)\\nHello and welcome to the 2nd module of Association Rule Mining. In this module we look at a very\\npopular algorithm that is used for association rule mining. As I said earlier the main problem\\nin mining association rules is counting part. So, typically how you would do, it is that\\nyou generate candidate frequent item sets then, count how many times they occur. And\\nthen you prune the candidates based on the count. So, if their count is above certain\\nfrequency then, you are going to call them as frequent item sets and if they are below\\na certain frequency you are going to reject that.\\nBut then, the problem with doing such an approach is that you have a combinatorial number of\\ncandidates. So, think about it. So, we have like 10 different items from which your transaction\\ncan be drawn; and that is say you are considering 2 item sets. So, that essentially gives you\\n10 choose 2 candidates item sets; now this is a small number. Think about real applications\\nwhere these candidate sets can be in millions. So, somebody like Amazon or a Flipkart is\\ngreat are like millions of candidates. And how would you even generate candidate item\\nsets some more than is very small size of candidates. So, really need a clever way of\\ngenerating fewer candidates. So, the very first approach for looking at generating fewer\\ncandidates came about late 90s which is the apriori property.\\nSo, the Apriori property is very clever observations and just says all nonempty subsets of a frequent\\nitem set must also be frequent. And even if one of the subset is not frequent, then I\\ndo not have to consider the larger item set. You do not even have to count the larger item\\nsets. So, the first table that propose this Apriori\\nproperty and came up with the fast algorithms for mining association rules was purposed\\nin 1994 and it is being a very similar algorithm. First they introduced the problem of finding\\nfrequent item sets and data base of transaction. It is said the tone for the entire field.\\nIn fact, this use of the term mining association rules, almost was the reason for the whole\\nsub field of data mining. And it since then there have been numerous improvements it have\\nbeen propose on top of Apriori algorithm, that allowed to main really large data sets\\nscale up to will be Billions and so forth. But still Apriori algorithm swap we have to\\nstart; alright explanation into association rules. So, in this module I will talk about\\nthe Apriori algorithm and give you more like introduction by an example and so, I will\\nleave it you to look up other algorithms if you are interested in association rules.\\nSo, here we will start up with a very simple example. So, look at very small transnational\\ndata base. So, it has got only 4 transaction learning, so each of these have different\\nid. And there are total of 5 different items which could be figuring in these transactions.\\nSo, we are going to call the A through E. So, let us look at the set of 1 item sets.\\nSo, we have A through E and this complete frequency of these item sets. Let us suppose\\nthat I have a minimum threshold for the support of 2. So, I have 4 transactions and item set\\ncan be called frequently appears in a least 2 of these 4 transactions. So, we can immediately\\nsee that item set D is not frequent. So, not only is the 1 item set D is not frequent,\\nany larger item set that contains D as element in it can also not to be frequent.\\nSo, when I am looking at the candidates, I have to use for generating 2 item sets I can\\ncompletely ignore D. And now, all the candidate 2 item sets or those that do not have E as,\\nI mean do not have D as part of it. So, essentially the 2 item sets that will have to consider\\nare: AB, AC, AE, AC and B E and C E. So, these are essentially generated by looking at all\\npossible combinations of the frequent 1 item sets.\\nNow, that we have these candidates 2 item sets. So, we are now on the second scan through\\nthe data then, we count them frequency of the 2 item sets. Now, we can see that A, B\\nand A, E on actually not frequent among these and therefore, we can thrown those away. So,\\nwe have a list of frequent 2 item sets, which compares AC, BC, BE and CE. Is it clear so\\nfar? So, we had we started out of with all the 1 item sets, these are the candidates\\n1 item sets. Counter that frequency we left out the 1 item sets, that was not frequent.\\nAnd then from the frequent 1 item sets, we generated a set of candidate 2 item sets which\\ncould be frequent. Then, we counted the frequency of these 2 item sets and from there we have\\nproven the way the 2 that were not frequent. So, these are the frequent 2 item sets. From\\nthese we can generate candidate 3 item sets. So, if you think about it. So, the candidate\\n3 item sets could be ABC, ABE, and BCE. So, can ABC be a candidate. It cannot be a\\ncandidate because the sub set AB is no longer, is not frequent. That sense the sub set AB\\nnot frequent, ABC cannot be a candidate item set; and likewise can we look at BCE. Can\\nBCE be a candidate frequent item set? Yes, because, BC is frequent, BE frequent, CE is\\nfrequent. And what about ABE? Again that cannot be a candidate item set because AA, AE is\\nnot a frequent item set. Essentially we have only 1 candidate 3 item set. And when we count\\nthe frequency of the 3 item set and then we find that exactly frequent. And now we have\\nthe complete set of all frequent item sets. So, one 3 items sets which is BCE, four 2\\nitem sets and then four 1 item sets which are all frequent. You do not have look for\\nlarger item sets, they both potentially possible because we do have 5 elements, 5 week events.\\nBut, we do not have to look for any further frequent item set because that is only one\\nfrequent 3 item set. So, the Apriori property allows us to proven\\nthe number of candidate item sets that we will have to generate. So, but still there\\nis a problem in that. So, every time we generate candidate sets of larger size, we essentially\\ndo another scan over the data. So, the more recent algorithm tries to minimize the number\\nof scans, you have to perform over the entire data sets; because that is very expensive\\noperation.\\nThis is a big challenge but, there are one other Caveat which I want to point out. So,\\nscaling up to large data set is a big challenge, but there is a one other caveat which I want\\nto point out. So, high confidence is not always a good idea. So, we will have to be really\\ncareful about what we mean by high confidence. So, I can say that somebody buys games, implies\\nthey buy videos with confidence of 66 percent; the support of 37 percent. So, this is something\\nthat was actually observed in a real data set from video rental company, which also\\ngoing selling video games with their shops. Because this, so from the real data they are\\nfound that somebody buys games, hence they will also buy videos with the confidence of\\n66 percent and support of 37 percent. So, essentially it means 37 percent of the people\\nthat came to their showroom, shop bought games and videos and 66 percent of the people to\\nbought games also bought videos. But then, if you just look at what fraction\\nof their customers bought videos there is 75 percent of the customers actually bought\\nvideos. And so, even though this rule has a high confidence, you can see that if you\\nbuy a game, actually implies negatively on buying videos. This is the video store after\\nall and so if we typically come into buy videos but, the occasional person who comes into\\nbuy a game, is not that interested in buying videos. So, it actually imply a negative correlation\\nand if you had just blindly been using support and confidence to determine rules, then you\\nactually trip this out as a important rule in terms of having a high confidence.\\nSo, one measure which people gives instead of confidence and support alone, is known\\nas Lift. Lift is essentially the ratio of the confidence of the rule to that of a default\\nrule. So, if you have a lift of 1 that means, that is really does not imply anything. So,\\nwhether A happened or not, B is always going to happen with same frequency. So, if I A\\nimplies B is the lift 1 and it is not a significant role, but if A implies B is the lift much\\nlarger than one that would be in that. So, the things tend to truly indicator of B. But,\\nagain if you think about in this case of games and videos, where lift will be lesser than\\n1 and which case is indicates a strong negative corporation.\\nSo, lift is a useful measure to have, and that lift is not only measure that people\\nhave to proposed the variety of different measures which people have proposed analyze\\nassociation rules and association rules mining is a very very active area of research. And\\nso this is essentially just an introductory module and if you are interested in association\\nrule mining you spend more time. And looking at the various modifications and additions\\nthat people have come up with of viewers.\\nSo, just to summarize we the major challenges in association rule mining is how do you extend\\nto these millions of transactions. So, there could be billions and billions of potential\\nitem sets. So, how do you do the pruning efficiently? How do you minimize the number of passes?\\nHow do you reduce the number of memory that is required when you are doing the counting?\\nSo, there are many many rules and many issues that we have to consider in trying these large\\ndata sets. So, we have talked about transnational data\\nnow, so some sets easy to count and it was discrete event that are happening; but, what\\nabout the continuous data like time series data and images? How would you go about even\\nidentifying what are appropriate items more which will be doing this counting. And data\\nwith rich structure like graphs. So, frequent pattern mining in graphs is a very important\\ntopic which is you used in diagnose area like graph discovery and social influence prediction\\nand so on. So, how would you can you average a structure or can you make computation or\\nefficient and it handle in see structure data. So, that is something which is a very active\\narea of research and experimentation. And one important twist to this whole frequent\\npattern mining issue is that, we are not sometimes not interested to in frequency within significance.\\nSo, if A occurs let us say in 0.3 percent of all transactions; but when B occurs it\\noccurs in 1.2 percent of the transactions. So, it means B implies A and that is a significance\\neffect, because it improves the frequency of A from 0.3 to 1.2. And A might be that\\nin the A stack that we are looking for and finding B. And actually gives us greater evidence\\nfor the presence of A. So, such patterns are significant, but if you just go by the frequency\\na loan, these are very infrequent occurrences of a data. So, how do we actually look at\\nsuch frequent, none frequent but significant occurances. So, that is a big challenge in\\nthe association rule mining community, again like it is a very activity process and people\\nkeep developing, that brings us to the end of these.\\nBig Data, A small introduction\\nHello and welcome to this module on big data analytics.\\nSo, what I would do in the next two modules, is trying to introduce you to the problems\\nof big data analytics.\\nWe will start of looking at what is big data.\\nOf course, I cannot hope to cover all the aspects of big data analytics in a couple\\nof modules in this course.\\nThe whole idea is to give you a very brief or as I say a small introduction to big data\\nanalytics.\\nSo, when people started talking about data, organizing data, and managing data they initially\\nstarted with data base management systems.\\nYou might have heard of rational data bases and other forms of data management systems.\\nSo, the goal here was ease of access, and to be able to answer simple aggregation select\\nqueries; essentially trying to do some form of analytics of the data.\\nBut while looking at very specific, very static, amount of the data.\\nSo, the volumes of data were not as large as we talk about now a days.\\nAnd then from data base management systems, when the data became much larger that you\\ncould not make sense out of the raw data itself; and people started talking about data mining\\nor data analytic systems.\\nWhere the goal was to detect?\\nPatterns in the data and people used lot of ideas from machine learning and applied statistics\\nin order to do this kind of data mining, try to understand the data better.\\nSo, what was happen now a days?\\nSo, this whole system is evolved to a point where you cannot just hope to run your machine\\nlearning algorithms directly on the raw data.\\nBut you have to worry about the data management aspects of it; as well as the analytics aspects\\nof it.\\nSo, that is essentially the crux of what people call big data in now a days.\\nThe fact that the data is so large, that you cannot look at analytics divide of data management.\\nSo, give you a feel of what this big data, it is court from rich man oppose the executive\\nchairman at Google.\\nSo, from the dawn of civilization until 2003, human kind generated five exabytes of data.\\nNow we produce five exabytes of data every 2 days, and the pace is accelerating.\\nSo, if you can just think about it, what we have produce for 1000s of years since till\\n2003, we are able to produce that kind of data every 2 days.\\nThat is a volume of data that humans are producing and obviously, we cannot make sense out of\\nthis data until some kind of organization and analytics goes handle with us.\\nSo, what are the main challenges that come about, when we are talking about data of this\\nscale.\\nSo, this is the some out of other the coming at shape that view of big data analytic, but\\none that is popular.\\nSo, I thought I should present it as part of this model.\\nSo, the challenges are essentially in capitulated under the 4V’s of big data: that is volume,\\nvelocity, variety, veracity.\\nAnd I would like to add a 5th V to it; is it called value, which is essentially goal\\nof all of data analytics and subsets.\\nSo, I will briefly introduce you to each of these ways and another may be the end of the\\nfirst module.\\nSo, why you said that we are able to generate such huge volumes of data.\\nSo, one of the reasons is that all aspects of modern life has been digitized, all aspects\\nof modern life has been digitized.\\nAnd nowhere is it more a parent than in your social activity.\\nSo, we have Facebook; lot of peoples friend significant portion of their breaking hours\\non Facebook and possibly significant portion of the sleeping hours reviewing about Facebook.\\nAnd then we have such a volumes of data that is probably Youtube, and WhatsApp and twitter\\nand flickr and all the web sharing totals at.\\nSo, if you think about it at almost every aspect of your life is now being recorded\\nsomewhere or the other.\\nAnd the next thing is just not the social activities that you perform social media,\\nany kind of interactions you are having.\\nThis with very high probability is being recorded; for example, phone conversations.\\nWe call costumer support calls they are being recorded any feedback and surveys; that will\\nhave full fill out of the form anywhere as being that preserve for posterity, any kind\\nof online conversation, chats or sub transcript and other things they are also recorded.\\nAnd so, it seems like everything that are working at a, it is actually being recorded\\nat some point of other.\\nAnother aspect is the increasing availability of cheap storage and cheaper bandwidth that\\nallows us to share multimedia content at rates and volumes that is never imagine possible\\neven 5 years back.\\nSo, here are few examples that I took from the internet.\\nSo, Instagram is growing at more than one billion photographs.\\nInstagram has more than one billion photographs.\\nYoutube has 100 hours of new videos uploaded to the site every minute.\\nOver one billion unique users vsit Youtube each month.\\nAt Facebook adds 30 billion pieces of content every day and likewise you could take any\\nof these online portals and you can come up these mind boggling numbers, that tell you\\nhow much data is being shared on these portals.\\nSo, apart from these the aspects of everyday life and regenerating this data, there is\\nbeing significant advance in technology in other area; for example, in biological data.\\nSo, 3 gigabases of a human genome can now be sequenced in a few days.\\nThis is like 3 times 10 power 9 base pairs which is significant volume of data, but then\\nthis is for one human genome.\\nAnd you can essentially get your genome sequence for few thousand dollars in the US and so\\nit should be made accessible at much lower price range.\\nAnd so the data has being generated at every enormous unbelievable mind boggling pace.\\nProtein data banks have 10s of thousands of structures amounting to several terabytes\\nof data and again people are continuing add to this experimentation.\\nSo, to put this in prospective; Donald Knuth one of the fathers of computing algorithms\\nand so on so forth.\\nHas this to say about computational biology.\\nI think the most exiting computer research, now is partly in robotics and partly in applications\\nto bio chemistry.\\nBiology is so digital, incredibly complicated, but incredibly useful.\\nBiology easily has 500 years of exiting problems to work on, it is at that level.\\nAnd not only is it in the data analytics appear which is most relevant to us, but the variety\\nof other area has to well including data storage and also in computing hardware and access\\ntechnologies; biology is challenging computer science at levels never seen before.\\nSo, this is about volume but, what about that rate at which this data is being accumulated.\\nSo, data is generated at tremendous volumes, but tremendous rates.\\nInstagram in may 2012, 58 photographs were being uploaded and a new user was being gained\\neach second.\\nIn Facebook adds half a petabyte of data every 24 hours, and then let’s not even talk about\\nsensor networks; which are generating data most of the continuous rate; for example,\\nat IIT Madras we get GPS readings from about 300 buses every 30 seconds.\\nAnd if you think about cities city wise transportation, this is really a small scale.\\nAnd if you are able to instrument all the buses that run in Chennai city we can get\\ndata at a much higher velocity.\\nJust hold on one minute, why where we worried about all these big data.\\nSo, we are not really, I mean not every one of us is going to work for Facebook or Instagram\\nor Google.\\nSo, tremendous amount of data is already available in the public domain.\\nEither through in Facebook APIs or twitter APIs or through Government data that will\\nbeing made available on open forums; and we can obtain significant insights from analyzing\\nthis public data.\\nAnd you could think about sentiments, you could think about trends and the variety of\\nthings.\\nIt is the kind of insights you can derive is limited by your imagination and access\\nto the data; and there is the significant amount of public data available, that we do\\nnot really have to work for Google or Facebook to think about big data.\\nThe second thing is that sensors are becoming ubiquitous.\\nSo, a lot of everyday I think that sees are being instrumented, and then raw sensors,\\nand buildings, there are sensors on bridges, and other kinds of infrastructure out there.\\nAnd ubiquitous of internet of things, this essentially the whole idea is to connect different\\nkinds of equipment through sensors on low power wireless networks.\\nSo, once you have these kinds of data being generated by sensors, then you do not have\\nany depth of volume or velocity of data.\\nAnd so you really need to develop technique like this; that allow us to work with data.\\nCan I say all ready mentioned and large volumes of public data is already available.\\nGovernment is pushing for more access to public information.\\nSo, the all of these factors really makes sense for us to look at big data and we do\\nnot have to work for such large company; is even though the examples I gave you through\\nmake you appreciate the volumes that we have to look at, where drawn from this kinds of\\nonline social media companies.\\nSo, the next V, I am talk about a little bit this variety.\\nSo, traditionally we talk about relational data bases, we talk about structure data with\\nvery well defined fields; may be of a few types in all could have strange, you could\\nhave numbers, you could have categorical variables things like that.\\nBut now with big data, data coming from the different kinds of sources, you have many\\nvariations of this.\\nSo, the first and very widely available source of data is unstructured text.\\nSo, we can get things from the web we can get scientific articles, news paper clippings,\\nvariety of sources review unstructured text.\\nWe keep these are not really marked into sink, this is the heading, this is the most important\\nkeyword here and these are all the attributes of this keyword.\\nI mean so you do not get this kind of organize data, essentially you have to just read free\\nfrom text and from that from some kind of a representation which you can then use for\\nyour data analytics form.\\nSecond source of information now a days is multi media.\\nAs I am mentioning you get pictures, you get videos, and you get variety of different sources\\nof videos and pictures, and as well as audio song clippings, and so on so forth on the\\ninternet now.\\nAnd so, you have to come up with technology for analyzing all of this, especially at a\\nscale.\\nAnd mention about sensor data and again this is going to look like unstructured data.\\nSo, it is going to be company wise time varying signals that we would be measuring from these\\nsensors.\\nAnd how do you even record these, how do you make sense out of this data?\\nThat is the other main challenge and then you have scientific data.\\nAnd scientific data comes in variety of different forms.\\nWe are taking about medical data, it could come in terms of reading from instruments,\\nit could be black and white images, it could be thermal images or here whatever seeing\\nhere is snap shot from a micro array data used in computational biology a lot.\\nAnd so, making sense of this kind of data its requires not only new computing techniques,\\nbut also significant understanding of the domain in which you are operating, and therefore\\nwe have to work about close being collaboration with the this scientist; who are handling\\nthis kind of data.\\nAnd data analytics cannot be performed in isolation here.\\nSo, lastly I would like to talk about link data; and this is essentially data that comes\\nwith some kind of structure, but not the kind of structure that we are normally used to.\\nthis is data and that comes with the network structure.\\nSo, we talking, you can talk about the entity; let us look at the small close up of this.\\nSo, each of this nodes here this very complex graph and each of this node here is essentially\\nan entity; and the links tell you how are these entities connected.\\nSo, such large volumes of link data which give you relationship between entities, already\\navailable in the public domain.\\nAnd apart from that many sources of data that you generate now a days, have this kind of\\nlinks structure associated to it.\\nSo, now the question is how would you mine such large volumes of big data?\\nSo, variety that is tremendous lot of variety and each of these come with its own challenge\\nand quite often significant requirement for domain knowledge.\\nSo, on the next V, we look at this veracity.\\nOne of biggest problems in social media data is figuring out if the data that is given\\nto you is true or not.\\nSo, the falsification data I am pretty sure that many of you have actually given false\\ninformation to create email ids and other things online, and this being cases of people\\nmaintaining multiple profile on Facebook; and so, that selectively share information\\netcetera.\\nSo, it is hard to disambiguate when you are giving false and when you are giving true\\ninformation.\\nSo, a significant amount of resources are spent by many of these companies in order\\nto verify the information which is provided to them.\\nOr if you take sensor of data, data could be noisy, that could be missing values.\\nSo, how do you account for all of this?\\nAnd it is how do you trust the data that you are getting from the sensor is it; or we sure\\nthat the sensor is not malfunctioning.\\nSo, how do you account for that?\\nMaybe there are few sensors; you can do that manually, but suppose you are talking about\\nmillion of sensors diploid over high rise building.\\nThen the question of manual verification is moved, just cannot do that.\\nAnd talk about how biological data is one of the biggest sources of future problems\\nfor us, but even though there are huge volumes of data available more often than not.\\nThese are estimated interactions, estimated data; and again you are not sure about the\\nveracity of the data.\\nnot sure with that lets if you say protein interact with another protein, you are not\\n100 percent sure interaction happen.\\nYou can say with 80 percent confidence, I am sure this interaction happens.\\nSo, in such cases how would you handle this kind of unsure data?\\nSo, this is just example.\\nSo, if we take any source of data coming from any kind of domains will always find that,\\nthere are issues of noise and trust worthiness in the data.\\nSo, the verification the people still work with this kind of data already.\\nWe can, some of you if you already worked or working in some kind of a data analytics\\ncompany you know that; you have to work with this kind of data already.\\nBut then verification becomes hard due to the scale that we are talking about.\\nSo, current technology people typically end up doing some kind of internal consistency\\nchecks and some amount of manual verification but, we need something more scalable to work\\nout this a big data scale we are talking about.\\nAnd at the last V, I wanted to mention very briefly this whole idea of value.\\nSo, I have all this big data; how do I monetize the big data?\\nSo, we have to think about problems where substance are returns or higher than the investment.\\nAnd now, the investment is no long at trivial because we really have to put in the lots\\nof resources in order to just manage this data at the scale; and then run analytics\\non top of it.\\nSo, all the 4 V’s play a role here, that is so basically have to figure out what is\\nthe right balance interns of the effort, that you put in and the kind of monetization that\\nwe can do.\\nSo some example which people actually try out or on recommendation is trying to build\\nbetter recommend assistance for different domains.\\nLooking at sentiment analysis trying to get a sense of what people feel about new products;\\nmay be movies or what do people feel about a particular candidates in election.\\nSo, we are looking at that and then other examples include business process optimization,\\nsurveillance, healthcare, smart cities; there are many many domains in which people have\\ntrying to find out monetization for big data.\\nSo, in the next module we will look at some of the challenges in running data analytics\\nat the scale that we have talked about today.\\nThat brings us to end of this module.\\n \\nBig Data - A small introduction (contd)\\nHello and welcome to the 2nd module on Big Data.\\nThat is we saw earlier that the variety and the volume of data; requires has to have a\\nnew paradigm for handling all the computing.\\nAnd one of the most popular paradigm that is used for handling big data computation\\nis Map-Reduce.\\nSo, this was a programming model that was pioneered by Google which were reusing initially\\nproprietary implementation.\\nBut later on a Hadoop an open source platform, that implements Map-Reduce became very popular.\\nAnd so, in the next few slides I will give you a very brief on introduction to Map-Reduce.\\nSo, Map-Reduce as we will see, is a very intuitive approach to parallelize computation typically\\non commodity hardware.\\nBut the big problem when you are using, especially using commodity hardware is that machines\\ntend to fail a lot.\\nSo, fault tolerance becomes a big problem.\\nSo, for example Google runs typically a million machines in one of the data centers and assuming\\non average 3 years lifetime for 1 machine.\\nYou would expect about 1000 machines to fail per day.\\nSo, Hadoop is a distribution frame work, that fundamentally uses Map-Reduce as that computing\\nparadigm, but on top of it takes care of data distribution, communication, and fault tolerance;\\nand makes it very convenient to use this kind of a distributed; set up and solving everyday\\nproblems.\\nAnd hence Hadoop is being wildly deployed commercially, and variants of that are continuing\\nto be very popular even today.\\nLet us look at the Map-Reduce computing model.\\nThe Map-Reduce as you can imagine consist of 2 stages: the map stage and the reduce\\nstage.\\nAnd in between there is a group and redistribute phase.\\nSo, the map stage, so the each mapper which runs on a individual machine takes a block\\nof the data; that is that you have to originally process, extracts some information from the\\ndata and output these as key value pairs.\\nSo, the key as you know, as identifier the value is any value that you would like to\\nsend along with the key.\\nSo, we will look at examples later.\\nAnd then before sending to the reduce stage, we sort and shuffle all of these key value\\npairs and you group them by the keys.\\nSo, that all the pairs which have the same key values, will go to the same reduce stage.\\nIn the reduce stage you essentially compute aggregate statistics corresponding to the\\nkey.\\nWe could add things up; we could summarize them, we could transform them, we could do\\nany kind of filtering that you want based on the key, whatever it is we can compute\\naggregate values based on the keys.\\nLook at this an example of a very simple word count, it is like the hello world of the Map-Reduce\\nprogramming.\\nSo, let us say that you have a document.\\nSo, what you would do is you would divide the document into different blocks as shown\\nhere.\\nSo, the 1st block I have colored it red, the 2nd block green and the 3rd block blue.\\nAnd we send it to the different mappers.\\nSo, what the mapper does is?\\nIt reads each word in the document, that outputs that as a key and the value as the count of\\nthe word in the document.\\nSo, every time it encounter a word, it just going to output it as that word with the count\\nof 1.\\nSo, the red mapper is going to output; concept, one, of, one; and then the green mapper is\\ngoing to output variety of things on this case is going to say for one, and concept\\none.\\nAnd the blue marker again outputs for one, various one, weighted one so and so forth.\\nSo, in the group stage what happens, you group things by the keys.\\nSo, all the key value pairs which have concept of the key get grouped and they are send to\\none reducer, likewise all the key value pairs which have for as a key get grouped and are\\nsend to another reducer and so on so forth.\\nAnd finally, the reduce stage aggregates all these counts regardless of which mapper they\\ncome from and then outputs the total count.\\nSo, in this case you would get (concept 2) (of 1 for 2 various 1 and weighted 1.\\nThis is very simple way of doing word count and your document could be very very large\\nas long as we have sufficient number of machines we can compute this very efficiently.\\nThat is not only in working with documents and the other things.\\nEven simple items, even simple computations is you would have thought they were straight\\nforward can become complicated when you are dealing with it scale.\\nSo, here is an example I would like to compute degree of a node in a graph.\\nOn very large graphs computing even degree of a node becomes little harder.\\nWhy is that, because the data itself is not available to you at one time.\\nSuppose I am trying to build the graph as of who called who.\\nSo, I am going to get things like A called B at time t and spoke for m minutes.\\nSo, that is an event that arrive to me after the call has finished.\\nSo, at no point of time do I have all the calls of A, stored in some place and we just\\ngoing to find the degrees.\\nIt is not like a single graph that already being constructed firm.\\nOr if you could think of trying to create some kind of an interaction graph on Facebook;\\nso user x posted on the wall of users y.\\nNow these events even if they are available to you post facto that are so numerous that\\naggregating them and to finding the degree of the graph is could take a while.\\nSo, you could actually use Map-Reduce to efficiently aggregate the each event into a graph.\\nHere is a simple program that would do now.\\nSo, the mapper essentially takes each edge event, each event, each interaction could\\nbe the posting of a message on Facebook; could be the making of a call, takes each of those\\nevents and then it creates 2 key value pairs for every event.\\nThat has of A call B it will say, A 1 and B 1.\\nand on the reducer essentially now takes in every event or every key value pair that have\\nthe same key; that essentially means is going to gather all the node A’s.\\nAdd up the events corresponding to the node A it will output the degree of node A. Fairly\\nsimple, fairly straight forward and so you do not really have to create a adjacency matrix\\nor adjacency list representation of your graph.\\nWe will be able to answer to queries like beginning.\\nWe can stick with the edge list representation and still answer these kinds of queries.\\nSo, let us look at some other questions that not necessarily need Map-Reduce; but become\\nharder when you are looking at large data.\\nSo, you looked at k nearest neighbors in one of the earlier modules.\\nSo, how would you find k nearest neighbors, if you have huge volumes of data?\\nSo, linear search is impossible; because any index structure should be small enough to\\nfit into memory for you to do linear search.\\nThat is not going to happen if data is very large.\\nSo, there is a new approach for finding neighbors in data call locality sensitive hashing introduced\\nby Andrei Z.\\nBroder and others.\\nSo basic idea here is to, find hash functions.\\nSo, you remember hash functions, they are functions that take a key as an input and\\nthen the hash it into one of n buckets.\\nSo, here what we do is?\\nWe want to find hash functions, such that 2 elements x and y, if their distance is less\\nthan a certain threshold the hash to the same buckets or same bin.\\nTwo elements, x and y; if the distance is greater than a certain threshold, then x and\\ny hash to different bins; and we would like this to hold with very high probability.\\nWe would like this to hold for sure, but then it is hard to get something that will work\\nalways.\\nSo, you would like to hold for this to hold with very high probability.\\nSo, now, if I want to find if y is nearest neighbor of x, then all I need to do is look\\nthrough the bin into which x hashes.\\nAnd since this is only with very high probability, so you might actually miss your nearest neighbors.\\nBut you will certainly get some neighbors that is close enough because since the data\\nset large, even close neighbors are usually sufficient.\\nSo, depending on the kind of distance function that, you want to use on your data.\\nIf you remember we talked about different distance function in the various neighbor\\ndepending on the distance function you want to use on your data; you are going to have\\nto define different hash functions.\\nAnd so for example, if you want to look at distance measure between sets; this has groups\\nof words and so on so forth.\\nYou probably like to usage the Jaccard distance, which is 1 minus the size of intersection\\nby the size of union.\\nOr if you could say Euclidean distance between points or between vectors you could want to\\nsee cosine distance; and for all of these people have worked out what are appropriate\\nlocality sensitive hash functions.\\nSo, we are not going to get in the details of the locality sensitive hashing, just wanted\\nto give you a feel for how hard it can be, when you are looking at large volumes of data.\\nEven what you thought are simple operation become harder, when you are looking at large\\nvolumes of data.\\nHere is another example; we talked about frequent pattern mining and association rule mining.\\nImagine counting frequent items in amazon’s transaction data; we have millions of transactions\\na month.\\nSo, just blindly running apriori is just not going to work.\\nSo, we need a different approach to the problem.\\nWe can still use apriori property, but we have to think of how you would handle the\\nmemory more efficiently.\\nSo, here is a very simple approach, to do distribute counting; you divide baskets randomly\\namong compute nodes.\\nSo, here we are talking about market basket data.\\nSo, essentially what I mean here is the transactions are randomly divided among all the nodes you\\nhave.\\nYou run apriori in each computing nodes separately.\\nAnd whatever turns up as frequent item sets in each of those nodes are now candidate frequent\\nitem sets.\\nSo, what we have to now do is, go back and count the actual frequency of these candidates\\nitem set; because the original candidates were determined on a small subset of the data.\\nYou will have to go back and count the frequency on the entire data to determine if these candidates\\nset are frequent.\\nSo, remember that when you are doing this in the distributed fashion the threshold that\\nyou are using for defining frequent item set should be lower.\\nSo, if you are splitting your data at 10 ways then the threshold that you had for frequency\\nshould be lowered by 10 times.\\nSo, once you have this candidate frequent item sets you do not have to count it in a\\nsingle machine, you could still do this in a distributed fashion.\\nSo, again each node will count the frequency of just this one candidate item set and then\\nat the reducer we could basically combine the frequency of frequency reported by each\\nnode, and then report frequency of the item set on the entire data.\\nSo, there are other computing models; I just spoke about Map-Reduce and this top.\\nAnd the other thing has Spark which is Map-Reduce variant with local memory and then there are\\nGP-GPUs, which are multi core massively data parallel computations.\\nAnd then there are other models with shared memory multi core repetition.\\nSo, a depending on what is the use case that you have, we will have to use different computing\\nmodels.\\nIt is not that Map-Reduces one solution for all method.\\nSo, depending on the solution, depending on the problem that you have, you will have to\\npick the computing model that the shows you.\\nOne thing which I would like to point out is that data visualization is still a challenge\\nfor big data.\\nSo, visualization is a challenge for normal data analytics, but it is a challenge for\\nbig data.\\nSo, people are working on adaptable interfaces or interactive visualization, but more often\\nthan not people are still trying to fit old visualization ideas to big data and that is\\nnot working.\\nSo, it is very active area of the research and several new generation methods are needed\\nhere; I just wanted to draw your attention to the fact that it is something that you\\ncould work on.\\nSo, in summary; data analytics has matured to a certain level and so now, people are\\nlooking at big data challenges.\\nSo, it is some sense evolving to a new discipline of data science, where just the analytics\\ntechniques are not themselves sufficient, but we need more understanding from the modeling\\nfront.\\nIt is very exciting time to be in this space, availability of vast amounts of data and the\\ninternet of things like picking up as going to be a lot of work, more data available.\\nThe new computing models and that could very well be a high impact on society if you are\\nable to come up with solutions, handle make sense of this big data.\\nThat brings us to the end of this module.\\nClustering Analysis\\nHello and welcome to our lecture, our first lecture on Clustering Analysis.\\nThis is the first of two lectures that we will have on this subject, this one is an\\nintroductory lecture.\\nIt introduces the basic concepts and algorithms behind clustering analysis and in the next\\nlecture; we will actually go into two specific algorithms and see how those two algorithms\\nwork.\\nSo, deriving to the subject, what exactly is clustering?\\nClustering is the idea of dividing data into groups and we do that, because sometimes there\\nis inherent meaning to doing such an activity.\\nAnd in other cases, it serves as the first step, it is fairly useful to do this.\\nNow, you might notice that you might have come across clustering, with some other names\\nas well, which such segmentation or partitioning.\\nSo, that is kind of why we have written out all three for you in this first bullet point.\\nBut, the core idea is the same, it is fairly interesting, it also partly what terminology\\nwind up using also has to do with the context or the background.\\nSo, typically something like segmentation is used more in a marketing sense.\\nSo, when you grouping people or customers, sometimes people use the word segmentation.\\nPartitioning, again interestingly comes more from the computer science community, where\\nyou are breaking a graphs, so you are partitioning graphs.\\nBut, again the core idea is that you have some data and taking the data and based on\\ncertain properties of this data, you choose to group it.\\nSo, you can think of it as actually grouping data objects based on various attributes associated\\nwith this data.\\nThe idea behind such an activity is that, the data itself is represented through various\\nattributes or features and some data points are similar to others based on these attributes\\nor features.\\nAnd you want to group those that are similar and put them into one cluster or group and\\ndifferentiate that from other groups or clusters, which are similarly formed based on some measure\\nof similarity.\\nSo, the core idea is to put things that are similar together and therefore, as a result\\nthe different groups are as dissimilar from each other as possible.\\nSo, data points within a group are similar and data points between groups as a result\\nare dissimilar to each other.\\nNow, the core idea the clustering often relates to and sometimes it is often confused with,\\nis classification.\\nI think an important thing to recognize here is the clustering is primarily seen as an\\nunsupervised learning technique, whereas classification is supervised learning technique.\\nThe idea is that you have some, in classification you have some input data and you use the historic\\ninput data and the specific output and in the case of classification, the output actually\\nhas classes, it is a categorical variable.\\nAnd, so you used some kind of supervised learning technique to look at the relationship between\\nthese input variables and the task there is to make a prediction and assign it a class.\\nIn the case of clustering, you are again dividing the input data space in some sense into groups,\\nbut the groups are not based on labels that have been explicitly given to you.\\nSo, in some sense there is no output variable with clustering and what you have is only\\nthe input data space and it is not even clear that, there is another there is a classification\\nscheme like in the sense of the, in the classification there was this output variable and this output\\nvariable had 1, 2, 3 or more categorical states.\\nSo, there were the states that are labels that was explicitly given to you and you are\\nnow trying to create that relationship between these labels and the input variables.\\nIn the case of clustering, not only that you do not have the labels or the output variable,\\nthere might be no meaning to having such labels.\\nSo, in some sense with clustering you are really breaking the data, input data set based\\non the inherent relationship between data points and how similar they are to each other,\\nyou do not have external label that is given to you about the data points.\\nSo, if two data points are very similar across these attributes, then you group them together\\nif they are not you group them apart.\\nSo, in the end you might create a few clusters and you can call them cluster A, cluster B,\\ncluster C, but that is not the same thing as classification, which is more of the supervised\\nlearning process.\\nSo, why really do this and the best way to see that is this brought the two major reasons,\\none is clustering itself might just be useful to understand the universe of the data that\\nyou are dealing with and we are going to look at some examples in that light.\\nThe other reason and sometimes the clustering is used is that, it serves as a precursor\\nto further data analysis.\\nSo, it has some utility from a machine learning sense itself to do a clustering.\\nSo, now, let us look at the first case, which is that in some sense you get a better understanding\\nof a data when you do clustering.\\nA fairly common example of this is in marketing or sales, your business is for instance collect\\nyou know lots of information about a customer.\\nSo, the way you should be thinking about it is, each data object is essentially a customer\\nand the customer has for instance various attributes.\\nThey could be things like gender, age and you know it could extend all the way to the\\nkinds of products of the person tensed to buy and so on and so forth.\\nAnd you have a full data set of a lot of customers or potential customers and you might be interested\\nin grouping that.\\nSo, there is no explicit output variable, but certain types of customers are very similar\\nto certain other types of customers.\\nThey could be very dissimilar to the third type.\\nSo, creating groups out there could help for instance like a market research initiative\\ninto looking into a particular group and coming up with ideas, so how to specifically target\\nour market to them.\\nAnother example you know, another very common example is in terms of just communicating\\ninformation; take a simple example such as Googling or you know searching for a movie\\non the internet.\\nNow, there are lots of things related to a movie.\\nFor instance, there could be a reviews of a movie, they could be trailers and videos\\nof a movie, they could be a ratings of a movie and they could be information on which theatres,\\nthe movie is running in or where you can purchase a movie.\\nNow, a search itself, the first search itself across the web might give you a huge large\\nbucket of things that could belong to any of these categories.\\nBut, if you have an ability to for instance look to see which pieces of information are\\nsimilar to each other, you might be interested in representing one of each type.\\nSo, you might have a cluster; that is created, which talks about essentially reviews of a\\nmovie and another cluster that gives of potential links that talk about, where this movie is\\nplaying.\\nAnd, so in some sense you have you could create this clusters and present the viewer or present\\nthe person, who is searching with representative of each cluster or typical value in each cluster.\\nSo, that a person, who is searching for a review of the movie does not get 20 links\\non the first page that give him trailer, give him or her trailers, which should be quite\\nfrustrating.\\nSo, in terms of information retrieval, communication of the information it might be fairly useful.\\nClustering is used a lot in biology mainly in taxonomy, where if you just said the wild\\nuniverse of mammals or insects or something like that, you kind of want to group animals\\nor mammals that are similar to each other and give them some taxonomy that is different\\nfrom animals that are different from each other.\\nAnd here for instance each animal would be the data object and the attributes would be,\\nyou know various animal related attribute such as, how they feed, what their family\\nor genes or species and so on and so forth.\\nIt is also used in climate, many times understanding ocean temperature ranging from ocean temperature\\nto hurricanes to various other things, it can be better done you know if you cluster\\nthe data on weather patterns.\\nMedicine of course, again a certain types of disease similar to other types of disease\\nor certain medicines similar to other medicines, it can simplify the word in some sense and\\nhelp people understand, how for instance certain medicine interact with certain disease and\\nso on and so forth.\\nNow, these give you an understanding and these kind of talk about, where clustering is useful\\nor has been used or just to get a better sense of the data.\\nBut, there is this completely different and other utility sometimes to clustering and\\nthat is mainly in the form of serving as a precursor to further data analysis.\\nThe idea here is that, clustering for instance can be used as a great way to summarize data.\\nEspecially, when the algorithm that a person needs to use.\\nLet us say a person is interested in performing some form of regression or another more complex\\nsupervised, unsupervised learning tasks.\\nSometimes with more and more data the task just becomes computationally harder and harder\\nand it might be sometimes beneficial to perform a clustering analysis on the data which could\\nbe in some cases it is computationally easier to do the clustering.\\nAnd then, just have a representative from each cluster as a data point, so you could\\nhave a representative or you could have essentially a cluster center the clusters representative.\\nSo, was the data point for all the data points in that cluster?\\nAnd, so you are now, doing that same machine learning task be it a regression or a factor\\nanalysis.\\nSo, whatever it is you are really doing it on the prototypes on the representatives rather\\nthan the full data set.\\nIt is also used an extension of that is also how it could be really useful like say in\\na nearest neighbor task.\\nWe in an earlier in earlier lectures we spoken about this algorithm called KNN: K nearest\\nneighbors when you think about the problem that is involved with that for any given point.\\nIf you need to find it is nearest neighbours you need to actually try and look at the distance\\nbetween the point under question and every other point in the data set.\\nSo, you need to evaluate the distance between the point you are interested in and every\\nother data point and then pick the K closest neighbours.\\nNow, that can become computationally very, very, very hard and you need to keep either\\nthe memory and you need to do the computation from scratch, a simpler approach could be\\nthat you just take the K you do the clustering on the data.\\nAnd if you look to see, which cluster center is closest and once you identify cluster center\\nthat is closest you now, take your point under question and only evaluate it is distance\\nto all the points in that cluster.\\nAnd the assumption here is that the points the points under that cluster are the once\\nthat are going to be closest to this point, because this cluster center was the closest\\nfor instance.\\nIt is also used in certain other forms of you know compression of data specifically\\nsomething called vector quantization, which is used a lot in image or sound or video data,\\nwhere you typically find that in a particular image, if you break it up into pixels there\\nis the whole host of data points that will look very, very similar to each other.\\nLet us say you take a photo of a person almost 20 to 30 percent might be the background behind\\nthe person and that might have the same color.\\nSo, it would be more efficient to just acknowledge that there is a very good chance that the\\npixel on all four sides are going to be the same color as this pixel.\\nAnd, so you kind summarize the data you reduce the data to few a number of pixels in a memory\\nvalues and; obviously, would this the some loss of resolution either some loss of information\\nessentially, but that might be acceptable then the data size itself get substantially\\nreduced.\\nSo, we spoke about, what clustering is and why use it, now let us just briefly talk a\\nlittle bit about the different types of clustering.\\nThe major classifications tend to be one hierarchical versus partitional and the major idea here\\nis that hierarchical clustering is essentially a nested form of cluster.\\nSo, think of it this way you can start with this one mega cluster, which is essentially\\na single cluster of all the data points and that is on one end of the scale and then,\\nyou go to the other end of the scale where you have as many clusters as the number of\\ndata points and each data point it is a cluster is its own cluster.\\nNow, some where hierarchical clustering works on creating this tree between a single cluster\\nof all data points to each data point being its own cluster now either of these extremes\\nare useless.\\nBecause, the single cluster of all data point is not really clustering.\\nYou just created you put all the data points in one group and its called it group one that\\nis not very useful.\\nAnd neither is it useful to call each data point its own group.\\nSo, somewhere in between these two extremes is the real value at, but hierarchical clustering\\nworks on some form of nested or kind of tree form of clustering when you start with this\\none you can you can start it either end, but you might start with one large cluster and\\nthen, break that into two.\\nAnd now, you have these two clusters, now you go into either of these two clusters and\\nbreak that into created division there.\\nSo, now, you will have three clusters, but the three clusters is strictly a division\\nof the two clusters.\\nSo, it is not like you are going to take some points from cluster when you have two clusters.\\nLet us say you had some cluster A cluster B it is not like you are going to take some\\npoints from cluster A and some points from cluster B and call it cluster C. It is in\\nthat sense nested it is in the sense that you make one split and very, very similar\\nto decision trees you keep making further splits and this is contrasted to an approach\\nof partitional clustering.\\nPartitional clustering is not of this kind of nested approach it is just that you explicitly\\ndecide on the number of clusters in some sense and you go and partition the data.\\nits it is It is simply a division of the set into non overlapping set, so it is essentially\\nclusters.\\nAnd, so in that sense there is no tree diagram or anything of that nature with this.\\nSo, for instance a partitional cluster that if I decide to do the partitional cluster\\nand create four clusters and I contrast that to a partitional clustering approach, where\\nI had three clusters.\\nThe four, need not be a further division of the three they just completely you know the\\none that said decided to break in into four has a very little or no relationship theoretically\\nto the division the separate exercise of the separate effort into creating a partitional\\ncluster with three clusters, whereas the same cannot be said for hierarchical.\\nBecause, hierarchical went in sequence, it could have started top down or bottom up meaning\\nyou could have started with all in hierarchical all the individual clusters, where it is each\\nday you know each cluster is a data point and then started grouping them or the other\\nway around.\\nBut, essentially with hierarchical one is nested into the other and so on.\\nThe other major type of clustering that people talk about is exclusive verses overlapping\\nversus fuzzy.\\nThe idea here is with exclusive clustering each object is assign to a single cluster\\nand that object therefore, cannot be assigned to another cluster.\\nAnd, so there is no there is no notion that one can simultaneously belong to multiple\\nclusters, where as in overlapping, in case in clustering algorithm that allow for over\\nlapping, you can.\\nBasically it is not exclusive a data point can choose to belong to more than one cluster\\nat a given point and decision on which, of these are really depends on the underlying\\nsystem.\\nAnd some cases it might just make sense to have some data points belonging to multiple\\nclusters and in some in some cases that that notion of division is just not sensical.\\nAnd finally, you have you have the notion of fuzzy clustering fuzzy clustering kind\\nof takes this over lapping even further, where each data point is not really assigned to\\na cluster, but it basically gets a number between 0 to 1 that that talks about the weight\\nassociated with that data point belonging to the different clusters.\\nSo, for a data point each data point gets total weight of 1 and it takes that data that\\nweight of one and says, I am going to assign 0.3 in belonging to cluster A, I am going\\nto assign 0.7 in belong to cluster B and I am going to assign myself 0 belonging to cluster\\nC. So, the constrained is that the sum of its weights in terms of belonging to the different\\nclusters adds up to 1, but it can use its weight of one in any way chooses to belong\\nto the different clusters.\\nSo, that is to give you some idea between of exclusive versus overlapping versus fuzzy\\nthe last that is worth mentioning is you can also have a complete process partial clustering.\\nA complete clustering basically just assigns every objects to a cluster, where as a partial\\nclustering does not it starts with the data points chooses to you know cluster as many\\npoints to different clusters and data points that do not really help in terms of belonging,\\nso clearly to given cluster just not cluster they are just left out.\\nAnd these might and the and the motivation there is that you are more interested not\\nin kind of pigeon holing each data point into a cluster, but you are more interested in\\nthe cluster formation itself.\\nAnd there you do not want data points are not, so clusterable, that do not really belongs,\\nso clearly into 1 of 2 clusters to kind of ruin the cluster center or ruin that nice\\ndivision that you created.\\nSo, these are set different types of clustering, now the algorithm themselves and these are\\nmore descriptive of the type of algorithm that goes into it.\\nNow, another area focused could be the kind of clusters that you are forming and while\\nthis has everything to do with the algorithm also here we are just looking at the end product.\\nSo, the algorithm that we are using what kind of end product does it can it give you.\\nSo, there is the first one is the well separated idea the idea the well separated is that you\\nwant to create clusters, where get the objects where each object is similar to every other\\nobject in that cluster.\\nSo, the way you defining well separated clusters are is more from the core idea that you are\\nvery interested in looking at the relationship between each data point with respect to each\\nother data point and you are measuring or your quantifying the clustering itself by\\nlooking at how similar that data point is to that other data points that are in that\\ncluster.\\nAnd therefore, how dissimilar this data point is to the data points that are in the other\\nclusters and this kind of thinking you know is very useful especially when the data itself\\ncan be very nicely separated.\\nWhen the distances between the clusters are fairly significant, then this conception becomes\\nvery useful.\\nThe prototype based approach really talks more about how each data point is close to\\nits cluster representative.\\nSo, that the commonly used terminology is to the proto type that defines the cluster\\nand when I describe it I can try to think of it as there is a cluster and there is there\\nis the main representative of the cluster, the prototype the one that kind of signifies\\nthat what the cluster is in the center of that cluster.\\nSo, in the prototype based approaches, because its representative is in some sense so central.\\nIt is also sometimes called center based clusters, but the idea is as following, where each data\\npoint is looked at more from the prospective of how close this data point is to the cluster\\ncenter to the prototype.\\nAnd, how far this data point is from the prototypes of the other clusters or the other representatives\\nand the classification is done in this fashion.\\nWe then, have the graph based clustering and this is really useful if the data is represented\\nas a graph and you have nodes, which represent each data point or object and you have this\\nlinks or edges, which represent some notion of connection between the data point.\\nAnd this notion of connection could be the one that talks about how similar a data point\\nis to the other data point you could have some kind of a threshold value or especially\\nwhen sometimes you are variable itself is not quantitative variables.\\nSo, you could have some notion that two data points are connected and the idea here is\\nto do some graph based clustering, where you really looking for a high density of connection\\nthis between the data points that belongs to a cluster and a very low level of connectivity\\nof the data points between two clusters.\\nAnd for that reason you know this is whole language that comes from the graph theory\\ncommunity, where you define things calledÊcliques, which essentially just means that at the set\\nof nodes in the graph of a completely connected to each other.\\nAnd, so a lot of that that kind of clustering ideas that go in to graph based clustering\\nare once that kind of look out for cliques and say these guys are all connected to each\\nother, so they must be a cluster.\\nFinally, you have density based clustering the idea behind density based clustering is\\nthat a cluster is essentially a dense region of objects that are surrounded by regions\\nof lower density.\\nSo, the idea here is that, because there are not such well defined clusters like in the\\ncase of the well separated idea that you are not really looking too often do a complete\\nclustering and there is a lot of noise in the data and you know the clusters themselves\\nare irregular or intertwined and there are lots of outliers and so on.\\nSo, the idea here is that you acknowledge all of that, but you just try to identify\\nspots of extreme density, where once you identify that spot that dense region becomes a cluster\\nand so that is fairly useful in defining a cluster, where there is a lot of noise and\\nso on.\\nWith that we will conclude our lecture on lecture that introduces clustering and the\\ndifferent types of clusters and where it is useful and so on.\\nIn the next lecture we will looking to basic algorithms one is the K means algorithm and\\nthat really belongs to that partitional camp and we will look at another algorithm called\\nthe hierarchical clustering algorithm which belongs to the hierarchical camp.\\nThank you.\\nClustering Analysis (contd)\\nHello and welcome to our second lecture on Clustering Analysis. In the first lecture,\\nwe introduce the idea of clustering, differentiated it from say classification and other supervised\\nlearning techniques. And we explain, what clustering is, how it is useful, where it\\ncan used and also gave you brief overview of the different types of clustering and the different types of clusters. In today’s lecture we are going to introduce\\ntwo very popular clustering techniques. The first is K mean clustering and the second\\nis called the hierarchical clustering, where both these techniques are fairy old, this\\nstill enjoy immense popularity in terms of being actually used. The first one, the K\\nmean clustering and this choice of K mean and hierarchical, you should find to be also\\nfitting. The overall, the first dichotomy that we used when we talked about clustering\\napproaches, which is partitioning based approaches and hierarchical approaches. So, the K mean\\nclustering is essentially the partitioning based approach. And, what we will do is, we will dive into the algorithm. So, we have already mentioned\\nthat is a partitioning based approach. So, it is one, where you partition the entire data set into K clusters essentially and you, it is also very useful to think of the K means\\nas a prototype based approach, where that is also something that we discuss in terms\\nof how clusters are formed. And the prototype based approach is one where, there is like\\nthis representative for each cluster and you use these representatives in some fashion\\nto express, what a cluster is and to also form the clusters.\\nSo, it is essentially this prototype based approach, where you create K clusters and\\nit is also noteworthy that it is an iterative procedure. So, even right at the end of the\\nfirst iteration you already have K clusters and they might not be good clusters, because\\nit is just the first iteration. And then, like most optimization procedures like steepest\\ndescent, any of these other iterative producers you will find that over time you are just\\nrefining a solution and at some point, it does not make sense to refine any more. You\\nare not, your clusters are not changing essentially prototypes or not changing either location\\nor who they are and, so you stop at some point. This procedure is ideal when all variables\\nare quantitative, whether what we really mean is that you should be able to take each data\\npoint and you might have some other data point or some other location and your location is\\ndefined across the multiple attributes associated with the data point. So, a data point, again\\nthe conception you have in your mind is these rows and you are basically grouping data points,\\nthat is your job. And each data point is described by some attributes, which are these columns in a table and each column means something. So, with the K means procedure, what you doing\\nis you want to say that that you want to take this conception and you want to actually,\\nyou want actually be able to compute distances between two points across these dimensions, which means that attributes themselves need to be quantitative. This in a lot of ways\\nto remind you of this K nearest neighbors, examples that we took up especially in the\\ncase of K nearest neighbors regression, where each of these dimensions for those each data\\npoint is a continuous quantitative variable. And once you do that you just able to be compute\\ndistances, typically whenever it comes to computing distances we always use Euclidean distance, you could also use other measures of distance and you would actually, it would\\nstill be a K means procedures, but with different approach, so common once are Manhattan distances,\\nEuclidean distances and so on. So, how does it really work? So, let us just go through\\nthe algorithm in some senses and then, we will see a graphical representation of it, the idea is to initialize some clusters centers K.\\nSo, what you mean by initialize cluster centers? What we mean is, essentially think of this\\nand it is easy to think of it in a graphical sense or you can think of it in a mathematical sense, but the idea is that each data point can be expressed in terms of an x axis, y\\naxis, z axis, w axis, so on and so forth, depending on the number of attributes there are that represent each data points. Now, while each data point is represented based\\non these different attributes, you can create a cluster center also on these attributes.\\nAnd often it really make sense to put cluster centers, where you think the clusters are going to be form, but you see how even if you do not do that perfectly sometimes the\\nclustering the K means clustering will adjust for it. But, the most important thing to take\\naway from this is that, the K means cluster algorithm therefore, could be sensitive to\\nwhere you place the cluster centers. If you do not place them in sometimes the right places,\\nyou could get to a solution, which is not necessarily a good solution.\\nSo, the idea is that you initialize some cluster centers, so specifically you would initialize\\nK cluster center, so you will, because it is K mean clustering. So, K is usually a number\\nbetween 2 and may be 10 or 20 depending on the… The higher limit is a little more vague,\\nmost problems you looking at between 2 to 6 or 7 clusters at the most. But, there are\\ncontexts, where your data set is really large or what you intend to do with the clustering\\nis such that you do not mind having more clusters. So, the algorithms initially you drop in K\\ncluster centers and then, what you do is you take each data point and if you figure out, which of this cluster centers is closest to the data point and then, you assign the data\\npoint to the closest cluster center. And, so essentially it is like if you kind of give\\nnames. So, just to give you some inside, this is where we are, where the second bullet point.\\nSo, what we are doing is we basically saying if you wanted to give those K-cluster centers name as cluster 1, cluster 2, cluster 3, then what we would do is, we would go to each data\\npoint and say, which cluster center is closest to you and then, I am going to assign you to that cluster center. Now, once all of these points have been assigned, what we are going\\nto do is we are going to recompute the cluster center to be essentially the centroid of this\\nassignment of data points derived from step 2. So, what we are going to do is essentially in step 2, what you did is you assigned a\\nwhole bunch of data points to different clusters. So, what we will do is, we then, for instance\\ntake all the data points that were assigned to cluster 1 to the centroid 1 essentially,\\nto the cluster center 1. And we gave them all the labels that you are assigned, the\\ndata point x you are assigned to the cluster sector 1, data point y you are assigned to cluster center 1. So, all of them that were assigned to cluster\\ncenter 1 we take those data points and compute the centroid of those data points. Centroid\\nessentially is in many senses like an average. It is, again it depends on exactly what measure\\nof distance you choose, but if you are choosing Euclidean distances, the centroid is essentially like the average. And we say it is the average of these data points, but it is called the\\ncentroid, because it is the average across all those dimensions. The number of dimensions are the number of attributes, so across those dimensions, what is the center.\\nSo, once all these points have been assigned you kind of compute a new cluster center based\\non the centroid. And it is essentially like you forget the old cluster center and now, this new cluster center is your cluster center and you do this for each assignment. If 10\\ndata points were assigned to the cluster center 1, then you do this step for cluster center 1 data points. And then, you go to the next 15 data points set for perhaps assigned to\\nthe cluster center 2. So, for these 15 data points find out the new centroid and call that the new cluster center. So, you got new K set of cluster centers.\\nWhat do you do next? You iterate, for these new K cluster centers you go to each data\\npoint and it is almost like you forgot, which cluster center that data point belong to,\\nbecause remember that data point was assigned to say cluster center 1 or 2 based on the\\nold centroid, based on the old cluster center. Now, because your cluster centers are moved\\nbased on the centroid, you now assigned them all over again and once you done the assignment all over again, you find the centroid. Once you find the centroid, you do the assignment\\nthat is how it is an interactive process. Essentially you repeating the steps 2 and 3 until the centroid is not moving any further or in some cases, you might say the centroid\\nis moving by an amount smaller enough that is within your tolerance.\\nSo, this was fairly abstract in terms of bullet points, so let us actually take a graphical\\nlook at this if you taken an ultra simplistic example, where there are only two attributes.\\nSo, let say attributes is x 1, x 2 always remember that with clustering, which is unsupervised\\nthere is no y there is no output variables, what we doing is trying to grouped the data and here is the data. The data is this blue squares and you are trying to grouped this\\nsquares. Now, fairly naive look at this can kind of\\nmake it obvious that perhaps this is one cluster and this is one cluster. So, the blue dots\\nare actually the data points, now that is to the naked eye and the things to remember\\nare that you know if for instance there were more dimensions that were there were more\\nattributes beyond x 1 and x 2 it would be harder to show you this visually. So, that\\npoint the math kind of take over in 3, three dimensions I can show you x 1, x 2, x 3, but\\nafter that whatever I do if you have more attributes. So, the squares are essentially the data points and let say the K means clustering algorithms\\nstarted with two centroids here obviously; that means, K is equal to 2. So, you initializes\\ntwo centroids and you know this like I said the algorithms itself to some extend could\\nbe sensitive to how you initializes the centroid. So, there is no right way and the wrong way. Ideal would be if you could actually plot the centroid in the middle of the clusters,\\nbut often we do not know that yet that is why we are doing the clusters. So, what is the first step in the clustering algorithms the first step is to take each\\ndata points and see, which cluster center is closest. So, let us say we take this data\\npoint 1 clearly this the yellow cluster center is closest and at least hope for the set of\\ndata points is yellow is closest. Obviously, for another set it looks like the green is closest, so each data point basically gets an assignment either gets assigned the yellow\\ncolor or a green color and that is essentially, what I have done. This diagrams are more conceptual\\nnot the scales. So, I kind of high balled it and said it looks like this 4 data points are close to the yellow that is this 3 this 4 are close to the green,\\nso that is how they have been assigned. What is the next step? The next step is given this assignment the new cluster center for the yellow data points is the centroid of the\\nyellow. So, what is the centroid of the yellow the, so you can think of it as the essentially\\nthe average of the yellow and average; obviously, needs to be on this axis as well as it needs\\nto be on this axis. So, where you would centrally places yellow, so that is you know minimizing the squared deviation, which is Euclidean distances to\\neach data point and that is the definition of centroid. And, so we move the yellow yellow\\ncircle to be the new centroid and it is probably going to come somewhere here and the green\\ncircle, now the green circle has a little bit of problem and it just cannot move to the center of this space out here. Because, it is; obviously, has an assignment.\\nSo, this is going to bias where the green moves, so perhaps it will move somewhere here and that is what I do in the next step actually move the yellow and the green. But, once I\\nmove it is like the assignments are completely lost you forgotten the assignment, because\\nremember that the assignments were made with the old centroids. Now, with the new centroids you just have the new cluster centers and no assignments\\nyou redo the process when you redo the process you see now, which blue dots are closest to\\nthe yellow and which ones are closest to the green and we repeat the process and as you can see we have already gotten pretty good results out here with the yellow and green\\nclassification, which matched our intuition about what the two clusters of this graphs was. Clearly in the next step for instance is green\\nwill move into this cluster center the assignments themselves won’t change too much perhaps\\nthe yellow will move little bit out here. But, essentially after maybe one or two more steps the centroids will stop moving your assignment will probably the same. So, even\\nif you stop the 1 or 2 stages earlier you would have gotten the clusters that you were interested in. So, I hope this gives some idea, what the\\nK means algorithms is, now another very popular prototype based approach, which can work and\\ngo beyond K means in some senses, which can go beyond a quantitative attributes. Because,\\nremember for you for this whole algorithm to work you had to able to be compute distances\\nand it is makes senses that you attributes x 1 and x 2 for quantitative and continue straight x 1 and x 2 there is a little there is a medium there is more and this is continuous\\nquantitative variable, now many times you do not have that.\\nSo, very useful alternative is K medoids in that case, now K medoids allows you to go\\nbeyond the quantitative variables. So, when you have categorical or rather more importantly\\nnominal variables, where the variables is are things like male, female and the attributes\\nis like male female or things like that you can use K medoids. But, more importantly K\\nmedoids allows you to go beyond attributes altogether, where x 1 and x 2, where attributes\\nto a point, where all you need is some dissimilarities matrices.\\nWhat do you mean by this dissimilarity matrices? What I mean is you have all this data points, let us start calling them 1, 2, 3, 4. Now, with K means the distances between and I am\\ngoing give you the same data points in the columns, now with K means I can compute the\\ndistances between the data points 1 and data point 2 through some form of Euclidean distances\\nand mark a value of x. But, what if, so the whole process of using Euclidean distances\\nrequire that I go into each attributes look at how different it is and then compute that square distances takes the square roots. But, what if I did not have that process,\\nwhat if I had a process, where I gave you just the differences between each data points.\\nSo, I have something like a dissimilarities matrix, so when I have a dissimilarities matrix,\\nthat I am giving you and you can get the dissimilarities matrices through completely differences ways\\nit can be user survey or it can be something extremely subjective, where you say I feel\\nlike 1 and 2 are different by this much and I can tell you how different 1 and 2 are and\\nI can tell you how different 1 and 3 are you know and I can tell you how different one\\nand four are and so on. But, I cannot really break it down into five different attributes and give it to you that way. So, people do this lot of times in social\\nscience research whether some kinds of a question I have that is given to people to tell, so\\nplease tell me how different this two are. And people are able to say that there are not able to break it down into a set of quantitative variables and say how different they are in\\neach of those dimensions. So, K medoids is very useful when you do not have these attributes,\\nbut you just have dissimilarities matrices of how each data points is different from every other data point. So, you will essentially just need some kinds\\nof leading diagonal of data. So, the data either on this side or data is on that side, because what is different from two the same amount that two is different from one there\\nis no different between those two. So, in cases like that K medoids primarily winds\\nup not having this arbitrarily cluster center, because you really cannot any longer compute\\nthings like centroids there is no centroids there are no dimensions on which. So, what winds up happening is you nominate A data points to be the prototype and you\\nuse the same core concept of K means in that you first nominate a data point and then,\\nyou do an assignment, where each data point chooses between the nominated data points. And once you have one assignment you find the new nominated medoids; that should become\\nthe representative data point. It is kind of like the idea of being the cluster center.\\nNow, while this works an important point is that the computational intensity associated\\nwhich such an approach. Now, when you had 10 or 20 data points which\\nhad an assignment, so let us take all these data points. The computing the average through\\nsum of square just was essentially like using sum of square minimization was essentially\\ncomputing an average of this points across each of this dimensions. Now, you do not have\\nsomething like that with K medoids once you have an assignment to choose which, for this\\nassignment, what should be the prototype requires you to see the dissimilarities between each\\ndata point to each other. So, the number of computations that you need\\nto assess really grows. So, K medoids is often seen as a very computationally intensive approach.\\nSo, this should give you some idea about K means and K medoids. The next approach that we are going to talk about is hierarchical clustering with hierarchical\\nclustering you do not really windup fixing fix number of clusters. And essentially the approach itself starts with either one very large cluster and breaks it down step by step,\\nso the first one, so one way of doing it is called the divisive way, which is you have\\none large cluster, which has all the data points in it is and then, after that I break it in to 2 clusters and then, after that I break it in to 3 clusters.\\nSo, I look at the two clusters that were broken up as choose, how to break them down further.\\nSo, and that is called the divisive approach you also have the other approach, which is\\nagglomerative and definitely the more popular approach, which is bottom up, where each data\\npoint becomes a cluster, so you have all these. So, you have the number of actual cluster\\nyou have is equal to the number of the data points, because of each data point is a cluster.\\nAnd then, after that you choose the two closest clusters and merge them together and we will\\ntalk about how closest is defined. So, you choose the two closest clusters and merge them together. So, the end of the first step you are essentially having we had n data points\\nyou having n minus 1 data points, so the end of one step and after that the next step you\\nhave the n minus 2. Because, now you have n minus 1 cluster where all of them expect\\none have one data point and one clusters has two data points. But, you are just treating them as clusters and you are saying, now order this n - 1,\\nwhich are the two closest clusters that I can merge and that is seen as bottom of approach.\\nNow, because of how we do this essentially you are going to have nested clusters. Think\\nof this divisive approach if you created two clusters. Now, when you go to create three\\nclusters in this divisive approach you are going to take either cluster 1 or cluster\\n2 and break it further and the others is going to be same, so one break up could be that the cluster 1 stay the same and cluster 2 gets broken up into two pieces.\\nSo, in many ways the clusters that you are creating are nested within one another you\\ncan think of them is parent and daughter. And this same of approach goes for the agglomerative, where you started with many individuals clusters and you choosing to group them. It is never\\nlike you are breaking one grouping you rethinking an action that you did in the previous step\\nyou never in some senses going back in time and recorrecting a decision to either group\\nclusters or break clusters, so in some sense you can think of this also partly greedy approach.\\nNow, we spoke about, how we are going take in with especially agglomerative, which is\\nwhat we focus on the rest of lecture, because it is the more popular one and for very specific\\nreason that we will talk about you need to take in the first step. For instance you have n data points the n clusters and you need to take two closest clusters and merge them,\\nhow are you defining closest clusters. And the way you would kind of define them is through\\nsome measure of dissimilarity between the clusters. An example is for instance that there are many definitions of the dissimilarity one\\nthe three of them that are listed here really talk about it more in terms of come more from\\ngraph theory. And the first one is called single linkage and the idea behind single\\nlinkage as the means of talking about dissimilarity is that it is essentially when you take two\\nclusters you take the minimum distances between two data points that can be in the two clusters.\\nSo, do not think of it as much in terms in the first step with in each cluster there\\nis one data points, so it is not a very interesting case. Now, think of a case, where you got\\nthis cluster and you got three data points; that is what I have shown here and let me give you concrete example, so here is one cluster and there are three data points within\\nthis cluster. Here is an another cluster, four data points five data points. Single\\nlinkage basically takes each combination and sees, which combination is minimum, so in\\nthis case probably this would be minimum. And, so you know it is for that reasons it\\nis actually called min as a definition of dissimilarity. Complete linkage in contrast\\ntakes the maximum this is which, of this combination I need you take one from cluster A and you\\ntake from cluster B, which combination of points can I take get the maximum distances and I am probably guessing that this connection that is just I drew would have the maximum\\ndistances this group average, which basically takes every combination of every point to\\nevery other point and takes the overall average. Now, in addition to that you also have some approaches that try the more prototype based ways and there you would for each cluster\\ntry to create a centroid and from that centroid you look at the distance from one centroid to the other and so on. And there are some other approaches there is a there is a wards\\nmethod, so on, where again trying to take some kinds of a more prototype based approach\\nto define the dissimilarity. But, essentially hierarchical clustering it is all of these\\nstill come under broad umbrella of hierarchical clustering. So, there are two important things in connection with the hierarchical clustering as you can\\nsee, because it is doing this nested clustering, where you start from with agglomerative you\\nstart with each data points being a cluster and you keep merging them till you have this one mega cluster of all the data points. And in the other way around with this divisive\\nyou start this one mega cluster with all the data points and keep breaking it till each data points is it is own cluster. So, therefore, you have not really committed\\nto creating clusters of a specific size K and you can therefore, take you can kind of\\nlook at the results over all and make it make a decision in terms of what here size should\\nbe. One important property that we see with all agglomerative when you know in some divisive\\nmethods is that they possess this property called monotonicity. And the idea here is\\nthat dissimilarity between merged clusters is see there is some way of measuring the\\ndissimilarity of a cluster. So, that that dissimilarity between merged clusters is monotonically increasing with the level of merger and that should be fairly\\nintuitive. When you think of the dissimilarity of a cluster which has just one data points\\nthere is no dissimilarity it is only when you have two data points you can say this\\ntwo data points at different by this much. So, with agglomerative clustering in some\\nsenses the more number of data points that you have in to the clusters the greater of the amount of dissimilarity there and that monotonicity is strictly maintain with agglomerative\\nclustering. So, a very useful way of representing it therefore,\\nespecially when you have this monotonicity you can graph quickly represent the agglomerative clustering through something called the Dendrogram and what is shown on this slide is Dendrogram\\nand it is essentially this binary tree, which is plotted. So, that the height of each node\\nis proportional to the value of the dissimilarity between it is daughters. So, what is node\\nhere? Essentially you can think of each partition is being a node and the height of this node. So, let us take something very simple height of this nod as to do with this height as to\\ndo with the degree of dissimilarity between b and c between see you are forcing b and\\nc to be in a cluster now that is what you doing by this part of the graph that is what\\nit is doing. This height as to do with dissimilarity of b and c, which is why for instance perhaps\\nd and e was merge first. So, with agglomerative clustering think that you are going bottom up and you are sequentially making decisions. And, so if you choose to put d and e together\\nfirst; that means, d and e would have been less dissimilar to each other than b and c and that is why d and e was done first and then, b and c was separately done perhaps\\nlater. Now, the really good thing about this dendrogram is that, now you have a full picture\\nyou can, now choose to say I am interested in this situation where there are three clusters and you can essentially draw this horizontal line. And you have the three clusters cluster\\n1 is just data point a cluster 2 is the data point b and c cluster 3 is data point d e\\nf and g you basically see what goes in each of this limbs and that is that is essentially\\nyour cluster. So, at any level when you draw a horizontal line you can the number of vertical line cuts across is the number of clusters that you\\nhave in your thing. So; obviously, if you had a line out here you drew it here this\\nis 1 cluster and here it is the 2 clusters does, so, on. So, this should hopefully give\\nsome idea about the two very popular algorithms K means clustering and hierarchical clustering.\\nIn hierarchical more specifically K agglomerative. Thank you.\\nIntroduction to Experimentation and Active Learning\\nHello and welcome to our first lecture on Introduction to Experimentation and Active\\nLearning. This is the first lecture of a two part series on these topics and in this lecture we intend\\nto motivate the use of these techniques as well as the concept of reinforcement learning.\\nSo, the reinforcement learning is a lecture, a separate lecture that you will have from\\nProfessor Ravindran. And in this lecture we will motivate the need for these topics experimentation, active learning,\\nreinforcement learning, why are we talking about them in a data analytics course and we will also briefly introduce the topic of experimentation or design of experiments and\\nin the next lecture, we will continue with experimentation and end with active learning.\\nSo, let us get into the subject, where we take a broader look into data science and\\nanalytics. The core idea is that data science and analytics need data and if you were to go with the big\\nbuzzwords now, we might even need big data, you know where we can really gain useful insights\\nand this is quite fairly motivated with the easy availability of storage, the easy availability\\nto process data and the internet of things generating a lot of data.\\nIt is quite easy to get lots of data and analysis the data and come up with useful insights.\\nBut, in not every situation do you start with a data base full of data and in not every\\nsituation, is it easy to create this data. It might either be costly or it might not be, the data that you might have is not the\\nrelevant data that you need, and in many cases, you just have not started the exercise.\\nSo, for all those cases the big question is, basically do you have no scope for data analytics\\nand, the answer really is that, there is this whole other set of tools and techniques, the\\nquantitative tools and techniques where which focus really are not just the analysis part\\nof data, but have something to say about what data gives creative, which then goes and gets\\nanalyzed. And that is the focus of these lectures on experimentation active learning and reinforcement\\nlearning, which is that data science is not confined or data analytics is not confined to lots of data that are already available, but it is moreover an iterative process, where\\nsometimes the question of creating the data is also intrinsically looked into this grand\\nproblem statement. Now, sometimes we will not even give a second thought to this dichotomy of creating data\\nand analyzing data, because some problems just inherently come with this creation.\\nSo, if you take a look at many of the, you know inferential statistics techniques that\\nwe saw earlier in this course. Let us take an example where we used a two sample t test, you essentially had to sample\\nn number from, you know class a and class b.\\nSo, and you know compare the means, this whole process or sampling was in some sense creating\\nthe data. So, let me give you a concrete example, one of our favorite examples might have then that\\nour 10th standard girls. Is a average height of 10th standard girls higher than the average height of 10th standard\\nboys and there we said, we need to go to take a sample of 10th standard girls and sample\\nof 10th standard boys and we came up with a conclusion that and we did a hypothesis\\ntest there. So, when you do a sample, you are essentially creating the data and in other situations\\nas well, where you would doing some kind of an engineering experiment perhaps and somewhere\\nwe did not really use a word experiment excessively, but the idea is that sometimes we never thought\\nabout it, but those are fairly the simpler cases. There are a lot more cases, where you need to explicitly think rule, the creation of\\nthe data before any kind of analysis can take place and that is what we will focus on in\\nthis lecture. Now, when we speak about creating data, a very important factor becomes whether this\\nis an online context or an offline context for creating the data.\\nWhat do you mean by that? Online and offline do notÉ Online does not mean that you are in the internet, it means\\nsomething different here. What it means is, essentially when you are in a online setting; that means, you are experimenting\\nor you are creating data on a system that is currently creating a product or producing,\\nyou know involved in performing a service or a task, which is going to the end user\\nand for the purpose of getting this data, you are not just starting a passive observation\\nexercise. So, you are actually either querying the system or you either playing with the system in order\\nto get the data that you need. Now, that can be a problem sometimes, because you are actively interfering with the system\\nto create the data that you need and this system is right now either producing a product\\nthat is going to the end user or this system is a live system, which could influence the\\nexperience an end user is having and I am using the word end user in a fairly broad way.\\nIf you experimented on a traffic signal, the end user or the motorists need to go through the traffic signal.\\nIf you clear on with a manufacturing process which is producing a product, the end user\\nis the ultimate user of the product that goes and reaches the, you know the customer.\\nIf it is a process, then again the process itself has some outcomes. It either gives the end user some information or the process itself performs a task for\\nthe end user and all of these things could be getting compromised, if you are playing with the live system.\\nAnd so, a major area that we would be looking at is the one of the reinforcement learning,\\nwhere you are looking at an online system. So, you care about learning, where you care about this learning in a very supervised learning\\nframe work, where you have some outputs and you want to understand how these inputs effect\\nthe output, that is one side of the story. But, this is second side of the story which is, you do not want toÉ You are interested\\nin doing as well as you can, even while you are experimenting. Because, some consumer or end user is experiencing the effects of your experiment and you yourself\\ncould be, the experimenter could also be the consumer. So, we are going a little abstract out here.\\nSo, but the advantage of going this abstract is that you can really envision any scenario and any domain and this kind of a frame work should apply there.\\nSo, we will see that primarily, the online setting or I liked to kind of call it the\\nlive setting, live experimenting gets covered in reinforcement learning and those lectures\\nwill support that. Now, in going forward now, in this lecture and the next one we are primarily going to\\ntalk about an offline setting. What we mean by an offline setting? Just the opposite of the online, which means, that the unit that we are experimenting on\\nis not producing products that are going to go to the end user.\\nSo, you either created and you can do this in many ways. So, if you want to conduct an experiment and you want to gather data, you can create a\\nmodel of your system and go experiment on that model, you can artificially create a\\nlab setting. Let us say I want to experiment on what fertilizers work on my fields.\\nI do not have to go pour those fertilizers on my fields, I can actually create a green house and choose some specific plans and try these fertilizers out and I can essentially\\ncreate this lab setting, which is suppose to mimic the real world. But, it is not always creating models I mean it could be creating models, it could be creating\\nartificial environments, it could be creating computer simulations and then you go experiment\\non that. But, it could also be the real process except, now if turned off the real process.\\nSo, let me give you an example. Let us say you wanted to do an experiment on a machine and this is a machine that used\\nto manufacturing and you wanted to know, what how to set various boiler plate stuff on this\\nmachine. So, you wanted to know what the speed should be, what the turning radius should be, different,\\ndifferent parameters associated with this machine. Now, if the machine is currently manufacturing the product which is going to the end user\\nthat is the problem. But, what if you stopped the entire manufacturing process and you said, we are going to now\\nexclusively commit some resources to experimentation. So, we learn about this system and so you clear on with the machine, clear on with various\\nsettings on the machine, make it you know create products and then you measure the products\\nand see how well you did or how badly you did and you learn about the systems, you created the data, you analyze the data, you learnt about the system, but these products that\\nare sacrificed in some sense, they are not going to the end user. In other words, you do not care how well you do when you are experimenting and that is\\nthe core of offline experimentation. A bad experiment is not one that gives you poor results, but a bad experiment is one\\nwhere you cannot learn about the system, your focus is about creating data to learn about the system and that is not mean there is no cost to experimenting.\\nThere is a cost, but you can think of it as a fixed budget that has been sacrificed or\\nyou can think of it as there being no cost at all. However, you want, but it is not that while you are experimenting you are trying to perform\\nas well as possible that would be the online setting. So, now, that we have understood this difference.\\nIt is also important to understand the difference between observational data and offline experimental\\ndata and I use the word in DOE. So, for the first time you started using the word DOE and here we mean design of experiments,\\nit is a more formal way of talking about experimentation of talking about statistical experimentation.\\nAnd the idea here with this difference between observational data and offline experimental\\ndata is the fact that with observational data you are not interfering with this system.\\nSo, let us go back to the original problem. We said, hey how do you do data analytics when you do not have data.\\nOne approach is to say, so I need to start collecting the data and so perhaps I will\\nturn on a few sensors or put some sensors in certain places and I have start collecting the data.\\nNow, that in itself is not the subject we are talking about here, that is just turning\\non this switch of collecting data and once you have the data, you analyze the data and that is fair game that is data analytics in some sense.\\nBut, that data analytics has coming from observational data.\\nWhat we focusing on right now is, is there some way for me as the agent that wants to\\ncollect the data to actively engage for the system and choose, what data need gets collected\\nand that is what we will be doing both in design of experiments and active learning. You are in an offline setting, meaning that you are right now committing all your resources\\nto collecting the data and you can collect whatever data you want. But, the point is that you are going to be controlling what data gets collected and as\\na result, the only way to do that is not to passively observe the data that get generated,\\nbut do actively in the case of design of experiments you will actively go and change some of the\\nsettings in a system or very specifically go query certain points and that is how the\\ndata gets generated. A typical problem statement and experimentation would actually say, go set the machine to\\nsetting a and setting b and setting, setting c and then let us see what the output is.\\nAnd in active learning is seen a little bit more as that entire data, the input space\\nis available and you get a query a particular point in the input space and then get the\\nanswer. Now, these are just two difference ways of describing it, because they come from slightly difference context of application, but the core problem statement is the same which is\\nthat you as the user and experimenter or learner has the choice of generating an output at\\na pre decided point in the input space and that is has some critical difference over\\nobservational data and lot of advantages just to give you some intuition, the biggest advantage\\nis one concerning multi collinearity, if you just observe the data it is possible that\\ntwo input variable are so highly correlated to the point where there is almost a perfect\\ncorrelation. In which case you would never know if the output was increasing or decreasing as a result\\nof variable input variable a or input variable b, because input variable a and b are so highly\\ncorrelated. So, in the case of design of experiments or active learning you would come to that realization\\nat some point without passively observing data and say o we need to try out an experiment\\nor we need to query a point, where input variable a is high and input variable b is low and\\nvise versa to try and understand which of these two input variables is having an impact on the output.\\nSo, let us start with focusing our first lessons on experimentation and design of experiment.\\nThe core idea design of experiments is that as long as you can conceptualize the operation\\nof a systems has some combination of inputs which when use together results in outputs,\\nyou have the scope for this kind of black box creation and a black box I mean essentially\\nand understanding of the way the inputs and the outputs relate to each other. So, the inputs themselves can be anything you know they can be really broad, the only\\nimportant thing is you want to be able to quantify them in some way. The black box that you are experimenting or can be anything, it can be a process, it can\\nbe a system, it can be an organization which is performing certain organizational functions,\\nit can be an actual product. And typically what are the outputs you are interested in?\\nThe result of the black box could be the some products being created and you can measure these products and therefore, measure how good or bad they are.\\nThe output could be some services or tasks that are created by this black box and again you should be able to evaluate the output.\\nAnd the third is a little bit more abstract which is the information is getting created, again as long as you quantify these outputs and quantify the inputs you have a system\\nwhere there is scope for experimentation. So, formal experimentation what is it and how is it different from just observing data\\nand analyzing. Formal experimentation essentially involves systematic, it is a very important word systematic\\nand purposeful changes that you make two input variables in an attempt to gain knowledge\\nabout this system and or find the ideal settings that result in the best output.\\nSo, that sentence is little long wind it, but let us kind of break it down, so the idea is that in experimentation you proactively go and systematically or purposefully change\\nthe input variables it would mean that you actually go and say, oh I need to understand\\nabout little bit about the systems. So, I am going to try setting input variable a to a particular value input variable b to\\nanother value and there I am going to go look at what output I get and the purpose of this.\\nNow, in some rare cases it could just be that you are not physically changing the input\\nvariable, but you are physically choosing the input variable that you want to observe.\\nBecause, you do not have the information about how the output is going to look at every point\\nin the input space. So, you are not making a modification to the system there exists this large enough repository\\nof information, which is very expensive to query. Because, if it is not all you have is a huge data set which requires a supervised learning\\ntask. But, for some reason if this is very expensive to query you could also think of experimentation\\nand light of choosing very carefully the input variables that you want to gain knowledge about.\\nBut, more often than not the typical context is that you have this system where you go actually change the input variable set them to different values.\\nSo, think of this perhaps this foundry process, where you are trying to create castes and\\nyour input variable could easily be the temperature of the molten metal that you pouring in, the\\npressure that is being applied the kind of material that the cast is made off and your\\noutput could be the number of defects that you see in the cast.\\nNow, in an experimental process you will go and systematically purposefully try different\\ntemperatures, different pressures, different materials to make the cast, different practices\\nin the cast, how long be you weight before you open out the cast, what kind of a room\\ndo you place it, you would go actually physically change these settings to different values\\nand observe what happens to the output, which in this case is the number of defects you see.\\nSo, the emphasis here is in the systematic and purposeful changes to the input variables. Now, why do you do it, you could do it for two reasons, you could do it to gain knowledge\\nand you or you could do it to find those settings that you want to set the inputs to get the\\nbest output. Now, the gaining knowledge could just be an independent process that could be the final\\ngoal, sometimes the approach is to gain knowledge and once you gain the knowledge.\\nAnd what knowledge we are talking about here? We are talking about the knowledge associated with how the input variables affect the output\\nvariables. Now, you could then once you gain this knowledge use that information to figure out what are\\nthe ideal settings for the inputs. But, you also have a algorithms which say you know what I do not care about the knowledge\\nmy goal is to right now find out what those settings should be. So, that I get the best output and that is takes on a different form.\\nSo, that is the core idea of experimentation. So, this point a very natural question arises see you got some two or three or four input\\nvariables and you got an output variable. What is the problem?\\nWhy do not we just clear on with each input variable, you know one at a time and see what\\nis the best setting for each input variable and do this sequentially, turns out it is\\nnot that simple. This is simple problem with that and we I am going to illustrate that with this diagram.\\nLet us say that this plot that you has two input variables, input variable one x called\\nx 1 and input variable two called x 2 and the output is nothing but, a hill that is\\nprojecting out of this green and this hill is shown with a contour plot.\\nA contour plot is basically is one which connects which is often seen in maps, where you basically\\ndraw a line, where the height about sea level usually is the same.\\nSo, here imagine that there is flat surface which is the base the rectangle and then there\\nis hill projecting out of this flat surface, out of this screen and coming towards your\\nface as if you are looking at this screen and these eclipse is that you see on this\\nscreen these kind of circular looking things are nothing but, the contour plots.\\nSo; that means, everything that is on this line is 70 meters or feet or inches whatever\\nyou choose to think of it. So, this is an example of response surface are basically try to characterize in this\\npicture, how the variables x 1 and x 2 have an influence on the output, why which is the\\nhill coming out of the screen. So, it is an abstract concept, now let us take a look at what happens, when you just\\nplay with one variable at a time and we are going to call that adaptive one factor at a time experimentation.\\nThe idea behind this algorithm is that what I am going to do is at any given point of\\ntime I am going to play with anyone variable. So, I am going to start with let us say x 1 and I am going try different values of x\\n1 and I am going make a conclusion at some point by saying at what value of x 1 did I\\nsee the best y and I am going go with that I am go on a fix x 1 now to that value and\\nplay with x 2. Now, sequentially do that with all the input variables until I come to a conclusion.\\nNow, take this example where I stop playing around with x 1 and I arbitrarily fix some\\nvalue for x 2 and it turns out that I fix the value right here. So, I put a star there, so I started with x 2 set to this value and I just started playing\\nwith x 1. So, what is that mean when you playing with x 1; that means, you keep changing x 1 and\\nhere we are actually just changing x 1 to different values and as you go to through different values you see different heights.\\nFor instance, when you set x 1 to this value you seeing a height of 70.\\nWhy? Because, this is the contour line of 70 when x 1 is equal to this value you see a height\\nof 80 and so you keep going through this process and until you find the highest point and the\\nhighest point is here, because of this point you are touching 80 for instance at this point\\nat x 1 you are not touching 80 your some across the 75. So, you conclude that this is the best value of x 1 and then you set x 1 to this value.\\nSo, x 1 gets set to this value and then you go about changing x 2.\\nSo, and basically when you changing x 2 you are staying on this grey line out here and\\nas you keep going through x 2, you find that the point where there is a star is the peak\\nand you conclude that is a highest point. So, you choose to set x 1 and x 2 such that you are at star.\\nClearly, what is the problem with this, your problem is that you miss the peak, there it\\ntwo problems on this, one is that you miss the peak the peak was really out here, if you will take look at this contour map in your understand the contour plot, the plus\\nsign is where the peak is and you erroneously concluded that the star is where the peak\\nis. And the reason for it is fairly simple, the way this hill is drawn it is clear that x\\n1 and x 2 have some interaction effects, there is x 1 is really good out here, when x 2 is\\nset to this value. Now, if you said x 2 to another value, let us say x 2 here the highest point of x 1 is\\nprobably somewhere here. So, this value of x 1 winds up being highest.\\nSo, it really what we mean by an interaction is where you conclude x 1 to get the best\\noutput depends on where you set x 2 to. So, this is interactive effect that you can sometimes gets fooled by, there is an another\\nthing that we have not really discussed here, which is that few do an experiment at a given\\nx 1 and x 2 setting are you always going to get the exact same value and the answer is\\nprobably not. Experimentation is typically carried out in sarcastic setting meaning that even if you\\nunderstand that your output y is some function of your input variables, in this case there\\nare two input variables. So, there is some mathematical function that is associated with input variables x 1 and\\nx 2 and the output variable y, but on top of that if you said x 1 and x 2 to specific\\nvalues are you going to get the same y. The answer is in a stochastic system you do not, because there is another factor which\\nis just you can call it noise, you can call it irreducible error, you can call that luck\\nwhatever it is, it is just concept of just there being uncertainty above and beyond you\\nare and this kind of uncertainty is what we deal with in supervised learning is what we deal with in much of what we have discussed and in this course.\\nSo, that aspect also can throw you off in an approach like this.\\nNow, this has been illustrated to you in a continuous frame work, where you are able to change x 1 continuously and see different, but in more practical settings you do not\\nhave infinite experiments. So, you might just set x 2 to a particular value and sample x 1 at someone two or three\\npredetermined values, more often than not in design of experiments you will see that\\nwe are only interested in linear effects most of the cases you are interested in linear effects.\\nSo, you would really look at trying out each variable at two different points.\\nBecause, where two points you can draw straight line and the two points are typically get\\ncoded. So, what is another approach that you can do, that you can employ to overcome this problem?\\nThe other approach you can do to overcome this problem and one of them is defined is called is a broadly called as orthogonal arrays and a specific type of an orthogonal arrays\\ncalled the full factorial which is what I show here. So, take a system where variable A can take on two states.\\nSo, let us go back to this casting problem and let us say you are interested in variable A which is let us say the temperature of the molten metal that is poured in and so the\\ntemperature could be something like 250 Fahrenheit and you might be interested in studying the\\neffects at 350 Fahrenheit. Now, I am not an expert in this I do not know those are reasonable temperatures for molten\\nmetal I am guessing it is a little too low actually, but who knows. Now, typically what you do is when you have just these two settings, you kind of lay code\\nthem and you called them 250 as minus 1 and 350 as plus 1. Now, you do the same thing with the second variable input variable of interest, now variable\\nB could be something like pressure, where you have lope or let us say the time that\\nyou weight before you remove the cast. So, that could be 1 hour or 2 hours again I do not know those are reasonable numbers,\\nthey to illustrate a point is to 1 hour again you call it minus 1 and 2 hour you call it\\nplus 1. So, what you go about doing in a full factorial is you try every combination of every variable\\nwith every other variable. So, you try the minus 1 minus 1 setting, you try the minus 1 plus 1 setting.\\nSo, and plus 1 and minus 1 setting and you get the picture, you essentially try every\\npossible combination and that is called a complete enumeration of the designed space. Because, you have discrete data points and you might choose to do some experiments at\\neach of these combinations of points.\\nIn this example of shown you two replicates these are called replicates and these are both the same output variable y, but you choose to take two readings, it is actually unfair\\nto call it two readings, because it is not like you do the experiment just once and just use the same measuring device and just take two readings, you actually redo the experiment\\nand the reason you do that is because of the concept we just discussed which is you acknowledge\\nthat your y is some function of in this case variables A and B.\\nBut, on top of that the sum error, this error often you sought of is being Gaussian which\\nsum with mean 0 and standard deviation equal to sum value. But, even without going to there.\\nThere is just some noise and you can see that even though you set A to minus 1 and B to minus 1 first time around you got 57, second time around you got 56 and you see different\\nlevels of uncertainty. Now, the same concept extended to three variables is what shown here, on the right hand side\\nand you have so a, b, c three variables and I am also showing you a case where you are\\nnot just interested in two levels, but you might be interested in three levels. So, variable A has 3 levels, variable B has 2 levels and variable C has 2 levels.\\nSo, complete enumeration of them would be nothing different than three times, two times, two which is equal to 12 and we call the 12 treatment combinations.\\nSo, you can have 12 treatment, 12 rows out here and again here I am choosing to take\\ntwo replicates just to get a better idea of the noise that is there in this system as\\nwell. But, the hope is that to taking on such an approach would enable us to perform a comb\\nof analysis on the data which helps us understand which helps us not get strict by this Gaussian\\nthis noise that is there on the system which can make you conclude the wrong things, if you an art where of it and at the same time also understand that they can be some interactive\\neffects between A and B or A and C or B and C and that is about we will be focusing on.\\nSo, in this part we are focused little bit more and how the experiments are itself designed and this is just one way of designing experiments and this is called the full factorial design\\nand there are other ways of doing the same thing. In the next lecture we will be briefly talk about, how do you analyze the data that you\\ngets from such an experiment and we close to be talking about active learning. Thank you.\\n \\nIntroduction to Experimentation and Active Learning(contd)\\nHello and welcome to our second lecture in the series, where we talk about Experimentation\\nand Active learning.\\nIn the first lecture we briefly spoke about, we motivated the idea of we using active learning\\nexperimentation or re enforcement learning in a broader data analytic setting, where\\nwe said these are some approaches that are fairly relevant when you do not have data,\\nwhen data does not exist or the data exist and it is not enough or you have only partial\\ndata.\\nIn the first lecture, we also went into design of experiments and spoke about, how perhaps\\nan approach, where you sequentially change one variable at a time need not be the best\\nway to conduct an experiment.\\nAnd we also spoke about something called orthogonal arrays and specifically, there we spoke about\\na form of experimentation called the full factorial design, where it is essentially\\na complete enumeration of a discrete input space, where even if you have continuous input\\nvariables you break them into 1, 2 or may be sometimes 3 discrete points.\\nAnd you essentially do a complete enumeration, which means that every variable is set to\\nevery possible value it can be set to with relation to every other variable being set\\nto all their possibility.\\nSo, all the possible points in the input space are essentially looked at and that was essentially\\nlooking at the design meaning, what points in the input space to you choose to experiment.\\nIn today’s lecture we will briefly take the just full factorial, which is a very basic\\ndesign and talk about some approaches that are used in analyzing such an approach, such\\na design.\\nSo, jumping into the subject in this slide, what we have is a full factorial design associated\\nwith three input variables A, B and C. And the idea here is that what we have is a full\\nfactorial design, because A can take on three values minus 1, 0, plus 1 and B can take on\\ntwo values and C can take on two values just minus 1 plus 1 and that gives you 12 combinations\\ntotally and we have taken, the output variable is Y.\\nNow, I call them Y 1, Y 2, Y 3, Y 4, only because they are Y 1, Y 2, Y 3 and Y 4 are\\nreplicates, so it is a same core variable.\\nBut, essentially you conduct the experiment at, the settings for instance minus 1, minus\\n1, minus 1.\\nYou conduct that experiment 4 times; again it can be a parallel effort or a sequential\\neffort either way.\\nWhat we mean by that is, when we say we conduct the experiment 4 times, you might have one\\nexperimental unit and you separately conduct the experiment 4 times on it or you could\\nhave 4 experimental units and you might, you choose to parallely try the same setting of\\nminus 1, minus 1, minus 1 on those four different experimental units.\\nBut, essentially this is almost a setting, where you conduct 48 separate experiments\\nand in design of experiments, language that just called 48 trails or 48 runs and these\\nare essentially your results, these are your outputs.\\nAnd the convenience of this is sometimes you can look at this raw data or you can look\\nat the average Y and this is nothing but, the average for instance for the average 80.75\\ncomes from taking the average of these four numbers.\\nSo, each row is average and it is represented and that is the, those are the results that\\nwe haven, this is essentially what we look to analyze.\\nSo, what is one way that you can take these results and arrive at a conclusion of what\\nvalues A, B and C should be set to, because that is kind of the goal.\\nThe goal is to figure out, what is said A, B and C to, I mean accurate one of the goals\\ncan be on to what values to set A, B and C; such that you get best Y and here we are going\\nto treat best as the highest value.\\nSo, how should I set value A, B and C, so I get the best Y?\\nOne approach is called the classical analysis.\\nThe idea behind classical analysis is to take each variable individually, each input variable\\nindividually and ask the question, at what setting am I getting the best results.\\nSo, A is set to minus 1, A is set to 0 and A is set to plus 1.\\nSo, if I am ask the question, what is my Y on average when A is minus 1, what is my Y\\non average when A is equal to 0, what is my Y on average when A is equal to plus 1 and\\nI look at these three numbers and I will choose the value of A, where my average Y is the\\nbest.\\nI will similarly do that for to B and C and I will come up with a recommendation on that\\nbasis.\\nSo, what is that look like for this data?\\nIt is a fairly straight forward calculation, when A is set to minus 1 you essentially get\\n81.81.\\nSo, it just means you can think of it in many ways, you can think of it as the average of\\nthese data points.\\nRight here, what I circled and the average of these data points or you can think it as\\nthe average of these data points, because ultimately each row comes from an average\\nfrom that respective row and the sizes are equal.\\nBut, you can think of it either way, either way this is the average Y when A is set to\\nminus 1 and that is 81.8.\\nAnd similarly you get an average for 0, you get an average for plus 1 and you would basically\\nsay, I like A at minus 1 it is giving me the best result.\\nSimilarly you do it for B at minus 1 and B at plus 1.\\nNow, when you do it for B at minus 1 and B at plus 1, how do you take the average?\\nIt is the same principle.\\nSo, you would for instance at B at minus 1 you would be interested in taking the average\\nof these points, B is minus 1 at these points, so it would ultimately be the average of these\\npoints.\\nSo, essentially that average would be B at minus 1 and that is 77.83.\\nSo, you do the same process and it is clear that A at minus 1 is the best, B at minus\\n1 is good, because B at minus 1 is greater than B at plus 1 and C at plus 1 is good,\\nbecause that is better than C at minus 1, so that is essentially classical analysis.\\nAnd it is a fairly heuristic approach and it is like a first cart approach.\\nNow that is one way of going about the analysis.\\nAnother approach is to take the best.\\nThe take the best essentially says, I am going to look at that treatment combination, which\\ngave me the average highest average Y.\\nSo, would that be at here?\\nSo, it looks like 85.75 is the highest average Y and that setting is I believe A 1, 1, 1\\nand, so we would essentially go with that recommendation.\\nSo, take the best would have selected A is equal to 1, B is equal to 1and C is equal\\nto plus 1.\\nSo, here are two fairly contrasting approaches and the question you need to ask yourself\\nis, where would you want to use classical analysis and where would you want to use take\\nthe best.\\nAnd to answer that question we need to answer that question in terms of, if you want to\\ntake a one factor at a time approach, what are two reasons that experiments that particular\\napproach fails or for that matter, what is the two major difficulties with design of\\nexperiments.\\nThe two major difficulties are the following.\\nOne is that, you could have some interactive effects between the input variables, which\\nmeans the effect that n input variable will have on the output variable really depends\\non how some other input variable is set.\\nSo, that is called an interactive effect.\\nThe other reason is that sometimes you get fooled, because yes, Y is some function of\\nA, B and C from top of that, there is also some noise.\\nSo, that noise would have given you results, which you are erroneously interpreting and\\nyou are essentially over fitting and you are getting fooled.\\nNow, the question is, under what circumstances would classical analysis work and under what\\ncircumstance would take the best work and the quick answer is, a classical analysis\\nessentially works really well in an environment of high error or noise.\\nSo, when this, when the noise, which is so we said Y is equal to f of a, b and c, but\\nit is also got this noise component and when this noise component, although this there\\ncould be no bias to this noise.\\nSo, this noise could be something like it is normally distributed with mean 0 and standard\\ndeviation, some standard deviation.\\nThen, if sigma e is very high and, so it is a very noisy environment, then classical analysis\\nwould work quite well because it is averaging a lot of data points.\\nNow, where it move to work well is when there is lots of interactions, so you need low interaction\\neffects for classical analysis to work, because it just taking the average at A and average\\nat B. In contrast, take the best would work only\\nwhen there is very low error or noise.\\nIt would not work well when sigma e square is high, but because it just takes the best\\ncombination, it is almost like it does not care about the interaction.\\nSo, high interactions work very well in its favor and, so you would you take the best\\nthere.\\nNow, the reason we talked about these two heuristics is to really motivate the statistical\\nway of doing things, which is this dichotomy that you see between high noise versus low\\nnoise and fitting to any shape you want versus not being able to fit any shape you want has\\na lot to do with something you seen before, which is the bias variance dichotomy.\\nAnd this bias variance dichotomy is what you seeing in these two extreme approaches.\\nThe statistical way can sometimes balance that out and, so the truth is almost any supervised\\nlearning algorithm could be fair game, the only context is that the data set is fairly\\nsmall.\\nBut, any data set, any supervised learning algorithm that is not require a very large\\ndata set that requires a very small data set, but they still give you meaningful results\\ncould be a fair game.\\nAnd in that regard step wise regression is usually popular in analyzing designed experiments.\\nYou start with the basic model that Y is equal to linear function of all the inputs a, b\\nand c and then, you also try to incorporate two way interactions in the form of saying\\na times b, a times c, b times c and you could even have a three way interaction a times\\nb times c.\\nAnd, because your variables are coded to minus 1 and plus 1, just multiplying a and b as\\nan input can have a meaningful interpretation.\\nNow, one thing that can be said about this process of design of experiments is the way\\nwe have done it with full factorials and the way we explained it makes it essentially a\\none shot approach.\\nIt basically means you decided even before you see any results the entire set of points\\nthat you want you look at the input space.\\nSo, if you look at this you already decided, so each treatment combination is a point in\\nthe input space.\\nWe already decided the all the points and how many times you want to look query each\\nof these points even before you look at even one result.\\nSo, it is ideal in some sense for a parallel deployment, but if you could do the sequentially\\nis there a better way of doing it, which is can you react to the data your seeing to say\\nin want to query this point more, because I am less certain about this point or variable\\nverses saying o I am very confident about something else.\\nSo, I do not want to waste my resources in conducting experiments in a particular place\\nand that is primarily the motivation of sequential experimentation this idea that you can you\\ncan conduct experiment sequentially and that gives you an opportunity to react to the data.\\nNow, in many ways while sequential experimentation has been an effort from the statistics community\\nactive learning essentially is this same core idea and it is been motivated more often the\\nmachine learning community and the context in which, the problems are applied to do sometimes\\ndiffer as the result.\\nActive learning is often seen as a semi supervised learning approach and understandably it is\\nalso called is optimal experimental design.\\nBecause, active learning is a process where the system sequentially chooses to query the\\ndesign space and therefore, be able to build knowledge, on what we see and how it can be\\nbetter understood.\\nThe key different, especially in a in the typical context of application except while\\ndesign of experiments often focuses a lot on the sole idea of starting with 0 data often\\nactive learning will start with this notion that there is some amount of data and it is\\nnot enough and you might want to sequentially query the system to improve upon your understanding\\nof the system itself.\\nAnd, so in many ways it is kind of seen as semi supervised learning, because there is\\nan abundance of unsupervised learning data just means there is an abundance of states\\nof the input space and you sequentially get you choose points in the input state for,\\nwhich you really want to get answers.\\nAnd therefore, get outputs for and by doing that the whole idea is that you can get away\\nwith much less data on the output space and you can still come up with predictions that\\nare meaningful.\\nSo, what are some prominence strategies in the whole active learning frame work is that\\nin a sense all of these strategies at we going to talk about now, rely on one thing.\\nIt relies on you know evaluating the, how informative different points on the input\\nspace can be if you query them and you got an answer, what you mean by queried is you\\nchoose a particular point in the input space and say can you please give me an output for\\nthat.\\nAnd that, now goes into your data set, where you can apply some kind of supervised learning\\napproach to make sense of, what you have.\\nAnd the strategies that are involved with active learning can be broadly classified\\nin these four and you know this these is an area of you know where there is a active research\\nand, so there they might be some strategies that also follow fall that are new that might\\nfall out it these have been historically the more popular ones.\\nSo, let us take look at the first one which, is uncertainty sampling, now this is perhaps\\nsome more simplest and also therefore, very fairly common frame work.\\nAnd in this frame work essentially the active learner chooses to query instances, which\\nit is least certain about, how to classify or how to predict.\\nSo, if it is a classification problem it says I am going to ask questions about I am going\\nto choose points on which, I want answers on which, I want you to give me an output\\nand I am going to choose points, where I am least certain currently given the information\\nI already have I am least certain about if it is a classification problem 0 and 1.\\nI am least certain about whether to classify it as 0 or 1 in those instances I really want\\nyou to I want to conduct my experiment in that point, where I am least certain.\\nNow, if you can also think of this as regression problem, where at a certain point in the input\\nspace you might have a prediction, but that prediction is not cast and stone some kind\\nconfidence bound.\\nYou have some kind of bounds around that predication is some amount of uncertainty associated with\\nparticular predicted value and you might choose to pick the point where your uncertainty is\\nthe highest.\\nAnother approach also fairly popular one is this idea of query by committee, let me just\\nmark, where we are we finished this and the second is query by committee.\\nQuery by committee approach essentially involves having committee of different models, which\\nare all trained on the current data set and when we say data here we are talking about\\nthe data set for which, we have the outputs you have a data set with the outputs in those.\\nSo, you have a some kind of supervised learner, which is capable of interpreting the data,\\nbut you might have a committee of models, which are all trained on the current data\\nset, but they might represent competing hypothesis.\\nNow, each of these members is now, allowed to vote and you basically choose to take you\\nchoose to get a data point from the input space, where the committee members have the\\nmost disagreement.\\nSo, think of it you know one way to one example that I always kind about liked about this\\nis it really maps on to lot of ensemble methods something that we saw earlier in the course.\\nSo, if you had a set of methods set of supervised learning approaches all trying to predict\\nthe same thing.\\nSo, think of it as a random forest, where we have multiple trees trying to predict same\\nthing.\\nSo, for a given input vector each tree does not going to come up with the exact same class\\npredication or it might not come up with exact same predication even if it is the output\\nis continues variable.\\nSo, this method simply says let us go with let us go get an answer for a data point,\\nwhere the trees disagree most about what to classify it or what where are the highest\\nvariance in the predicated value of the models.\\nThe next approach is expected model change the idea behind expected model change is to\\nsee if we knew the label of a particular data point.\\nThen, which label of the input space if we knew would contribute to the greatest change\\nin the current model we have based on the current data we have of the inputs and outputs.\\nSo, we have some data on the inputs and outputs and you basically extrapolate to this to the\\nbroader question of asking the question saying you could get data of you could get an output\\nfor any point in the input space, which point would essentially lead to you making the largest\\nchange in the model.\\nAnd the idea is to kind of query that that point very specifically the last two sets,\\nwhich is the expected error reduction and various reduction use the following approaches\\nthe approach is to basically say with error reduction the idea is there some deviation\\nof the predication verses actual.\\nSo, you can think of it as essentially the residuals in a regression case and you know\\nin other in every other case other it is bias or variances for whatever reason you are unable\\nto predict you are unable to predict the exact value at a particular location.\\nThe question we need to ask our self is and answer to which, point in the input design\\nspace could lead to the largest expected error reduction.\\nIn that sense its it is fairly close to a uncertainty sampling, but it is not just uncertainty\\nsampling is the where it really differs from uncertainty sampling is, uncertainly sampling\\nask this question about each data point in the input design space with respect to that\\ndata points.\\nSo, I go to one data point in the design space and ask the question saying, how much uncertainty\\nthere in that point.\\nIn terms my predication at that point, where as expected error reduction does not talk\\nabout a single point it talks about, how if I got and answer to a question at any particular\\npoints.\\nSo, I go to a particular point to the input design space and I query the oracle and I\\nget an output.\\nNow, that output if I now, refit my supervised learning with this extra data point there\\nis going to be overall reduction in my residuals across the boat, because this new data point\\ncould change the entire regression line fit.\\nNow, this new regression line fit will create new residuals and, so I am looking at the\\noverall reduction in error between the data points and the fitted model.\\nAnd, so I choose to query that data points in the input space and get an output for that\\ndata point, which lead to an overall reduction in error.\\nNow, the variance reduction approach is a deviation from that it is a deviation in that\\nit is you do not look at the error between the fitted model and the data points, But\\nyou just look to see reduce the overall variance in the output space.\\nNow, that is done probably because it is much easier to do this, but what the core idea\\nhere is that you have some variance associated with the outputs.\\nAnd you can still continue to reduce generalization error indirectly by minimizing output variance\\nand that also can sometimes you do that because it is mathematically a easier you can kind\\nof get close forms solution an and that is essentially the core idea.\\nSo, I hope this gives you some feel for experimentation and the whole idea of active learning and\\nat least some strategies that we could use an experimentation of active learning.\\nThank you.\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw File_2.txt'}, 'embedding': None, 'id': '764493da05d5027d61c906b1daa1396b'}>, <Document: {'content': \"\\n1)Introduction\\nhello and welcome to the course introduction to data analytics my nameis nandan sudashanam i'm a faculty here at iit madras department of management studies and i'm ravindran a faculty in the computer science and engineering department here at iit madras we are really excited to be bringing you this course the focus of this course is to introduce you to the tools and techniques that are used currently for understanding data and to derive useful knowledge from it so the course itself follows the broad contours of the different types of data analytics that are there the early part of the course is going to focus more on data analysis through statistical methods and in that we will be covering descriptive statistics which is the idea of how do you describe data how do you represent or summarize data and in that we will be covering lots of visualization techniques and so on and then we move on to inferential statistics the context here is that you see the data as a sample as a sample from a broader universe that's generating this data and the idea is at this point you're not content with just summarizing the data you really want to say something about this broader phenomena that is generating this data so can i make some kind of generalization or some kind of inference about this data so this leads us to the main part of the course which is machine learning and data mining so here we'll be diving deep into the art and science of predictive analytics right in predictive analytics we are looking to build algorithms that build that learn models from historic data that relate one or more variables okay so note that the emphasis here is on automatically learning these kinds of models from the data and therefore we'll be focusing more on the algorithms that learn these models rather than the models themselves so the models these algorithms learn should be such that they are able to predict values about different variables given only a subset of the variables that describe the data in addition to learning these models we will also be looking at learning interesting patterns in the data as well as any hidden structure that is present in the data we will finish the course with the final section on prescriptive analytics this is what you might have heard as data-driven decision making the idea here is that you in the earlier part of the course you've learned to make predictions you've learned to mine for interesting patterns in the data you've learned to make inferences but how do you use this information to make concrete decisions i hope this has given you a broad overview of what we seek to cover in this course and we really look forward to having you join us thank you see you soon\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'c97eea9ef3363867876b4f864037e90c'}>, <Document: {'content': '2)Course Overview\\nHello, and welcome to our first lecture for the course Introduction to Data Analytics. In this lecture and perhaps the next few lectures, I am going to be providing you with the Course Overview and giving you a detailed description of what we will be covering during this course. Let us start off with a little bit of logistics associated with the course. First thing is, there is a good amount of information available on the course website. So, I just wanted to make sure that everybody can avail of the course outline, the syllabus, the reference books and so on, it is all uploaded on the NPTEL website and you should access it, so you have the appropriate information. The second thing is that the forums in the course are a great resource for answering many of the questions that you might have or that you might come across during the course. The most of the cases, the professors myself and Professor Ravindran, we will try to answer some of your questions, at times we will have teaching assistants help us with answering questions and most importantly, many of these questions can be answered by your own pears. So, I really encourage you to use that resource and also contribute to it in any way that you deem fit. If for instance we feel like a particular question is not been answered appropriately, clear it keeps coming up; we will definitely join in and get in on the discussion, so definitely try to use that as a resource. Before, I started there are already a lot of questions on the forums, so I just wanted to address one or two of them, they pertain to the course at large. So, the first question relates to the course style. For instance, is this course going to be very theoretical, is it going to be a very case study base, so on and so forth. So, to give you a feel for what this course is going to cover, the course is not going to be only theory or highly theoretical, we are not going to emphasize extensively on the pure math of the course. They might be some amount of math and some amount of programming, but we are not going to go in depth and derivation and so on. The course is going to be heavily conceptual and it is going to try and have as many applications as possible. So, we are going to give you the whats and whys of data analytics are ranging from the statistics to the machine learning, what techniques and tools will you apply, where, why do some methods work in certain situations and not others, how do these algorithms work, what is the background logic behind it and we are going to give you lots of applications. We will be assisting you with little bit of how. So, how do you implement this algorithm or how do you, what kind of software could you potentially use and so on, but the course itself is not going to be a tutorial style course. So, it is not, you know anything that you can essentially get from the help file of package that you using for data analytics, we are not going to be repeating that here. So, that we believe that we are adding more value by spending our time and giving you really the core concepts. Syntax, you can learn from one software to the next. Which brings me to the next core area, which is the software and programming area. Many of you have expressed, concern or have questions about, what software we would be using, what programming languages, how much do I need to know. So, the main languages that we are going to be using in this course are R and Matlab and occasionally, we will show you, how it can also be applied in Microsoft excel. But, again it is not going to be tutorial style, we will not teach you the nuts and bolts of how to use R and the code itself that we would be expecting or teaching would be very fairly basic and more like command line instructions. So, it would be fairly easy to pick up, even if you did not know the software per say, but you had some comfort level with basic programming. So, without I have covered some forum questions and basic logistics, so let us dive into our course overview. So, in our course overview, I just wanted to give you the basic module list. Now, this is available on the course website and the basic module list just gives you the topics that we are going to be covering in this course. So, we will be having descriptive statistics, an inferential statistics that is the first two broad areas. Within descriptive statistics, we would also be covering some amount of probability and probability distributions. We then move on to more advance concepts in inferential statistics, namely something called the ANOVA or analysis of variance. We will be talking about what, where and why we apply that. We will introduce regression, something that you might have heard of. And then, we move on to what we see as the main focus of the course, which is machine learning. Both an introduction to it, core concepts, how do they tools and what are the tools and techniques are there, how do they work and within that, we would be talking about both two classes called supervised and unsupervised learning. And finally, we come to this module on, what do we do when you do not have data. How do you go about, what is data analytics when you do not already have the data? So, there is a lot of interesting work there. So, do not worry if at this stage you do not understand some of the words that I have used, that is what this overview is for to give you some idea of what it is that we are going to be covering. I am now going to go step by step, talk about each of these modules and place them and give you some idea of, what is the core concept behind them. Now, again the purpose of this is not to replace the actual class. So, in actual class for instance, I will be giving you far more thorough treatment and so Professor Ravindran, but this is really to help you get a first class view of what it is that you will learn at the end of this course. So, great, let us start with our first module, which is descriptive statistics and exploratory data analysis. Here we are really talking about, how do you describe data. So, we would be talked the, one of the main things that would be introduced here is data visualization techniques. So, this primarily concerns different forms of graphs, how do you represent a single variable graphically, how do you represent relationships between two variables graphically and typically, we are dealing with a data set. So, what kinds of graphs and what kinds of tables are best suited, especially also given that different variables are of different types and for different variable types are there, different ways of representing this data. So, visualization is one part of the descriptive statistics and we will spend a few short classes there. We then move on to summarizing data. So, this is part of descriptive statistics, which is you have a data set, how do I you know summarize the data set, how do I present it to you in a single statistic or a set of statistics. Out here for instance, we will be mostly concern with something like measures of central tendency. What do we mean by that? You might have heard of the words means, median and mode and even if you not, I am sure you all colloquially used the words saying, you have this data set, but what is the average. So, if it is a single variable especially, what is the average or what is a typical value in that data set, what is a value in between and you know these are all very colloquial ways of talking about it. But, there are various different measures of central tendency and different measures express different properties associated with data set and we would be going into that, another area is also measures of dispersion. So, you might have heard of the words variance or standard deviation and essentially, what that captures is an inherent variability. So, we spoke about this concept of mean or measures of central tendency, but how does the data itself vary around that average and that is what we will be talking about there. We will focus on measures of dispersion and measures of central tendency. But, beyond summarizing data through these measures, we can go even further and probability distributions are the richest way of expressing a data. And they do so, because you do not just stop with saying this is the mean or this is the average or this is the standard deviation. You almost literally express it as you know, 20 percent of the data is below this value, 30 percent of the data is below this value and so on. So, when you do that you not only get an idea of the dispersion and the centrality, but you know the entire characterization of the data set. So, you will be talking about probability distributions as well there. So, I think in that is and that gives you some sense of, what descriptive statistics really seeks to accomplish and that is, what we will be covering in that module. We move on to the next part of the course, which is inferential statistics. The idea here is, the idea is one of populations and samples and I am going to explain to you, what these words really mean. The idea here is that, the data that you have is essentially some sample from a broader universe; that is that could be creating this data. And this broader universe is what we call the population and the population can be like a finite, very large data set and you do not have that entire data set, which is why you need a sample from the data set. But, it could also be something more conceptual, essentially this data does not physically exist anywhere. The population data does not physically exist, but it is the concept of this really large universe of potential data that you can get, but you essentially have only, you can only get a sample of the data set. So, perhaps I mean, just to give you a stronger intuition for it, let us just talk about one or two examples. If you took for instance the height of all IIT Madras students, let us say the height of all IIT Madras students. Here is the case, where you actually have a finite population, you define your state space, you define that is space as the data is ultimately the height of all IIT Madras students as of today, let us say as of 2015, that is a very large number and you might not have the data. So, you might choose to take a sample from the data, so you might choose 20, 30 students and go actually measure their heights and that is your sample and here the population was, population would have been the height of every single IIT Madras student as of 2015. I give you another example, where it might not be a finite population. So, you might say well I have this manufacturing process, that makes a certain product and I am interested in the dimension of the product. Now, let us say I go and change that manufacturing process and I am interested in measuring, so let us say it is the width of a particular piece of metal that gets that comes out of this machine. Now, the population here would be essentially infinite in size, it could potentially be the width of infinite number of metal pieces that come out of this particular machine and you know, you are not interested in actually getting that entire population you could not. But, you might, you could still have a sample, you might have just collected about 20 pieces of such metal and you have the sample. Now, what is the point of all these? Why is this an important concept? It is a very important concept, because in many instances, we find that we are not satisfied with just describing or giving you statistics about the sample. You are far more interested in saying something about the population. So, for instance I might be interested in saying something like the average height of IIT students is less than 130 centimeters or some such number or 150 centimeters. The average height of IIT students is less than 150 centimeters. Here note that you making a statement about the population, you making a statement about the average height of IIT students and we said the height of IIT students is the entire population. But, you taken only a sample of 20, 30 students, you do not have data associated with the population. So, is there something you can do with this statistic that is, you can do with the sample of 30 or 40 and say something about the population. Another example, for instance institute could be something like let us say I have a tooth paste company and my job is to put certain amount of fluoride in the tooth paste. And let us say, I am interested in putting about 1000 parts per million of fluoride in my tooth paste use. Will every single tooth paste you have exactly 1000? Probably not and what is my population here, it is potentially every single tooth paste tube that I have sent out to the market or I could be sending out to the market. But, I can do one thing; I can go, take a sample of about 10 or 20 tubes or 30 tubes, measure the amount of fluoride in that and see, if the average amount of fluoride in my tooth paste tubes is equal to 1000 or less than 1000 or greater than 1000, this is with the full recognition that not every single tube is going to have that exact amount, because the world is not a perfect place the world is a noisy place. But, is my average equal to the value I think it is or is my average less than or greater than the value I think it is. So, the whole idea again just to kind of recap, what inferential statistics tries to capture is that, you now have split the word into a population and a sample. A population could be finite or it need not even be finite, but it is essentially you can think of it is very large universe of data and all you are getting is a sample, all you can measure is from the sample. But, is there something I can do with the sample to make a statement about the population and that is, those of the tools and the techniques that we will be discussing there. And it goes beyond for instance the two examples that I gave, in for instance the two examples that I gave we got a sample and we compare that to some number we had in our head. We said, is the average IIT student height less than 130 centimeters or is the amount of fluoride equal to some number x. But, you could also be comparing two samples and there by essentially comparing their populations. Is the average amount of fluoride in tooth paste brand A equal to the average amount of fluoride in tooth paste brand B and extend that even further, A versus B versus C and so on. So, the core idea again is that you not are confined to one sample, you could go to many samples and the idea of going to many sample is what is get captured in a technique called ANOVA or analysis of variance. We are going to have a separate module on that as well, but again stepping back are we living in a world, where we get samples of data whether is a bigger phenomena, but can I use these samples to make statements about the broader phenomena. So, that is what we will be covering in the inferential statistics part of the course. We then, move on to something that many of you might have heard or encountered, probably has bend the motivation for you to even may be take this course and I am talking about regression. When I say the word regression here, I am going to actually start using the word regression analysis, because the word regression itself can be used to cover many highly relative, but slightly different concepts. But, we are going to talk about regression analysis and regression analysis is a great segue, it is a great step going after inferential statistics and going before our machine learning. Because, regression analysis uses inferential statistics, it is you cannot say regression analysis is only inferential statistics, it uses inferential statistics, but it also uses many others tools. It uses optimization, it uses some concepts from descriptive statistics and so on and it has a lot of overlap with machine learning. Regression analysis per say might not in many courses or depending on, who you ask might not come under necessarily the umbrella of machine learning, because regression analysis has been there, been around far before, say machine learning analysis. But, in a sense it tries to tackle the same problem statement or a very similar problem statement, that some of the more advance techniques in machine learning try to do. So, it is a great thing to learn and understand right of the back and so we would be introducing a regression to you in this course. With the regression itself, we are going to start with the simplest form, simple linear regression and we going to use fairly simple techniques of doing it called the ordinary least squares techniques. And right now, let me just give you again a very basic intro to what a regression is again like I have mentioned before a this is not to replace the class, that we will have on regression all of these things and the talking about today we are going to go into it in great detail as we go through the course the purpose out here is to let you know what you getting into in the course at this stage. So, let me just spend few minutes and give you very brief in true to what regression analysis tries to do. The regression analysis is essentially about creating a relationship between the dependent variable, you can also think of that as the output variable and one or more independent variables and you can think of the independent variables as input variables. We definitely going to talk about some examples here, but the core idea is to create to this relationship and one of the simplest ways of the creating this relationship is to fit a line through the data, what do you mean by fit a line through the data. Take a simplest example you have the independent variable and may be your best suited with some examples here, but let is say you are independent variable is the amount of rain fall that a particular region in India receives particular rural region and this is collected over the course of yours, so it is the annual rain fall that a particular region in India collects. And the output variable is the amount of crop yield, so amount of rice that grow the amount of wheat that grow that particular year. The core idea is the independent variable is the rain fall the dependent variable or the output variable is the crop yield, why because we might suspect, that how much rain fall it is, depending on how much rain fall there is that is going to influence the crop yield, The amount of crop yield is the output depends on the amount of rain fall may be we are right may be we are not, but what we are essentially doing is taking pairs of data from different years. So, year one how much did it rained, what was the crop yield how much wheat or rice of crop did we get. Year two how much did it rained how much not. So, you have this data set right of inputs and outputs independent variables and dependent variables. The core idea is can I create a relationship between these two variables and the simplest way is to create fit a line through this data. And you can see in the slide in front of you that I have just created an example graph where the x axis is essentially the independent variable how much did it rained the y axis is how much what was the crop yield and the idea is if you can fit a line through this data that line in some way represents that relationship between these two variables. You can think of many more examples like this for instance may be your independent variable is exercise and your dependent variable could be something like weight loss in this particular case the line; however, will look different the more you exercise the greater the weight loss, but if you can if you thought of it as if you thought of the loss it depends on how you think of the loss right if your y axis the dependent variable for instance is your overall weight and not the weight loss. Then, you would have a line that started from the top left corner and came down to the bottom right corner, but that is fine that is still a regression and one way or the other as long as you are trying to fit a line through the data that is what you are trying to do. There are many more examples that even come from engineering some of the early experiments at then, in trying to understand voltage as the independent variable and current as the dependent variable the frequency to the inductive impedance was another one that is tried like this. Another great example from mechanical engineering is a essentially speed of the vehicle to the mileage that you get from the vehicle. So, at different speeds are you getting different mileages is there some relationship between these two variables and how do I quantify that. But, the examples can be endless, but the core idea with such an exercise typically tends to be one of two things or either both of them one is to capture that functional relationship. So, you understand the system you are dealing with right and this requirement is prevalent and it is not mutually exclusive from this other requirement, which is given the independent variable can I predict the dependent variable. So, the first one the more obvious one is I need to know one I need to understand my system. So, by doing this data analysis I understand, how rain fall effects crop yield, but there might be another goal another not unrelated goal, but another goal which is given some amount of rain fall can I predict what my crop yield would be. Now, this has some critical advantages this has many business applications and to give you an example keeping with the same example. Now, given the amount of rain fall there is if I can predict, what the crop yield would be, then as a government can I institute certain policies for relief performers given the amount of rain fall there is if I for instance providing insurance for farmers can I come up with the prediction of what their crop yield should have been. So, at least I am not in the dark about, what it should be now with each of the other example we spoke about we can also think of cases, where being able to predict what would happen or what the case would be for a given instance has as a lot of advantages for us. And many of the machine learning techniques for instance that we are going to be focusing on really emphasis this prediction part, how accurately can I guess, what is going to be and some amount of it might be that its really helpful to guess that dependent variable the output variable, because it is not possible for me to always measure the dependent variable. I have measured that you for some amount of time and given you some data. So, that you can go and build models based of it, but it is not possible for me in real time continuously keep monitoring that variable, so prediction becomes an advantage there. Another advantage could be that the dependent variable actually takes place at some point of time; however, small, after the independent variable takes place and an example that is often sighted is ambient temperature and number of heat strokes in a hospital. So, that if I just knew if clearly these two variables could be related that the dependent variable, which is the number of heat strokes appearing coming into a hospital is some function of the temperature outside if the temperature outside is really cool and the sun is not out you are not going to that many heat strokes I mean that is reasonable right. But now, because there might be a time lag between when temperature speak and when people actually physically arrive at the hospital can I be better prepared I use some historic data between temperatures and heat strokes and build my and fit my line and build this regression model. Tomorrow, I am just going to use this regression model and say the temperature is x at a temperature x can I use my regression line and predict how many heat strokes are going to arrive at my hospital and therefore, I will be better prepared at the hospital. So, I hope this gives you some insight into what regression is what it seeks to achieve and with that we conclude our first session of the course overview. In the next session, we will be continuing with giving continuing with giving you an overview of the course and introduce the more detailed sessions on machine learning and so on. So, look forward to having you join us then. Thank you for participating today.', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'a5f802125b947e0a53aa727c782a751d'}>, <Document: {'content': \"3) Course Overview (cont'd)\\nWelcome to the second lecture of the course Introduction to Data Analytics. In this session, we are going to continue from our previous session, where we presented to you a brief overview of what, we are going to be covering in this course. And we started of talking about in the previous session we spoke about descriptive statistics, inferential statistics, the use of ANOVA in inferential statistics and finally, we spoke about regression and regression analysis and how we would be using that and we would be talking about that in this course. We now move on to the next session of the course, which is machine learning. Again, just to remind all of you there, this is not the introduction to machine learning part of the course. This is the part, where I am just giving you an overview of everything that we are going to be covering in this course. Obviously, with each session we are going to separately introduce the topic and go over it in great detail during that particular session. But, this is just again to give you an idea of what it is that we are going to be covering in this course. So, let us talk about machine learning. Machine learning is what we feel is, a primary focus in this course and having covered concepts in probability, statistics and also in with regression analysis should set you up fairly robustly for understanding machine learning. So, many of you might have heard of the word machine learning, come across it in some form or the other or you might have also come across machine learning through one of it is related topics. So, you might have come across data mining, you might have heard of the terms data mining, you might have heard of the term pattern recognition or statistical learning in some cases. Now, all of these are highly related topics, but they are not necessarily the same. For instance, the focus on the machine learning is more focus on the algorithm themselves that are going to be used to convert data into usable knowledge. So, and that is also going to be the focus of this particular course. So, let us talk broadly about machine learning, topic of machine learning is itself broadly divided into two areas, one of supervised learning and unsupervised learning. And now, I am going to give you a brief idea of what we seek to achieve in both these topics separately. So, let us for instance take supervised learning. Before we jump into a definitional understanding of supervised learning, you already saw the first glimpses of what supervised learning tries to do and you saw that when you cover the module on regression analysis. Now, to jump into the definition, the core idea of supervised learning is essentially a task of creating a function or a relationship from training data. So, based of historic data, which has at least one explicit output variable, traditionally this is also indicated as data that is labeled, so that is coming from the computer science camp, where people say the data is labeled. But, what that essentially means if you are not familiar with the terminology, is there is this clear single variable, which I can call as the output variable and I am primarily interested in create a functional or algorithmic mapping between this output variable and one or more input variables that I might have. So, that is supervised learning and we can take the same examples that we were looking at when we were speaking about regression analysis as examples of supervised learning. So, you might have data, where one of your inputs is something like the rainfall and your output is the crop yield or you might have data, which says that your input is a square footage of the house and your output for instance could be the price of the house. Again, there are many, many, many examples we can think of, but a supervised learning is this idea that we have an output variable and your primary focus is to either predict the output variable or create a functional relationship between the inputs and outputs, which can be used or it is useful for the future. Now, within supervised learning itself there are two broad classes of problems. Now, this classification of problems does not mean the algorithms themselves are really different. So, essentially you are, the idea here is that your supervised learning problems can be classified into two broad classes and they called classification problems and regression problems. The word regression means something quite differentiate, it does not mean the exact same thing as a regression analysis, but once I explained this division you will understand better. Classification problems are essentially problems, where you still trying to do what supervised learning tries to do, which is create a relationship between the inputs to the output. But, here your output variable is a discrete categorical variable and more often, the not is a nominal categorical variable, meaning there is no explicit ordering of the classes. So, an example of this could be something like your output variable is either male or female. So, you are trying to predict something and the output variable is not something like the previous example, where we said how much, what the crop yield was. So, how much crop did I get is a continuous variable, meaning 20 kgs or whatever per hectare is a very, is exactly twice 10 kgs per hectare. So, that is a continuous variable, you can get any value between 0 to infinity or negative infinity to infinity. But, with classification problems you are trying to predict based on the inputs as to which class the output variable should belong to and that just means that the output variable is discretized and in all likelihood, it is a categorical variable and it is typically nominal. Now, move on to the class of problems which are called regression problems within supervised learning, that just again quite simply means the output variable is a continuous quantitative variable, such as the crop yield given some amount of rainfall, how much crop are you going to get given the rainfall. The methods themselves are just marginally different and many of the supervised learning tools and techniques are perfectly capable of being deployed in classification scenarios as well as regression scenarios. And, but at the same time there are techniques, which are just suited for one of the two and you need to make some modifications to the technique for it applied to the other. But, we will be discussing this dichotomy as we go through the course also and even as we go through the techniques themselves, we talk about them a little bit. We now move on to unsupervised learning and I am, let me just give you a brief idea of what we will be covering in unsupervised learning. An unsupervised learning is the task of creating patterns from data, which have no explicit measure or signal guiding us. In other words, there is no single variable, which we can call as our output variable. Again, here people say the data is unlabeled, but ultimately if you are familiar with the terminology great, so if you think of it is labeled data for supervised learning unlabeled data for unsupervised, I find it easier to think of it as with supervised learning there is an explicit output variable, with unsupervised learning there is not one or two you know variables that I can just point to once, so say these are the output variables these are the input variables with unsupervised learning you just have the variables. Now, that we have basic definitional understanding of supervised and unsupervised learning. I am just going to give you some idea of what are the tools and techniques that we are going to cover in them. I might not, I am not again, because we are not in the supervised learning class, I am just giving you an overview I am not going to go into what each of these techniques are, but this is more to just familiarize you with the names of these techniques and in some cases, you might have come across or heard of these names somewhere, so I just want to make sure that you have familiar with that. So, with supervised learning we are going to be looking, we would have finished our module on regression analysis. We would be looking a little bit at more advanced methods in regression, modification setup available with regression analysis approaches. We will be looking at logistic regression, which is used for problems of classification, regression styled approach for not predicting continuous output variables, but categorical output variables. We will talk about an algorithmic approach called K NN methods. You might also come across this module on Classification and Regression Trees. It is also called CART, we will be talking about that. Other methods that you will come across are Support Vector Machines or SVMs, Linear and Quadratic Discriminant Analysis LDAs and QDAs, Artificial Neural Networks or ANNs and there are breed of methods called ensemble methods, which kind of use multiple predictors together, so that is also something that we will be covering this course. We do not stop at only tools and techniques, because just knowing the tools and techniques and sometime, some of these tend to be buzzwords, is good in that you know what you might be talking about. But, you also want to understand some of the concepts that go behind, creating some of these techniques and these concepts can be critical to fine tuning some of the parameters that are there in the techniques. So, we will be talking about some common supervised learning concepts called regularization, dimensionality reduction or cross validation and so on and at this point, if you do not understand some of these words, that is fine. The purpose of this is to just give you an idea of, what it is that we will be covering, fine. We then move on to unsupervised learning and in unsupervised learning, there are two major areas that we would be covering. The first is the concept of clustering. You might have heard of the word clustering and that is a topic that we are going to cover in unsupervised learning. And the next, the other topic that we will be covering in unsupervised learning is called association rule mining. So, let me just briefly give you an idea of what clustering is and what association rule mining is. This way you also get a slightly better idea of unsupervised learning. See with supervised learning, you had the example of the regression very concrete example it is easy to imagine. But, what is it mean to do machine learning, where there is no output variable, what is that feel like, perhaps talking through these two will give you some idea. With clustering, the core goal is that it is a task of grouping a set of objects into clusters or you can think of them as group into groups based on their similarities. How are these similarities defined? It is defined across a common set of attributes or features that each of these objects have and again, the easiest way to digest what I just said I will repeat it, is that clustering is the task of grouping a set of objects into clusters or groups based on their similarities and the similarities are defined based on a common set of attributes or features that these objects posses. So, let us take a couple of examples. Now, we understand a definition version of what clustering is, but let us take a concrete set of examples and may be, what we mean by objects and what we mean by features becomes more obvious. The easiest example to think of our customers for a business, so let us say that I am an online retailer or let us say I am a taxi company, take whichever business is close to your heart and let us say I had a database of my potential customers or may be my current customers, either database. The objects here would be the customers; the features are attributes, our features and attributes associated with the customers. So, a simple feature could be is my customer male or female, another feature could be, what is the age of my customer, another feature could be is this returning customer or is this a new customer, another feature could be the actual amount of rupees per transaction spent by this customer each time they come to me. So, these are all some attributes and features associated with the customers, the customers are objects. So, what are we doing in a clustering, what we are doing is we grouping these customers and why would we want to do that, for various business reasons. If I can group these customers, so nobody is coming and telling me, what is the right answer wrong answer, there is no output variable. But, I have taken these customers and now, I have created two or three groups and that could help me in a variety of ways. If I understand that there are only two or three types or groups of customers that come to me, knowing which group a particular customer belongs to. It might help me behave differently potentially to the customer or it might institute certain policies in my business environment based on the groups that get formed in amongst my customers. So, again there are many, many examples. We just spoke about businesses incoming customers for a business, this is been quite prevalently used in biology for instance, where the object here are different genes and the different genes performed different functions. So, these functions tend to be features or attributes. So, can I group genes based on the functions that they performs, so that is one application. There are also many applications in medicines. So, you have a whole plethora of disease and the disease form the objects, but are there certain set of symptoms or are there certain set of responses to treatments that these different disease have and so, can I group these disease based on the attributes, which could be symptoms or their responses to different kinds of treatments. And for instance, a grouping like that might help establish wings in hospitals or medical treatment facilities, where disease of a certain kind get grouped together and people are sent there. This is just thinking allowed, it might be a good idea, it might not be a good idea, but point is clustering can enable you to create these kinds of groups and how a business or an engineering application uses it is more domain specific in that sense. Some of the techniques in clustering that we are going to covering include K NN, so you might have heard it as K means clustering. We are going to be talking about hierarchical clustering, graph based clustering and also density clustering. So, this is just to give you some idea of different types of clustering techniques that we are going to be covering in this course. Let us now talk a little bit about the other major unsupervised learning technique that the course is going to focus on. And this is essentially association rule mining. Association rule mining is essentially this task of identifying relationships between features across a set of objects. So, keep the same object and feature definition that we created with the clustering. With clustering, your goal was to use the features and thereby group objects. With association rule mining you want to use these objects, you want to use these data essentially to create relationships between features. So, let me give you a concrete example and this is a seminal example that introduced in many ways association rule mining and it is called the market basket application. In fact, association rule mining was you know, times also called like a market basket analysis and so on. The idea here is that, let us say you are a point of sales system, you are a super market and your rows or your objects are essentially customers and these customers are come in and they buy some sub set of the products that you stock in the super market. And the super market now represents this whole transaction, where each row is a sale that a customer makes and the columns or the features or these different products that the super market stocks. So, a particular sale will have a stream of zeros and ones, where if I did not take product A I get marked as 0 for that product, if I do take product B, then that is a 1. So, each sale you can think of this stable, where each sale is a row in that table, each column is a product that the super market stocks. So, if a particular sale includes a certain product, then that is marked as a binary, it is binary systems gets marked 1 and if a particular sale does not have that product, it gets marked 0. So, let us say you have this table now, this table could potentially enable you to answer questions of the nature such as people, who buy coffee and coffee would represent a particular column in the table. You could say something like people, who buy milk tend to buy sugar. Because, typically when there is a 1 in my data set, it looks like under the category milk there also tends to be a one under the category sugar in my same data set. So, and this can be extracted to go beyond a one to one mapping say I give you an example, where people, who tend to buy milk tend to buy sugar, but you have lots of other combinations you can say things like people, who buy coffee and milk tend to buy sugar or people, who buy milk almost never buy milk substitute. So, why is this exciting well now, a super market knows, where to keep which product in its setup. So, you can lay things out where if coffee and milk bought, bought together can coffee and milk been next to each other and so on and so forth. But, association rule mining again need not be confined to a market basket context as long as you can again break down the data into simple thing of objects and features that will enable you to perhaps considered association rule mining. But, the important thing is again look there is no one variable that you are targeting. So, it is this is not a classification exercise it is not a prediction exercise of you know, who is most likely to buy sugar, but it is that any variable can be can become the relationship variable. So, there is no strict output variable within association rule mining there are a couple of challenges and we will be talking about that as well in this course. So, the idea of you know creating many complex rules becomes computationally very hard, so what do you do with that. And how do you how do you say a particular rule and when I say the rule of here something like people who buy coffee and milk tend to buy sugar how do you evaluate how good particular rule is in that is some of the core challenges in association rule mining. Finally, we come to the last module in this course overview and this is a module on creating data and we feel fairly strong about this module, because we see that this is a problem that is often faced with many organizations, where their interested in data analytics their interested they see this buzz word floating around sometimes big data and so on. And there a little unsure about, how that applies to them when they just is no data available or they have not really captured it. And we feel like a set of topics in here should help companies or organizations understand this part of the data analytics process better. So, there are three major topics that we are going to be covering here the first topic is on design of experiments. So, you have no data, but you want to take a data centric approach this whole idea of the data driven decision making you want the data to tell you what is the right thing to do. Perhaps the best thing for you to do is to conduct an experiment you try certain options and essentially you explicitly change that input variable in different settings and different points of time and then see, what happens and you use that data that just generated from the experiment to essentially do something like the regression or a supervised learning technique and thereby make decisions. So, design of experiments is would be one way of going forward there this is other really exciting area called active learning, which is the part of the machine learning machine learning words active learning comes about when you might have some data or very little data broader umbrella of machine learning and the idea here is also one of you can think of it as one of experimenting you could think of is one of sequentially quarrying the system. In other and its fairly expensive to gather this data. So, instead of just doing a blind approach of fixing senses all over the place which might be an expensive proposition and therefore, you do not have this data. Is there something we can to do the partial knowledge and can we sequentially quarry this system in what we mean by quarrying the system here is can we sequentially put senses can we sequentially see, which data we want to gather. Because, we do not have lot of data in we do not have enough to make conclusions. And at the same time we cannot just say let us start lets commission data gathering exercise only because it is hard to do that. So, given that we have fixed resources towards gathering data active learning is an area, where you sequentially go and gather data, but you only gather the critical data that you need in order to mine it or in order to process it for coming up with insides. Now, the third area that we will be covering in this section, which is creating data for data analytics is the area of reinforcement learning and this is also reinforcement learning is also subset of the larger machine learning umbrella and most specifically in the reinforcement learning re now, we are going to focusing on series of problems called the bandit problems and the context here is that you do not start with you do not have to start with any data or you might have some partial data. But, you just cannot go about experimenting to create data for a verity of reasons one may be cannot create this kind of lab setting which might be needed to conduct the experiments in create data. But, more importantly it is also possible that you cannot experiment only, because it effects the end user in some way you cannot essentially go off line and do your experiments. So, here you are do not have any data you want to try something is right because you do not know of what you doing is the best thing. But, you cannot just commission in experimentation exercise to create the data, which then gets analyzed, and then tells you what is the best, because when your experimenting you might be doing some horrible things and those horrible things might affect an end user. So, the whole idea behind banded problems is you can think of it one way of think it it is a form of experimentation and learning in an online setting. So, you do not only care about how much you are learning from the data, but you also care about how value of performing. And in fact, the experimenting itself becomes consequents because your grand objective in the banded problems tends to be one of performing as well as you can over sometime horizon. So, because you need to perform as well as you can, which is defined by some notion of how you do cumulatively you wind up a trying a few things out just, so that over time you are not continuously doing something that is not in your best interest. So, these are three these could be fairly useful techniques or tools to use when you are in an online setting or when you are in a setting were you do not have much data. Finally, I just wanted to mentioned that in addition to some of these topics that we have discussed we will also going to have some a couple of modules on major challenges for big data analytics and what I guess big data analytics its of means in the world today. And we are also going to be talking about some of the more popular techniques contemporary techniques like deep learning especially when we cover concepts and artificial neural networks and so on. So, with that we conclude this second session of the course overview and starting next session we would be directly diving into the content itself and I mean we cover the content today. But, again the spirit of it was to give you an idea of what it is that we are going to be covering in this course and I hope you found it interesting and I look forward to having you join us in the next session. Thank you.\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '7d6731ea40231e72fa6f11f1591f2efe'}>, <Document: {'content': '4) Descriptive Statistics - Graphical Approaches\\nHello, and welcome to the third lecture of the course Introduction to Data Analytics. My name is Professor Nandan Sudarsanam and today, we are going to be talking about Descriptive Statistics and more specifically about Graphical Approaches used in descriptive statistics. Now, before we jump into the content into descriptive statistics in the different types, it make sense for us to take a step back and talk more generally about data, what is data and most critically, what are the different types of data that you will encounter. And it is important to do this, because the descriptive statistics that you use in the approaches that you use vary according to the different types of variables or different types of data that you will encounter and this is a recurring theme in many other aspects of this course. So, make sense for us to talk about that for a few minutes right now. So, data is essentially numbers, now it can also be texts, symbols, but more often the not, you will encounter numbers, which represents some kind of information. And therefore, it is a kind of helps to think of data as values, because values is broad enough to cover numbers, text, symbols. So, you can think of data as values of quantitative and qualitative variables. Now, the variables themselves can be of different types and that is, what we are going to talk about right now. In statistics, you have various classifications of variables and so different depending on, who you ask, you find different classifications in different text books as well. But, one broad classification that make sense and that is very useful for us is to differentiate variables as quantitative and qualitative variables. So, quantitative variables are also called as numerical variables and these variables are essentially, you know the best way to think of them is that they have meaning as a measurement, such as a personÕs height or weight or IQ or they can be some kind of count, such as the number of something number of days it is rained and so on and so forth. But, a very intuitive way for me to, that is always been useful for me is to think of quantitative variables as variables, where some form of basic arithmetic like either adding or averaging or subtracting kind of make sense. So, I mean a definite requirement is that the quantitative variable is numerical, but some time you can also have numbers being used as symbols not as the actual numbers. So, a really simple rule that kind of helps me identify quantitative variables is to say that it has to be a numerical variable, that the values that the variables takes on are numerical and that, some basic forms of arithmetic kind of make sense on such a variables. Now, within the quantitative variables you have continuous and discrete variables. Continuous variables are essentially one square within a certain interval and this is an interval that the variable could, where the variables could take on values. Within this interval any value is possible, if any value within this interval is possible, then this variable is said to be a continuous variable. So, let me give one example. Let us say that the variable we are interested in is the height of students, who are registered for this course. And so, let us say I go take a sample of a 1000 students and write down and their heights and so this is a data set that I have. The data set have 1000 values and let say an interval and the interval can, you can may be get the interval from taking the highest value to the lowest value or you can just, as long as it is a real interval that covers this data. The question is, is any value possible within this interval and the answer is yes. So, let say my interval was 120 centimeter to 140 centimeters, is it possible that someone, who taken this course could have a height of 135 centimeters. Absolutely, there is nothing about heights that inherently stops people from having that particular height. Now, I can go even further, I can say is it possible for someone to have the height of a 135.156 centimeters and the answer is still yes. I mean I might not have a measuring scale that goes to a certain accuracy, but that is the measurement problem. There is nothing about heights that prohibits this value from existing in this data set. So, continuous data is essentially one, where any value between certain interval and the interval is sometimes formally defined as the highest value in the data set to the lowest value. So, any value within this interval is potentially possible, then you have a continuous data set. The second kind is the discrete, so the discrete quantitative data is one, where this condition is not true essentially and it again helps to think of, what kind of a data set would be such that a value not all possible values are there and I will give you another example for that. So, let say that I was interested in looking at the number of people, who enter IIT Madras every day, so the number of people, whoÉ Let us make it interesting, the number of unique people, who enter IIT Madras every day. So, if you come in and go out, come in go out and times, I do not care, you are still one person. So, number of unique people, who enter IIT Madras on a given day is my variable and the actual values of this variable I get from doing this kind of a survey or a study for 1 year. So, I have 365 data points, one data point for each day, which says the number of people who enter IIT Madras. Now, clearly this variable is discrete, because let say there is a lower bond, which is may be 0 people, nobody enter the IIT Madras highly unlikely, but on a given days. So, zero is the lower bond and the upper bond is some, you know 100,000, 50,000 something. Now, within this range, can I have is every value possible, no. You could for instance never have a day, where two and half people entered or let say, you know 133.2 people entered. So, here is discrete, because it is discrete in the sense that only the integer values are possible, all the more; no, where you can never have values that are non integers. So, that is an example of a discrete value and in this particular case, it happens to be one of being discrete at the integers, but that is just, because of the example I came up. You can come up with the other examples, where the variable is numerical and it is I mean its quantitative, but the values you can take up are discrete. We move on to the other class of variables, which are qualitative variables, these are also known as categorical variables and categorical variables essentially represents some characteristics, some characteristics, which can be categorized, which can be grouped. So, examples of that are things like a person gender, so that is the variable and the values of variable can take a male and female. And then, you might have something like marital status or more interesting, one might be home town of or state within India. Let say, let us take all the people who registered for this course from within India, which should be bulk of them. And the variable we are interested in is, which state are you from, so that is the variable the state that you are from and the values that this variable can take up are the different states of India. Now, the categorical variables again, because of their definition and their nature are always discrete, so that should be obvious. But, within these categorical discrete variables there are two classes again, there is nominal and the ordinal. With nominal, there is essentiallyÉ The big difference is that, with nominal there is no order. So, the great example of that would be this home state, which state are you from, variable. There is no order, which says that Madhya Pradesh is greater than or lesser than another state and so on and so forth. So, these are all, the values that these variables can take up cannot be ordered in a sensible way you know, whereas with ordinal data by definition of the variable, there is an order. Let me give an example of that, let say that we created a variable for, which is the color for terror alert, so some countries have this, the terror alert color signify something. And let say the possible values this can take is green, yellow, orange and red. So, the variable stated terror alert color green, yellow, orange and red are the four values that this particular variable can take, where green represents low risk and red represents very high risk. See, so there again it is a categorical variables, because it is not like you can do arithmetic operation on green, yellow, red. The variable itself is a qualitative variable, but yet there is some order. Because, you can say things like if orange is worst than yellow and yellow is worst than green, that must mean orange is worst than green. So, you can get an idea also, that is an ordered categorical variables, whereas with a nominal variable you could not say if Madhya Pradesh is greaterÉ First of all, you could never say greater or less than, so creating more complex relationships becomes impossible. So, that is just to give you a very quick idea about the different variable types. So, now, let us jump into descriptive statistics. So, descriptive statistics is the idea of quantitatively describing data and you can do that through various means, you can do that through visualization techniques like graphical representation, tabular representation, but you can also do that through summary statistics. The idea here is that, you crunch the data, you work with the data and come up with 1 or 2 or 3 or 4 different numbers that summarized the data for you. In this class we are going to be focusing more on the graphical and the tabular representation and the next module is going to be on the summary statistics, so that is the idea. Now, this is a very good time for us to just quickly review, you know in our overview classes we spoke about descriptive versus inferential statistics and this is the good point to just bring that up again and to kind of have a very quick idea, what descriptive statistics are really means. The core idea in this dichotomy is that descriptive statistics focus or is the way to say something meaningful for the data that you have at hand. So, you have some data at hand, whether you call it sample of population or whatever, if you are making the statement based of that data about that data derived from that data, you are dealing with descriptive statistics. Descriptive statistics do not; however, allow us to make conclusions beyond the data we have. So, you cannot look at the data, do something with the data and make and based of that make the generalization about potentially the source the data that, the data came from, you would need inferential statistics for that. Now, having said this; however, descriptive statistics is still very important, because you cannot just simply present raw data, it would be very hard to visualize, especially when the data is a lot. When you have a lot of data, you cannot just show the data, you need to present the data in a more meaningful way, which allows for some kind of simpler interpretation and that can be through the graph or through numbers, great. So, and a final point I just want to make is that, descriptive statistics is not just confined to a single variable, it can be about multiple variables and when you are dealing with multiple variables, our topic of interest is relationships. So, how does one variables change with respect to another variable. So, in essence you will be doing two things which is summarizing each variable or describing each variable, but you are also interested in showing interrelationship between variables. So, let us go ahead and now, that we have an understanding of different variable types, let us talk about some graphical representation techniques. If one is dealing with a single variable and let us say it is a categorical variable, a great way of representing data could be through a bar graph. So, that is the graph that you see on your left hand side here. So, here for instance let us say the example is one, where we sent out a survey and ask people, what their highest level of education is and highest level of education be the variable, the possible values that takes up our high school, bachelorÕs, masterÕs and doctorate. Hence, therefore, this is a categorical variable, there are only four possible qualitative states, that this particular variable can take up. And, what we plotting is the number that we, number of responses or the number of observations we counted in having this values. So, you sent the survey out let say it about 50000 people, may be 60000 from the local way. So, and about 15000 of them said that their highest level of education was high school. So, the height represents the frequency about of occurrences of this particular value of this variable. So, this kind of a representation can quickly summarize, which is more which is less and so on. But, an interesting thing to note out here is that this categorical variable is actually ordinal, meaning there is an order of going high school, bachelors, masters, doctorate. You could have flip the whole graph around, but you would still have the order that is a sense that a doctorate is a more years, I guess than masters and which is more than a bachelors, which is more than a high school or whatever. So, in some sense the variable itself has an intrinsic ordering and the good thing is something like a bar graph, I love for that. Just, because of the fact that there is this concept of a x axis, makes it very convenient to represent ordinal variables, which are categorical. Another way of representing categorical variables could be something like a pie chart, this is an example, where let say we looked at the number of students, who were in different engineering departments. And your different engineering departments here are mechanical, civil, electrical and computer science. These are just some random departments I chose and again the frequency of occurrence is more represented as a percentage of the whole. So, this percentage of this full circle is computer science students and thus the idea behind using the pie chart. And clearly a pie chart is not very suited for ordinal variables, which is more suited for nominal variables. Because, there is no order, one that the fact that computer science shares the wall with mechanical and civil is just coincidental, that is not what a pie chart seeks to capture. Sometimes people will keep similar things together, but that is not a requirement. One important thing about pie charts is that, usually you want to represent all the values. So, if there are some engineering departments that are not being represented, usually a pie chart need not be the best way, because there is an impression that this is all the departments together. So, if there are more engineering departments, but you wanted to only show 3 or 4, may be you could use a bar graph rather than a pie chart. Now, we move on to quantitative variables and with quantitative variables, you have a couple of different ways of representing a variable. One example is a box plot. So, with quantitative variables, remember that is numerical data and, so you might be interested in representing things like, what is the average, what is the variance and in our class on summary statistics, we are going to go to a great detail about it. But, for purposes of this, a box plot is essentially something that captures central tendency, which is this red line that is there in, typically that tends to be the median of the data set. And you have the two bounce of the data set, so the top and the bottom of the box itself and that tense to capture in some way the variability in the data and the way a box plot does that by representing the lower quartile and the upper quartile. Now, the lower quartile and the upper quartile in really simple words is just 25th percentile and the 75th percentile and, so that kind of that range a gets captured there and the whiskers themselves take on different meanings depending on the, which version of box plot is using, but more often they are not it tends to be lowest value and the highest value in the data set. And typically red dots like this represent outliers in the data. The box plot is itself something that will make more sense to you and we will talk about summary statistics, because you will understand, what exactly a median means you will understand, what a different way of representing spread an outliers and so on. But, it helps you at this stage to kind of say that this is one simple way to take a data set, which is full of numbers. So, let say this data set had you know 500 or 1000 numbers and it looks like these numbers are pretty much within the range of like 25 to 33 or, so and to represent all of these numbers in a single graphical representation, so great. Another way of representing quantitative data is through a histogram the histogram is what you see on the right hand side and histogram is arguably ah the richest representation of numerical quantitative data, because histogram essentially says how many data points do you have with in this range. So, the x axis out here represents the different possible values that this data set has. So, if you take this as 8 to 10 this should be something like 8 to 8.66 and this should be till like 9.33 and this should possibly be 10. So, the question is in your data set how many data points do you have that have values greater than 8 and values less out here. So, another way of showing that I am going to try highlight it is, so here interested in this range right here this range. So, this is 8 and this is 8, let say 8.66 is from reading the graph right, how many data points do you have. Because, this is a numerical quantitative data set that have values greater than 8 and less than 8.66 and the answer to that question is it looks like 6 data points right approximately may be 7 may be if I am reading graph correct. So, you answer that question for each of this bins this are all called bins each of this columns are called bins for each of this column you answer that questions and what you get is histogram and the histogram is the first step towards empirically constructing, what you will we will later learn is it distribution. So, once you capture this, this entire picture out here you have a very clear representation of the entire data set. So, again just keep in mind that we are going to be talking about distribution we are going to talking about medians means and variances, but keep in mind also that this is the graphical way of representing these things. So, now we move on to the multiples variables and the last section in graphical representation and there are three major of forms of representing ha this data and they are the following. The first is scatter plots this are very useful when you have two quantitative variables that is you know. So, two variables, which are both numerical you can you can very easily represent using scatter plots and, so in the key thing you should notice in this scatter plots is that it really helps you understand the relationship it does not do a very good job of understanding each individuals variable. So, may be if you done distribution of x and distribution of y separately you would understood those two variables, but what it does a good job is of capturing the relationship between x and y. And this particular case the fact that in general when x increases it look like y also increased or y also high or you know you can always flip it the other way around in general when y is high x is a high when y is low x is also low. So, that relationship gets captured for that reason this is the great graphical representation of two variables usually you can extend box plots if you feel like one of your variables is categorical and the other is quantitative. So, you are not just interested you are interested, let say one variable, which is country and the other variable, which is some indicator let us say of the economy or crime or whatever I have just called it values here because it does not matter. But, this variable is continuous right its mean I should not say quantitative I do not know if it is continuous it could be continuous or discreet, but this variable is quantitative, where as this variable is qualitative. So, one great way of look comparing different qualitative variables, which have data set that are on the quantitative scale is to perhaps use multiples box plots on the same graph and that gives you not just an idea of how on average his country is different, but there are also different in terms of their variability and their out so on, so forth. Finally, we come to the use of contingency table out here on the extremely right and the idea here is that when you have two categorical variables and what you are interested in representing is the frequency of occurrence. So, the frequency of occurrence is the theme of the data set. Then, contingency tables are great ways to do that, so an example that I have come up with out here is how many people let say you go to a company and you take a survey of all the managers working in the company may be interested in asking how many of them have MBA. So, y represents yes they have an MBA n represents no clearly this is categorical variables right it has only two states. Similarly, you could say before this people join the company did they have work experiences before they joined as managers and answer could be again be yes or no, so that is also categorical variables. So, here is an example why you have two categorical variables and what you are interested in his how many people belong to each combination. So, how many people have MBAÕs and had work experience before joining how many people had MBAÕs did not have work experience before joining. So, this can be complex data set where it very neatly summarized in the contingency tables and that is something that could be quiet useful. So, I think that is about it for graphical approaches to representing data in the modules descriptive statistics and in the next class we will be looking more at summary statistics as a means of as the sub modules in descriptive statistics. And then, we will move on to inferential statistics great, thank you for joining me and look forward to seeing you in the next lecture.', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'bd6fb895f651412d67358b10db4ac3c6'}>, <Document: {'content': '5) Descriptive Statistics - Measures of Central Tendency\\nHello and welcome to our next module in the course, Introduction to Data Analytics. In this module, we continue our previous work on Descriptive Statistics and weÉ In our last session, we spoke about descriptive statistics through the use of various graphical and virtualization techniques. In this module, we start with the use of summary statistics or the idea that you can describe data with numbers, with numbers that summaries the data and most specifically, we are going to be talking about measures of central tendency in this lecture. So, just to jog your memories we spoke about the idea that there could be a data set and a data set essentially would be representing, could potentially be representing a particular variable and so, I provided for you a simple example of a data set. And this data set is what is been captured in this histogram. The histogram is a visualization tool that we spoke about in our last lecture. Now, the histogram essentially is a very rich representation, because it not only captures just some parameters associated with this data set, but it captures various new answers associated with it. So, just to give you a quick reminder on how this works, essentially the entire x axis breaks down possible values that the data sets could take. So, for instance this bend is the series of values between 10 and from the looks of it 10.66 on this side. Now, depending on the number of data points that you see here, that fall within that range of 10 and 10.66 that would get counted here and out here it looks like that is about 15 points. So, that is essentially how a histogram is calculated and the idea is that, using a histogram you could then fit something called distribution and the distribution is this red line that is shown on top of the histogram. And, so in some sense the histogram and the distribution that some time follows, tells us the full story associated with the data. And usually this red line is represented through some kind of a formula and we just call that, let say f of x for now. But, the basic idea behind summary statistics is that you do not even need to go this deep, I gave you this full picture just to tell you what the richest or the most detail story could be and our next sessionÕs, next modules are going to be about distributions. But, now let us take a step back. Is there something simpler that we could do? Is there something simpler without even fitting this distribution or even creating this histogram that could tell us a part of the story? And the answer is yes and there are these various summary statistics that do exactly that and I am just going to talk about a few of them now. The first and the most common one are these measures of central tendency and what they mean is that, you have this data set and for now, let us just occupy ourselves with the histogram not the distribution that is fit on the top of the histogram. But, essentially with this histogram, what is a fairly central value. So, it is clear that the values of this distribution go from here to about here, but what is something in the center and how do you define that. So, one idea is often to say, well one measure of central tendency is to see the minimum value going all the way to the maximum value and take something that is in between, so that is one way. Another way could be to say, at what point in this histogram am I really covering about 50 percent of the area. So, this histogram is defined by these blue bars and at what point am I covering about 50 percent of that area, so that could be one way of saying, what is central. There are other innovative ways, one of the most common one is to say think of this as a balance, as a sea saw essentially, this x axis line and all these blue bars are weights on top of it. So, the idea is, where would you want to put a fulcrum, such that this whole thing balances. This does not, one does not tip off the sea saw is essentially in balance. So, that is the core idea behind measures of central tendency, what is a central value. Now, you then have measures of dispersion and the idea behind measures of dispersion are, that so you might have something that is central value. But, how a data point actually dispersed around the central value? Are they are very far away from them or are they very close to it and so on. And these are the two major forms of summary statistics that we will cover in this course and that you are likely to encounter. But, you might have also heard of the concepts of skew and kurtosis and so, it just briefly what mentioning it, skew concerns the shape of this distribution itself and the like, the fact that sometimes distributions lean to one side versus the other and this is the fairly colloquial way of saying it. But, instead this distribution the red line being the way it is could you have a distribution that looked like that, which means it is leaning, the whole thing is leaning to one side. Again, I am just giving you conceptual feel for it, it is not a formal definition, we and then, in the same line kurtosis is the idea that how fat are the tails of the distribution and what we mean by that is, you know having a similar distribution that looks like this. So, that tails themselves are fatter than the other and that property gets captured with kurtosis, so great. So, this is, this should I have given you some a brief overview of the different, the over hatching idea of summarizing statistics through mean, describing statistics through summary statistics or numbers as a means of describing distributions. We now, go into the major subject of this lecture which is measures of central tendency. So, the best way to do this is through a concrete example and we, that is what we will do with this step. So, there are three major measures of central tendency and they are the mean, median and mode. With the mean, the core idea and many of these you might have encountered, you might have come across before. So, bear with me if you already heard this, the idea behind mean is just that it is the concept of average. If you have a data set and I am just given you a sample data set out here and, so it is the numbers 3, 4, 3, 1 and I have kept the data sets smalls, so that I can illustrate the concept typically might be dealing with much larger data sets, but the idea remains the same. So, for this data set the mean is nothing but, the sum of all the numbers divided by the total number of a numbers there are. So, we take each numbers 3, 4, 3 and we add them all up and the 12 that you get out here is the actual number of numbers that there are in this list. So, once you divide it and that is the concept of mean, which can also be represented mathematically in this form and I have just shown that, you use of that, when you see that it is, you not surprised by it. So, great and incidentally the mean is the concept that I was speaking about in this histogram of balancing the seesaw, where would you place the fulcrum; such that this seesaw gets balanced, that is the same concept of mean. We now move on to the next measure of central tendency, which is called the median. The median is calculated by arranging all the numbers in order. So, you had this data set and it was 3 comma 4 comma 3 that is what you had out of here, but when you bring it to median, you basically takes the smallest number put it first and then, in some order ascending or descending you arrange the numbers. Once you do that, you choose the central number and that is your median. Now, choosing that central number is quite easy when you have an odd number of numbers, so if you had 9 numbers the 5th number would be the central number, which you have 4 numbers before and you have 4 numbers after words and that is your central number, but when you have an even number of numbersÉ So, in this particular case we have 12 numbers, the central number is really not 1 number it is 2 numbers. So, at here it winds up at being a 6th and the 7th number. So, typically there you choose the 6th and the 7th number and take the average. In a particular case that is not a problem, because it happens to be the same number and quite easily we say that the answer is 4. The mode, which is a third measure of central tendency and there might be a few others, but these are the three most common ones that you will encounter, the mode essentially says what is the most common value. So, if you look at this data set, the number 3 appears 3 times, the number 4 appears twice and then, all the other numbers just appear once. And, so out here it is fairly clear that the number 3 is the most common one and hence 3 is the answer if you, if the question is, what is the mode. It is the most common number, but and that kind of make sense, if you have a data set, where there are only few numbers that are recurring, but the concept is again generalizable . So, even if you had a data set that look like this, where numbers you know, it does not make sense to ask the question, which is the most common number in fact, no number might repeat itself. But, out here the more essentially is based on the range itself. So, you would say this is the most common range and so this is the mode and so the mode out here would have been something like if 9.5 to 9.6, so great. So, we now understand how mean, median and mode are calculated. Now, let us take a step and see, where do we want to use which measure of central tendency. So, how do we choose between mean, median and mode? The main concept really comes between mean and median. So, let us talk about that for a minute and becomes kind of obvious, where mode is more useful, because it has a very different property associated with this central tendency, so great. If you have to choose between mean and median, much of the debate usually comes down to outliers. The idea of the outliers is that, it is a number or a value that is not really within that set of most of the other numbers that you see. Now, that can be, because of quite of few reasons. When this come about because of an error in the data, so the data set itself could have an error, then it is easy to say that is around you, so this is a bad outlier. But, sometimes this state can be that outlier is very much not an error and it tells an important part of the story. In really simple words, the median is not influenced much by the outlier, whereas the mean is greatly influenced by the outlier. For that reason, the median is often kind of expressed as been a more robust metric to outliers. But, that we need to take a step back, just a second about saying you know outliers can either be good or they need not be good and so, it really depends on what we think about the outliers, ask to whether we choose to go with the mean or median. Obviously, when we think the outlier is a bad think, it should not be there or it is not contributing towards a story that we want to tell. Then, we call it a bad outlier, we prefer the median in that case. So, to actually give you some insight as to, how the outlier affects the mean and not the median. Let us just go back to the previous example. Let us say that in our data set instead of 8 we had 800. So, that is clearly a mistake. Someone by mistake type two 0Õs next to 8 and, so it is 800 and let us assume it is a mistake, it is not obvious. Now, in the case of the median, this would have a huge impact instead of 8 being here you would put an 800 and that would greatly change this number. So, your mean is largely affected, it probably send the number into the 100Õs. Now, it will have no impact on the median, this 8 gets listed, it becomes 800, which means that it is not in it is place, it comes after 9. So, make some space here, so the 800 comes here, but the central two numbers still remain these two 4s, I mean these two 4s, so your answer really did not change. So, in many ways the outlier has like this huge impact on the mean, it has no impact on the median. Now, clearly if what you have faced with this kind of an error, you like using the median, because the mean is susceptible to this problem. There might be other situations, where you want to use the median and again it pertains outliers, but here we are not as much scared about errors, but you are scared that there is this one a typical case, which is just skewing a story that I want to tell. So, a classic example of where medians are used is, when looking at salaries of people, where the idea is that salaries having some sense of exponential. Many people earn a consistent salary and then, there is these few peoples who just earn these catastrophically high amounts and so something like mean, where and here the idea is the catastrophically high amounts are these outliers. And here talking about a mean will not give you the typical salary that a person earns, because of these one or two people who earn very high salaries. So, it is not just errors there might be other situations, where you have outliers and you feel like the presence of these outliers is moving you away from talking really about, what is a typical value, now having said that there are many situations again, where you are dealing with outliers, but these outliers are of very important part of the story. So, let me give you an example of this. So, let us say you have this data set, where you were looking at a particular financial strategy. And in this financial strategy you are looking at how much money you made on a daily basis and, so you have taken some historic data and you want to see you want to see, what is a typical scenario of the strategy you want to evaluate the strategy based on based on this data set. So, let us see this financial strategy actually made you lose 1 rupee every day on 99 percent of the days, but on 1 percent of the days this strategy gave you 10 crores, so large enough number. So, this strategy made you lose money on 99 percent of the days and on 1 percent of the days gave you 10 crores is this strategy you would like to take the very straight forward answer is if you like making money you really like this strategy, because despite the fact that you lose just 1 rupee on 99 percent on the days as long you as you can play this game or you can trade on this strategy in a stock market for long enough period of time here bound to in the long run make good amount of money. Because, 10 crores more than compensates for the 99 days or during, which you lost the 1 rupee. Now, let us see how mean and median would have represented this data right if you got a sufficiently large enough data set of having actually play the strategy. So, let us say you go and you actually collect your data set of the strategy over a 1000 days or less than 10, 1000 days, what would be the median of this strategy the answer is said the median would have been the minus 1 that you that the 1 rupee that you lost. So, quite simply, how is that on that data set for this would look something like this. It would look like this minus 1 comma 1, 1 comma minus 1 comma dot, dot, dot, dot, with the lot of minus 1Õs 99 percent of them are minus 1Õs and then, the odd time you are going to find this are really large number I am not even going to talk about how many 0Õs lots of 0Õs dot, dot, dot for the 0Õs. So, you put this in ascending order and you choose the middle value that is going to be a minus 1, so the median gives you a minus 1. However, you put these numbers you add all of these numbers up together include put your 10 crores in there with lots of 0Õs and then, divided by the total number of data points, which is you know 1000 or 10000 or something whatever the number of data point you have and you going to get a very large positive number positive and large great science. So, here is a story where yes youÕre not you have an outlier you got this huge outlier which is 10 crores. But, the story was in the outlier that is as much real money that you made or lost as the 1 rupee is that you lost. So, here is the case where the mean is probably a great measure to go by if you had to choose whether to play this strategy or not. So, I have that gives you some idea between mean and median, now let us talk a little bit about mode is an interesting one, because it just blank it says I am going to take, the value, which is the most popular and that works fine for you know distributions, which are fairly symmetric. But, in many cases that that people do not find that too meaningful the one big advantage; however, that the mode has is that you can even use a nominal variables. So, you might have a situation, where you are just counting the number of you are coming up with the count associated with a categorical variable an example could be in the number of reds the number of greens the number of yellows and all the mode is going to do is say lets pick the one which has the most number of it and it can also be fairly useful in multi modal distributions. Let me give you an example of where a multi modal distribution and multi moral just means that there are many peeks to the distributions. So, if you go back to this slide and let me just erase that for you. So, the red line is the distribution and we going by the we going talk a lot more about distributions multi modal distribution is one that might look like this, so there are like two peeks to this distribution. So, let me give you an example a real life example of where, you could have a multi modal distribution and, where you might want to use the mode. So, let us say you we looked on this street and this street was a 100 meters long. So, one end of this street is 0 meters and then, there are markers on this street. So, this 1 meter, 2 meter, 3 meter and the street goes all the way to 100 meters. So, if someone said this 75 meter you immediately knew, which point of the street or road we are talking about. So, all the residence of this street need to make a decision, on where to place a garbage can a trash can, which lets say for whatever reason people do not people have strong opinions of that. So, people are all going to go or we going to take we are going to take a survey of all the residence and each ones going to come up with the number. So, person one says I want the garbage can in the 25 meter mark another person says I have want it in the 50 meter mark so on and so forth. Now, let us say we collected all these data and we found that 40 percent of the residence said they want the garbage can in the 25th meter mark. Let us say another 40 percent or let us say 45 percent said they wanted the trash can on the 75th meter mark. So, just to recap 40 percent of the people say they want the trash can on the 25th meter mark you know 45 percent of the residency they wanted in the 75th meter mark and the remaining 15 percent they just its all over between its like uniform somewhere between 0 to a 100. Now, the problem is both mean and median might windup saying the average preference is to keep the trash can somewhere in the 51 52 meter mark, because that is bound to be a central value. Now, that might be something that nobody wanted, where as something like a mode would just categorically say keep it in the 75th meter, because that is the most populist preference. So, in case is, where kind of taking two extremes and averaging them out and in some sense median also does that as long as there are enough data point does not work and in those cases the mode could be fairly useful application. I have just gives you an idea of the difference measures of central tendency. In the next lecture we will take up measures of dispersion.', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '1c2bec1e596a535c272c5108a995b524'}>, <Document: {'content': '6) Descriptive Statistics - Measures of Dispersion\\nDescriptive Statistics: Summary Statistics: Measures of Dispersion Hello and welcome to our course Introduction to Data Analytics. In this lecture, we continue our work in Descriptive Statistics to give you a timeline of where we are. We have discussed within descriptive statistics, the various graphical and visualization techniques and the second half of the descriptive statistics deals with Summary Statistics, The use of numbers to describe and summarize data. Within summary statistics, in our previous lecture we spoke about Measures of Central Tendency. In this lecture, we will be concluding the use of Summary Statistics with discussion on Measures of Dispersion. So, we should be at this point fairly familiar with the use of this data set, essentially the data set is just a sample, which talks about different data points over a certain range and the histogram that you see to the right hand side of this is a histogram that is generated from this data set. So, we covered histograms during our lecture on graphical techniques and we use them extensively in our discussions of measures of central tendency. Again to be very clear, measures of central tendency and measures of dispersion and the specific matrix that we would discussed in them. So, for instance in measures of central tendency we spoke about mean, median mode and today we going to be speaking about some matrix associated with dispersion. All of these matrices don’t in any way shape or form need this histogram. They directly operate on this data set and in some sense, you might say why even talk about the histogram. And the idea is that you are absolutely right, we do not need this histogram, but it really helps to explain the concepts and it is also healthy way to start thinking about these matrices and start thinking about these distributions. It will help us also in the long run, when we cover concepts and probability distributions. So, now, having said that let us talk about what measures of dispersion seek to capture. We spoke about in the last lecture, how measures of central tendency try to capture in some sense a central value; some central value within this range of values that this data set takes up. So, the range of values of this data set takes up is shown here and in some sense, the histogram captures their likelihood in this axis. So, the range is here in the x axis and the y axis is in some sense, the likelihood of seeing that data and we spoke about, how measures of central tendency try to captures, what appears to be like a central value and we spoke about different matrix that do that. Measures of dispersion talk really about, how the data is dispersed around this value, how does the data deviate from this value. For instance, if every single data point and let us for now assume that 10 is our measure of central tendency. One measure of central tendency is the mean, so for now let us just say we are using the mean. So, if 10 is the mean, then if every single data point in this data set was equal to 10 and it is not, what is here, but it is every value is equal to 10. Then, there would be no deviation of data from 10, you would just see a single tall line in this histogram and none of the sides, none of these would exists all together, but that is not the case. Typically, most data sets, the values are going to be different and you might have some measure of central tendency, but there is going to be some amount of deviation of the data points on either side to the central value. Now, measures of dispersion try to capture that, do the values deviate a lot from the center or do they deviate very little from this center and that is what measures of dispersion seek to capture. To understand the different measures of dispersion, let us go back to the data set that we were using, when we were speaking about measures of central tendency. So, I have used the same data set out here and the simplest measure of dispersion is range and the range is quite simply nothing but, the highest value minus the lowest value. So, in this particular case the highest value is 9, the lowest value is 1, so quite simply 9 minus 1 is 8 and that should be intuited for you. The given that the mean of this data set is about 4.5, 4.6, a measure of dispersion is just the max minus min. Now, if for instance if there was very low dispersion, then the highest value would be close to 4.6 and the lowest value would be close to 4.6 and so that, range between max minus min could have been smaller. At the same time, if this dispersion is very high, on the high side and on the low side your max and min value is going to deviate a lot from the 4.6 and so, you would have high dispersion. So, that is the simple one. This second one is the Inter Quartile Range and the idea here is highly related to the concept of median, where you would arrange the data points and you would kind of take a central data point. Another way of saying that, we discussed that procedure of median during measures of central tendency, but another way of thinking of it is that, you are taking the 50th percentile point. With the Inter Quartile Range, what you are doing is you are taking the 75th percentile point or the 3rd quartile and subtracting from at the 25th percentile point. The idea being that within this data set if there is a high level of dispersion, then that range between the 75th percentile point to the 25th percentile point also be high and if the dispersion is low, then this range would be low and it is really noteworthy that this is the concept that gets captured in box plots, which we discussed in a graphical techniques. We spoke about, how in the box plots the upper line of the box plot and the lower line of the box plot, correspond usually to the third quartile and the first quartile of your data set. So, and this is also known as the Inter Quartile Range. So, it might be abbreviated to IQR in some text books, but this is also a measure of dispersion. We then come to, what is a fairly popular measure of dispersion and the idea behind this is to essentially look it, how much each data point deviates from the mean that you just calculated. So, x_i represents each data point, because i goes from the first value to the n_th value, when in a particular example n is 12. And, so we wanted to take each data point, see how much it deviates from the mean. In a particular case for this data set, the mean is 4.58. So, we will take the first data point which is 3, so the data point 3 and we subtract them from 4.58 and square that value. We would take the second data point to the same thing and we would keep adding up these squares and once you add up these squares, you take something that kind of looks like an average and it is not an exact average, because you have this minus 1 and we will talk about that in a minute. But, in concept you essentially are trying to get an average of the square deviation and you will ultimately take a square root of this. Now, when you take the square root, what you get is the standard deviation and when you do not take a square root, you get this measure called variance and variance is also a measure of dispersion. In concept, the only difference between standard deviation and variance is that, 1 is the square root of the other. Again, now that you understand how a standard deviation is calculated. Let us go through some questions that might have come up, when we discuss standard deviations. Given that the other two methods that we have discussed are of fairly straight forward and clear. So, here is some questions that always go with standard deviation. Why do we use this square function on the deviations and what are it is implications? So, what we are referring to here is the fact that, we actually take the square of the deviation. So, why, what is the purpose? If you want to calculate, see if you want to get some measure of average deviation, why not just take the deviation and take the average of it and the answer is fairly straight forward to that. The answer is that just by definition, because you are looking at the deviation from the mean, there are going to be some points that deviate from the mean on a positive side, there going to be some points that deviates from the mean on the negative side. So, 3 minus 4.58 would lead to a negative number, whereas 9 minus 4.58 would have been a positive number and again by definition, because of how you calculate a mean and the math for this is fairly straight forward. You will find that if you just took the deviations, some positive numbers and some negative numbers and you added them up, you would always get zero and that is because of, how the mean is calculated, because the mean is nothing but, the sum of all the numbers divided by the total number of such numbers. So, by definition just taking the deviation would result in some positive numbers and some negative numbers, which should cancel each other out and give you zero. So, what you really trying to capture is an average deviation, but you do not want the signs. So, what is one great thing you can do is to square it all. So, when you square a number, whether it is negative or positive, you always get a positive number and the other really interesting thing is the, only thing that matters is the magnitude. So, minus 3 square is 9, which is also the same as plus 3 square. So, the idea is that the square function is symmetric on the plus minus side and always gives you a positive number. So, for that reason we use the square function. Now, are there some implications of that and the answer is, yes there are some implications. The implications is, the effect that squaring has. So, let us say you had two separate deviations of one unit each, so let us say you had two data points that signified that there was a deviation of one unit. So, given the average is 4.58, let us say you had a 3.58 deviation. So, 3.58 and you had another data point, which was 5.58, so both of these would have a deviation of minus 1 and plus 1 and when you square these two numbers, so you square these two numbers, the answer comes out to be 2. So, that is what happens when you do this entire squaring process. Now, what happens when in one case? So we had two… We just focused on two data points. Now, what happens in one case when you just… In one case, you are right on the mean. So, you are right on the mean, in the other case you are deviating by two points essentially or whatever the unit you are using, a deviation is two units. So, here the deviation, because you are comparing it to the mean, you are essentially just replacing this 4 point, you are replacing this 3 with this 4.58 and we are looking at what would happen. In case, because you are doing that your deviation is zero. Here your deviation is 2 and because it gets squared, that becomes 4 and so, your cumulative deviation in some sense is 4, whereas in the previous case your cumulative deviation was only calculated as 2. In both cases, you deviated by two units from your mean across the two data points. In one case, you deviated by one unit in the first data point and one unit in the second data point, but the sum of the squares led you to a number 2. In the second case, you deviated from the mean by zero data points in the first, by zero units in the first data point and again two units in the second data point. So, in both cases if you just look at the actual deviation from the mean, in both cases you have deviated by only two points, but in the second instance, in this instance you would be recording a square deviation of four units, which is twice as much as the square deviation of the first case, which is two units. Now, many people like that and there are many contexts, where that makes a lot of sense. There are some contexts, where this justice not make sense, but that is one of the implications of squaring the deviations. So, second question is, why do we work on standard deviation and not the variance? So, the idea is, why do we take this square root. Why not just report the variance, why do we report mean, because they both the same function and the answer again is fairly straight forward. You have a data set and some units, that is 3, 4, 3, 1 could have some units and these units could be things like, simple things like rupees or kilometers per hour, whatever it is that you know. You might have then collecting data own and when you report a deviation from the mean, the units would then be in squared if you are using variance. So, if you use variance you will have to report a value that is in square and, so what is it mean to say rupees square. So, what is it mean to say a dispersion is a 500 rupees square and you know, rupees squared is not something that we can understand, it is far more intuitive. When it, truly it is form a meaningful to say a deviation is 23 rupees from the mean, you can make decisions based off of that and you can gather some insights based off of that. So, third question and often a very interesting question is why do we average by dividing by n minus 1 and not n. So, the idea here is that the sum of the deviations is always zero and so the last deviation, because you are essentially doing a series of deviations. Now, the last deviation you can be found, once we know the other n minus 1 deviations. So, we are not really averaging n unrelated numbers you are really averaging only n minus 1, a squared deviations. In some sense, it is almost like only the n minus 1 square deviations can vary freely and we average by dividing the total, essentially by n minus 1. This is also the concept of degrees of freedom, which is how many of the values can actually move freely with, can move freely and still maintain the final statistic and in this case, the final statistic is the mean, because you subtracting each number from the sample mean. Now, the important thing is, this mean which is 4.58 in our case is something that was calculated from this data. So, from the same data, which we are using to calculate the standard deviation, you calculated the mean and that is the reason essentially that you are using the n minus 1. If instead you are not using this mean, but someone came and told you, what the true mean of this data was. Someone said, here is the data set and by the way, the mean of this data set is 5. So, they just told you the data set or you knew the data set from past experience or you are able to compute that, then you would not have to do the n minus 1 and you would do the n, but also out here you would not substitute 4.58, you would be substituting 5. So, in each of these places you would be substituting 5, which is the true mean. We call that the true mean and we call 4.58 the sample mean, because 4.58 you calculated from this data, whereas 5 is something that you knew on principle or you are able to use some other source to know, what the true mean was. Now, another way the people like to describe this is also to say for instance that, if this 3, 4, 3, 1 this data is ultimately a sample from some other population, then you need to essentially do what we just discussed, now which is to use this n minus 1 and take the sample mean. So, again we are talking about the case, where nobody comes and tells you what the true mean is. So, your only hope is to calculate a mean from the data and you calculated 4.58 and because this data set is a sample from something else that generating this data, the right way to do it is the way the standard deviation formula right now is shown. But, if in some sense this data is the population, it is not a sample from some universe, but it is the real deal. Then, again the idea would be to use n and not n minus 1, because this is the true mean and again out here, you would be substituting the 4.58, but then this should be called a population standard deviation. So, it is POP, population standard deviation. But, more often than not in terms of the more realistic situation that you will encounter in life, I think it is fairly safe to say that, if you are taking the sample and you are calculating the mean, use n minus 1. If you given a sample data set, but you already know the mean, that is you are not calculating it from this data set you already know the true mean. In that case, you can just go ahead and use n instead of n minus 1 and that would be the right standard deviation. So, that is as far as standard deviation goes, but before we conclude on measures of dispersion, it is worth mentioning that there are some other measures of dispersion out there and these are called mean absolute deviation and there are many variance to it. But, the core idea is that, with mean absolute deviations you replace what you use in standard deviation, which is the deviation of each point from it is mean and squaring it, you replace that with an actual deviation. So, the deviation and this sign which is the two vertical lines on either side, what the essentially mean is that the negative symbol just goes away. So, a 3 minus 3.58 minus 4.58 which would result in minus 1 would just be written down as a 1 and so would a 5.58 minus of 4.58. So, negative signs are just taken off and then you do and the other operation of the same. The good thing with mean absolute deviation is that, it has lot of variance, so it is like what is the average deviation from the mean, that is the typical case and that is what I have written down here, but you can also replace this x bar with the median of the x’s. So, the mean absolute deviation from the median is another case and you also have cases like, what is the median absolute deviation from the mean, the median absolute deviation from the median. Obviously, our previous lecture on understanding the pros and cons of means and medians would play an important role in making such a selection. So, that should conclude a lecture on measures of dispersion. In the next lecture, we will continue with descriptive statistics, but focusing more on distributions. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'c0314970dfb5dd6114761134ae658434'}>, <Document: {'content': '7) Random Variables and Probability Distributions\\nHello and welcome to the next lecture in our course Introduction to Data Analytics. In this lecture we are going to be talking about Random Variables and Probability Distributions and this would be the first lecture of the series that cover this topic. Just a recap on what we have completed so far, we finished looking at Descriptive Statistics and the use of various use of various graphical and visualization techniques in descriptive statistics as well as the use of summary statistics. Within summary statistics, we looked at measures of centrality and measures of dispersion. So, jumping into this topic, quick question is why do we need to talk about probability distributions. What does it have to do with data? It is just like a mathematical concept, why, what does it have to do with data. And the quick answer to that question is, if you go back to the use of the histogram we express that as a way of describing data. Essentially the histogram is, if you look at this picture on the slide, the histogram is those vertical gray, grayish blue bars that you see on this graph and that describes the data that summarizes the data in some way. But, you might be of the belief that if you redo this exercise, if you collect a new sample you will get bars that looks slightly different and the question is, is it really coming from a probability distribution, is it coming from some other mathematical function that closely approximates this histogram that you are seeing. And that red line that you see on this is the attempt to fit this mathematical function and the core idea here is that, this data is being generated by this probability distribution function, which is that red line and the histogram is, what you see in terms of data. Because, not every time you going to get data that looks exactly like the red line, so it is in this context that you can think of a probability distribution also as a way of just describing your data. But, I would say describing and not summarizing, because it is fairly comprehensive, it just does not give you one number or one thing. It gives the full form and shape of that data and you can think of it as an exercise also in modeling your data. So, you are not just describing it, you modeling it. So, in that context probability distributions are very important and we will also see how in various other things, not just describing data, but even in terms of doing more advanced analytics in the machine learning parts in the statistical inference parts, the use of probability distributions is critical. The think of a data set has random numbers that are being generated in accordance to some mathematical function is the whole idea behind, the use of probability distributions with respect to data. To do this, we kind of have to understand some basic concepts, which is and the first basic bond is to understand what random variables are. Random variable is essentially a variable whose value is subject to variations due to randomness as a post to variations due to some other phenomena. So, we are all familiar with the concept of constant, which just means it is a fixed number and variable. Here I am talking about the variable that you probably learnt in algebra in high school, non random variables and there you learnt that the variable is essentially something that can take on many possible numbers or any possible number. But, the distinguishing factor between a variable and a random variable is that, with a regular variable once you fix all the externalities, then the variable takes on a specific value. So, let me give you an example. So, you take something simple like, force is equal to mass times acceleration. So, force is equal to mass into acceleration and you might say, all these are variables and they are and that is true. So, force can be any possible value, given what I have just told you, force can take on various numbers as it is value with some units. But, once you fix mass and you fix acceleration, force will take on a very specific value. As opposed to a random variable, where even if you fix all the externalities, the best way to describe the random variable would be to say that it can still take on a set of possible values and that set could be a very large set, it could be infinitely large set. But, the variable itself can take on many possible values and each of those values have a specific probability associated with it and beyond that, you are not going to, you cannot reduce the variable beyond that by definition. Even after you fixed everything around this variable, you still have to describe the variable with a probability state space. So, let us, the best way to again get this even deeper, to understand this even better is to talk about somewhere you specific probability distributions and that is what we are going to do in the next part of this class. But, before I proceed I just wanted to tell you that, I am broadly breaking up the idea of probability distributions into discrete probability distributions and continuous distributions and that is just to give you some structure into it. Those words might not make immediate sense to you right away, but what we are going to do right now is to look at discrete probability distributions. You might also notice that I am using the word probability density functions and you might not know over that word means yet, but very soon we are going to be talking about that is well. So, great, so we are going to look at the most simplest discrete probability distribution and this, it is really simple, because I think we all used it in some sense in our daily life. So, we are here, so let us look at the first example, I will give these numbers just show that going forward, it is clear. So, we are looking at number 1. In number 1, matches the closest with a colloquial use of probability, chance, likelihood and so on and we are saying something simple, which is that the probability something happens is x. So, you might say the probability that rains today is 10 percent, is the 10 percent chance it is going to rain today. What goes unsaid is that, there is therefore, a 90 percent chance that it does not rain today and that is what is captured in this graph. So, we more used to saying, this is 30 percent chance there it is going to rain, this is 20 percent chance that there will be an accident, there is a 10 percent chance that the product that I am manufacturing is not fit to be shift. But, essentially we are talking about these kind of binary events, where one of the possible outcomes is x and therefore, by definition the remaining possible is just 1 minus x or if you thinking of it in terms of percentage, this is the 100 minus x. Again a very simple example of this could also be something like this, there is a 50 percent chance that, if I toss this coin I am going to get a heads and what goes without saying is therefore, that there is a 50 percent chance that you would not get heads. In this case, that is called tails, so great. So, that is one very simple conception of probability and this is a probability distribution, it is called Bernoulli distribution, but we can move to multiple outcomes. So, if you look at number 2, what we have there is, what you get when you role a dice. So, you role a dice and a dice has six faces and on each face you have a dot. So, you have, if you role a dice you either get a 1 or a 2 or a 3 or a 4 or a 5 or a 6 and the idea is that the probability of each of these is one sixth, if it is a fair dice and so, that is a different kind of a probability distribution. We will soon learn in our next class that is called discrete uniformed distribution, because they are all the same probability, but the possible outcomes going back to our definitions is set. So, the possible outcomes possible values are 1, 2, 3, 4, 5, 6; the probability associated with each of those possible values is one sixth and one thing that you might have noticed by now is, if you take all the possible values and you take each probability and you add the mole up, you always get 1. So, in the case of 1, we saw that if the probability of it raining was let us say 30 percent or let us say the probability it rains today is 30 percent and the probability it does not rain therefore, becomes 70 percent. You add those two up you get a 100 percent or you get 1. Similarly, you have six possible outcomes when you role a dice and there is a one sixth chance of each of them happening and six times 1 by 6 is 1 and the intuition for this should also be obvious, that if you role a dice or if the day passes means something has to have happened. So, you have had to have gotten one of those six numbers or you know it either rained or it did not rain, but as long as you have comprehensively covered the universe of possibilities, then something needs to have definitely happened within that universe. So, therefore, that should also been intuition us to why, that the probability distribution sum to 1. What we have in number 3 is the idea that, again it ties to this notion of probability not just being a theoretical exercise and you might actually have some data and you might choose to define the probability distribution based off of what you see in the data. So, if somebody came to you and said look, I do not want you to assume that, so I wanted to take this coin and I wanted to describe this random variable, which is the probability of getting a heads or a tails and that is the random variable and I do not want you to assume, there is a 50, 50 chance. So, you might say fine, I have nothing I cannot assume anything and you might toss the coin a few times. So, you do a data collection exercise, where you toss the coin 30 times and you notice that you get 14 heads and you get 16 tails and for whatever reason, if you do not want to assume anything about the distribution and let us say, you also do not want to do any statistical inference, again a topic that we will cover soon. You might just be contained and saying, I am going to describe this random variable with the actual data that I see. So, I am going to actually say that a 14 out of 30, there is a 14 by 30, because you actually got 14 heads when you toss the coin 30 times. So, I am going to actually say 14 by 30 is the probability of getting a heads and 16 by 30 is the probability of getting a tails. So, there is nothing wrong with doing something like that, we would have to see if that was actually made sense do, but nevertheless if you said I just wanted to take data and I wanted to describe a probability distribution with the data that I see, then you can definitely define a discrete distribution in this way. And now, we go on to something that is a little more complicated, which is continuous distributions. In the previous case, in the discrete distributions and let me erase this, all the ink. So, in the discrete distributions, what made as discrete, were that the possible outcomes would discrete. So, it was either an event or a non event, so that is discrete, there is no half event. So, there is no half event, not there. Same way are here, you either get a 1 or a 2 or a 3 or a 4, this set of possibilities that is the here x axis essentially has some countable number of possible states and so, you cannot get a 1.5 when you roll a dice and similarly, you cannot get a half head, half tails when you toss a coin. So, that is essentially the concept of it being a discrete distribution and with continuous distributions; however, that is not really true. The idea is that the x axis are here, so it is the same idea which is the possible outcomes are on the x axis, same thing that we saw on discrete and the probabilities are on the y axis. Case of this is the same core concept of describing the distribution, but here the x axis is not discrete. So, what is that mean? What it means is, you take something like the probability of a certain height. A height can be a 130 centimeters, so that could be one number out here, but it could also be 130.001 centimeters, it could also be a 129.999 centimeters. So, there is no inherent discretization. You might turn around and say, look what if I had a measuring scale that could only measure in 5 centimeter intervals. So, the way I mean or I can only measure up to a centimeter, I cannot measure less than a centimeter, because the scale that I have does not have more resolution and that is fine. You know, if your resolution for measurement is still accurate, meaning that anything between 130 and 131 gets called a 130, because of the inherent resolution of the scale. That is fine, you can create a discretized version of it, but the idea is this nothing about height or any, essentially the measurement of space. There is nothing about height that is inherently discretized, like the measurement of a dice is inherently discretized. You cannot possibly roll the dice and get a two and half, whereas if you had a five and half measure of height, you could get any possible value within the certain range. So, you might have the lower end of this being 20 centimeters and the upper end of this being a 180 centimeters. But, essentially any value between it is possible and we kind of spoke about the same concept when we spoke about discrete and continuous variables. The same concept of discrete and continuous variables applies to discrete and continuous distributions. Now, again another important thing to note is just like in the discrete distributions, where we said the sum of all the probabilities for each possibility should add up to a 100 percent or should add up to 1. Here you cannot have a countable number of possibilities, so you cannot take each possibility. Take the probability of that and added to 1, just because there are infinite number of them. So, you basically what you do is, you take the entire interval and sum the probability within that interval and the best way to do that is to look at this area as a whole. So, this way when you look at this area, it is like you are taking all the possibilities within that area and summing all the probabilities for each possibility and when you do that, you will be getting 1 or a 100 percent. So, now let us now that we understand both continuous and discrete random variables. Let’s just briefly talk about the use of probability density functions and cumulative density functions. So, far what I have been graphically showing you, have all bend probability density functions. For each probability density function, there exist a cumulative density function and so, I will describe it in the continuous case and that is the easiest and I separately do that for the discrete. The idea with the PDF is that, like we said for, because there are infinite number of possible states, you really does not really make sense to ask the question, what is the probability at a given point. It turns out that the answer to that question is that the probability of that given point is zero, although there is some y, there is some height for that given point. Because, there are infinite such points out here, because there are technically infinite such points. As a result, you can only say what is the probability of a given area, so you can say I want to look at this area and you can get the answer to that question, you can get the probability of this given area by just measuring the area under this curve and that is what I have done with the gray line on this graph as well. But, the idea behind PDFs to CDF is that, the CDF describes the cumulative probability up to a certain point. So, if I would ask the question, what is the probability within this area, you could answer it from me by looking at the area under the curve. If I ask the probability density function at this given point, you can use the function to figure out what this value is, but the probability itself is zero. But, the cumulative describes the probability from zero, from the lower end of this axis and that can be zero or that could be something like minus infinity or it could be some other value. You know this starting point essentially, from that starting point all the way up to the point of interest. So, this is x, the PDF describes the height out here for x, the CDF describes… So, this is your point x, the PDF describes the height of this curve at x and which is y, this entire curve out here this curve that is here on the left hand side is the PDF, whereas the CDF describes the area to the left of this point x and that area is the CDF. So, this graph is nothing but, the same as the graph to the left, where at each point x you are looking at what is the area to the left of x on the PDF and that is what you are plotting here and that should logically be equal to 1, when you complete and the reason for it is the following. We already discussed in the previous section is to how the overall area under this curve. The overall area under this curve is equal to 1, correct. We discussed that if you take all the possible states and add the probabilities of all the possible states, as to how you are getting up, you would get a probability of a 100 percent or 1 and. So, the CDF is nothing but, this description of the area to the left of the curve and when you reached your entire set of possible heights or in this particular case heights, but it can be anything else, then the CDF hits at the one mark when it ends. And it is, it would be helpful for you to know that therefore, the easiest way of getting from a PDF to CDF and a CDF to PDF is, from a PDF to CDF you would essentially want to integrate. For those of you, who used integration in high school or if you heard of the term integration, that is that symbol that looks like this and the idea is that an integration covers this core concept of area under the curve. So, if you integrate up to a point x, so let us say just for as an example that this started at zero, but you could change that. It could start y, it could started minus infinity, but you essentially if you integrate the area under this curve to get the CDF. You would just want to integrate from 0 to x, some f of x and the f of x here is your PDF function and that will give you the CDF and the idea of going from CDF to PDF is exactly the opposite, which is to differentiate this CDF and that will give you the PDF. So, we will conclude this lecture with that note and starting from the next lecture, we will be talking about some actual probability distributions and we will go through at least the most popular ones in that lecture. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'e4adbe8262b232f584747441eb49a979'}>, <Document: {'content': '8) Probability Distributions(contd)\\n Hello and welcome to our second lecture in this series on Random Variables and Probability Distributions for a course Introduction to Data Analytics. In the previous lecture we saw, we made a more broad introduction to the concept of random variables and distributions, we spoke about the concept of having continuous distributions and discrete distribution, we presented some examples. And we also spoke about the idea of a probability density function versus cumulative density function. In todayÕs lecture, we are going to pick a 5 or 6 very common distributions and discuss them one at a time. And the first one that and these distributions are going to be both, some of them are going to be discrete, some of them are going to continuous and some of them could be both. So, we start with the most common distribution, which is the uniform distribution. The uniform distribution has a discrete version and a continuous version. Now, we will start with the most simple one, which is the discrete version. You already seen examples of this. So, for instance the example that we saw on the last class of the six sided dice, where we were quantifying the probability of getting any particular face values. So, the face value of the dice is essentially, you throw the dice you get something on top, so you see the 1 or 2 or 3 or 4 or 5 or 6. So, the probabilities associated with these 6 possible outcomes, assuming a fair dice is one sixth for each and, so that is example of discrete uniform distribution. We also saw the case of the coin toss, where here you have only two possible outcomes and as long as they are both equal, it is still uniform. So, the formula in terms of the PDF the Probability Density Function, which we discussed in the last class, it is fairly straight forward, it is just 1 by k when there are k outcomes. So, if this is 6 sided dice it is 1 by 6 for each of those six possible outcomes. If it is coin toss, which is two possible outcomes it is going to be 1 by 2 and the ideas that for each of those k possibilities, it is 1 by k and for the rest of the universe, it is 0. So, the probability of getting a 7 when you role a 6, a single 6 sided dice is 0, so you cannot get 7 and you cannot get minus 45 either, so that is what this formula says. With the continuous version, here again you are looking at a uniform distribution, so the probability is a uniform, but like we discussed the variable that we are quantifying is continuous. So, the variable in the six sided dice was, what is the number that shows up and that number is either 1, 2, 3, 4, 5, or 6 and that 6 discrete possibilities, but you might have many things that are not discrete and this goes back all the way to our discussions and quantitative variables, which can be continuous or discrete. So, examples of this and the truth is the uniform distribution, while it is theoretically very intuitive, could be quite convenient in some cases. There are not a lot of examples, real world things that tend to be uniforms, some of them are something like number of seconds pass the minute. So, if you were to randomly, if you have to have random process, which just looked at the clock over the course of date during some random intervals, the number of the seconds pass the last minute could be uniform and that is essentially a space, where you can have 0 seconds pass the minute all the way to 60 seconds pass the minute. So, your interval is from 0 to 60 and where you find yourself in that interval is described potentially by uniform distribution. And again the idea that it is continuous, just means that you not discretized the seconds as 1 or 2 or 3 all the way to 60, you could be in that in 4.5432 seconds pass the minute. So, it is continuous, the variable time is being monitored continuous as a continuous variable. Another example could be the exact age of the randomly selected person between the ages of 50 and 60 perhaps in a certain country. Now, again you know you have to come up with fairly specific examples, because something simple like the age of the randomly selected person is not likely to be uniform. You are not likely to find as many people between 80 and 90, as you are between 70 and 80. And even if you stop the clock at a certain point with typically with things like age, you see that the number of people or the probability of a person in that window decreasing as sometimes age increases and it depends on your state space and it depends on your countries, is the population increasing decreasing, so on and so forth. But, sometimes over a short enough interval, even if the overall distribution is not uniform you can create an interval and say that is my universe. So, my universe is people between the age 30 to 40, within that interval perhaps the distribution of the exact age of people. So, imagine I take the universe of all people between the ages of 30 to 40 in India and then, I try to create a distribution of the exact age of a randomly selected person from this bucket and that could potentially be a uniform. The uniform is also a great distribution, think of when you want to make absolutely no assumptions. So, if I want to make the assumption that the countryÕs population is and some of these need not even be assumptions, so it could be a fact. So, something simple like the countryÕs population is increased and so on, then I would be hard pressed to come up with a uniform distribution and I might find richer distribution to represent my data. But, this uniform distribution can be thought of us like, this ground 0 I do not make any assumptions, the probability of everything occurring within a certain interval is equal and, so I could potentially use it. I have also come across the usage of this a little bit in different aspects like physics and chemistry and so on, for instance the probability of certain types of molecules be in certain locations over certain space could be uniformly distributed and so on. So, we have spoken about, so the formula for the uniform distribution, we spoke about the discrete uniform PDF and we said, the PDF is essentially 1 by k. So, with the continuous, the PDF is essentially 1 by b minus a. So, the idea here is that, this is essentially a and this is essentially b and as you can see in the graph, the probability between a and b is uniform and just like in the discreet case, if this distance b minus a, then the probability is 1 by b minus a. And another way, another quiet simple way of thinking about it is, between this interval, over this entire interval from the lower limit to the upper limit to the area under this curve should be equal to 1 and if you look at the simple math of it, 1 by b minus a times b minus a, which should be, so the height is 1 by b minus a, the length of this rectangle is b minus a. So, 1 by b minus a times b minus a, would give you 1 and, so you can essentially think of it that ways well. And; obviously, like we discussed it has to be 0 for if you are less than a if you are greater than b. So, what is this CDF of this distribution? Again, it is x minus a by b minus a, we will not be exactly deriving it out. We will give you formulas at the end of this lecture to give you an idea of, how to get to the CDF, how to get to the mean, how to get to the variance. But, the core idea here is that, essentially you take some point and let us call this point x, so this is x, there are better x. And the whole idea is that we know this CDF is essentially the area under the curve to the left of x. So, the question could be if you knew that this height is 1 by b minus a, how do you go about writing out this area, how do you calculate this area and it can be a function of x. So, that general formula would be the CDF and the idea is fairly simple. If this is x and you are looking at the CDF is a ratio, it is essentially the area, we know that the total area is 1. So, this total area of the blue rectangle is 1 that we have discussed. So, what percentage of that rectangle have you essentially covered and the idea is, because it is uniform, because this height is constant. If this is x and this is a and this is b, then of the 1, you covered x minus a is this area that you have covered and b minus a would be the full area that you could be possibly covers. If you think of it as a ratio of, how much have covered and how much I can potentially cover, where this b minus a kind of represents the 1 in some sense of the full area. Then, I have covered x minus a of the b minus a that I could cover and therefore, I covered the percentage in some sense, where; obviously, if x was equal to b, then I would have covered the 100 percent and that would be equal to 1, so that is the over all idea behind getting this formula. Again the formula for the mean should be fairly intuitive, if this is a and you know, you have this is b, this central point in some sense is b by you know half of between b and a essentially. And again with variance we will not derive it, accept to tell you that the core concept is to say how much do we deviate on average from the mean. So, we will talk through some of this formula some of it is to give you an intuition, some of it is to actually give you these problems in the assignment. So, you get a feel for actually figuring out what the mean of variance exactly is. Let us move on to the next distribution. The next distribution we are going to talk about is the binomial distribution. So, the binomial distribution is also another distribution, where you have a lot of toy problems associated with it, but by nature of it in the real world sometimes it is more useful to approximate it with another distribution and user, but what exactly is the binomial distribution, let us start there. So, we spoke about this class of distributions and if you did not, then let us just do that right now, so let us. So, there are these class of distributions where called the Bernoulli distributions and the idea behind that distribution is that, it is very similar to the first example we saw in the previous lecture, where you have an event and it has some probability. So, the 30 percent chance is going to rain, that is what the exact example we use. And, so therefore, the probability that it is not going to rain is 70 percent. What is key about it is that, there are only two possible outcomes and these two possible outcomes, discrete outcomes sum to 100 percent and that class of distribution is called Bernoulli distribution. So, our standard example of tossing a coin and saying, what is the probability of heads and what is the probability of tails, would be an example for Bernoulli distribution. Now, if just, so that there is no confusion if it happened that the probability of heads and tails are both equal, you can also think of it as a discrete uniform distribution and, but out here we are just saying we saying something different. We saying that Bernoulli distribution are distributions, where there are only two possible outcomes. The probability of the two outcomes need not be the same as required by the uniform. But, the key out here is that there are only two possible outcomes, two possible discrete outcomes that sum to a 100 percent. So, great, so we spoken about the Bernoulli distribution, what is it happen to do with the binomial distribution, is not that, what this slide is about. Well, if you take the problem and you quantify the probability of heads and tails, then you are talking about Bernoulli. But, instead if I rephrase that problem and said, what is the probability of getting 5 heads out of 10 tosses. Then, I am describing the problem associated with a binomial distribution. So, an example of a binomial distribution would be, if I said I am going to toss the coin 10 times, I am going to take part in a BernoulliÕs process, n number of times and I asked myself the question, what is the probability of getting k successes and success could be defined as getting a heads or it raining or whatever it is. It is one of those two outcomes essentially; you call one of those two possible outcomes of a Bernoulli process as a success. And then, you say, what is the probability of getting k successes out of n possible trials? So, if I say I am going to toss a coin 10 times, can you tell me the probability associated with 0 heads, 1 head, 2 heads, 3 heads, 4 heads all the way up to 10 possible heads. Now, note if I toss the coin 10 times, I can either get 0 heads or any of those numbers in between till 10 heads, I cannot get minus 1 heads I cannot get 11 heads. So, the probability associated, if I toss the coin 10 times, the some probability that I am going to get 0 heads, the sum probability that I am going to get 5 heads. But, each of these numbers each of these probabilities when added together should again be equal to 100 percent or 1. So, quantifying the probability of getting k successes out of n trials of a Bernoulli process is the binomial distribution and you can think a various real world example. So, for instance the probability of, you know let say there are 10 mergers the companies considering. So, in mergers and acquisitions since it is a small enough number, what is the probability of getting 3 out of 10 of them or you might say, probability of having 5 defective products in a batch of 20 products, what is the probability of the 3 defective products, answering questions like that. And one just I noticed, you might have noticed that I have take an example of small enough a numbers. So, technically the binomial could be answering a question like, what is the probability that out of you know 1 million possible toys. Let us say that I distribute, I am a toy maker and I send out 1 million, what is the probability that I get 500, 5000 toys as broken. And technically that would still be a binomial distribution, but what we will learn probably in the next classes, why thatÉ The computation of that is a little messy, you get very large numbers and you have some other distributions that can approximate something like this to solve problems like that. But, the core concept is this. You have a Bernoulli process, where something isÉ You are looking at something that is binary a or b, 1 or 0 and you turn around and you say, what is the probability of getting k successes out of n trails of this process. And, so logically the formula and note that I have used the word PMF of here, that is an important distinction that is worth mentioning. PMF is the same thing as PDF, PDF Probability Density Function, we spoke about that we spoke about probability density functions and cumulative density functions. PMF is just the standard way of calling a PDF if you are dealing with a discrete distribution. And since this is a discrete distribution, because you cannot get out of 10 tosses of a coin you cannot get 3 and half heads. So, you can only get either 0 heads, 1 head, 2 head or 3 head and so. Since this is a discrete distribution, it is technically called Probability Mass Function or PMF. So, the PMF, this distribution should be fairly intuitive, it is nothing but, n choose k, so this symbol out here just means you might have seen it like this. So, that is n choose k and that is has to do with a combinations, something you might have studied in the permutations and combinations. The idea is, how many ways are there of choosing n from k trails. So, here I think the words n and k are swapped as suppose to, what I was mentioning earlier here. So, here for instance I was interested in finding out, what is the probability of getting 5 heads, then it would really be 10 would be the n c 5. So, how many ways are there of getting 5 of choosing 5 out of 10, so I toss the coin 10 times, there are many ways in which, I could get 5 heads out of 10 tosses. It is either that the first 5 could all be heads and the next 5 could all be tails or you can have 1 head 1 tail 1 head 1 tail. So, that number of different ways in which I can get 5 heads out of 10 tosses is, what is being quantified by this number, which relates to this part of the formula. Once I figure out the numbers of possible ways, this part, which is p power k tells me, what is the probability of getting the 5 heads in the first place. So, it might be 0.5 power 5, if the probability of head was different from a tails, let us say we were dealing with the problem where 60 percent chance of heads following, because it is an un even coin, then it could be 0., it will be 0.6 power 5 and this part, which is the remaining part talks about the probability of getting the tails, the remaining 5 as tails. So, essentially the 3 parts of this formula are the different ways of getting those heads, probability of getting so many of those heads, probability of getting the remaining number as tails. So, that is essentially, what the PDF captures and the formula for the cumulative density function again is the same logic that we were discussing with the uniform, which is that for a given k, which might mean for let us say I am interested in knowing, what is the probability of 6 heads out of 10 tosses. You are essentially looking at nothing but, everything to the left of the curve and to the left of the curve here means, what is the probability of getting 0 tails I mean. So, let us see what is the probability of getting 6 heads is the question out of 10 tosses and that would be nothing but, the PDF of getting 0 heads out of 10 tosses, 1 head out of 10 tosses, 2 heads out of 10 tosses, all the way up to 6 heads out of 10 tosses, that summation of those probabilities, because that is essentially what would be to the left of the curve. And here I am doing nothing but, thinking of a curve, where 1 out of 10 tosses. We should start with 0 out of 10 tosses and going all the way to, you know 2 dot, dot, dot, dot 6 and then, it will go 7 and it will go all the way till 10 out of 10 tosses. So, if this was some kind of PDF, I am essentially looking for this area and the curve and you know, the top part of the line is not uniform I am just representing, which side I am interested in, that is all I am doing there and also it would be discrete. So, you have discrete rectangles popping out of the 1, 2 and 3 and you just summing them up and the formula also with the summation does not really simplify too much, so you are dealing with the CDF as it is. As I mention it is more useful for small values of n, when you get really large values of n, you took it other approximations. The mean is nothing but, the number of times you are looking at times of probability. So, if the probability of getting the heads is 60 percent and you are going to toss the coin 10 times, the average number of heads you are going to receive is nothing but, 10 times 60 percent, which would be 6. So, on average I should get 6 heads out of 10 tosses, because this is 60 percent chance of getting heads. And, so the intuition of that should be fairly obvious and again with variance, the intuition itself might not be very obvious. It is really about looking at how much the deviation, how much of a deviation there is from the mean. So, how much does one out of 10 deviate from the mean and we would look at the probability that you would first of all see of 1 out of 10. And again I think there the formulas for variance might probably help you to get a better idea of it, great. So, let us move to our next distribution this is also a discreet distribution and this is very similar to the binominal in many ways both of them are discrete, but essentially while this is discrete, which also counts the number of possible occurrences x. So, it says, what is this quantifies the probability of having getting x occurrences. So, probability of getting one of those occurrences two of those occurrences, but it is over a certain period of time or space see in the binominal we were saying, what is the probability of 5 heads out of 10 tosses. So, the 5 heads was discrete but, so was the 10 tosses here you are asking the question, what the probability of x is where x is discrete meanings, what is the probability of there being 5 people. But, instead we wonÕt say out of 10 people we will instead say, what is the probability of there being 5 people coming to this bus stop over the next 5 minutes the key difference is 5 minutes is continuous as oppose to 10 tosses, which is discrete the ramifications of that is; however, low the probability is of people arriving in a given time there is still technically a probability, that you have a really large number of people infinite number of people coming over the next 5 minutes, where as I can say with certainty in the case of the binominal that out of 10 tosses I cannot get 11 heads. And this has nothing to do with the probability of getting a heads in the first place even if I had a probability of 0.1 of getting a heads in a toss, because of such an unfair coin I still know or 0.9 whatever you know whatever the probability is I still know that its technically not possible to get 11 heads out of 10 tosses. Here, you are looking at a discrete occurrence such as probability that of n number of defaults in a given month or you know number of people, who are going to arrive at the bus stop in next 5 minutes the number of defaults the number of people, who arrive in the bus stop are discrete you cannot have less than 0 people I mean this in this particular case you cannot have less than 0 and you cannot have two and half people who arrive at the bus stop, so it is a discrete distribution. But, the distribution is defined over at time or a space, which is continuous. So, it is not how many heads can I get out of 10 tosses its more, how many of a certain occurrence can happen over a certain period of time and that is the core concept there the PMF again note it is not the PDF, because it is a discrete distribution its characterized as lambda power k and lambda here is a parameter. So, it is essentially a number that that you have and it represents like the average rate. So, 3 people are arriving per minute that would be more like a three out there and k is the variable of interest where you say k is equal to 1 and you get a probability you say k is equal to 0 you get a probability. And technically this k can go all the way up to infinity like I was discussing right and the sum of all those possibilities discrete possibilities is equal to 1. So, if you wanted to know the probability that 3 people will arrive at the bus stand all you will do is you will say, what is the average arrival rate. So, may be the average arrival rate, which is what is essentially represented by lambda. So, the average arrival rate could be something like well two people arrived per minute, so that is two and I want to ask the question what is the probability 0 people will arrive over the next minute. So, I would say on a average two people arrive at the bus stop k is that 0. So, I will put this I will substitute k with 0 and this will this formula will spit out a answer, which is the exact probability of 0 people. And you will replace k with 1, 2, 3, 4 and so on, all the way up to infinity and the sum of all those probabilities are is essentially, what is essentially the distribution is essentially, what quantifying. Because, of how it is defined the mean is lambda we defined lambda as essentially that rate parameter and its interesting property of the that variance also is lambda and lambda has to be greater than 0. But, again just to recap this stateÕs space for a Poisson distribution that is the possible value that k can take are always greater than 0 in greater and less than and goes all the way up to infinity. But, it is a discrete distribution, because you can never have two and a half people arriving at a bus stop. So, the next distribution we are going to look at is the geometric distribution the geometric distribution is also an discrete distribution, but it is a very interesting counterpart to the binominal distribution. So, take the fact that we said that is a Bernoulli process, which means there is some probability of event happening and some probability that the event will not happen and they add up to one. So, probability of a heads, let us say is 60 percent. Therefore, by definition probability of the tails is 40 percent that was Bernoulli and then, we said the bi nominal is nothing but, the probability of getting k successes out of n trails. So, what is the probability of getting 2 heads out of 10 tosses 3 heads out of 10 tosses that space is defined by the binominal the geometric defines the probability of the number of times you need to toss the coin before getting your first heads or you can think of it as getting the next heads. So, number of attempts before an event is what you are looking at and before you can think of it as in inter arrival counterpart to your binominal distribution. So, if you take the coin toss case right essentially it is more like, how many times do I need to toss the coin before I get my first heads or a next heads. So, you start at some point and then, you say so; obviously, anything that I have tossed before it is not influence my future tosses, because they are independent. And now, I am going to start tossing the coin and tell me the probability that I will have to toss the coin 0 times before I get my first head or 1 you can count you can say, what is the number of tails I am going to see before I get my next heads. So, that could be 0,1,2,3 and technically it can be infinity meaning they could be this really bizarre world, where even though there is a finite non zero probability of getting a heads whatever that number is it could be 0.9 or it could be 0.1 its technically possible that I wind up tossing the coin and infinite number of times I keep getting tails and, so am still waiting for my heads. So, this is again a distribution, which starts with 0 and goes all the way to infinity depending on how itÕs defined exactly the geometry is sometimes defined as the number of tails you see before the next heads. And there the distribution starts at 0, because you can see 0 tails another version of this distribution can could start with, how many tosses do you need to make to see the first heads in that case the very you need to at least toss the coin 1 time to see the heads. And, so both these distributions you might find in text books the PMF and CDF should be fairly intuitive all we are doing with the probability mass function you are saying, what is the probability of getting k minus 1. So, here you might say k minus 1 is nothing but, the number of tosses before the actual success that you keep getting the tails. So, if p is the probability of getting the heads you are saying, what is the probability of getting k minus 1 tails. So, if you here we are defining the distribution as on the first toss I get the heads then what is the probability that, so let us say I want to know the probability of getting it taking 3 tosses before I get a heads, then you are saying; that means, for the first two tosses 3 minus 1 is 2 the first 2 tosses I should have gotten a tails. So, what is the probability of getting a tail like that and that is 1 minus p. So, 1 minus p to the power of k minus 1 says before that success happens I need to say I need to say k minus 1 failures and what is the probability of that and then finally, the probability of that one success. Again essentially the CDF you are doing a summation, but the summation kind of neatly simplifies to the formula that we have shown here and the mean being 1 by p should be fairly intuitive meaning if the probability of getting a heads is let us say 10 percent it should be intuitive that it on average should take 10 tosses before I get my first heads, so 1 by 0.1 would be 10 and so on. And again even if it is not entirely intuitive to you may be working through some problems and formulas, where we will be discussing those can help. Finally, the variance is also 1 minus p by p square and even that is something that you might have to work through a couple of times. So, we come to the last distribution that we are going to be discussing in this class and that distribution is the exponential distribution in many ways the exponential distribution essentially you know how the geometric was looking at the inter arrival time of a binominal distribution the same way the exponential looks at the inter arrival times of the Poisson distribution. So, what does that mean in terms of our examples you know how we spoke about the Poisson distribution as being discrete distribution, where you said, what is the possibility of three people arriving at the bus stop in the next 5 minutes what is the probability of two people arriving at the bus stop in the next 5 minutes, so on. So, you have fixed the time 5 minutes or ten minutes whatever is of your interest and you looked at the probabilities of various discrete possible occurrences. So, you said what is the probability zero people arrived one person arrives two people arrive three people arrives. Here with the exponential you are describing the inter arrival that is how long should I wait before the next person arrives. So, it is the exact same thing the exponential is the same thing to the Poisson the way the geometric is to the binomial. The binomial is quantifying the number of the probabilities of getting 3 out of 10 out of 10 tosses the probability of getting 3 heads, where as the geometric saying how long should I wait before the next head heads arrives. The Poisson is quantifying over some time scale or space scale its quantifying the probabilities of different occurrences like, what is the probability of 1 occurring 2 occurring 3 occurring this is how long should I wait before the next thing occurs. So, you can think of the exponential as moving in time and waiting for that next occurrence and how long should I wait, what you can think of it a space I keep walking and you know I encounter occurrences over some length scale the poisons gives me the probability of seeing n number of such occurrences. But, if I start walking on that scale how long should I walk before I get the next occurrence and long can be in length or it can be in time. So, think of it is time or space, but essentially the continues version of the you can think of it as the continuous version of the geometric distribution. The more again the PDFs and CDFs it is there for your reference, but the important thing about this distribution is that people call it memory less meaning that the probability of something occurring over a time if the same if you condition that it is not happened yet. So, think of this way I am not saying for instance, so let us think exponential is often used to say to describe may be the failures of a light bulb over time. So, how long should I wait before this light bulb fails, but as Poisson would describe saying out of 100 light bulbs over a tenure horizon how many would fail, but let us leave that. So, out here an exponential am dealing with one light bulb I am saying how long should I wait before it fails. So, if this distribution were uniform the probability that it would fail between year 1 and 2 would be the same as probability that it would fail between year 5 and 6 that would be uniform what we mean by seeing exponential is memory less is that. The probability that the bulb would fail between the year 1 and 2 is the same as the probability of the bulb would fail between year 5 and 6 if I tell you at the start of year 5 that the bulb is already not failed. So, if you are standing at time 0 the probability the bulb will fail between year 1 and 2 is very different from the probability the bulb will fail between the year 5 and 6. But, the probability that the bulb will fail between year 1 and 2 is the same as the probability of the bulb will fail between 5 and 6 if I go to year 5 and tell you that the bulb has not failed yet same way if I go to year one and tell you that the bulb has not failed yet. So, condition on the bulb not failing the probability is over a future time horizon or length horizon would be the same. And just for your convenience we have kind of put together the four distributions that are partly related and saying that these are discrete over the time arrivals versus how the distribution is counted. And I have used the color coding to describe which are continuous distribution and which are discrete distribution and you can see that clearly its only the exponential that is continuous the other three are discrete distributions great. Now, before I sign off from this class I just wanted to give you some formulas on how to calculate mean and standard deviation and so on. But, before we do that let us start with the more basic thing, which is given a PDF, how are you going to get the cumulative density function and the idea here is we have always said it is the area to the left side of the curve. So, the left side of the curve is from minus infinity, so this is the PDF and the left side extreme is referred to as minus infinity. Now, if you know that distribution starts from another location you just essentially want to get to the starting point all this minus infinity means it is the starting point of the distribution if a distribution can technically go to minus infinity it can, but like the uniform for instance it starts at a. So, that would be a you would replace that and x is you leave x as it is, because your cumulative density function is a function of x and this integral is what would solve it. Obviously, if you are dealing with a discrete distribution you would instead have this summation not integral, but everything else about the formula would stay the same way if you are going from CDF to PDF you will essentially differentiate the CDF and there is not much more to that. So, now, coming to mean and variance the idea here is that the mean x bar is the kind of mean that we have discussed, so far. So, you have some data points you take them up you average them divide by, but if you wonÕt start thinking of a mean in terms of a distribution. So, you given instead a distribution not actual data or you use the data and created a distribution out of it you created an f of x out if it. Then, how do you calculate a theoretical mean of the distribution and that you do by essentially this formula for the continuous distribution and this formula for the discrete distribution and these you can call them expected values these typically gets called mu it is that Greek alphabet mu, that the core idea here is that you take each possible outcome in the discrete case multiplied by the probability that there outcome can take on and we all know that these probabilities multiply all the way to the sum of these probabilities is going to be equal to one. But, you are multiplying pi and xi for each I and; that is the discrete case the equivalent of that is nothing, but f of x is the equivalent of pi and x is x in both cases. So, this is the equivalent of the continue this is the continuous version of the same problem. The standard deviation is the same here you have the n minus 1, because you are dealing with x bar, but if you have a theoretical mean that is given to you can use the n and in the discrete case this formula, which is the same formula that you have used to with the exception of one by n takes place. Now, this actually simplifies and for your convenience on the continuous version I have given you the simplified version this to me this version, which is used for the discrete is more intuitive but this formula simplifies to a formula, which looks more like summation of x square and so on. Separately just like on a case of the mean discrete uses a summation and continuous uses an integration we have the same distinction even here, but for your benefit on this one I have shown you the simplified formula. So, even here originally you have started off with integral minus infinity to infinity x minus mu the whole square dot f of x and so on. But, that is not what we are doing here and here actually this would not be dx this would just be probability, so it will be p of I out here. But, out here it would be d x and it would be f of x because it represents that. So, I hope these formulas give you some idea and we will definitely be looking to see you apply some of these formulas on distributions to get answers. But, thatÕs it for this class next class we will talk about the distribution we have not talked about, so far which is the normal distribution. Thank you. English - NPTEL Official\\n9) Probability Distributions(contd)\\nHello and welcome to the third and last lecture on the series on Random Variables and Probability Distributions. In the first lecture we spoke about, we introduced the concept of random variables spoke about, how probability distribution can be discrete or continuous and we also introduced the idea of PDFs and CDFs, Probability Density Functions and Cumulative Density Functions. In the second lecture, we targeted about five or six distributions, commonly used distributions and we introduced them. As well as talking a little bit about, how one can get the CDF if you are given the PDF, what is the relationship between the PDF and CDF and vise versa and how do you get to the PDF given a CDF, symbolically. And we also spoke about, how you can mathematically using given a distribution compute it is mean, compute it is variance and so on. In this lecture, we are going to focus more on a single distribution called the normal distribution, many of you might have already heard about it. But, we are also going to look at some applications associated with this distribution and one really important application has to do with inferential statistics, which is something that will be quite central to the next 4 or 5 lectures. So, it is in that idea that, we are introducing the normal distributions. So, the normal distribution itself you might have come across it, if you not you might have heard of this thing called the bell shaped curve. So, the distribution itself looks like the shape of a bell. So, just like the uniform looks like a flat line and you know different distribution have different shapes, this looks like a symmetric bell, bell shaped curve and the probability density function of this distribution is characterized by this formula. This formula that is shown here and one thing that is noteworthy is that, this distribution has two parameters mu and sigma. So, the distribution itself is defined by the mean and variance, so the mean and variance of this distribution go into the formula and they defined it. So, there is no point saying tell me, what is the probability of value x for a normal distribution, because that question does not make sense. In order to say for a distribution with this mean with this variance, what is the probability of value equal to a greater than x? So, that question means more or you know, what is the probability of finding a value between x and x plus delta, for a normal distribution with a mean mu and a sigma equal to sigma and standard deviation equal to sigma. But, once you given the mean and sigma, it is quite simply this formula that you would use and you can compute the probabilities. So, what is the mean of a particular normal distribution defined by mu and sigma? Well, that is very straight forward, it is the mu, because the distribution is defined by mu and the variance is nothing but, your sigma square. So, you can, it is quite straight forward there is well. The CDF; however, is not something that simplifies very elegantly. So, to define the CDF you would still use your traditional procedure of using the integral and by the way the normal distribution goes from minus infinity to plus infinity, so it make sense to actually use the minus infinity here. So, you would actually use the minus infinity to x, f of x, which is the PDF, which is nothing but, this formula, so dot d x. But, while in many distributions this whole thing simplifies and you are able to do the integration and there is an actual value, with the normal distribution it does not simplify very elegantly without using more complex algebraic terminology. So, the CDF is often just stored in tables, sometimes especially for the normal with mean 0, standard deviation 1 or it is just something that you integrate each time to get. Now, this is a very interesting distribution, because there are lot of things that are normally distributed. So, things like peoples height, weight well height; obviously, with each gender, grades in a class, marks that people score in exams. The core idea with the normal distribution is that, unlike the uniform distribution, which says everything is equally likely. It is the normal distribution says that things in the extremes are less likely, things in the center are more likely within certain limits, which is what gives it its characteristics bell shaped curve. I mean, if this is any attempt to the bell shaped curve, we basically saying that things that are on the extremes, like here and here are less likely and things in the center like here are more likely that is why they have a greater height, with all of these things the y axis is the probability. So, if you take a look at something like heights or let us say weights and you fix a gender, let us say male and you take something like people, who are registered for introduction to data analytic course, then you will find that there might be very few people, who weigh less than I do not know 40 kg, so or 50 kgs, men especially. And you will find very few of them probably weighing more than 100 kgs or so and then, you know and so that kind of tapers off, an either extreme you find less, in the center you find more. But, there are many other distributions are also like this, but this that this is that is key feature of being in a bell shaped curve. The other thing is you know many things after you remove outliers start to look normal and we will talk about an example of that. In this slide, I am just not going to talk about the other things that we will talk about in this lecture, so I am not kind of rushing through it. We will especially take up from here and till here and go through them in detail with slides. But, you are also encounter that there is this things called the binomial approximation, which isÉ We briefly spoke about this when we introduce the binomial distribution that certain problems, which just by definition look like they fall so cleanly as a binomial distribution, for computational reasons could be quite easily approximated to a normal distribution. Although, the binomial is a discrete distribution and the normal is a continuous distribution. We will also talk about something called the central limit theorem, which makes the normal distribution very useful for many applications and also a very interesting concept per se and finally, we will look at the idea of sampling distributions. The core idea being that, if you take a random sample of size x of associated with any variables, so I randomly select five people and measure their heights. Is there a distribution associated with the parameters that I get like the mean and standard deviation? But, we will talk about this in greater detail. So, the first thing is things after removal of outliers. So, here is an example of some real data, where we looked at the total annual household income and you know, so the graph that you see to the left hand side is you know, it is essentially all these households with income up to and we just stop the x axis at a certain point and so, we said let us look it income up to a certain value and the y axis is the number of households. So, I have created essentially a histogram, but that is a proxy for finding the probability distribution itself. So, you can think of the probability distribution as something that looks like this, in this particular case. People cannot have incomes less than 0, so on and so forth. Now, look at the same graph, where I said I am not going to look up to 4 lakh rupees income, but I am just going to concatenate the x axis in 90000 rupees. So, the whole idea was to say that some of these values could have been outliers and we took a certain value beyond, which we go. And already you can see that this graph is starting to look a lot more bell shape. Probably not perfect, but the core idea is this, which is that sometimes once even though the distribution originally might not look normal with sufficient amount of outlier removal, the distribution could truly be know. The second concept that we want to speak with respect to this is the binomial approximation. So, let us just very quickly review, what the binomial distribution is about. We spoke about, how this term in the PDF of the binomial distribution was really, n choose k. So, n combinations, k combinations out of n was the core idea and that is fine. So, if you have problem of the type saying, what is the probability of finding, you know 3 heads out of 10 tosses. This works fine, you can substitute the values get the PDF. Now, somebody came and asked you saying, what is the probability of getting 2100 heads out of 5000 tosses. Then, you essentially need to, if you want to use this formula you need to plug in 5000, you know c 2100 or whatever the number is and you know; that is a very large number; that is a very hard computation and you could 5000 and 2100 just an example that could be 5 million and you know 200000 and it is very hard to do those calculations. So, one thing that you can do, when n becomes really large is you can essentially use this formula that you have for mean and variance of the binomial distribution and construct a normal distribution with this mean and this variance and used to answer distribution related question. So, you for instance if there is a 50 percent chance for instance of a coin falling head and tails, you can say well the mean of 5000 tosses is 2500, because you have 5000 tosses times 50 percent probability. So, that is 2500 and that is your mean and your variance also you would similarly calculate by plugging in n equals 5000 and p equals 0.5. And once you do that, you can essentially construct a normal distribution with these parameters and you can answer questions like, what is the probability of there being more than 2100 heads or what is the probability that the number of heads would be between 2000 and 2500 out of 5000 tosses. You; obviously, cannot answer a question like, what is the exact probability of getting 2112 heads, because you essentially converted this to continuous distribution. And the idea of answering a question like, what is the exact probability of 2121 tosses out of 5000 or I mean 2121 heads out of 5000 tosses becomes relatively meaningless, because as n keeps becoming large the probability of any one thing exactly occurring becomes really small becoming close to 0. So, you are interested more in intervals, which is in spirit this, what you can do with continuous distributions and you can use a normal approximation of the binomial distribution to achieve that as long as n is fairly large. Next, we will move to something called the central limit theorem and the core idea here is that the aggregation of a sufficiently large number of independent random variables results in a random variable, which will be approximately normal. So, what is that mean? It just means that look, if you have some process and it is some distribution from that process, so let us say flipping a coin or throwing a dice is the process. Now, central limit theorem says as long as I am aggregating many such processes. So, if I said instead of asking you the simple question of the distribution associated with what I would get, if the roll the dice once. I instead say I want to know the distribution associated with rolling the dice twice and I am going to add them up. So, the first time I will roll the dice and then, I get some number I write it down, I will roll the dice another time and I will get another number and I am going to add those two numbers. Now, the distribution associated with that sum is also probability distribution, because you know it is still a random process; there is still some chance that I can get each value. I clearly cannot get any value less than 2, because first time I can roll 1, second time I can roll 1. So, I cannot get 1, I can only get 2 as the minimum value and the maximum value is 12, I can roll 6 and 6 and that is 12. So, the idea that is being put forth here with central limit theorem is that aggregating it and the word aggregating can be thought of is, you know taking the sum or you can think of it is taking the average, both a forms have, both are essentially the same thing. The difference between sum and average is, average is just divided by the number of times. But, this form of aggregation of a sufficiently large number of random variables results in a random variable, which will be approximately normal. So, let us see how that works. So, on the left hand side of graph out here, I talk about the distribution associated with the single row and this view seen and we have discussed this is uniformly distributed. Why? Because, the heights are all in the same, which is discrete distribution and you see, it is uniformed distribution and it is 1 by 6; that is what I have shown here today. On the right hand side, I show you the distribution of the sum of two rows. So, you can think of it is, rolling it once writing it down rolling it second time. So, you can think of your hands having you know two dice and you roll both of them and you sum up, what you see and what shows up. And already you can see that the distribution is started moving from uniform to something else. This happens to be triangular, but that is just the first step towards starting to look more and more bell shape. What is happening? Now, although the probabilities of rolling 1 through 6 were uniform, the summations; however, are not equal. So, the probability of getting a 2 is lower than the probability of getting a 3 and that should be fairly intuitive. For you to get a 2, you need to roll a 1 the first time and roll a 1 the second time. But, there are many ways in which you can get 3, mainly 2. You can roll a 1 the first time and then, the roll a 2 or you can roll a 2 and then, roll a 1 and that kind of keeps increasing till you hit the point at 7, where 7 you can get in so many ways, you can roll a 6 the first time and then, roll a 1 the second or if you not, if you thinking of rolling both of the same time you can get a 6 and 1 or 1 and 6, 3 and 4, 4 and a 3 or 2 and a 5. So, there are more ways of achieving the same thing of achieving a 7, there are fewer ways of achieving a 2 or 3 and so, you already have something that is looking more like a normal. Now, you go further as I discussed; obviously, the average of two dice is the same as the sum of two dice. So, these two graphs are identical, this one and the next one on the next slide. These two are identical, except that are changed to average, so this axis is different. It goes through 1 through 6, the other one went from 2 to 12, but these are essentially identical graphs and this is also a triangular distribution. But, look it, what is already happening. Now, if I say the average of three dice, so I am going to roll three dice at the same time or I am going to roll one after the other after the other. There are all independent either way. What you going to see is that, now this is started looking a little bit more you know triangular, let us starting to get that little bit of inflection and so on. And, so as you increase this number more and more, as the idea I said you get something that looks fairly normal and that is about the central limit theorem is about, that you aggregate a sufficiently large number of distributions when you start getting a normal distribution. Now, this is the really important point for what we are going to say in next associated with sampling distributions. So, we going to start a fresh and sampling distributions, but I just want you to keep in mind, what we have discussed now in central limit theorem. So, jumping give us, now to sampling distributions the idea here is very simple. So, lets the you have some original distribution and lets for now, say this distribution is normally distributed. And let us say this normal distribution has some mean, which I have shown with this blue vertical line and lets call that mu, so this point is mu. And let us say it has some standard deviation I am just referring to the dispersion through the arrow that is not the exact link of the standard deviation. But, it is have some standard deviation, which can be represented is the variance is represented as sigma square and by the way this mu comma sigma square is fairly norm nomenclature that just means it is a normal distribution with that looks like a with mu and sigma square all though that looks like an m, so may be a little bit more like mu norm. So, you have this distribution, now let us say N let us give it a name, so let us say this is the distribution of heights this is the distribution of what we lets keeps weights. So, this is the distribution of weights staying consistence with the previous example of the men or the male members, who registered for introduction to data analytics. So, may be this distribution starts somewhere at I do not know 50 kgs and goes all the ways to say 100 kgs this is this is the distribution technically it can go all the way to infinity. Because, by definition and normal distribution can go to infinity and on this side it can go to minus infinity. So, this is this is the normal distribution. Now, let us say that I took a sample from this distribution. So, these data points represent the different samples and in this particular case I have taken just six samples, but well that can be more and the heights mean nothing the sample just mean, where they fall on the distribution. You can; obviously, use that and build a histogram and the idea is that if you build a if you take a sample large enough that histogram will fit very neatly to this curve, which is the normal distribution if that sample is very large. If the sample is not you might get a different histogram, but what we most interested in is taking this sample and computing some key statistics from this sample. For instance if you took this sample and computed the arithmetic mean of the samples you will take each data point right and let us say you call it x 1 and the next data point got called x 2 and so on. Then, what you looking at is x 1 plus x 2 plus dot, dot, dot divided by N that is your arithmetic mean and, so you compute an arithmetic mean. But ,since you got some finite sample got like 6 points and may be you have in an others instance 10 points the question is will your arithmetic mean always be equal to mu remember mu was, what defined this distributions this distribution is by definition mu comma normal mu comma sigma square. But, if you take a sample if you take some axis and compute x bar we differentiate between mu and x bar meaning mu is the theoretical mean, where as x bar is the sample mean it is. If you take a sample of size n and compute an x bar will this x bar be equal to mu and both intuitively another wise the answer is no theoretically if your sample size is equal to infinity; that means, you take infinite number of samples, then perhaps your sample means will be equal to, then your sample mean again an theory will be equal to mu. But, that is not a practical situation, who takes infinite samples like that that by definition does not does not make is not very useful. So, if you take a finite sample and in this case 6 and another case can be 20 next less say 20 and you compute a sample mean it is not going to be equal to mu. But, the idea is that it might the idea is that it is also a random variable, what do you mean by that you mean that say I mean one time you go about you take a sample you take a sample of 10. Let us say and you take the mean of that sample you will get a particular value that will not be equal to mu it could be equal to mu. But, you know it could be little less than mu little greater than mu now, you go do that exact same thing again you will get some other new value. So, what; that means, is you have a random variable on your hands and the random variable is about the distribution of the sample means for a given size end. So, that is what that is a core idea associated with sampling, which is that from the original distribution you take a sample and you compute a means and you get a certain value and, but that value itself belongs to a distribution that distribution changes based on the sample size. So, suppose we were like I said if you took infinite if your sample size was really large if it was infinite, then perhaps you will not even have a distribution you just have a line out here which is that you almost always get mu because your sample size is, so large. But, if you , but think of the other extreme suppose your sample size is equal to one that is each time you took one point from the distribution and you computed the mean of that point, what is it means to compute the mean of a single points it is that number itself. So, let us say we were looking at 50 kgs to a 100 kgs you took a random sample of one. So, 75 you know 65 kgs this time that was the random number I picked the average of 65 is 65. So, if you had a sample size of one what could the distribution of sample means look like the answer is it would look exactly like this distribution, because you taking a sample size of one its essentially like and you computing the average of that, which is nothing but, that number itself. So, it is essentially like just re plotting that graph, now if your sample size was greater than 1, but less an infinity, what happens is if your sample size as the sample size gets larger and larger you are dealing with the distribution step. Because, each time you take a sample of, let us say 5 or 10 or 20 you are going to get some sample mean from that and that sample mean is not going to always be equal to the exact overall population mean. And, but it is going to be some number nearby and the idea is that as in this particular case we had a normal distribution and the idea is that as long as we taking the average of a some number. Let us say 10 or 20 or 30 or 40 or 50 samples you are going to get a mean. But, that mean is not certain it is not certain, what that mean is going to be you know it is you know it need not be mu you know that for a fact. So, what you are essentially getting is another distribution you getting a random number from another distribution and this the distribution of the sample means now it is. So, happens that when your original distribution is normally distributed the distribution of sample means is also normally distributed, but they might be some questions you have in this regard. So, for instance what is the shape of this distribution the quick answer to the question is when the original distribution is normal like we said this distribution of sample means is also normal. But, we also went through this central limit theorem where we said as long as you are aggregating is sufficiently large number of distributions the resulting distribution starts to look normal. So, even if your original distribution is not normal as long as your aggregating a sufficiently large number this distribution of sample means becomes normal. So, that is the shape, now what is the mean of this distribution, what is the mean of the distribution of sample means the quick answer is because your just taking the average of some numbers if you what to do this is sufficiently large number of times you should not get a mean that is biased. So, the mean of this distribution will also be equal to mu, but it is clear that the standard deviations are not the same right the standard deviation would be the same if your sample size was one in which, case you are not really sampling you are just taking a single data point. But, depending on the size of the sample the standard deviation is going to be typically lower a it will always be lower as long as the sample size is greater than 1 and the relationship is nothing but, sigma square. So, if you are using sigma square it would be sigma square divided by n when you use small n refer to the sample size, but you can also think of it as taking the square root of this you can also think of it is sigma divided by square root of n. So, this would be the standard deviation and this would be the variance. So, this is var this is the variance and this is yours standard deviation. So, that is the that is relationship that is very useful to remember, now you might have a question saying. So, we did all of this work to say let you have an original distribution you randomly sample from that distribution and you compute a mean an arithmetic mean then that arithmetic mean that you compute belongs has a distribution of its own and we spoke about the mean and shape and standard deviation. Similarly, if you take a sample from the original distribution and you compute a standard deviation of that sample. Then, would you be is that sample standard deviation also coming from a distribution and the quick answer to that question is yes and in the if you are using a normal distribution to start with that is distribution of the sample standard deviations tends to be chi square distributed and that is also something that we will encounter. But, are focus for now has when on the distribution of sample means and the important things to take away are if you start with an original normal distribution, then by theory you will have a normal distribution for your sample means for whatever sample size. But, given that we also learnt about the central limit theorem even if you start with an original distribution that is not normal as long as you aggregate sufficiently large number of as long as your sample size is large enough and the distribution of sample means is likely to be normally distributed. We spoke about how the mean of the distribution of sample means should be no different from the mean of the original distribution, because you are not adding or subtracting any number you just taking average numbers you are just taking numbers and taking the average of that. So, if you do that many times the distribution that you get from that should also be centered around the overall grand mean of the original distribution we spoke about how the standard deviation. However, keeps reducing, so as long as you are aggregating more numbers your standard deviation will reduce in this rate it which, is reduces is as a function of this square root of n the sample size. So, sigma divided by square root of N is the rated, which the sample size your standard deviation of the distribution of sample means is with respect to the original distribution and actually that phenomena you should be able to see even in the examples that we took of the central limit theorem just two kind of show that you again see in this particular example and I will erase the red mark in this particular example I was focusing more and showing a central limit theorem about how the shape changes. But, if you take this graph, which is this uniform distribution out here and there is some standard deviation out here right the sum spread around the mean correct this sum spread. Now, take a look at the average of two dice the mean is the same centered around the 3.5, but this spread has decreased right before this spread was like this. So, there was there was the higher probability of seeing values in the in the earlier graph up here you had data points that were with the higher probability further away from the center at 3.5. Now, you do not see the probability of finding points far away from the center has reduced the these are low probabilities, but the probability finding things close to the center is increased. So, therefore, the standard deviation of this distributions is lower than the standard deviation of the uniform distribution and that effect is going to just increase the probability of extremes keeps becoming lower there by the standard deviation becomes lower given that you are for all of these you are starting with one and ending with 6. So, the that example shows both the central limit theorem meaning the change in the shape, but you can also capture this idea which is the distribution associated with sample means and in the previous cases the sample size was two in the first example and the sample size was three right because we were averaging two dice or three dice. So, the distribution that results from that is having a lower standard deviation. So, that should give you an idea of the whole idea behind sampling distributions and this is the good concept to revise or understand deeply. Because, a lot of inferential statistics is based half of this and with that we conclude our lecture on random variables and probability distributions. We will continue a next class and focus more on inferential statistics. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '2f66064960e79ca5b36d9548ef0ec314'}>, <Document: {'content': '10) Inferential Statistics - Motivation\\nHello and welcome to our lecture on Inferential Statistics. In this particular lecture, we are going to be talking more, giving you more about Motivation, for why we need inferential statistics and we will be talking a little bit about, what kinds of problems you can solve using these techniques and you know, where you would applied and when you would applied. In the subsequent lectures, we will talk a little bit about how you would applied and also, why you do some of the math that you do, why you do some of the computations. So, understanding it a little bit more on the nuts and bolts level is something that will follow. But, in this lecture we are going to focus on why you need inferential statistics in the first place and we are going to do this through the lens or something called hypothesis testing. So, hypothesis testing is very widely used an excepted tool for a lot of data analysis and if you understand inferential statistics through hypothesis testing, many other concepts in inferential statistics just fall in place. So, things like confidence intervals and so on, which you might have heard become fairly easy to understand and process. So, having said that let me jump into the subject. The idea behind inferential statistics is to make some inference about the population from the sample. Just to jog your memories, I think we have spoken about population and sample a couple of times. But, this is different from, what you would have done with descriptive statistics. With descriptive statistics you do not care about population or sample, in the end of the day you have some data set. And for simplicity sake, assume that was a sample that you got from population and this sample you will very content in descriptive statistics with describing that data set with describing that sample in case you have a sample. But, here the kinds of problems that you are more interested with inferential statistics, a ones where you have a population and you are getting as just a sample from that. But, from this sample I do not want to just say something about the sample, I do not want to talk about the mean of the sample, I do not want to talk about the variance of the sample and I do not want to talk about the centrality dispersion. My goal is to say something about the population. I only have data, which is the sample. I do not have the data associated with the population, but with this sample can I say something about the population. So, that is the core idea and just a kind of jogs some of your memories, the idea behind population and samples is fairly simple, we looked at it to the couple of examples. But, you can think of the population in one of two ways. The first and the more obvious way is that, there exists this really large data set associated with the phenomena. So, let us say the phenomena was the height of all boys, who are in 10th standard in public school. So, that is, so that could be a very large data set and let us say that was for India. So, it is very large data set, there are lot of children, who are in the 10th standard, who are in public schools across in India. And you can think of that as a population and you want to say something about that population and you might not have the data. So, you go to take a sample, so you select 5 schools or 10 schools or you select 200 students through a census process randomly and take that as a sample. So, you take a sample of, you know some subset of students from the population, that is one way of thinking of population sample and another way could be that the population itself is moreover theoretical abstract concept. So, you could not have an actual data set, but it could be something like I have created this new machine and this new machine is going to start making certain products and let us say, you are very interested in a product dimensions. So, let us say the diameter of a product is machine makes. So, you put a raw material in to this machine, the machine splits out some finished product and this finished product should have a diameter, I mean has some diameter. Here you do not, because it is a new machine you might not actually have a population; that is a really large data set that exists somewhere. The population here is the concept that, if this machine going to create infinite such products without any change in time space, the dimensions of these products would be the population and ultimately you might say, hey let us just for the first time run this machine and make 10 products and these 10 products that I physically have and I measured and so on, is the sample. So, here is the case, where I still want to use the sample to say something about the machine in general. Not just the 10 products the machine is turned out, but the concept of the population here is not an actual finite large data set in my hands, it is more about concept. Now, having revisited population and sample here let us again see the statement, which is that inferential statistics you make, you want to say something about the population from the sample. So, as I said the major aim of this lecture is to motivate you to see why inferential statistics is important. So, I felt that the best way to inference to that might be to give you some examples. So, that is what we are going to do and I start with some simple examples. I have broken them down in to one sample and two sample examples and pretty soon, that will become clear what that distinction is. So, let us start with the first example which is a one sample example on that upper left of your screen. So, the idea here and I am just, so you know we are in this part and the idea here is that, let us say we were interested in noting the average phosphate levels in our blood and I do not have a medical background or anything, so do not look at the medical aspect of this examples. But, let us say that your doctor or doctors in general or you know public health advocates, say that the average phosphate levels in bloods should be less than 4.8 milligrams per I do not know deciliter. So, again irrespective the units, so the whole idea is that this number, which you can get if you go measure your blood should be on average less than 4.8. The key here is an understanding that they should be less than 4.8 on average. So; that means, the doctors or the public health advocates understand that sometimes it could be greater than 4.8 and that perhaps in this particular case is not a cause for along. Again do not focus on the medical aspect, I do not know if it is not, but this is the situation I am creating. So, but the important thing the doctors have told you, it is on average it should be less than 4.8. So, let us say you say and you know this; obviously, variation. So, it really depends on what you ate that day, it depends on what time of the day you take the measurement, it depends on what instrument you used to take the measurement, it depends on how much water you had. So, let us say there are lot of factors that you do not seek to control and that is the whole idea behind this. But, you want to take a set of measurements and you want to take a set of measurements and answer the question us to whether the average phosphate levels in your blood. In general, not just on the sample that you have taken, you do not I mean the sample could be anything, but you care about, in general is my average phosphate level and blood less than 4.8. So, why you… So, the question might arise you know, why you distinguishing between what the sample says and what reality is and that is going to become clear in second. So, let us first go about trying to answer this question. So, the first thing is, if you took a set of measurements and let us say, you got consistently very low values. So, you let us say you got 2.4, 2.5, 2.1, 2.7, 2.3, 2.9 fill of four more numbers in the two points. Then, I guess you really do not need a statistician, you can look at the sample that you got and you can say, look I am fairly certain that even if I went on taking more and more samples or that if I woke up another day through all these data or took another sample or if I took infinite many samples, in either case it looks like my average is going to be less than 4.8, that is fine, you know intuitively that seems obvious. Similarly, the flip sides, suppose you wanted to take this data and you consistently got 5.5, 6.1, 7.7, 6.9, so on, where every single data point is significantly greater than that 4.8 mark and approximately in that same region, meaning it is not widely moving around. So, that is also an intuition that you might have, that if one second it is 6.5 and the next second it is 2.1, you know it is widely moving around. But, here I am giving you examples, where 6.1, 6.7, 7.1, 7.3, 7.4. So, it consistently significantly greater, again you do not need a statistician. Somewhere your intuition, you just say look, I mean based of the sample I am willing to bet that my average phosphate levels are less than 4.8 mg per dl. But, then it gets a little tricky. What happens if you had, you know numbers, you know some of them below some of them above. So, some of them are less than 4.8 some of them are greater than 4.8 and in some sense, what you do then and the instinct to the intuition there sometimes just to say well, let me take an average of the sample. If that average is greater than 4.8, then perhaps I should conclude that my average is greater than 4.8 and this is small problem with that kind of an approach. I mean, assume that you got the readings like as such as 5.1, 4.8, 4.9, 4.7, 4.7, 5.3 and you got some set of variance and you took the average and that average was 4.85. So, you saying, you know what the sample showed that it is greater and you conclude, for instance that the average phosphate level in my blood in general is greater than 4.8, based off of the sample that I just saw. The problem with that could be that may be if you just took two more data points. Let us say you took two more data points, you increased your sample by two more and you got a 4.6 and a 4.7 and all of a sudden, because of these new data points, your average you know slights just below 4.8. Something about doing this process, we just take the average of the sample and make a conclusion, does not seem correct for this reason. And you know, another way of looking at it is also that, if we looked at this notion of sampling distributions. So, we know that, let us take a look at the same graph that we looked at the end of the last class. So, you had this thing, which we described as the original distribution. And let us say for now, let us just say for now, the truly your… This distribution by the way represents the amount of phosphate in mg per dl that you will see in your blood if you do a test at any given point of time. So, you get numbers from this distributions, so sometimes you get a 5.1; that is what, that is the dot there, sometimes you get a 4.4 that is the other dot there. But, in the end of the day it looks like on average, given how I have drawn it. On average it is only 4.7 mg per dl, which is less than the 4.8 mark that we were interested. Now, you go and take a sample, this is the same example as a last time, so you took the sample and you some data points. So, you took 6 data points and this is what you got. Now, if I want to just take the sample average and I am just eye balling it here, if I just took the average of these numbers I would say that average falls somewhere here, would it to see. So, this might be the average of these numbers or maybe it will fall right on this data actually. So, let us say this is somewhere out here is the average and the true is this sample average is greater than the 4.7 and if you want to just go by the sample average, we might have conclude it that the amount of mg per dl is greater than 4.8 or whatever. But, here is a good part, you now had a class in statistics that told you that the average that you get from the sample is not always going to fall on this 4.7. As your number of samples tends to infinity? Yes, it will converge to this point. But, if you got a finite number of samples and that is not, so it is not going to always be exactly on 4.7. So, what is it going to be? What it is going to be is another distribution. So, if you had N samples and this distribution will change with more samples, if there are many, many, many, many samples, then literately if as the number N tends to infinity, this distribution will pretty much like flat on this line. But, if not, you still getting the sample mean, the mean that you calculate from the samples literally a random variable that you getting from this distribution. So, it is literally like you just pick random points from this distribution. So, you get a point any where here, you probably will not get a point here, because it does not… The probability of getting this point from this distribution is very low, it is almost 0. So, you will be getting points from this distribution and as a result, just because this number is greater, that 4.7 should not make you conclude that this mean, which is what you are trying to conclude. You are trying to say something about this line; you are trying to say something about this line, which is the mean of the population and, because you get a sample mean which is nothing but, the number from a distribution should not make you conclude that therefore, it is greater than 4.8. So, that is why you need to do something more, you need to do something more complex than just blindly taking the sample average. So, again we are going to talk later about how you do it, but and what and when you do, but right now I am just trying to motivate for you why you need something else. So, take another example and in this example, we take a problem of proportions. So, let us say the health department or some dentist related body says; only 5 percent of the toothpastes of any given brand can be out of specification. So, out of specification might mean that, you have some ratings on the amount of fluorides tooth paste can have. So, let us say you are allowed any 1000 parts per million of fluoride and you, there are other chemical limits. But, the health department understands, that not every toothpaste can match exactly the ideal requirements. So, let us say the set out limits on the chemicals and they say, look if you are a toothpaste manufacturer, only I am going to only allow 5 percent of your toothpastes to, you know be out of specification, 95 percent of your toothpastes that I see in the market need to fall within my guidelines. Same problem comes up again. You can take a sample of 10, 20 toothpastes and it could very well be that truly this toothpaste brand is involved in a chemical process or manufacturing process, that creates on average only 4 percent. Only 4 percent of the toothpaste that this company makes are actually out of specification. But, it is perfectly possible that you went and took 10 toothpastes from the market and your luck, 7 of those 10 are out of the specification; that is perfectly possible. It might not be the most probabilistic thing, but it is perfectly possible. It is possible that, this toothpaste company is involved in a manufacturing process and chemical process that creates toothpastes and on average, 4 percent of all the toothpastes they make. So, when I say toothpaste, think of it as a toothpaste tube; on average 4 percent of all the tubes they make. Have a chemical composition that is not acceptable, which is fine, because the health department says you cannot go more than 5 percent and this toothpaste company has rising it is hand and saying hey you know, which only 4 percent. But, I now go and randomly sample 5 toothpastes, 10 toothpastes and I find that out of the 5 toothpastes that I randomly sampled, 3 of them are defective. All of a sudden, I am saying 3 out of 5 that 60 percent, you say only 5 percent is allowed and I find 60 percent. And so, is that does not mean the company is not creating toothpastes less than 5 percent rate, which are in conformance less than 5 percent rate, probably no. Again you need a little bit more new on thinking and you need little more statistics to actually answer this question, based off of the sample you cannot just take the sample average. Another example, the third example that we have on the one sample cases, imagine that you are in an insurance company and you find, that there is this particular mechanic shop that is new garage, which does repairs and because most people are required to have insurance. Once the garage kind of writes out an invoice and people who file the insurance claim attach this mechanics invoice and tell the company to reimburse them for this rectification that is claim to the car. And let us say, this is a new garage and you know the insurance company is suspecting that these guys are cheating, that their set up as a place to not do any real work, but just write really high invoices. So, that the insurance company, so they are involved in some kind of a fraud. So, one thing that the insurance company can do is saying, I am going to look at that next 10, 20 or whatever repairs. So, let us say up to this point this garage is not made a single you know claim, but it is just being set up, but the inside word is that they are trying to cheat this system. So, once a garage gets set up, this insurance company is ready. The first 10 claims or 20 claims at this garage files or 30 claims, they take those claims and they see how that compares to the national average in terms of average claims. Again the problem is just, because this sample is greater than the national average, can we conclude yes these guys are cheaters and just, because his sample average is less than the national average, can we conclude these guys are not cheaters and the answer to both those questions is no. In some cases, like when I was talking to you about the case, it might be brutally obvious, where every single data point is so high or so low, that you are like I do not need a statistician to tell me that the answer to this question. But, when it is not that case you need little more you need inferential statistics to answer that question. So, let us we look at the single sample cases by that we essentially mean that there was one data set and you are essentially comparing that data set to some bench mark number that you had in your head. Lets now, move to two sample situations the example here is let us say that I am running up foundry and some guy comes in consultant comes in says you know if you change the temperature a little bit of the molten metal that you pouring in whatever. Let us say change the temperature down by degree over two and I assure you that average number of defects that you see in your costs in metal costs will decrease. So, like a mechanical engineering application you like may be the consultant knows what is talking, but how do I test it, how do I test it and the answer to that question is it is you can do the following, which is before you do that before you go change things you can measure the average number of defects in your costs and you do that for 10, 20 data points. And then, you do with the consultant, which is change the temperature and then, you collect the another 10, 20 data points. Now, let us go back to the question if the average of the sample the first sample, so now, we have sample a sample b sample a corresponds to before the temperature was change sample b corresponds after the temperature was change. Now; obviously, I mean in all likely hood there is going to be some average to sample a and there will be some average to sample b. In all likelihood these two are not going to be the same one is going to be higher or lower than the other just like in the single sample case if they. So, dramatically different these samples and when I say dramatically different I mean dramatically different with respect to some amount of variability there is in the two samples as well. Then, you do not need a statistician to tell you it is like, so obvious that changing the temperature dramatically reduce the number of defects. But, in many cases you do not know and in those cases it is not obvious to say the average of sample A was different from the average of sample B, I mean go back to I am going to erase this, but erase the red ink, but go back to this example. Let us say that, lets say the this is the original distribution of the number of defects, so this is the original distribution and let us say this is 3 defects and this is like 9 defects, now that is too high let us say this is 5 defects and this is 1 defect. So, this original distribution is what I care about that goes from here to hear these numbers of defects you will see in a costs. Now; obviously, if I take a sample of 6 or, so I get a random point I let us say this is the random point I get and the erase the other one done erased. So, I basically I took the original distribution I took the sample of 6 costs and when I take the sample of 6 costs like we discuss we the average is not going to always we exactly 3 is going to be some number that falls in to this distribution the distribution or sample means and I drawn a random number that I got out here from that distribution. So, good now let us say this consultant who is telling you to go increase or decrease the temperature was completely wrong. Let us say he had no clue, what he will say he was just lying, but fortunately, unfortunately whatever is said does not make a different I mean he lied in that you know he said is going to improve the process it did not improve the process luckily it dint make the process was. Now, you go take and because a processes not change the original distribution is not change after that temperature is change the number of defects you going to be receiver also exactly in conformance of this distribution it is in conformance to this outside distribution the original distribution. So, that has not changed essentially this mean has not changed this mean has not changed. Now, you go take a sample of sets you get another sample average and this sample average again is going to belong to this distribution and let us say this sample average was this value. So, this is the new sample average, so this is new it is that is new now you cannot say hey this new sample average is higher than the old sample average. Therefore, the population mean is different it is not I mean I just gave you an example, where the population mean truly did not changed, but how you could have seen two different sample averages in concluded that one is greater than the other. So, this is why again you need more than looking at the sample average of a and sample average b and saying hey one is higher than the other. So, we should believe that what the consultancy said was correct or the other way around you know if the if you conclude that what the consultancy said is definitely made things was that is also marked correct perfectly possible that there was truly a change, but because of luck you know you saw things other way. Now, another example you can think of and this the next example is one where I wanted to emphasis variation the rather than just the mean, let us say you have two different manufacturing processes and you want compare their variance of the finished product in each batch. So, you have manufacturing process A manufacturing process B and they make batches of you know finished material finished product and within each batch this some amount of variance of each products that some amount of variance right variances the inherent variation between one part to the next. And, let us say I care about that the variance let us say I do not in a particular batch I do not want one product to look very different from the next product I do not care about the mean. But, I want them to all be consistent again you would use the same concept you would take the first batch, which is made from machine A calculate the variance of it calculate the variance of that sample take the second batch made from machine B calculate the variance of that sample. And again the overall concept exists you cannot just say sample of sample variance of machine A is lower than sample of machine B. Therefore, I conclude that machine A better than machine B it is perfectly possible than machine A actually worse in machine B. But, because you ultimately only have a sample that machine A got lucky ultimately it goes back to this idea that you see in this distribution. But, sometimes you get number on this side of the distribution sometimes you get a number on this side of the distribution. The more data point should take the overall variance of this distribution reduces, which is why if you had infinite number of points right none of these problem is exists, but you do not. And, if you dealing with that then there I think to do this to use inferential statistics to look closely at the data. Another example that is often coated is things like are tenth standard girls taller than tenth standard boys in India for instance we all know that in terms of average heights men have larger average heights than women, but we also heard that girls start growing taller earlier. So, I do not know is 10th standard breakeven point at least some statistics text books sink to think, so. So, you could have a simple question like the population here is 10th standard girls in India tenth standard boys in India you are ultimately taking the sample and based of the sample, what can you say about the population. And you know sometimes the story is obvious from the sample itself sometimes you need inferential statistics to come in and tell you whether you can say something concrete or perhaps you cannot say anything concrete and that is also something inferential statistics will tell you. But, ultimately it is not as simple as just saying the average of sample A is greater than the average of sample B, therefore I am going to conclude one way or the other. So, let us go to what it is that we will be trying to do with inferential statistics I am just going to go through the overeating principle and keep this in mind will revisit this slide a couple of times. But, I think the ultimate test in some sense of you understanding, what it is in, what it is and how it is and why it is will really become clear once we do the actual math with each test. And ultimately these will come in the form of test I mean some of you might have heard of these test like t test, z test, chi square test, f test, ANOVA and so on and we will go through each of these tests. But, here is the overarching principle with respect to hypothesis testing is to have this have a null hypothesis and an alternate hypothesis. So, for instance in the fluoride case the null hypothesis could very well be let me erase this, the null hypothesis could very well be that you have less than 4.8 mg per dl. So, the null hypothesis is that the actual phosphate levels in the average phosphate levels in blood for person x is less than or equal to 4.8. And, so the alternate hypothesis would be that its greater than 4.8 the important thing is the null hypothesis and the alternate hypothesis in some sense together should be mutually exclusive, which means that if it is greater than 4.8, then it cannot be less than or equal to 4.8 and vise versa and you know collectively exhaustive there should essentially cover the entire space that you are interested in talking about. So, I mean meaning that the average phosphate level is either less than or equal to 4.8 or greater than 4.8 it cannot be neither. So, its collectively exhaustive, so then you what we will be doing and you not been talk this yet, but you will be doing some basic calculations or arithmetic on the data to create a single number call the test statistic. So, you do not know what that is yet, but what you will do is you do the reason I am explaining this to you is to give you an idea that you are going to be working with the sample. So, it is not like a magic in that you are not going to say something about the population without dealing with the sample. So, you will be doing some math you know and it some of that might involved taking things like the sample mean sample standard deviation, but you will be doing some math on that and when you finish with that math you will getting something called the test statistic some of you might have heard of the these test statistics it may be called the z statistics or the t statistic and so on. The crucks of null hypothesis the crucks of hypothesis test is that if we assumed the null hypothesis is to be true. And make some assumptions about distributions of various variables and those we won’t go into that much, but if we assume the null hypothesis is to be true. Then technically the test statistics should be no different than drawing a random number from a specific probability distribution. So, in some sense what we saying is if the null hypothesis is to if that if the true it is a true mean is equal to 4.8, 4.8 this time, because the null hypothesis is true the null hypothesis was is it less than or equal to 4.8, so here the null hypothesis is true. So, let us take the extreme case the true mean is 4.8 and let us say there are some assumptions like that this distribution is normal may be that these vary the samples that you are taking are independent of each other some set of assumptions that you have to take. If all that of its true, then we want to do certain things such that you will get another new distribution you will be doing some math with these data points. See these data points that you got from the sample you will be doing some math with those data points you will do that math’s such that the test statistic would be no different than a single random number that you draw from a very specific probability distribution. If that is the case, then you test the probability that the test statistic you calculated belongs to this theoretical distribution you basically say hey it looks to me like if the null hypothesis is true, then whatever I have calculated of here with the sample should be like drawing a random number from this distribution. So, let me calculate the actual test statistic and see how likely it is that this number came from that probability distribution and that is what we call is a P value. Now, once you have done this process you might say look if the null hypothesis is true, then in the test statistic that I should have calculated should have come from this distribution. But, look at the number that I got in my hand it is, so unlikely that I could have gotten this test statistic from this distribution. Therefore, the null hypothesis perhaps was not true and I am going to reject the null hypothesis or in some cases the test statistics that you get looks like it does belong to this distribution the specific distribution. And therefore, you can only you cannot really say anything it is like you just have no grounds for rejecting the null hypothesis you can just say I feel to reject the null hypothesis the important thing for you might want to look at the this procedure a couple of times, but the important thing that you might want to digest from this is that the P value itself is associated with the probability of seeing this data if the null hypothesis were true and not really the of probability of saying of this hypothesis being true given the data. So, it is probability of data given hypothesis not probability of hypothesis given data. So, I hope that kind of clarifies inferential statistics for you and in the next class we will look at some specific tests and go over how you actually do these tests and even go deeper and talk about why we do some of the mathematical operations that we do. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '7855718606412d25299488b4acf0f8d6'}>, <Document: {'content': '11) Inferential Statistics - Single sample tests\\nIn our previous lecture, we spoke about the need for… We tried to motivate the need for inferential statistics through the context of hypothesis testing. So, we spoke about why we needed it, where it would apply and so on. We concluded that lecture by coming up with a template, coming up with a rubric – essentially of what it is that one needs to do with hypothesis testing. So, we started off with instructions like you need a null hypothesis, an alternate hypothesis; and, the whole thing was fairly general. So, in today’s lecture, what we are going to do is… Today’s lecture continues our focus on hypothesis tests. And, we are going to talk about something called single sample tests. In the last class, you would have seen that we gave two sets of examples; we gave examples of single sample tests and two sample tests. And, today, we are to going focus on single sample tests. And, what we are going to do is we are going to illustrate that template that you saw with by illustrating one test. So, the test is going to be the single sample z-test and we are going to show you the mechanics behind it and kind of give you the reason of why we do some of the math that we do. And then, we will talk about some of the other tests that are there as well. So, the single sample z-test is a test that is used when you want to make some inferences about the population mean. Note that again there is the clarity here is that, you are not saying using the sample, but you are not interested in just reporting the sample mean, which is what you might have done with descriptive statistics; but, you are interested in ultimately making a statement about the population means. And, this is a test, where you need to know the variance of – you can think of it as variance or standard deviation; but, you need to know this variance of the population; and, that is a requirement. So, it is not the same thing as computing the variance from the sample. So, that is called the sample variance. And, there is a formula for that. There is actual data. But, this is more useful when there is pin historic data and you actually know the population variance. But, ultimately, you are doing a test to see if the population… You are doing a test about the population. So, another case where this test finds an application is when the sample size is fairly large. So, this is the one exception to this rule that you need to know the variance. In some cases, when the sample size is fairly large and larger gets defined by approximately 30. So, that is the magic number that people use here. And, when the sample size is 30 or larger, you consider, you reason that you have a large enough sample; and, in that one instance, you do not need to know the variance; you can actually calculate the sample variance from the data itself and use that for this test. So, just to quickly summarize with the single sample z-test, you have a single sample and then you are testing… You are making some inferences about the population mean based up of the sample. And typically, in a single sample z-test, you need to know the variance with the small exception that you can also use the single sample z-test if you have a large enough sample size and you do not know the variance. So, the example we are going to use to motivate this test is one that you have already seen in the previous class. So, we spoke about this problem on average phosphate levels in blood; and, we said that we created… I want to be very clear; we created this kind of a medical scenario; I am not… I am not saying this is medically accurate; we are interested in the statistics of it. So, we imagine this doctor or this public health system, which says that your average phosphate – the average phosphate level in your blood should be less than 4 point 8 milligrams per deciliter. And, the idea here is that, doctors or whoever understands that not every time. So, you take a blood reading; you take a blood and you take a reading of the phosphate level. Not every time is it going to be 4 point 8 or less; sometimes it might be more, sometimes it might be less. But, the whole idea is you are trying to see if the average is less. And, when you say I am interested in the average, you are not interested in the average of some sample that you have taken; and, here a sample would be if you took 5 blood tests; perhaps different machines give different results; perhaps different times of the day give different results; perhaps what you ate in the morning affects it; but, the whole point is you just have that sample at hand; but, you are interested in saying something about the population. So, let us just step back and go through each of the bullet points one more time in the context of this example. So, we said what are we testing? I am interested in the population mean. So, you might ask yourself in this particular example what is the population? And, you can think of the population in a couple of different ways; you can think of it as a concept of what your true average and true distribution is. So, let us say that, yes, you can take a blood test. And, when you take a blood test, it is like your taking a sample. But, there is the concept of what – of what the phosphate is in your blood at all times. So, doing a test just gives you one peak into the reality; but, there exists this concept of reality, which is that, there is some distribution. And, we are assuming that distribution does not change over time; and, that distribution has a certain mean and we are very interested in this mean. And, this is the distribution of the phosphate levels in your blood. This is that reality, which we do not know and you can think of it as this oracle somewhere that knows what your true phosphate levels in blood is at all times and at some distribution. And, at any point of time, you go take a blood test, it is like you are getting a random, you are getting a number from this random variable, which is in the form of distribution. And, you are very interested in the mean of the distribution. So, that is the population; population is your true phosphate levels in your blood at any point of time. And so, it is a concept. And, if you kind of like to think of population as actual data points; another way you can think of this is to say the population is what is this data set – this is very large data set that you would see if you were to continuously take blood tests – infinite number of times with infinite number of machines over multiple days or whatever. So, you can think of constructing this population in your head as a very large data set. But, the truth is ultimately, you never have the data set or you do not… otherwise, we would not be doing inferential statistics. But, what you do have is a sample. And, the sample can be of some size; we have not discussed that. It can be 5, 10, 20, 30, 40 data points. And, the idea here is that you know the variance. So, I have said that it is a known standard deviation of 0.4 milligrams per deciliter. So, just mind you that, this is the standard deviation, not of the data set, of the sample set you got; but, it is the standard deviation of the population. So, it is like this. This population distribution – we are trying to answer some questions about its mean. But, someone somehow you know what the standard deviation of this distribution is someone whispered it to you or you know it from fundamental principles or you might reason that historically it has been equal to this value and should not have changed. But, you know the standard deviation in this particular example. And, we have already talked about how – if you have a large enough sample size, you do not actually need to know the standard deviation, you can compute it. So, here is the data. So, I have told you that, we know the standard deviation; but, you also… This out here is the data and I have explicitly not given you the exact data points. So, just to give you an idea, each of these data points comes from going and doing a blood test. So, you did a blood test and you got 4.1; you did it again, you got 3.9 and so on. And, there is this list. And, this is what we call as a sample. So, what do you do with this data? How do you conduct the test? Let us go back to this rubric that we created that is just it is like this template that we discussed in the end of last class and to conduct any hypothesis test. So, the first bullet point have a null and alternate hypothesis. And, that is what we are going to do. The null hypothesis here is to say that, mu naught, which means – which refers to the population mean. And, you can use the… I have used mu naught here, you can use mu as well; that also you might see text books do that. But, the idea is that, the null hypothesis says that, the true mean of the population is less than 4 point 8 I do not know the answer to this question; that is what I am hypothesizing. We are going to do some mechanics; we are going to go through some process. At the end of it, we are going to see if the null hypothesis is true or not colloquially speaking. So, a null hypothesis like we said, the null and alternate together need to be mutually exclusive, collectively exhaustive. Mu naught says that, null is less than 4 point 8; that is the null hypothesis. So, the alternate hypothesis should be that mu naught is greater than 4 point 8. The next step we said is do some basic calculations – arithmetic on the data to create a single number called the test statistic. And, the math that we are going to be doing here is fairly straightforward. We are going to take x bar, which means a sample mean. The sample mean here would just mean that, you take these data points out here and you take their average. So, you take all the data points that you have collected and you take their average. So, we do that. And, we then calculate x bar minus mu naught. And, here mu naught is 4.8; it is the number that you are hypothesizing. And, you divide that by sigma divided by square root of n. So, here sigma refers to that standard deviation that you already know. So, that is the 0 point 4 that we spoke about out here. So, we are already given that, the standard deviation is 0 point 4. So, given this, we compute x bar minus mu naught. So, x bar is the average of the sample; mu naught is the 4 point 8 – the number that you are hypothesizing; sigma is the standard deviation that you are given; and, n is the number of data points in that sample. So, if your sample size is 10, 15, you would substitute 10 of 15. So, that is how you calculate something called the z-statistic. So, why you are calculating this value? We said in the next bullet point that, if we assume the null hypothesis to be true; and, make some assumptions about. Distributions – I would not say that every time; but, if we assume the null hypothesis to be true, then technically, the test statistics should be no different than pulling a random number from a specific probability distribution. So, if that… The whole idea is if the test statistic – if the null hypothesis is true, then this test statistics z-stat should be equivalent calculating z-stat. So, you can plug in numbers for x bar, mu, mu naught, sigma and n. And, you will calculate a z-stat. Once you calculate the z-stat, if the null hypothesis is true, then the z-stat that you are getting should be equivalent, should be the same thing as pulling a random number out of a specific probability distribution. What is this specific probability distribution? In the case of the single sample z-test, this distribution is called the z-distribution. And, the z-distribution is nothing but a normal distribution with mean of 0. So, I have used this nomenclature; we have discussed how this is standard nomenclature; but, the n here means it is a normal distribution with a mean of 0, which is the first number that you see. So, you have a normal distribution with a mean of 0 and a standard deviation of 1. And, 1 square is a convenient way to represent it because you know very clearly that, you are talking about… 1 refers to the standard deviation and 1 square is also equal to 1. So, the standard deviation or the variance is equal to 1. And so, this is what is known as z-distribution. So, we are saying that, if the null hypothesis is true, the z-statistic that you compute with this data should be the same or should be equivalent to pulling a random number from a z-distribution. This is a very useful thing to say. And, what we are going to do is we are going to come back to what we do next, because of the statement. But, before we do that, I want to make sure that, you understand why. If the null hypothesis is true, that the z-statistic – computing a z-statistic from the data is equivalent to pulling a random number from a distribution that is normally distributed with mean 0 and standard deviation – 1. So, let us go to the next slide to kind of do that. So, at the start, you had this distribution. So, this distribution is the population distribution. So, this is the population. The population – if the null hypothesis is true, would have a mean equaling mu; correct? I am using mu and mu naught little interchangeably out here. But, I do not want you to get confused by that; but, the idea is whatever your hypothesis is, if your null hypothesis is true, then the mean of this distribution is equal to 4.8. Technically, it is less than or equal to 4.8; but, we are going to take the extreme case. So, we are going to come to one end of it and say it is equal to 4.8. So, this is the extreme situation, where the mean is actually equal to 4.8. And, we are already given the standard deviation. So, you have already been told that, sigma is 0.4 and you are given that value. So, first, let… This is just the distribution of the population. So, you have built the distribution of the population. Now, when you go to take a sample from this population of some size n, what do you get? What you get is as we have discussed, if I take sample of 5 data points, 6 data points; and then, compute a mean from that sample, we know that, the arithmetic mean that you compute from a sample need not always be equal to the mu exactly. Sometimes it is mu, sometimes it is little higher than mu; sometimes it is a little lower than mu. But, we have already discussed as to how. That is also a distribution and that distribution called the sampling distribution is also normally distributed with the same mean mu, but with a standard deviation of sigma by square root of n; where, n is the number of samples you took to compute that mean. So, it you took 5 data… So, if you technically just take one sample and compute the mean from it, you will get the same distribution again – get the same original distribution. So, if you substitute n is equal to 1, nothing changes. And, that should be intuitive. If your sample size is just one data point; when you are computing the average of one data point, it is literally like you are recreating the distribution. If that n goes to infinity, then your variance goes to 0. So, as your sample size keeps on increasing, you are literally going to be sitting on top of this line and you really would not have this distribution. But, for all finite sizes of samples, what you have is a distribution for samples, which is normally distributed with mean mu and standard deviation sigma by square root of n. Now, what we are going to do? Now, mind you, this is the distribution of sample means. So, in some sense, this is the distribution of x bar. Now, what we are going to do is we are going to subtract mu from this distribution. From this distribution, we are going to subtract mu. What effect does that have? From a distribution – from a normal distribution, essentially, if you just subtract a number, it is literally like just shifting the distribution and centering it. So… because you are just subtracting a number, you are not affecting the standard deviation. So, this gets unaffected. But, when you just subtract a number from the distribution, you can think of subtracting from a distribution as you take each data point and then subtract the same number from it. Or, you can think of it as computing that x bar and then subtracting it. But, in either case, it has the effect of just shifting the distribution; it has the effect of centering the distribution at another location. And, that is what happens. When you subtract mu, when you take mu out of the x bar; so, it is… This is what I have done; I have taken mu out of x bar. It has an effect of moving this distribution to another location, which is now centered around the mean equal to 0. Now, what happens if you then take this distribution and divide it by the standard deviation? So, what happens if we divide it by this number – sigma by square root of n? The effect that has is in re-scaling the standard deviation such that you now have a distribution, which is normally distributed. When you divide, something that is already centered around 0. So, the distribution is already centered around 0; which means that it has some positive values, some negative values. And now, you go divide by a number. The effect of the division is that, because it is already centered around 0; it is not actually going to change the central location of the distribution; it is instead only going to widen it or narrow it depending on what you divide it by. So, I have kind of shown the distribution getting narrower; but, that need not be the case; the distribution could have just got wider. It just depends on whether sigma by square root of n was greater than or less than 1. So, if it is greater than 1, then you would by dividing by sigma by square root of n, you will be making the distribution more narrow. But, if sigma by square root of n was smaller than 1, then it would have the effect of widening the distribution. But, ultimately, when you go divide the normal distribution, which is already centered around 0, all you are doing is you are either stretching the distribution or kind of crunching it. You can think it as scaling it. And, now, you have a standard deviation of 1. So, that is how you get the whole idea of x. So, on the first step, we took x bar and then we subtracted the mu from it. That is where we got this value. And, now, after dividing by this number sigma by square root of n, you went and divided this by this number to get your normal distribution with mean 0 and standard deviation 1; which is what we said was the z-distribution; got it; perfect. So, now, what we are going to do is go back to this rubric. So, great; so, we have reasoned that you have to calculate the z-stat. And, we have already said that, the z-stat if the null hypothesis is true, should be like pulling a random number from a normal distribution. What we are going to do now is we are going to test the probability that, this statistics that you got; we are going to say – if your null hypothesis was true, I should have gotten a random number from here. But, let us actually look at the actual number that I got. Does my z-stat actually look like it could be something that I could have pulled out of a normal 0 comma 1 square, because if it does not, then something that I assumed was wrong. I said that, this z-stat should look like something that I pulled out of a normal 0 comma 1 square if the null hypothesis is true. So, let me go to take an actual look at the z-stat number. And, if it looks like it could not have come from this distribution; if it looks like it was unlikely to have come from this distribution, then I can reason that, maybe the null hypothesis was not true and I can reject the null hypothesis. So, that is what we are going to do the next step. The next step is to take the z-stat and plug it in to this normal 0 comma 1 square and see how extreme is this actual number given that we know the normal 0 coma 1 square; and so, we know the potential values it can take. For instance, I have already told you it is a normal distribution – mean 0 comma 1 standard deviation. So, from this distribution, if I were to pull a random number, how likely is it that I would see a number like 55. That is too high. The standard deviation is 1, the mean is 0; it is almost impossible that you would pull a number like 55 or minus 20; so, for that matter. So, if it turns out that the z-stat is too extreme a value to be coming from this normal distribution, then we can maybe make a statement about the null hypothesis. So, that is what we are going to do as a next step. Let me just clean this up. So, I have just restated the official definition. What we are going to be calculating is something called a p value. And, this is the probability of seeing a test statistic as extreme as the calculated value if the null hypothesis is true. With the core idea being that, if it looks too extreme that, if the p value of the probability of seeing this test statistic is really low; then, perhaps the null hypothesis was not true to start. So, for instance out here, if the z-statistic you computed was 1.2; so, I just… I am just giving you a number to go with. Then, the core idea is that, you would calculate a p value based on the standard null hypothesis, which is that, mu naught is less than 4.8. And, you will say… Let me take this distribution, which is the z-distribution or the normal 0 comma 1 square. So, this is the normal 0 comma 1 square. And, I am going to go place 1.2 here. And, I am going to compute a probability; and, this – the probability is a probability to the right side of 1.2. It is the area under the curve out here. And the idea is because you are then quantifying the probability of seeing something as extreme as 1.2 or greater is equal to the area under this curve. And, that might happen to be any value. So, I mean I think in this case, it happens to be something like 13 percent or whatever. But, if this number was really low; if this number was 0.001, you might say look – this number is so low that I am going to reject the null hypothesis. And, if you cannot find something that extreme, the standard thing that you do is you fail to reject the null hypothesis; you technically never accept the null hypothesis. So, that is the core idea. And, you can also think of this in a couple of other ways. For instance, if your null hypothesis were that mu is greater than 4 point 8; then, you would be looking at the area to the left of your curve. But, typically, in a situation like that, you actually would not do the statistical test. So, it is not common to see p values greater than 0 point 5 because at that point, you start by saying look I have computed a z-statistics that is already positive, that is, 1 point 2. And so, I know even before I go put this line out here that, I am going to get a probability greater than 0 point 5. So, for instance, your z-stat was computed to be exactly equal to 0. You know that, if your null hypothesis was mu is greater than 4 point 8; that, it will be 0 point 5. So, any z-stat even greater than that is bound to be greater than 0 point 5 when you do not need statistics for that. But, another interesting situation, which a lot of people work with, is called the two-tailed case. These two are called 1 tailed – 1 tail. So, you also have the 2 tailed case, where your null hypothesis is really that, mu is equal to 4 point 8. And, you are interested in rejecting this null hypothesis whether that mu is too large; meaning it is large enough that you can say that it cannot be equal to 4.8; or, if it is small enough. So, you are happy to reject if you see evidence that shows that mu cannot be 4.8 on either account; maybe because it is the data suggests that it is too large, maybe because the data suggests that it is too small. And, that is called the 2-tailed case. Now, there is lots of different software, where many of the steps that we have done we are actually taking care of and it does not take much. Even a simple excel sheet if you just go down the data and say do a z-test; it will do it. But, somewhere understanding the mechanics of this and getting it to the stage of the z-statistic, at least brings about some sense of control and transparency in your understanding. But, once you get off the stage, computing this area – whether it is on the left-hand side, right-hand side or either side can be done fairly; it is not something that can easily be done by hand. So, what text books do is – if you have taken, most statistics text book will have these pages towards the end. And, they are given in the form of tables. So, you can take z-statistic number that you computed and go plug that in to this table. And, it will tell you the probabilities. And, usually there will be a diagram on top to hint whether they are giving you the probabilities to the left hand side or the right hand side or both sides. And, you know – as long as you know what they are giving you, it is fairly easy to figure out whatever it is that you need. If you want the right-hand side; but, they are giving you only the left-hand side; then, you can just subtract the number they are giving you from 1, because you know the total area under the curve is equal to 1. But, what I am going do is I am just going to give you some simple Excel functions that do this for you. For instance, in Excel, you can just… The convention is to give you the area to the left-hand side. So, instance, what I do here is I subtract that from 1. And, the norm s dist is what refers to the z-distribution. And, the true refers to the fact that I am not interested in just height, I need the area under the curve. So, that is what that… So, for the right-hand side tail, you can use this; for the left-hand side tail, you can use this. A simple multiplication by 2 with the left-hand side case gives you the 2-tail situation. So, with this, we have discussed greater detail the single sample z-test. So, now, let us look at a couple of other single sample test. What I provide you with here is the list of them and the formulas. And, I will give you some idea of the context in which they are used. But, we would not derive it or go through it in the same detail as the z-test. So, we have already discussed the z-test. Let us now look at the next test, which is the t-test. So, we have finished with the z-test. So, now, we are going to look at the t-test. So, with the t-test, it is a very useful test and it also tries to test this… It essentially tries to do the same job the z-test is doing; which is to make some statement about the population mean; so, same problem statement in some sense. The one big difference is you are not given the variance. And, in most situations in life, in statistics, you would not know the variance; I mean just think about how fairly unrealistic it is that you already know the variance of the population in a situation, where you are trying to make a statement about the mean. I mean the only reason you are doing this test is because you do not know what the population mean is. So, you are trying to… You are making a hypothesis, you are taking a sample, and then you are testing that sample, you are working with that sample to make a statement about the population mean. So, there is… I mean think of it as there is some uncertainty about the population mean in the first place and that is why you are doing this test. To assume that in such a situation, you already know the population variance is not very – need not a very realistic. So, this test works the same way. So, if you look at it, it has got the same x bar; it has got the same mu; it has got the same square root of n. But, this s is different from this sigma. And, the difference is here sigma was given in this z-test. So, in the z-test, sigma is given. But, in this test, the s is computed; it is computed from the data. So, you actually go back to the sample data and you calculate the variance or standard deviation from the data using the same formula for dispersion that we would have discussed when we spoke about standard deviation and descriptive statistics using the n minus 1 idea. And, if you do not remember, you can go back and see that lecture. But, the idea is that, you compute the standard deviation and you plug that value in to get the t-distribution. Now, a couple of things that are worth noting is that, we spoke about how if you know the variance, you can use the z-distribution; if you do not know the variance, you can use t-distribution. But, there is this exception. We said if your sample size is large enough; then, you can technically use the z-distribution and just compute the variance and consider the variance to be the truth; consider the variance to be the sigma and go ahead. I personally find that a little confusing; I think that is fine; if… That is what is there in tax; that is what people do and that is the reasonable approximation. But, the point is you cannot go wrong with using the t test when you do not have the variance. So, even if you have a large enough sample size, the idea is that the t-distribution becomes… It approximates z-distribution quite well when your sample size is greater than 30. There for all practical purposes, the t-distribution looks exactly like the z-distribution. But, keep the things really simple; you can just follow this simple rule that, you do not know. If you know the variance, just use the z; if you do not know the variance, just use the t. And, that should keep you clear. The other thing to mention out here is that, this DOF or degrees of freedom – we have mentioned that, out here we briefly spoke about that concept when we were talking again about the standard deviation. Without going into too much detail into degrees of freedom again, the simple thing to keep in mind is that, the t-distribution is not one distribution. I mean it is one distribution, but in the sense that, the t-distribution – you can think of the degrees of freedom as a parameter that goes with the t-distribution. So, just like the normal distribution, if you say the normal distribution, you need to mention the mean and the variance for you to have a… to actually draw the exact distribution or to do some computation on it. It is no point coming to someone and saying how likely is it to see a 1.2 in a normal distribution? That question does not make sense. Normal distribution with what mean and what variance? And then, I can answer your question. You can think of degrees of freedom in a similar light; which is that, the t-distribution itself is not completely defined until I mention to you what the degrees of freedom are. So, t-distribution with three degrees of… – with degrees of freedom equal to 3 looks different from a t-distribution with degrees of freedom 4. And, the core idea that you need to know is that, the t-distribution has a mean of 0. And, it looks very similar to the normal distribution of mean 0 and standard deviation 1. But, the exception that as the degrees of freedom keep increasing; so, when you go to degrees of freedom of 30 and greater; and, at some point, it is exactly the normal distribution. So, the t-distribution with a large enough degrees of freedom is exactly like the normal distribution with mean 0 and standard deviation 1. But, as the degrees of freedom keep decreasing and come all the way down to, the lowest degrees of freedom you can have is 1. When it comes all the way down to degrees freedom equal to 1, you will find that it still looks a little bit like the normal distribution with mean 0 and standard deviation 1; but, it is a little shorter – shorter in the center and has fatter tails on the sides. And so, that is how it deviates from the normal distribution. But, all that you need to know is that, the degrees of freedom get defined by the concept n minus 1. So, number of data points minus 1 tells you the degrees of freedom. And, once you know the degrees of freedom, you know which t-distribution to look up in the tables. So, you know how to draw the curve and then calculate probabilities from it. Again Excel uses – Excel has some slightly nicer functions for it. So, if you are just interested in looking at the left-hand side of the distribution, you just use T-DIST – T dot DIST. If you are interested in T dot… On the right-hand side, you do T dot DIST dot RT; or, on both sides, you do T dot DIST dot 2T. So, you do not need to actually do the 1 minus and so on that we were talking with the z-distribution. Excel already has some inbuilt functions to just point to which side of the distribution you are interested. So, we go now to the next. We are finished with the t–distribution; we go now next to the next test, which is the chi square test. And, the chi square test has a couple of different types of tests. But, the one that we are interested now is the chi square test for variance. I am using the words variance, standard deviation a little interchangeably; one is just the square of the other. The test is ultimately one for variance. And, if you are testing variance, you are essentially testing standard deviation. So, if it is easy for you to think standard deviation, you can keep that in mind. And, an example for instance of the chi square test is you are really interested in looking at a sample, but you are not interested in making a statement about the population mean. You are instead interested in making a statement about the population variance. So, you are interested in saying is the population… Just like in this z-test and the t-test, you are interested in saying something like – is the population mean equal to 4.8? Or, is the population mean less than 4.8? Similarly, here you would be interested in saying things like – is the population variance equal to 0.5, 0.3 – whatever number you have in mind. The important thing is you have a number in mind and you are trying to see if the sample that you are taking… With the sample that you are taking, can you say something about the population variance being equal to this magical number that you have in your head. And, the mechanics of the test is fairly straightforward. And, it is here the sigma naught is the hypothesized variance. So, this is the number that you want to compare it to. This is the equivalent of the 4.8 that was there for means. The s square is the sample, is the variance that you compute from the data, from the sample. You take that data set of the samples and you compute standard deviation, you compute a variance from that. And, that is s square. And, the way you do that again to remind you is this that, 1 by n minus 1 in the formula for the calculation of standard deviation; you would be using that. And, that is how you calculate the test statistics, which then gets compared to a chi square distribution with n minus 1 degrees of freedom; just like in the first case, it got compared to z-distribution and this got compared to… The t-statistics got compared to t-distribution. This is the same way the chi square distribution gets compared to a chi square distribution; great. So, a couple of things to note is that, if again chi square also uses the concept of degrees of freedom; so, think of the degree of freedom as something that defines the exact distribution you are interested in, because a chi square distribution with 3 degrees of freedom is a different distribution than a chi square. It is a different density function. It looks different. It has different mathematical properties than a chi square distribution with 4 degrees of freedom, 5 degrees of freedom. So, the degrees of freedom help you define the exact distribution and its parameters and its mean variance and so on. But, essentially, that is what you would use. You would need to use the degrees of the freedom and that is also the same as before; it is n minus 1. So, number of data points minus 1. And, chi square… With Excel out here just uses chi square dist; this is the left side and chi square dist dot rt is the right side. I do not see them having something for 2-tailed, but I might be wrong. But, as long as you have these two, you can quite easily just draw that graph in your head and figure out which side; if you are interested in a 2-tail distribution, how you would compute that; great. So, we finally, come to our last single sample test, which is called the proportion z-test. And, the idea here is you are testing something that is a proportion. So, you are testing something like… If you are given… So, you are testing a hypothesis like less than 30 percent of the shoppers, who come to my online store are women. So, you can say again; we can go the 2-tailed way or you can go the 1-tailed way; you can say less than 30 percent; you can say 30 percent of my shoppers in my online store are women or you can say greater than 30 percent of the shoppers are women. The key out here is that, whatever sample you collect to actually test this hypothesis, the hypothesis… So, let us fix on the hypothesis. Let us say the hypothesis is less than or equal to 30 percent of the shoppers, who come to my online store are women. The idea is like all hypothesis testing, you will now… – you have this hypothesis; you will now go and collect a sample. The sample in this particular example could be something like you actually give a survey at the end of the purchase or something and people actually say them – male or female. So, you collect a sample. How does this sample look? The answer is that the sample unlike in the previous examples, where you would have seen an actual number. So, in the previous example, in the phosphate examples, you saw numbers like 4.1, 3.5 – these were actually readings from blood tests. Here you are going to get something that is binary. The person is either going to say that either they are female or not. So, it is a series of 1s and 0s – very similar to the idea behind Bernoulli trials. And, what you are doing is you are now looking at the sample data of 1s and 0s, which… and then answering the question of whether… and then saying something about the hypothesis, which is less than 30 percent of the people, who come to my shop are women. And, this has the same intuition as all the other forms of inferential statistics, which is if for instance, you take 100 samples and you know all 100 of them point to the shoppers being women; then, you are likely to reject the idea that only less than 30 percent of the shoppers have come to my online store are women. But, the idea is if you notice something, this looks in every way shape and form like the Bernoulli distribution. And, the fact that you are counting how many. So, the Bernoulli distribution had to do with probability of heads or tails. So, that would be probability of it being male or female. But, if you are interested in out of size of n people, who arrive; how many of them? Sort of hundred people who arrive are… Is it less than 30 who are women? That ties us to the binomial distribution. And, yet you do not see the binomial distribution being used in the test, you instead see the same idea of z. So, you are again calculating a z-statistic with this test and you are comparing to the z-distribution, which is normal – which is normal 0 comma 1 square. And, the idea here is fairly simple. If you remember, we spoke about the binomial approx… – we are approximating this binomial distribution to a normal distribution. That is a right way to put it. And, that is what you are doing out here. And, it should also be intuitive in that the p hat out here is a calculated proportion. So, you will take a data set. Let us say you took 30 people or 40 people or 50 people who came to your store and you actually found that, exactly 23 of those 50 were females. So, that proportion is what? The proportion that you get from the sample is what you have is p hat. p naught is the hypothesized proportion. So, p naught would be the 30 percent. So, this would be sample. And, this is the equivalent of mu naught. So, this is the population – proportion – the number that you are hypothesizing; and of course, is the sample size. And, if you look at it, this also looks very much like that x bar minus mu concept. And, in some way, this formula out here in the denominator should remind you of the formula that you saw for standard deviation in the binomial distribution class. So, you are doing something very similar to the x bar minus mu by sigma over square root of n; it is in construct identical to that. But, you are using the binomial distribution for calculating things like the variance. But, you are also saying – hey, this I believe should be… you can approximate to the normal distribution. So, I am just going to use the z-distribution to calculate my p values or I am going to use the z-tables. Just in quick conclusion, just going back to the rubric that we created, I will just clean this up. The final idea is that, you use these z-tables; you can use z–tables; by z-tables, I mean like you can use the back of the statistics text book, you can use Excel or Matlab or R. Just important that you know how to do with at least one of these softwares. And, the core idea is that, if you get a low enough p value; all of these help you calculate p value or a probability. And, if you get a low enough p value, you can use that as grounds for rejecting the null hypothesis. And, I am saying I reject the hypothesis that mu naught is less than or equal to 4.8. And also, just keep in mind – on the flip side, you never say I accept the null hypothesis and you can only say that, I fail to reject the null hypothesis. I hope that clarified the use of… Give you one illustrative example of the single sample test and the idea behind the mechanics of it; and, introduced you to the other tests. And, in the next class, what we are going to do is we are going to talk about 2-sample tests and we are going to go also beyond that. We are going to talk about the idea of having multiple samples. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'd61b0d28619226a1bd6d8db10463f29a'}>, <Document: {'content': '12) Two Sample tests\\nHello and welcome to our next lecture in inferential statistics. So, today, we will continue our series of lectures and hypothesis tests; and specifically, we will build upon our previous lectures. So, just as a reminder, we motivated inferential statistics and the use of hypothesis tests two lectures ago and, in the previous lecture, we focused more on the single sample tests. Now, if you remember from our previous classes Ð two classes back, when we provided examples, I spoke about the use of the one-sample situation as well as the two-sample situation. So, this table that you see in front of you is something that we discussed previously and we have gone over this. And, what you see on the left-hand side, the one sample situations Ð some examples; and, on the right-hand side, here are some examples of the two sample situations. In the last class, when we did some tests, we focused on the one-sample case; and today, we are going to focus more on two-sample case. So, what is the big difference? So, again, just if you jog your memory, we spoke about various single sample tests. So, let us go back to the single sample tests. In the one-sample testÉ I am using the word single in one sample interchangeably. In the one-sample situation, we were either testing Ð doing a hypothesis test on the population mean or we would do a hypothesis test on the proportion Ð a proportion Ð again associated with the mean proportion Ð associated with mean; and, or sometimes we would do a hypothesis test on the population standard deviation. But, in all these cases, whether it is a proportion that you are testing, whether it is the mean that you are testing or whether it is the standard deviation/variance that you are testing; in all these cases, you always had a single dataset. Your sample came from a single distribution. And so, you had a single sample in your hand. A single sample does not mean a single data points; you had many data points, but the data points represented one particular distribution or one particular context. And, in all these cases, you would invariably compare this dataset to a single number that you had in mind. So, you would test the hypothesis that, the average phosphate levels in blood were less than 4.8. Here the dataset that you had was the dataset associated with phosphate levels in blood for may be a person or a machine or a set of people; whatever the context is, there is still one dataset and that set was used and compared to a specific number. In this particular case, the specific number was 4.8; that number could be something else. In the case of the proportion test with the health department, it was 5 percent that you were testing against. And, in different cases, in the case of the garage, it was comparing it to the national average, which conceivably would have been a particular number. So, that is the third example. But, in all these cases, whether you are testing for mean, whether you are testing for proportion or whether you are testing for standard deviation, just keep in mind that, what makes it a single sample test is that, you do not have multiple sets of data; you have the single set of data. And, you are always comparing that set of data to the set of data you sample. But, using that sample, you are saying something about the population. And, what you are saying about the population is essentially a comparison with a particular number that you have in your head. So, you are trying to see if it is less than 4.8. So, those are the single sample cases. But, in the case of the two samples, you actually will get two sets of data. And, typically the way it happens isÉ For instance, in the first example, you see there in this table, you actually go change the temperature; so, you actually go mess with a particular variable and then you look at the number of defects. So, you have dataset one, which might be a set of 10 or 20 or 30 data points and the number of defects in different casts. And then, you went and changed the temperature and you get another set of 10 or 20 or 30 data points. Sometimes these numbers are not always equal Ð meaning Ð cannot have the same number of data points on either side, more often they are not; it is good that you do. But, the core idea is that, you now have two sets of data that represent two separate distributions. Distribution 1 is for the number of defects that you would expect to see before you change the temperature. And, distribution 2 is the number of defects you would expect to see after you change the distribution, after you change the temperature. And, these two distributions correspond to their respective populations. And, what you are doing is you are taking a sample from both of these populations and near comparing these two samples and you are ultimately trying to make a statement about the mean of population 1 versus the mean of population 2. So, that is essentially the first example that you see here. Now, the second example is again one where nothing was changed over time; but, let us say you just had two parallel different manufacturing processes. And, this is a case, where you want to compare the variance; you do not really care about the mean of the manufacturing processes. So, let us say these two processes are coming up with some finished product. You are more interested in the variability of this finished product. So, you do not care really what the mean is. But, again the idea said you will have manufacturing process A and there will be some distribution associated with it. And, there will be a dataset associated with that sample and you will have manufacturing process B and you will have another dataset associated with that sample. And, even the last example which is tenth standard girls taller than tenth standard boys is also fairly is just a straightforward extension of what we discussed, where you have two different datasets and you are comparing the two datasets. So, keep in mind that you do not have to have a number in your mind. And, I will talk about the odd case where you do have a number in your mind; but, the one big difference is with two sample situations, you are ultimately having two separate datasets. So, instead of thinking of the words as one sample and two sample, that sometimes gets confusing. I just like to think of it as a single set of data versus two sets of data; that kind of emphasizes that there are many data points. The other way of thinking about it, which sometimes helps me is just think of it as for the first time, the two-sample test, you are dealing with two variables; you are always going to have the variable that you are measuring. So, in the case of theÉ Let us take each of these examples. In the first case, the variables Ð I am going to call them 1, 2 and 3. So, let us do that. So, this is 1, this is example 2; and, this is example 3. You do not have to circle it. So, in example 1, the variable that you are measuring is the number of defects. So, that is your output variable. So, in some sense, you can say you have got one variable, which is the number of the variable as a number of defects. But, in addition to that, you have another variable, which is the temperature. And, the temperature takes on only two values. And, that is why it is a two-sample test. The temperature takes on the value that it was before you changed it and the value Ð it is after you changed it. So, for the first time, you are seeing two variables. When you go back to the single sample situations; so, if you take the average phosphate levels example, the phosphate levels in blood was your response; so, essentially your output variable. And, that was the only variable that was involved; there was nothing else that you were changing. For the first time in the two-sample case, you will have two variables, which is the number of defects, which is your output, your response variable, your standard variable that you always see. And then, there is another thing that is changing, which is the temperature. So, the temperature beforeÉ And, the temperature can take on only two states: 1 and 2. And, those are the two datasets before and after; same thing with the manufacturing process to compare variance of the finished products; so, the variance of the finished product, the variance of a certain number. So, it might be the dimensions of the product of which you are interested in the variances of. So, you are interested in the variance of the dimensions of a certain product. Let us say that was the diameter of a finished product. So, the diameter itself is the output variable and you are concerned about the variance of that. So, that is your output variable. But, the manufacturing process is your other variable with two difference states. So, there are two different manufacturing processes. So, if you call it manufacturing process A and then manufacturing process B; then, manufacturing process becomes your second variable. And, this variable can take on only two states: A and B; you can call it 1 and 2 or whatever you want. Again out here in the third example for instance, what is your output variable? Quite straight forwardly height. So, you are measuring height in both cases; and that is your output variable in the third case. And, the second variable of interest; and, you can think of it as the input variable is gender. So, girls verses boys. So, you have datasets for girls and then you have a dataset for boys. Tenth standard is not a variable, because it is consistent across both. So, it is just a detail in some sense. But, what you will be measuring is height 1, height 2 and so on dot dot dot; and, same thing out here also. So, it is like you haveÉ One way to think of the difference is obviously that you are dealing with the single dataset with the single sample case and you are always comparing that to a number. In a two-sample case, you are dealing with two datasets. And, another way to think of it isÉ For the first time, in the two-sample case, you will actually be encountering two variables. One Ð very clear output variable or the response variable that is the variable of interest, that is, what you are comparing. And then, the input variable; essentially, what are the two classes that you have created in your system that you are comparing this response variable across. But, otherwise, you will find that the core steps; this rubric that we created last time in terms of steps for hypothesis test statistics, which is that you still need to have a null and alternate hypothesis. In this case, the null and alternate hypothesis looks a little different. If you remember, in your previous example, you would have a null hypothesis such as a mu 1 is equal to 4.8 or mu 1 is less than or equal to 4.8. Here you will typically have a hypothesis like mu 1 is equal to mu 2; you can have different types of null and alternate hypothesis. If you remember in the single sample class, we spoke about how you can have a single tail tests and two-tail tests, where typically something going back to your single sample class thinks like mu 1 is equal to 4.8 would lead to two-tail test. And, we explained how that works. So, the same thing applies here. When you say mu 1 equals to mu 2, that is a two-tail test; whereas, when you say mu 1 is less than or equal to 4.8 or mu 1 is greater than or equal to 4.8, that would be one single-tail test. Similarly, out here you can have something which says mu 1 is less than or equal to mu 2; that can be null hypothesis. And correspondingly, the alternate hypothesis will also change and you can also have something that says mu 1 is greater than or equal to mu 2. So, all of these are possible. But, accordingly, just remember Ð you will calculate the same test statistics. This formula will not change. But, accordingly, where you are, which part of the distribution you are interested in will change. And, if you still have some doubts about how that works, I strongly suggest that you go back to the lecture on single sample test and see how, which part of the distribution gets covered. And, we will also try to support that with some problems in your science. But, the important thing for you to remember with the two sample case is that, the same core concept of single-tail and two-tail tests hold. There will be a small exception to this and we will quickly cover that. It is not really an exception, but it is more of an addition and we will cover that towards end of this lecture in terms of creating more complex a hypothesis. But, for now, just seals as a simple extension of the single sample tests; but, you have a mu 1 and a mu 2. The second step still hold, which is that you will do some basic calculations or arithmetic on data to create a single number called the test statistics. Last time we solved different formula for the single sample z-test. Here, you have given your formula for the two-sample z-test Ð two-sample z-test. And, the core idea here is for instance thatÉ So, let us just go through this formula to get you some idea. The idea as such you have an x 1 bar and an x 2 bar. So, it is different from your old formula of x bar minus mu divided by sigma by square root of n. Now, this is the formula that you would have seen for the single sample z-test, because you had a single dataset and you have calculated an x bar from that; and, mu was a number you had in mind. Here you have two datasets: 1 and 2. So, for both these datasets, you are going to calculate x 1 bar and x 2 bar. So, just to give you some idea, I mean I have represented as a table. Just for your convenience, just focus on this table. We are going to use this table for some other purpose. So, on this table, with the arrow, you have two sets of data. So, one set of data corresponds A; one set of data corresponds to setting B. You can think of it as the boys and the girls or you can think of it as manufacturing process A versus B or whatever it is. And, this number out here in the bottom, which is x 1 bar and x 2 bar Ð essentially, there needs to be a small dash above it, if it is not clear. So, there is x 1 bar and x 2 bar. And, these correspond to the average. So, x 1 bar corresponds to the average of these numbers; x 2 bar corresponds to the average of these numbers. So, you can think of it as x A bar and x B bar, but you can alsoÉ x 1 bar and x 2 bar are also just fine; it is just convenience. So, this x 1 bar and x 2 barÉ Let me clean this page up. x 1 bar and x 2 bar is essentially what gets plugged into these two formulas. And, I am going to come to d naught in a second; but, similar to the z-test of the single sample case, you will need to know the standard deviation of the populations associated with the two means. So, x 1 bar and x 2 bar are essentially the sample means that you get from this distribution. But, you have this distribution associated with 1 and 2 separately with A and B separately. And so, if you are going to stick with the nomenclature of sample 1 and sample 2, sample 1 has a mean Ð mu 1; sample 2 has a mean mu 2. We are testing the hypothesis that mu 1 is equal to mu 2; but, there is a standard deviation associated with sample 1 and a standard deviation associated with sample 2. That is sigma 1 and sigma 2. We need to know sigma 1 and sigma 2. So, those are numbers that need to be given to us. So, you need to be able to derive it from first principles, not from the data. So, you can plug those in; and, n 1 and n 2 would correspond to the number of the data points that is there. So, if these are 10 data points that are here; then, you would actually see. So, there are 10 data points in A; maybe there are 15 data points in B. So, you would plug in that 10 and 15 correspondingly and that would be n 1 and n 2. So, you plug all of that in. And, I said I will come back d naught. And, this d naught is this small addition to the simple hypothesis that you can form here. So, if have a simple hypothesis like mu 1 equals to mu 2, then your d naught would be equal to 0. This is the idea where you say something like manufacturing process A; that mean of the manufacturing process A is equal to the mean of the manufacturing process B. And then, that is fairly straightforward. But, what if you said something a little bit more complex like the mean of manufacturing process A is 3 units higher than the mean of the manufacturing process B. In that case, your null hypothesis is really that mu 1. Suppose you were to say mean of manufacturing process A is equal to 3 units; mean of manufacturing process B plus 3 units. So, it is three units greater than mean of manufacturing process B. So, that becomes your null hypothesis; then, your null hypothesis becomes something like that, which is mu 2 plus 3 is equal mu 1; and then, d naught will take on a value. And then, d naught would take on the value 3. So, you can create more complex hypothesis. More often than not, d naught tends to be capital 0. You are more interested in questions like is A equal to B. But, sometimes if you are interested in questions like is A 3 units greater than B or is A equal to B plus 3 units just to keep it consistent as a two sample test. But, if you are more interested in saying is A greater than B plus 3 units. In those cases, again it will become a single sample test, but you would still need the user something like 3. So, now, you do this and you calculate test statistics the same way as you did before. The formulas are little different. But, once you calculate it and you have a z number out here, the distribution you are talking about is the same distribution that you were dealing with in the single sample test, which isÉ So, if the null hypothesis is true, which is that mu naught is equal mu 2; mu 1 is equal to mu 2. And, you make some assumptions; and then, the test statistics, which is what you have derived z should be no different than a single random number from a z distribution or a single normal distribution with mean 0 and standard deviation 1 square. So, that part of the logic is exactly the same. And, the way you test the probability using either the z tables from that is, these are some tables that you see in the back of statistics test books or you could use Excel or Matlab or R. And, we discussed some formulas in excel during the previous class. Those formulas are all identical; you are doing the exact same thing. And, the core idea is that, you use the software to calculate the probability, which is called a p value. And, if the p value is low enough, then you reject the null hypothesis; you reject this hypothesis that you created, which is that mu 1 equals mu 2. So, guys, like everything out here in this part is exactly the same as your single sample test. The only thing that change is this formula and your null and alternate hypothesis. Again the core concept of using two-tailed and one-tailed tests also is the same. So, having described this, let us briefly go into the tests that are actually available. So, we have already discussed the z test. Another example that I can think of if you kind of learn by examples isÉ So, for instance, it is often thought that calcium could reduce Ð calcium supplements could reduce blood pressure. So, you could conceive of a simple test, where you give calcium to some people, let us say 20 people; and then, for others, you give placebo. So, people do notÉ These are just sugar coated pills where they think they are taking calcium. And, you can compareÉ So, let us say you gave it a 20 in 20 people; you can compare the blood pressures of these two sets of people. So, you will have one set, which is a calcium set and then you have one set, which is the placebo set; and then, you can test the hypothesis that the mu of calciumÉ So, mu of calcium is equal to the mu of the placebo. Or, in this case, because there is the hypothesis goes out and says that calcium should create lower blood pressure, you can test the hypothesis that mu of calcium is less than or equal to mu of the placebo. So, just another random example for you on that; you can alternatively use the t-test. And, the reason for using the t-test Ð again the goal of the z and the t test are always the same with the same difference as in the case of the single sample tests. The difference being that, in the t-test, you do not know the standard deviation already. In the z-test, you know sigma 1 and sigma 2. This is given to you in the t-test. This is not given to you. And so, you have to calculate s 1 and s 2 in order to figure out. Now, if you remember, in the single sample case, we said there is an exception to this rule. So, we said the basic rule is if you know standard deviation, use z; and, if you do not know standard deviation, use t. And, in most cases in life, you are not going to know the standard deviation. So, the t tends to be popular for that reason; I mean think about it right; it goes back to saying you are actually testing the hypothesis associated with means because you do not know mu 1 and mu 2. If you knew mu 1 and mu 2, you would not be doing any of this ifÉ Now, imagine a world, where you do not know mu 1 and mu 2; which means you do not know the population means of the two distributions. But, someone comes and tells you what the population standard deviations are; kind of rare to find. So, more often than not, the t gets used; but, we also spoke about this exception, where if your dataset is really large; and, large was defined by greater than a dataset size of 30, then you can actually compute the standard deviation and then the t-distribution approximates to the z-distribution. So, you can still use the z-test. If some of you find that confusing, you can keep it simple; you know the standard deviation, use the z; you do not know the standard deviation, use the t; you cannot go wrong with that. Now, there is one small complication. And, I know these formulas can sometimes be a little scary. But, we are going to step through each of them. In the t-test, there are two versions; there are actually multiple versions. And, there is another version called the paired t-test; and, we are going to come to that. But, we are not talking about that now. For now, I am talking about a straightforward t-test just the way you did the z-test for the same purpose of defining the difference in mean. But, in this particular case, in the t-test, you need to figure out whether the standard deviations or the variances are equal or not on principle. If you can say the standard deviation should be equal, you would use this formula, which is the equal variance formula; if you do not and you believe the standard deviation should not be equal, then you would use the unequal variance formula. So, the quick idea is that, the idea of x 1 bar, x 2 bar and d naught is the same as in the case of the z-test. And, you use a single formula for standard deviation because its equal variance; and, that you get from computing s 1 square and s 2 square, which is nothing but the variance that is computed from the actual dataset. So, here is the dataset. And, this s 1 and s 2 are actually the standard deviations that are computed from this data. By the way, just for your reference, the sigma 1 and sigma 2 what you see here are not computed from that data; I have just put them under A and B to tell you where they belong; but, sigma 1 and sigma 2 are given to you as s 1 and s 2 are the standard deviation ofÉ Like for instance, s 1 is the standard deviation of the this dataset; hope that make sense. So, let us also clean up this table grid. So, when you have equal variances, you can compute something calledÉ And, this is called a pooled variance and that is what you will plug into this formula. And, we have also covered the concept of degrees of freedom. And, the degrees of freedom are nothing in this case, but the idea of the total number of data points. So, using the total number of data points to figure that out. And, you will see that you can use a similar formula for the unequal variance case, where you will not have a concept of pooled variance and you will have a separate s 1 and s 2 square and the formula for degrees of freedom also differ. Again if you have any questions on degrees of freedom, feel free to refer to the previous lecture; that should give you a similar insight. You will also notice that again in both those cases; this is extra term d naught; sometimes some texts will not even include this formula; but, the idea is that, if your null hypothesis is quite straightforward, which is x 1 equal to x 2, or is x 1 less than or equal to x 2 or is x 1 greater than or equal to x 2; then, d naught is equal to 0. But, if there is an offset such as is x 1 equal to x 2 plus d naught. Then, you will need d naught. Or, is x 1 greater than x 2 plus d naught. We will now move on to the next form of t-test. And, this is called the paired t-test. It is also trying to test the same core concept, which is mu 1 equal to mu 2. But, it is doing so in a slightly different way. And, the idea behind the test is the following. In a typical test, you have two datasets. And, here I am referring to everything that you can see on your left-hand side. Dataset A and you have dataset B. And, if you are trying to see if mu A equals mu B; and, you are doing that using x 1 bar and x 2 bar and so on. Now, if there is some kind of logical pairing between the rows; so far, we have actually ignored any connection between this data point to this data point, because there need not be any connection. And in fact, the number of data points in B need not be equal to the number of data points in A; which is why we have two completely different terms n 1 and n 2 in all these formulas. But, if n 1 was equal to n 2 and there was a logical connection between each point, then you want to use something called the paired t-test. What do we mean by a logical connection between the two points? We mean the following. Either thatÉ Let us say you are doing this same test of the calcium and placebo. We discussed this test in the context of a z-test. Suppose you did not know the standard deviations, you could have very well used it as a t-test. But, let us say that you were doing a calcium and placebo; but, the way you were doing the test was that, for each person Ð for each individual person, you would give a calcium tablet; look at the change in blood pressure; and then, on a separate day, give the same person the placebo tablet; that means, for each person, there would be one recording in calcium, one recording in placebo; which could mean that, each row could signify the calcium for person x and the placebo for person x. So, there is actually a very clear logical pairing. So, the second row could be something quite simply calcium for person y and placebo for person y. So, in that sense, these two data points whileÉ Yes, A continues to mean calcium, B continues to signify placebo; but, this is specifically forÉ There is a logical connection that both of these are connected by this person y. So, lot of times, applying the same treatment on the same x essentially experimental unit could be that logical paring. And, when you have that logical paring, a great way to get more out of your test is your paired t-test. And, the idea here is that, instead of computing x A bar and x B bar, x or B; used to call it x 1 bar and x 2 bar, we instead take the differences of A and B. And, each difference is computed here. So, this is the difference between 23.1 minus 21.1. And, that is 2.2. Once like that you compute all these differences; and then, you get a d bar, which is nothing but the average of these differences. And, you get an s d, which is nothing but the standard deviation of these differences. And, you go ahead and plug that into this formula. So, you have this d bar s d Ð square root of n is just the number of data points and this d naught is the same concept as the d naught here; where, if you are saying if A is equal to B, then your d naught is 0; if A equal to B plus 3, then d naught is 3. But, outside of that, d naught is the same core concept; but, the idea here is that you are using d bar s d instead of x 1 bar, x 2 bar separately. You can think of other common examples about a logical pairing. For instance, if a common example is Ð if we are interested in knowing Ð if a particular track creates more wear and tear on the left-hand side of the particular tyre; let us say you are on a formula 1 context; it is a sport and you are really captain of a particular track creates more wear on the left-hand of the tyre than the right-hand of the tyre; then, you might do a two-sample test and you might compare left tyres and measure their wear. So, wear just means how much they have eroded. So, let us say there was some logical way of measuring that. Then, you would measure the wear on some sample of 20 left tyres and the wear on 20 right side tyres. Now, if these 20 left-hand side tyres and 20 right-hand tyres came from completely different cars, you would have to stick to your standard t-test. However, for each car, if there was one left tyre and one right tyre as a data point; and then, you had 10 cars or 20 cars. So, you had 20 left tyres and 20 right tyres; then, you can use the paired t-test, because for each particular left tyre, there is one corresponding right tyre. You could use that logical association to use the paired test. We then move on to the equivalent of a proportion test. We saw this in the single sample case; but, again it works the same way as your old proportion test; and, that you have two samples here and you want to see if the number of defects through process A is worst than process B. So, again you are doing this comparison not against a particular number; but, you are doing this comparison between two samples andÉ But, you are comparing the proportion that you see in both the samples. So, that is what you are doing in the proportion z-test. The formula here is quite straightforward. The proportion associated with sample 1; the proportion associated with sample 2. Always remember that, in a proportion test, your data is essentially extreme of 1s and 0s. So, you are not getting the actual numbers; you are getting either yesÕs and noÕs or males and females or whatever; it is that, you are measuring. But, you are getting this p 1 comes from a dataset, which is 1, 0, 1, 1 dot dot dot. And, that p 1 is nothing but the average. And, p 2 is similarly coming from another dataset 1, 1, 0, 0, dot dot dot. And, for the sake of variance, you would use this. This is kind of like the concept of pooled variance. And, that p hat in general comes from this, where x 1 and x 2 are the number of 1s that you see overall. And, n 1 and n 2 are the total number of data points that are there overall. So, that is a fairly straightforward extension of the proportion z-test. And, the final we come to is the F-test. The F-test is used when you want to compare the standard deviation of dataset 1 with the standard deviation of dataset 2. You can conversely think of it as comparing the variance of dataset 1 with the variance of dataset 2 because that is essentially what you are doing. And, the idea is that, you take sample variance from dataset 1, sample variance from dataset 2. And, that should logicallyÉ This is very similar to the chi square test; but, that should logically give you something called the F-distribution. This is the first time we are saying that distribution called F-distribution. But, the one difference between the F-distribution and some of the distributions we have seen so far is that, the F-distribution has two parameters that define it and there are the two degrees of freedom. So, it is actually called numerator degrees of freedom and denominator degrees of freedom. And, numerator degrees of freedom is defined by n 1 minus 1, which is what is a number of data points on dataset 1, which is what goes on the numerator. And, the denominator degrees of freedom is n 2 minus 1, which corresponds to the s 2 square that you calculated, which is the sample variance of the dataset 2. Just to again recap for you the concept degrees of freedom with a z-distribution, that concept does not exist; the z-distribution is nothing but a normal distribution with mean 0 and standard deviation 1 square. With the t-distribution, by definition, t-distributions have mean 0. But, to describe exactly the t-distribution, you are talking about you need to signify the degrees of freedom; and, that gets signified by these formulas for equal and unequal variances. And then, you have the special cases of the paired t-test; where, again the degrees of freedom is n minus 1. Again with the proportion test, because of the binomial approximation to the normal, you are only getting a z-distribution at the end of it. So, ultimately, you will be Ð you use this test statistic; but, once this test statistic is computed, you are still dealing with a normal 0 Ð mean 0 standard deviation 1. And finally, for the F-test, which does a test of compares two variances to see if they are equal or not; you have to define it based on numerator degrees of freedom as well as denominator degrees of freedom. I hope that was clear and that you have a good feel for the use two-sample test. Again there is a lot of software out there, where you can just plug in dataset 1 and dataset 2. So, for instance, you might be able to just completely dump the entire dataset in its native format and tell the software what test to do. But, using these test statistics you have a better understanding of what you are doing when you understand the formulas. And, once you compute the formulas and carry out your tests, then you can use a software to get the exact probabilities. Thank you and look forward to seeing you in the next lecture . English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '532cd762d99126fa1bb9b5bf13553ceb'}>, <Document: {'content': '\\n13) Type 1 and Type 2 Errors\\nHello and welcome to our lecture on Type 1 and Type 2 Errors. This is something that you might have heard and it is a fairly central and important concept with respect to hypothesis tests. So, let us get into, what the core concept of type 1 and type 2 errors are. So, the idea is the following. Take any of the hypothesis tests that you learnt so far, in this course we provided you with a template for how the hypothesis tests are conducted. And in this template, you would notice the first step is to form a null and an alternative hypothesis and your last step is that you essentially get a p value associated with this particular test and based of the p value, you either reject the null hypothesis or you fail to reject the null hypothesis. So, this is what we have characterized in this table. So, you actually make a null and an alternative hypothesis and you do not know which is true, that is why you are doing this test. If you knewÉ So, your null hypothesis was that mu 1 equals mu 2. So, if you knew that was true you would not be doing this test, so you do not know that is true, that is why you are doing this test. So, you might have a null hypothesis that says mu 1 equals 4.8, which is a single sample case or you might have a null hypothesis, which says that mu 1 equals mu 2. In either case, you do not know which is true. But, let us say there was this, all knowing world, where you knew which was true. So, that is what marked out here in actual, you say actually what is true. So, here I am saying the null hypothesis is true and in this part, I am saying the alternative hypothesis is true. So, here I am saying mu 1 is equal to, let us say mu 2 and here the alternate hypothesis is true, mu 1 is not equal to mu 2. Now, this is the truth. But, based of some data, you did a test. So, you might have done a two sample z test or you might have done a two sample t test and let say, that you did the test you actually computed the z or the t statistic. Based off of the z or t statistic, you calculated a probability or a p value and based of a p value, you took a decision and the core idea has always that p value is too small, then you reject the null hypothesis and the term too small is a subjective term and typically people use some line in the some threshold and that threshold is called alpha. So, if the p value is less than alpha, we reject the null hypothesis. Typically, the value of alpha tends to be something like 0.05 or 5 percent. So, based off of this, let us say you rejected the null hypothesis, that is one decision and here you fail to reject the null hypothesis, for whatever reason. So, the idea is that out here you would have rejected the null hypothesis, because your p value would have been less than your alpha. So, p value is less than alpha, so you rejected the null hypothesis. Here the opposite was true, p was greater than or equal to alpha that is it. You do not have to have that equal to sign or so, just say greater than and I am not sure, you might want to say less than or equal to. So, this is what you did; now here is the problem. If the truth was that the null hypothesis was true, the null hypothesis was true which means that mu 1 was equal to mu 2. But, you went ahead out here in this quadrant, you went ahead and you rejected the null hypothesis. You said mu 1 is not equal to mu 2, but in reality mu 1 was equal to mu 2. You did something wrong and that error is what is called is a type 1 error. It is when the null hypothesis was true, but you went ahead and you said that I am rejecting the null hypothesis and we are going to come back to quantifying that value which is called the type 1 error. It is just noteworthy that is also called the producer risk, in sometimes the false positive or the alpha risk. We will not go in to each of those terms, for instance producer risk is seen more from a manufacturing context, false positive is seen more from a medical context. But, a few kind of think about it, it definitely makes sense. Now, here is the case out here, where the null hypothesis was true and you fail to reject the null hypothesis. So, that is okay, you are happy with that, reality was that the null hypothesis was true and you did not find any evidence to reject the null hypothesis. So, you did the right thing, so you are happy in this quadrant and let us come to this quadrant out here. Here, the null hypothesis in reality, because this is truth, so this is truth. So, here mu 1 was not equal to mu 2 and you correctly rejected the null hypothesis, this is still the null hypothesis. Your null hypothesis is this, this mu 1 not being equal to mu 2 is the alternate hypothesis, but it is so happened, that the alternate hypothesis was reality that the null hypothesis was wrong and when the null hypothesis was wrong, you correctly rejected it. So, you did the right thing even here and that is a very good thing that you did. You actually detected that mu 1 was not equal to mu 2 and you said no, I am rejecting the null hypothesis, so that is a great thing. Now, come to the final quadrant. Here, the truth was that mu 1 was not equal to mu 2, but you failed to reject the null hypothesis. You are not able to say, I am rejecting the idea that mu 1 is equal to mu 2, I am rejecting the null hypothesis. So, you are not able to reject the null hypothesis, whereas you should have, because mu 1 is not equal to mu 2 and that is the truth. So, that is also not a great thing that you did and this error is called the type 2 error and again, it is call the consumer risk in the more manufacturing production settings, it is called the false negative in medical settings in couple of other settings or beta risk. Now, in really simple words, type 1 error is the concept. So, just to be really clear, we are right here. So, the idea is that your type 1 error, even before you see any data, even before you do anything could be as high as your alpha and I say after analysis; it is exactly equal to your p values. So, let me explain that a little bit. So, let see you have not collected any data, we just discussed what alpha is. Alpha is the idea that you are going to do this hypothesis test and you are going to get some p value and you already said, if the p value is too low I am going to reject the null hypothesis. How low is too low that is the line in the stand that you draw and that value is called alpha. So, if your p value is less than alpha, you are going to reject the null hypothesis. What is that mean when your p value is less than alpha? First of all, you are going to get a very low p value, let us say you are going to get a low p value and this p value is less than alpha. Because, you type one error first of all comes about only, when your null hypothesis is true and you go ahead and reject the null hypothesis. So, by definition; that means, your p value must have been less than alpha, only then you would have rejected your null hypothesis. Now, only when you reject your null hypothesis or you even putting yourself up for the possibility of a type 1 error. So, the idea is that I am going to tell you that up front, your type 1 error is associated with this idea that you rejecting the null hypothesis and you reject the null hypothesis when your p value is less than alpha, which means your p value is really low. Now, let us take a step back and think about, what the p value is. The p value is this idea that, if the null hypothesis is true, this is the definition that we looked at much earlier, even before we discussed type 1 and type 2 errors. P values by definition express the idea that if your null hypothesis is true, this is the probability of seeing a test statistics as extreme as this, if your null hypothesis is actually true. So, this probability is really low, then essentially what you are saying is that, you are willing to take the risk and saying, the probability of seeing this statistics is just 1 percent if the null hypothesis is true. If the null hypothesis is true, the probability of seeing something so extreme is just so small, it is just 1 percent, then I am willing to take the risk and reject the null hypothesis. So, write there by definition, the risk that you took was that 1 percent risk, it was that p value. So, whatever your p value is, essentially if you do the entire calculation and you calculate the p value, then your p value is essentially your type 1 error, that is the risk you are taking, that because that is you actually calculated the probability that this data could actually occur with the null hypothesis being true. So, if you are still going ahead and rejecting the null hypothesis fully knowing this probability, then that is the risk you are taking of making a wrong decision. Now of course, before you even see any data and even before you calculate the p value, given that you set yourself with an upper threshold of alpha, means that you could get a type 1 error as high as alpha. So, I hope that makes type 1 errors fairly clear. Now, type 2 errors are more complicated, now and there is a reason for it. It is a function of variety of parameters, it is a function of something called delta, which is not the delta that we would have discussed so far and the core concept behind delta is the following. Type 2 errors, when do they occur? They occur in the situation, where your alternate hypothesis is actually true. So, mu 1 is not equal to mu 2, but you failed to reject the null hypothesis. But, in order for me to quantify how likely that is, you need to tell me, how much is mu 1 not equal to mu 2. So, if I said mu 1 is not equal to mu 2, because mu 1 is equal to mu 2 plus a very, very, very, very, very small numbers. So, let us say 1 micro meter or whatever it is, whatever the metric of mu 1 mu 2 is, but it is a very small numbers. So, mu 1 let say is 4, but mu 2 is 4.000001, then it is true mu 1 is not equal to mu 2. The alternate hypothesis is true, but the probability that I am going to reject the null hypothesis, just became very low. My ability to discern between a difference, this difference between 4 and 4.000001 is much lower than my ability to discern between the difference of 4 and 5, all other things being equal. So, it is really a function of how different are they and that is what gets captured in delta, it is also a function of the sample size we take, which is as the sample size becomes infinite. The uncertainty around mu 1 and mu 2 becomes smaller and smaller and theoretically, even a small difference between 4 and 4.000001 could potentially be found as long this sample size is large enough. It is a function of type 1 error, because your type 1 error tells you, how conservative or liberal you are being in rejecting the null hypothesis. So, at the more conservative you are in type 1 error, meaning that you do not want to make that type 1 error, the more likely you are to make a type 2 error and similarly, the more liberal you are with the type 1 error, meaning you are with some amount of error the better you are going to be with type 2 errors. So, it is a little bit of given take in terms of the type 1 and type 2 error and essentially, a lot of people are very interested in understanding the relationship of beta, which is this type 2 error. So, it is a bad thing versus delta, which is something we would discussed here and for a given sample size and they will show it from many different sample size and that is known as an OC curve or an Operational Characteristic curve. Another word that you might come across, it is known as the power of the test and that is equal to 1 minus beta and it essentially a very positive thing. It is 1 minus a bad thing. So, the higher the power of the test; that means, the more strength you have in being able to detect a difference between mu 1 and mu 2 and I have use the concept of mu 1 and mu 2 coming from the two sample scenarios, but all of these core concepts also applied to a single sample tests. I hope that clarified and that give you an idea of type 1 and type 2 errors. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'b4b1aaf1247d5d0fa60cb6fad214cc2e'}>, <Document: {'content': '14) Confidence Intervals\\nHello and welcome to our next lecture in inferential statistics. Today, we are going to be talking about confidence intervals. So, statistical inference even in terms of classification mainly is discussed in terms of hypothesis testing and estimation. So, these are the two broad categories in statistical inference. And, hypothesis testing is something that we have discussed in good detail in this class; we have talked about single and two-sample tests Ð various tests. Today, we are going be primarily talking about estimation. And, you might notice that, almost any text that talks about statistical inference, talks about these two topics. And, some of them might be introducing estimation or confidence intervals before hypothesis testing. But, that does not really matter. Essentially, these are two sides of the same coin if you meant So, today, we are going to be talking about estimation. Now, the idea behind estimation is that estimation can be in terms of point or interval. What we mean by that is it is a same core concept as what we introduced with inferential statistics during hypothesis testing; which is that, we are interested in some population parameter. What we mean by that is that, there is this concept. So, the examples that we have used in this class are things like amount of phosphate in our blood, the height of tenth standard students in public schools in India. In all these cases, you define some population and you are interested in some parameter. More often than not, we would discuss the parameter being the mean. So, what is the average amount of phosphate in blood? What is the average height? But, it does notÉ That is just one of the parameters. That is the most common one. But, it can be other parameters. So, stepping back, we are interested in some population parameter. But, you do not know this parameter. It is like it is some truth that you do not already know. If you did know that, there would be no need for any of this or any of the statistics. But, what you do have is a sample. So, you have 5 data points, 10 data points, 20 data points, 30 data points. Some sample from this population. And, what you are most interested is you are most interested about this population parameter. So, in hypothesis testing, you would hypothesize that, this population parameter is equal to 4.8 or 2.3 or it is less than 4.2. And then, you would go about and look at this sample and see if that is true or not. With estimation, you are not having any hypothesis; you are not having any hypotheses in mind in that sense. What you are trying to do is you are trying to take the sample. And, with point estimation, you are trying to come up with a single point estimate of the population parameter. And, that might seem fairly straightforward. So, for instance, let us say you are interested in the population parameter, which is the average amount of phosphate in blood. And, you took a data, you took some sample. And, that sample was about 20 data points. A simple point estimate of the population mean could be the sample mean. So, you take the sample of 20 data points; take their average. And, that is your best; that could be one of your best point estimates of the population mean. So, point estimate just means you are making as good a guess on the population parameter based on the data that you have. But, today, we are going to be talkingÉ And more interestingly, this is what getsÉ This is what people are more interested in, which is interval Ð estimation in the form of an interval. So, this goes back to again the core concept with hypothesis testing, which is fine. You do not know the population mean; you have a sample mean; and, you acknowledge that, if you go to take another sample of another 20 points, you might not get the exact same value. And, both these values the first time around and the second time around might not be exactly equal to population mean. If it is not exactly equal to the population mean, then can I come up with some range around my point estimate. So, I have a point estimate, which is actually my sample mean; my sample mean is my best bet; let us say at my population mean. But, I acknowledge that, I might not have exactly hit target. The population mean might be a little higher or a little lower than my sample mean; in which case, I ask the question Ð can I come up with the range around this sample mean? By which means it is essentially like I am giving myself a margin of errors by which I am fairly certain that I have covered the population mean. So, that is the goal that we are going to embark upon. And, in many ways, it is the same map, because we have introduced hypothesis testing; it becomes a little easier, so that we can reason by the same logic in map that we have already discussed. So, let us do that. So, the core idea is that, let us take an example that we have looked at many times. So, which is that we might be hypothesizing the amount of phosphate in blood is equal to exactly 4.8. Now, we discussed that, in confidence intervals, you do not have this number; you do not come up with the hypothesis. You are only interested in coming up with some bounds around your point estimate. So, what you essentially do? One way of thinking about confidence intervals given that we have already introduced hypothesis tests is well, for different values of mu naught. So, let us say you have some dataset and you calculate some x bar. Essentially, 4.8 gets plugged in out here to calculate your z-statistic. If you are doing a z-test, you are given a sigma; if you are doing a t-test, you take an s; but, in neither case, that gets plugged in; n is again the number of data points. So, you get some z-value Ð some z value; that is out here. And, based out of that z-value, you calculate some probability; you calculate a p value. Now, the core idea with hypothesis test was that, if this p value is really small, you reject the null hypothesis. So, the question with confidence intervals Ð one way of thinking about is given that, I do not have some hypothesis, I ask myself the question within what range can my mu be such that I will calculate a z such that I will get a p value, which I will not reject. I am just going to repeat that; given that you do not have a mu naught, you can think of a confidence interval as what is the range of values that mu could potentially be such that given that, for a given dataset, you will get some x bar, some sigma, some root n such that you will calculate a value z such that you will get a p value, which you will not reject. So, if you would not reject, that means you need to have some bounds. Suppose you start off by saying well, I am going to reject any p value less than 0.05. So, that is something you started off with. Now, given that you started off with that; then, is there some rangeÉ For a given data set, is there some range of mus such that you would not be rejecting this hypothesis test. And essentially, to compute that, all you do is just rearrange the terms out here. So, you keep the mu naught onÉ Ð the mu or the mu naught Ð I am using those two terms here interchangeably. But, you keep that on one side and you essentially move the terms to the other side to get this formula for confidence interval. So, typically, if you know the formula for the hypothesis test or the test statistic, you can just essentially rearrange it. But, the core idea is that, this is your point estimate, which is your x bar. So, for your best estimate of mu is your x bar; but, you create a margin of error or you create a range around it as plus or minus the z associated with this alpha. And, alpha here is that 0.05 that you said. Essentially, you said within a certain range. So, within that range and sigma and square root of n are the same. Another way a more formal definition associated with confidence interval is that, if we were to repeatedly take identical samples of the same size and build similar confidence interval bounds for each sample, then you are building a bound such that 95 percent of such confidence interval bounds will cover the true mean. Or, in other words, we are 95 percent confidence slash certain that the true mean mu is within our confidence. So, for single sample tests, the process of creating confidence interval is fairly straightforward. I explain to you how in the z-test, it essentially just becomes a rearrangement of terms and this sigma byÉ This concept of sigma by square root of n essentially goes towards the z and then the x bar goes here such that it is a plus or minus. And, that is how you get the formula for that interval. The same core idea for the t-distribution; the formula is no different, except that, the t-distribution gets also defined by the number of degrees of freedom; so, not just the alpha, but the number of degrees of freedom. And, in all these cases, mind you, because I am putting a plus or minus, this is the equivalent of the two-tailed z-test or the two-tailed t-test. You could also create a one -sided bound if you were interested. And, that would be again the formulae equivalent of having a one sample test. Again it would be the same sigma divided by square root of n. The formula itself would not be different. But, the way this alpha gets used up will be different. Again it goes back to the core concept of how you would shade that region under the probability distribution. With the chi square distribution, that rearrangement is not obvious. To some extent it is. But, the plus or minus is not, because you do not have a plus or minus term; it is essentially like this sigma naught goes out here, the chi square distribution comes down here. But, the way we differentiate between the lower bound and the upper bound is by changing the alpha in the bottom of the chi square distribution. So, it is the same rearrangement; it is a same core concept of taking the hypothesis test and rearranging; that is, in this case, it would be to put the sigma naught out here and bring the chi square down here and you would have the same formula that you see here. But, you get an upper bound and a lower bound by looking at the 1 minus alpha by 2 and alpha by 2. And, by the way, this notion of alpha by 2 depends on how you define it. Now, if you say I want a 95 percent confidence bound; that means, you are left with 5 percent. And, if it is two-tailed test, that 5 percent gets divided into 2.5 percent times 2. That is how you get the alpha by 2. So, it really depends if someone starts by stating alpha and you know it is a two-tailed test; then, technically, the correct way to do this would not be to just have an alpha out here, but it would be to have an alpha by 2, so that you are being technically correct. And, the same thing goes for here as well; you will have an alpha by 2. Again the same core idea with respect to the z-test. If this alpha is more generic term that I have used out here. So, if somebody comes and says I need a two-tailed test; so, there is a plus or minus and the alpha gets divided by 2. But, if it is a one-sided test, that alpha can stay as alpha. So, depends on how it gets firmly defined typically. If it is a two-tailed test, you will represent it as alpha by 2. But, it isÉ Again if you look at it, it is a same rearrangement from your test statistic to create the confidence interval. So, the same idea goes towards two-tailed tests. Just to give you an idea of how it works for a two-tailed test, I have given a single sample for the z-test, for the t-test and the proportional z-test. We have consciously left it out; that might be a part of your assignments that you could work on. But, the idea is the same. We are interested in this term. We are interested in the term x 1 bar minus x 2 bar. And so, we want to create a confidence bound around x 1 bar minus x 2 bar. If you remember this simple way of thinking about x 1 bar minus x 2 bar was to say to test the null hypothesis that x 1 bar is equal to x 2 bar. But, that is logically the same as asking the question Ð what is the confidence bounds around x 1 bar minus x 2 bar? And, seeing if that essentially covers a 0 or not. Of course, this d naught is an extra term. Suppose you were interested in a hypothesis that looked more like this; if this was your null hypothesis, then you could Ð you would have this d naught being something nonzero; otherwise, d naught is just equal to 0. But, this is the idea. So, you are interested in some bounds around the term x 1 bar minus x 2 bar. And, again you would do the same logical rearrangement. This goes here and the bounds go around x 1 bar minus x 2 bar. And, you are essentially creating bounds around this value. So, there is x 1 bar minus x 2 bar and what is your range around that; great. Again similar to the chi square test, the F-test is not so straightforward. So, I am including the formula associated with that out of here. You do the same thing that you did with the chi square test, which is you have the same s 1 square by s 2 square. That is your core upper bound and lower bound. But, you divide by this F. This F kind of comes down and you divide by that. But, the lower bound is 1 minus alpha by 2 and the upper bound is alpha by 2. Of course, also remember that, when this is a two-tailed test, the alpha that you see here becomes alpha divided by 2. If it is a one-tailed test, meaning you just had a plus or a minus; you can keep that as alpha. I hope that clarified this concept of confidence intervals. Thank you. English - NPTEL Official', 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': 'e679dacc76f9e8a26a58421462ccf721'}>, <Document: {'content': \"15) ANOVA and Test of Independence\\nHello and welcome to our lecture on ANOVA or also known as analysis of variance; and, that chi square test of independence – the TOI is test of independence – the chi square test of independence. We present this as the last lecture in the series on inferential statistics. But, it is only fair to say that, while some set of techniques are presented is a part of inferential statistics. Most of the techniques in statistics use statistical inference in some way or the other. So, for instance, after this lecture, we will be talking about regression and so on. And, there is a significant component there, which is associated with statistical inference. But, in any case, this lecture, we are going to talk about the analysis of variance and the test of independence. So far, statistical inference was confined to input variables that could take up two possible values; and, that was the two sample cases – the two sample tests, where you would compare two different samples. And, the variable of interest would be for instance, either male or female. So, we came up with an example, where we said the heights of tenth standard boys in public schools versus the heights of tenth standard girls in public schools. So, if you were interested in seeing if the average height of a boy in tenth standard in public schools in India is equal to the average height of a girl in tenth standard in public schools in India; here the output – essentially you can think of there are two variables that are involved and you can think of the output variable as one of height and your input variable you can think of as gender. So, you have two different sets of data and you are comparing them. Now, there are also these cases, where it does not make sense to think of these as input and output variables; that is just confusing. So, and, these are the single sample test cases, where you just have one variable height and you have defined all the other parameters around it. And so, it could be something like I have already defined that I am interested in studying the average height of boys in tenth standard in public schools in India; and, I am interested in seeing that average height is less than a 120 centimeters or something – some such number. There is no concept of it; there is just one variable, which is height and that is it and you are comparing it to some number that you had in your head. So, that is essentially summarizing the two sample case and the single sample case. For the first time with something like the ANOVA, we take on a case, where this variable, which is essentially like this input variable, can take on two or more states. So, essentially, you will see it taken three or more states. And, that is the real differentiator about ANOVA. So, just to kind of give you a little bit more color on this, a standard application of ANOVA would be something like the following, where you are still dealing with an output variable that is typically quantitative and continuous like for instance, like height. But, your input variable need not have just two states, it can have three or more states; and, you are still probably interested in comparing their average. So, extending the example that we saw in the two sample t-test case; if you wanted to ask a question such as – is the mean height of tenth standard boys in… is the average height of tenth standard boys in Tamil Nadu equal to the average height of tenth standard boys in public schools in Maharashtra; is it equal to the average height in Karnataka. So, you are not just interested in comparing two sets of samples; but, you had multiple sets of samples. And typically, what you are comparing, the output essentially, variable is still like a continuous quantitative variable like height. It can be anything else; but, essentially, it is a quantitative variable. And, you have multiple discrete settings of your input variables. So, you are not just interested in comparing boys versus girls or method A versus method B; but, you are now going from… In the t-test, you are comparing method A versus method B; you can now say is method A equal to method B equal to method C equal to method D or are they different, or a some subset of them different from the others. So, that is essentially an application of ANOVA. Now, the chi-square test of independence is one that can be used when you want to compare again multiple proportions. When you are interested in two variables; and, both of these variables are categorical variables. So, it could be the same thing like Tamil Nadu, Karnataka. So, the different states of India could be one of the variables. And, the other variable could be the number of people at different age groups. So, people between – number of people between 0 to 20 years – between 20 to 40 years and 40 to 60 years. So, there is one variable, which is the state, which is a categorical variable. And, here is another variable age, which I have in some sense made categorical. But, essentially, I am just interested in these two variables, which are categorical and I am just interested in looking to see there is any relationship between them. Another way to think of chi-square test of independence is see it as some form of an extension of your proportion z-test. In a proportion z-test, you would typically handle problems of the following nature. You will say if you took a question like our men more likely to purchase a certain product than women. That would be a proportion z-test, because you have two categories: men and women. It would be – has to be precise; it would be a two sample proportion z-test, because you would have two categories; men would be one category; women would be the other category. And, their likelihood of purchasing a certain product would be represented by a binary. So, upstream of 1’s and 0’s. So, 30 percent women purchase this product out of a 100 samples; and, 20 percent men purchase this product out of 25 samples. But, you take that same problem, where you had just two categories, which is purchased or not purchased, men and women, and you extend that to multiple categories on both dimensions. So, that is what we did when we said state could be multiple different states; and, we said age could be multiple different ages. So, if you are able to take these two variables and extend them to multiple categories, you can use a test of independence in sets. So, let us first jump into the ANOVA and try and explain the core concept and the math behind it. The idea behind the ANOVA again would be to test a hypothesis of this nature, which is mu A equals mu B equals mu C equals mu D. A typical t-test for instance would have just looked at something like is mu A equal to mu B. So, I think a very natural question that quite often comes up is what so special about 2 versus 3? Why is it that for 2, you can use the t-test? 3 and more… It said the ANOVA is built for this kind of multiple comparisons; but, it works just as well even if you just want to compare two samples. So, you can essentially replace your t-test with a two sample t-test with an ANOVA and you would be doing something mathematically identical. So, before we go any further, we have identified the hypothesis. Now, let us get some nomenclature ready. So, these are the different samples. So, this is the sample corresponding to A; and, A could be anything; it could be fertilizer A versus fertilizer B versus fertilizer C. A could represent the state Karnataka; B could represent Tamil Nadu; C could represent Maharashtra, whatever. So, it depends on the question. Let us take fairly consistent examples. So, let us take the example of a height of boys in public schools in tenth standard for four different states. So, these are the four different states. And, this is the dataset. So, this is data point 1. So, this is data point 1 for state 1. So, that is represented as y 1,1. Now, data point 2 for state 1 is represented as y 1 comma 2 and it goes on till y 1 comma n. And similarly, it goes in this direction; y 2 comma 1; and ultimately, you have y 4 comma n in the bottom corner. Now, having defined this, I am going to throw you in some sense in the deep end with the formulas of how an ANOVA is actually computed. So, this is the idea. And so, let us just go through this step by step. The idea here is to compute an F-statistic. And, what if we learnt about… If there is one thing we have learnt about hypothesis testing, it is that you come up with a hypothesis, which we did; and a hypothesis was said mu A equals mu B equals mu C. And, the alternate hypothesis by the way out there is that, not all mus are equal, because there are many ways in which mu A – that you can violate the statement mu A equals mu B equals mu C equals mu D. You can violate it by saying mu A equals mu B, but it is not equal to mu C equals mu D or you can put that not equal to anywhere in that equation. So, quite simply, the alternate hypothesis is that, not all the mus are equal. So, we took care of the first step, which is to create a null and alternate hypothesis. The second step is to do some kind of computation to come up with the test statistic. We are going to talk a little bit about how… First, we are going to talk about the overall structure, which is the math behind this table leads to an F-statistic with a minus 1 degrees of freedom and n minus 1. If you remember, we discussed about how the F-statistic has a numerator degrees of freedom and a denominator degrees of freedom. And, you will calculate their F-statistic and the rest of the hypothesis testing is the same. You will then calculate a p value and then choose to reject or not reject. Now, let us take one step back. We say the F-statistic that we have computed here is nothing but something called MSB divided by MSE. We are going to talk a little bit further about these terms. But, for now, you can you can take this; again go one step back to these two values. And, you technically do not need to know the mean square total or any of these terms. So, this entire part you do not need for actual calculation of the test statistics. But, it is there to give you a bigger picture. So, again we say this MSB is nothing but SSB divided by something called the degrees of freedom. And, that is fairly obvious in terms of what math needs to happen there; and, again the same concept with MSE. So, at least we know how things flow from one side to the next. Now, let us get to the core of the formula, which is calculating the sum of squares between and the sum of squares error. The sum of squares between is essentially y bar i dot minus y bar bar dot dot; what do we mean by that? So, let me first… We will talk about the terms here and then I will come to n and a and so on. Essentially, what y i bar dot means is y A bar dot, is nothing but the average of all these terms. y bar bar dot dot is nothing but the average of all the numbers here. So, if I want a row-wise addition, I say y A bar dot. And, similarly, I would say B bar dot for this row. So, this would be y A bar dot; this could be y B bar dot and so on. And, the idea is that, if you are going to change A, B and so on; and, A, B and C and D can be coded as 1, 2, 3, 4. Then, I might as well just say y i bar dot and put that through a loop essentially. So, that is what I am doing here. I am saying y i bar dot to say I am going from i equals 1 to a; where, a is the total number of treatments, which in my particular case is 4. So, there are four different treatments or there are four different sets of samples that I am comparing. So, essentially, I am taking i from 1 to 4 and I am taking each of those means that I calculate. So, I am essentially taking this average, this average, this average, this average separately. I am taking each of those and I am subtracting that from the grand mean – the overall mean. The overall mean is nothing but every single output variable. This entire table in a sense; this entire table average is y bar – y bar bar dot dot. So, essentially, what I am doing out here is I am doing something… If you look at this formula very similar to standard deviation. But, it is essentially the standard deviation of the means. So, there is an overall mean and you have individual treatment means. So, I am essentially looking at out here this term looks… It is not the standard deviation because I have not done the square root and I am not divided by the number – n minus 1. In this case, it would be a minus 1. So, I have not divided that. So, that is why it is still; I am going to divide it. Once I divide it, it is kind of like variance; but, I have not divided it yet. So, out here it is essentially like a sum of squares. Once it gets divided by that a minus 1, then it becomes that MSB. But, in concept, this is… And, we are going to talk about the multiplication by n in a second. But, in concept, this is a lot like the standard – the variance of the means. So, it is like the variance. So, if you were to compute a set of means for each row, a set of like y bars i dot for each row, the standard deviation of these values or rather the variance of these values is essentially what you are computing with MSB. Now, what is the concept behind here? Here what you are doing is you are looking at the standard deviation of each data point. So, i comma j just means that i goes from 1 to a; which means I am going through each row and I am going through each column out here; j goes from 1 to n. So, j is going from 1 to n; i is going from 1 to a. So, it is literally like I am going through each data point and all through this matrix; and, I am looking at the deviation of each data point from its row average. We saw these two are the same terms that we are using here. The concept here is that, we are looking at essentially the standard deviation within each row. I am not looking at the deviation of y 1 1 from the grand mean y bar bar; but, I am looking at the deviation of y 1 1 from this rows – essentially, this rows average. And, I am averaging that across every row. So, it is almost like I am taking a row-wise average of every data point and I am averaging that. So, that is called the sum of squares error. And, again you are dividing by the total number of points that you see; it is the set of N – capital N by the way out here. There is a difference between the small n and the capital N; capital N is essentially nothing but the total number of data points. So, small a is the total number of treatments; in this case, it is equal to 4, n; we have not actually defined out here; but, essentially, it is the total number of columns. So, that is the number of data points that each combination has. And, capital N is the total number of data points. So, essentially, it is nothing but a times n. So, a times n is capital N. So, using that nomenclature, again calculating MSE also should be obvious. So, the mechanics of how you do the ANOVA with the F-test and calculate a p value should at this point be fairly clear to you. But, let us spend a few minutes and try and get a little bit more intuition on this; which is, why are we using an F-test? Which we know from earlier experience is used to see the difference between two variances. We use the two sample F-test for detecting the difference between two variances. Now, why are we using that to calculate the difference between means? And, the core idea is the following that, the mean square between is one way of calculating the total variance. Now, mean square error is another way of calculating total variance. Now, these three, which is mean square between mean square error and mean square total, which is what is described here are all going to be equal in a sense. Again they are going to be statistically equal, not actually equal. If the null hypothesis is true; so, if mu – if we go back to the null hypothesis, which is that mu A equals mu B equals mu C equals mu D; then, MSB, MSC and MSD would be statistically equivalent. However, if it is not true, then you will find that MSB will be greater than MSD will be greater than MSE. So, you have got two different ways of computing variance. And, what you are doing is you are doing an F-test of these two different variance calculations. So, you are doing an F-test on these two different variance calculations and you will not reject the null hypothesis, if the null hypothesis is true, which is mu A equals mu B equals mu C equals mu D. Now, if those means are not equal, then these two computations of variance do not really represent the total variance. And, this computation of variance becomes an over estimate and this computation of variance becomes an under estimate of the overall variance. And therefore, a test of whether these two are different variances will show up to be true. Essentially you will wind up rejecting the null hypothesis that these two variances are equal; and thereby, commenting on the fact that the null hypothesis of means was probably not true. So, let us get a little bit more intuition on why this becomes if mu A… if the null hypothesis is not true. And, here the null hypothesis is that, mu A equals mu B equals mu C dot dot dot. If this null hypothesis is not true, let us take a look at why this computation becomes an overestimate and this becomes an underestimate. So, here is the first case where the null hypothesis is true. So, the null hypothesis is true; that means, mu A equals mu B equals mu C equals mu D; which means the mean of mu A. So, this is the distribution of A; this is the distribution of B; and, this is the… So, this is just a sample case. So, there is A, B, C. And, here are the distribution. And, hey, look at the means of this the same. And, here I have gone further and even made the distributions look identical. So, if you were to take samples from A, samples from B, samples from C and you are just going to put them all in one bucket called total distribution of Y, this is how it would look, because essentially, since these distributions are identical and their means are identical, it looks this is also identical. So, how do you compute mean-squared error? The way you compute mean-squared error is – essentially, look at the variance of each distribution; you basically take each data point and look at the standard deviation or variance of the data point with respect to its mean. So, you are essentially computing the variance of each of these distributions and you are just averaging them. And, because the variance of these distributions are going to be the same, that is going to match the variance of the total distribution of y; and, we just explained why. Now, let us step to mean square between. Mean square between is… So, here is the total distribution of Y and you are interested in capturing the variance of the total distribution of Y. And, the way you are going to do that is you are going to take the mean of distribution A; you are going to take a sample mean of distribution of A. And, as you have known from the earlier part of this course, if you take 5 data points or 6 data points on A and you compute a sample mean, that sample mean need not exactly fall on the population mean. So, you might get another value here for distribution B. Now, as long as the null hypothesis is true, which is the distribution A equals distribution B equals distribution C equals [FL] then you are going to get some sampling distribution if you take the means of each of these distributions. And, if you take the means of each of these distributions; then if all of these are equal, then you are going to get a new distribution called the distribution of the means of A, B, C. And, what is it that we have discussed about the variance of this distribution? We have discussed that as long as you are sampling from essentially the same distribution – you are sampling from the same distribution, because A now looks identical to B looks identical to C. They all have the same means. As long as you are doing that, we know that this standard deviation is equal to the overall standard deviation, that is, the standard deviation of the total distribution or you can think of it as the standard deviation of any of these distributions, because they are all identical – divided by square root of n. Or, you can think of it – you can think of it in variance terms and say sigma square divided by n. So, as long as I can compute this variance and then move this and multiply it by n, I could cancel this out and I can get an estimate of this variance. So, that is what I am trying to do. What I am trying to do out here is I am calculating means from each of these distributions and I am taking the standard deviation of those means; I am taking the variance of those means. I am taking the variance of those means and multiplying it by n with the belief that, if the null hypothesis is true, that should be a great estimate of the variance of the total distribution; which is fine. That is going to work great as long as the null hypothesis is true. So, as long as the null hypothesis is true, this approach to calculating the variance of this distribution – variance of this distribution; and, this approach to calculating the variance of this distribution. So, the MSE approach to calculating variance of the total distribution and MSB approach to calculating the variance of the total distribution – both should work perfectly fine. But, what happens when the null hypothesis is not true? Take this case, where the null hypothesis is not true. Now, if you take the total distribution of Y; because the null hypothesis is not true, you get some data points from distribution A, some data points from distribution B, some data points from distribution C; it is going to fall over a much larger region. Now, if you try to estimate the total variance; let us take a look at how MSE would estimate the total variance. MSE would take the variance or take the deviation of each data point with respect to its mean, not with respect to some grand mean. And, even that mean is essentially a sample mean that you are going to calculate. But, the idea is that, your estimate is going to – of variance out here is going to be of some value; your estimate of variance here is going to be another value. And similarly, out here it is going to be of some value. And, all you are doing with MSE is you are taking the average of these three numbers – of 1, 2 and 3. Now, what is the average of these three arrows? It is an arrow with a magnitude that is much smaller than the real variance. So, when the null hypothesis is not true, the MSE method of calculating variance becomes an underestimate of the total variance. Now, let us look at what happens on the MSB side. Now, the null hypothesis is not true. And so, despite the fact that there is one source of variation, which is that the sample means are not following exactly on top of true means; but, in addition to that, the sample means are further separated from each other, because they are trying to go after a totally different line. They are true means themselves are different. So, the standard deviation or the distribution essentially of the sample means is going to be a much wider distribution, because the means are not equal. And now, when you multiply that by n, you wind up making a huge over estimate of the total distribution of Y, because you are essentially out here you are trying to look at different – you are trying to get sample means of distribution, which have truly different means and you are looking at the standard deviation of the sample means of distributions with truly different means. It is going to be an overestimate of the total variance of the distribution of Y. So, I hope that gives you some intuition on why the F-test for the ANOVA works the way it does. But, the important thing for you to remember is that the F-test; the F is the ratio of two variances. And, the core idea is you are using the F-test, which is used for ascertaining whether two variances are equal; you are using that in an indirect way to make a statement about the means of many distributions. And, you are doing that by saying that, if the null hypothesis is true, method A of calculating variance should be equal to method B of calculating variance. But, if the null hypothesis, which is mu A equals mu B equals mu C is not true; then, these are not accurate methods of calculating variance; great. The big question is what do you do? So, now, we know the mechanics of it; we have some intuition for the ANOVA; but, the idea is... So, you did this; you calculated an F-statistic; you then went and plugged that F-statistic and found out the probability. And, let us say your p value was really small. So, you want to reject the null hypothesis. So, what do you do after rejecting the null hypothesis? So, what is the statement you are making when you reject the null hypothesis? You are saying I am rejecting the null hypothesis that mu A equals mu B equals mu C equals mu D; great. But, can you tell me which of these mus are different from which others? So, one great way to do this is called the Tukey test. It is not the only way to do it; but, it is a fairly common way of doing it. So, that is why I have called it method 1; it is the Tukey test. But, there are other methods as well. The idea here is to decide on an alpha value, which we do it many hypothesis tests; even before we start, we say if that p value is less than a certain value, I am going to reject it. So, let us say you start with an alpha value and then you calculate something called the critical Tukey distance based on this alpha value. So, there is a distribution called the Tukey distribution and that is again something that you can get usually from software or you can get that from the back of text books. But, you will essentially plug in the values of alpha, which you just decided in one step above. And, you know the value of i; and, i out here is the number of treatments. So, it is the equivalent of what we had as a. But, I did not want to confuse you in between alpha and a. So, I have called it i out here; N is the number of replicates. So, small n remains the same – the same concept; and, the large N remains the same. The only thing that I have changed is I have called what we used to call a; I have called that i and the reason I have done that, so that I avoid confusion with alpha out here. So, we do that and we use that mean squared error formula out here and we calculate something called a critical distance. And then, you do a very simple thing; which is you do a complete enumeration of all pairs. So, for instance, this is the mathematical way of showing it; but, essentially, all I am saying is let us calculate y bar A dot. And similarly, calculate y bar B dot, y bar C dot. And, look at the difference between each pair of y bar i dots; where, the two i's are not. So, the different combinations you can come up with are – in our particular example, you will also have A comma B, A comma C, A comma D, B comma C, B comma D, C comma D. So, there are 6 possible combinations and because… And, you are essentially just taking the absolute value. So, b minus a is the same as a minus b. So, you just calculate 6 differences and you see which of these 6… It is 6 in this particular case, because we had 4 treatments; we had 4 different… We had A, B, C and D. So, we had 4 possibilities, 4 sets of data. So, that is 6 treatments, because you get the concept here is combination. So, you will have 4 c 2 combinations. But, how many ever treatments you have, you will similarly have the concept of whether you like to use the word i or whether you want to use a, c – two combinations; you will calculate that many combinations of distances and you will see which of these distances are greater than the Tukey distance. And then, those are the ones that are not equal to each other. So, you will calculate a critical distance. And, to figure out between… See you just rejected the null hypothesis that mu A equals mu B equals mu C equals mu D is wrong. So, you have said that; you said that, that is wrong. But, which of these are different from each other? For that, you calculate a critical distance and see which of these combinations have a mean difference that is greater than the critical distance; and, those are different ones. So, hope that gives you an idea of what the Tukey test does. So, now we move on to the next topic, which is the chi-square test of independence. It is noteworthy at this point to say that, we have already looked at one chi-square test; we have looked at the chi-square test for standard deviation in the case of the single sample test. And, you might also encounter another test called the chi-square goodness of a test; and, that is used when you have a set of data and you want to see how well it fits to a particular distribution. But, out here what we are interested in is essentially using that chi square test of independence; and it is used here, where you have two categorical variables. So, here is the first categorical variable, which is the smoking habit. Is it heavy, regular, occasional or never. And, here is the second categorical variable exercise – frequent, some or none. And, what you are trying to do is you are creating a contingency table of how frequently various values occur. And, you are trying to see if they are independent of each other. Does smoking habit have anything to do with exercise or vice-versa. Again not implying causation, but you are taking two categorical variables and seeing if these two variables are independent of each other. So, the core idea between that is to convert this table to a set of percentages. So, for instance, you would take each of these values and you would see how frequently they occur within each row essentially. And similarly, you would do that for each row. Finally, what you would do is would create theoretical values for this table in accordance to the assumptions of independence. So, for instance, if I believe that these were completely independent, I would take the row-wise sums and the column-wise sums and I would for instance conclude that row 1 – there would be a 48.7 percent chance of seeing a particular value occur in a particular spot. And so, I would essentially say look since… So, this is… I am getting that 48. So, the numbers I am getting are for instance out here is 48.7. And, the number for this category is 41.5. And, for this would be I believe the remaining, which is about 9.7 percent; so, 48, 41. But, the idea is the following. The idea is that, you get percentages based of the sum of each rows. So, you do 7 plus 9 plus 12 plus 87; and, you divide that by the total sum of all values. And, that is how you get the 48.7 percent. Now, given that, there is a 48.7 percent chance that you will find yourself in row 1; I ask you the question that, if you are a heavy smoker and there are a total of 11 heavy smokers, the number of heavy smokers that frequently exercise that I should expect is 11 times 48.7 percent. So, just note the map that we did here; we computed the probability of being in each row. The way we did that is we took the sum of each row. So, the sum of the first row is 7 plus 9 plus 12 plus 87; and, we divided that by the sum of all the numbers here. And, by doing that, we got a number 48.7 percent. So, we told ourselves the probability that, if I did not know anything; that I would find myself in the first row is 48.7 percent. Now, assume that I am a heavy smoker; then, there are a total of 11 heavy smokers; if I would randomly guess whether I was a frequent exerciser or not, then my probability is nothing; the number of heavy smokers that frequently exercise – the expected value of that is nothing but the 11, which I get from adding 7 and 3 and 1 and multiplying that by the 48.7. And, a small correction from before… So, you would… This I know for a fact is 48.7. This probability is also 41.5; you would just need to compute the remaining to get this value. And, that is close to about 10 percent. So, I do not believe it is exactly 9.7 percent; that is why I am correcting this up. But, that is the core idea. The core idea is you calculate percentages based on frequency of occurrence and then you come up with what is an expected value for each cell. Once you come up with an expected value for each cell; and, you can do that row wise or column wise; I explained it to you row wise. So, I took each row, calculated a percentage; and then, I calculated an expected value. But, essentially, what you have computed is a set of expected values out here. Now, you compare each original value. So, you compare that 7. So, I compare this 7 to what I get when I multiply 48.7 by 100 – 48.7 percent times 11. I believe that is approximately like close to 5 point something. So, what I am essentially doing is I am doing that 7, which I see minus that expected value, which is the 5 point dot dot that I see; and, I am squaring that. And, I am doing that for each cell. So, I am doing that for each row and each column and thereby doing that for each cell. And, I am dividing it by their expected value. So, this 5 will also come in the denominator. And, that summation should essentially give me a chi-square distribution with r minus 1 times c minus 1 degrees of freedom. Now, note that the chi-square distribution has only one parameter associated with degrees of freedom. So, if you just expand this, you will get this formula, which is nothing but just linear algebra r times c minus c minus r plus 1. So, you have just calculated a test statistic using this formula. And, that test statistic should be chi-square distributed with this degrees of freedom. And again, you can use that same core concept to calculate a p value associated with it and thereby either reject the hypothesis. And, what is… And, just as a refresh, what is the hypothesis here? The hypothesis – the null hypothesis is that there is no relationship between this variable and this variable; that is, these two variables are independent of each other. And, the alternate hypothesis is the rejection of that. There is some relationship that depending on whether you are heavy regular or occasional smoker, that impacts whether you are going to be a frequent exerciser or not; I mean it is not a causal relationship; but, knowing this, helps me make – say something about this other variable. Or, it can be the other way round – knowing that you are a frequent exerciser, does that help me in anyway predict or in any way make a statement about your smoking habit. Is there some relationship or are these two variables independent of each other? The null hypothesis being that they are independent of each other and a very low p value in this chi-square test statistic will enable you to reject that null hypothesis or you would fail to reject that hypothesis. So, I hope this gives you an idea of the two tests that we have introduced in inferential statistics that have to do with multiple samples; that just do not have to do with two samples; that is the ANOVA or the analysis of variance and the Chi Square test of independence. In our next lecture, we are going to start with regression. Thank you. English - NPTEL Official\", 'content_type': 'text', 'score': None, 'meta': {'name': 'Raw Data_5.txt'}, 'embedding': None, 'id': '66d0d4a4ba6c4531105b7f3cf04631a7'}>]\n"
          ]
        }
      ],
      "source": [
        "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http\n",
        "\n",
        "\n",
        "# Let's first fetch some documents that we want to query\n",
        "# Here: 517 Wikipedia articles for Game of Thrones\n",
        "doc_dir = \"/content/input\"\n",
        "#s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip\"\n",
        "#fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
        "\n",
        "# Convert files to dicts\n",
        "# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n",
        "# It must take a str as input, and return a str.\n",
        "docs = convert_files_to_docs(dir_path=doc_dir, clean_func=None, split_paragraphs=True)\n",
        "\n",
        "# We now have a list of dictionaries that we can write to our document store.\n",
        "# If your texts come from a different source (e.g. a DB), you can of course skip convert_files_to_dicts() and create the dictionaries yourself.\n",
        "# The default format here is:\n",
        "# {\n",
        "#    'content': \"<DOCUMENT_TEXT_HERE>\",\n",
        "#    'meta': {'name': \"<DOCUMENT_NAME_HERE>\", ...}\n",
        "# }\n",
        "# (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and\n",
        "# can be accessed later for filtering or shown in the responses of the Pipeline)\n",
        "\n",
        "# Let's have a look at the first 3 entries:\n",
        "print(docs)\n",
        "\n",
        "# Now, let's write the dicts containing documents to our DB.\n",
        "document_store.write_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMI2VFmse7qD"
      },
      "source": [
        "## Initialize Retriever, Reader & Pipeline\n",
        "\n",
        "### Retriever\n",
        "\n",
        "Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered.\n",
        "They use some simple but fast algorithm.\n",
        "\n",
        "**Here:** We use Elasticsearch's default BM25 algorithm\n",
        "\n",
        "**Alternatives:**\n",
        "\n",
        "- Customize the `BM25Retriever`with custom queries (e.g. boosting) and filters\n",
        "- Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n",
        "- Use `EmbeddingRetriever` to find candidate documents based on the similarity of embeddings (e.g. created via Sentence-BERT)\n",
        "- Use `DensePassageRetriever` to use different embedding models for passage and query (see Tutorial 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rnvxanJe7qE"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import BM25Retriever\n",
        "\n",
        "retriever = BM25Retriever(document_store=document_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "ThLTSay-e7qF"
      },
      "outputs": [],
      "source": [
        "# Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes with SQLite document store.\n",
        "\n",
        "# from haystack.nodes import TfidfRetriever\n",
        "# retriever = TfidfRetriever(document_store=document_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqnFUVpNe7qG"
      },
      "source": [
        "### Reader\n",
        "\n",
        "A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n",
        "on powerful, but slower deep learning models.\n",
        "\n",
        "Haystack currently supports Readers based on the frameworks FARM and Transformers.\n",
        "With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models).\n",
        "\n",
        "**Here:** a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2)\n",
        "\n",
        "**Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n",
        "\n",
        "**Alternatives (Models):** e.g. \"distilbert-base-uncased-distilled-squad\" (fast) or \"deepset/bert-large-uncased-whole-word-masking-squad2\" (good accuracy)\n",
        "\n",
        "**Hint:** You can adjust the model to return \"no answer possible\" with the no_ans_boost. Higher values mean the model prefers \"no answer possible\"\n",
        "\n",
        "#### FARMReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "SjCP3Vvee7qH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631,
          "referenced_widgets": [
            "3e109722d15f4ab6b283f0d9be1bcf62",
            "064e3a168f70491683520794e38ce77c",
            "a1fcfb7d2ccc467786b6cc620d2a2eae",
            "bdaeafc7586f4b0aa76b11b5bcd0a752",
            "21045411d0934ab287b74ae4e7136535",
            "4dc238fb51f34e73ae6b0f3d69d04a29",
            "29b10325735e47c2b39a6bb027b03533",
            "6c92b1ed407347d69686be5860de463e",
            "44bb6005947e463398e82b1417015131",
            "176534704bc842b6a40614ed0e70d945",
            "3f412f5344a14e31959631f71bcef044",
            "e06f12624e274ad5ad68de626e4faca9",
            "aaf9939160bf4085bfa4781be9a0e394",
            "105e33b2c68f49f48e14b31359596755",
            "6a3137a954da4d24bd9b62e6ce6f5b73",
            "cd66f2dbc3894247ba92e015d822592a",
            "92a6fd733aae4ae0ae5ee2c208065748",
            "8710a9932e334925ae6f7865cee0a882",
            "c3cbe683fc9f4bd8bca5f5549a0dce90",
            "0d527a94a52347b58e7f4515a7e69b17",
            "a62f5c70cb034f3b8a174f48c88acf00",
            "6869b75271b24b4cbf5b021ea86e3f61",
            "1d5ef232646f415ca80f1f977ffa0a43",
            "84736337d4aa4420833e9aed597d5c85",
            "ea9b613a94dd4adcb62d9f63e996d4fd",
            "7f307c985bd74127934b7f20ae649e01",
            "ce95f85f785d4196898742b6423cc817",
            "862477e421b94e589ddb69b55cdba918",
            "d116bf76e5ce4849ad58565b289d71b1",
            "43c1b8a0266a483c82636a43385a295e",
            "7c4a25f22d2a47edb2ac592bd03fd4d8",
            "c00c5803dd4c40c5a3bf8b1357ba980d",
            "149bcfc62d664120abbbb7136c955a7c",
            "9a9ff3f91d86404f8ced2bcb0c1d7cf5",
            "012a1d1e472949768b6018c7618c480a",
            "3e291dab6c8f4cefb088c5438691130b",
            "f9f761e51b884c3c877a60cc61c88098",
            "b0d361674e7e49678067e4cdab734143",
            "c3b2367d1b0f456d85fa70aff2cae17b",
            "2432e9dd65104dda88760d137fc4460e",
            "08d8eacc884c417ca1b91773b6eb6342",
            "41a798c0222848e7a9e45a0b6aaa0fc2",
            "0f8fd505913846a3a76e17b5d0299626",
            "97604979479d4d4484bdb463487d5bec"
          ]
        },
        "outputId": "e3a599a6-9821-4546-e935-222249d59272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e109722d15f4ab6b283f0d9be1bcf62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e06f12624e274ad5ad68de626e4faca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d5ef232646f415ca80f1f977ffa0a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a9ff3f91d86404f8ced2bcb0c1d7cf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-af4a9ff92850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2SeqGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt2-medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#generator = Seq2SeqGenerator(model_name_or_path=\"facebook/opt-125m\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#generator = pipeline('text-generation', model='anas-awadalla/gpt2-span-head-finetuned-squad')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Call the actuall __init__ function with all the arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper_exportable_to_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haystack/nodes/answer_generator/transformers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, input_converter, top_k, max_length, min_length, num_beams, use_gpu, progress_bar, use_auth_token, devices)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, T5Config, XLMProphetNetConfig."
          ]
        }
      ],
      "source": [
        "from haystack.nodes import Seq2SeqGenerator\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "set_seed(32)\n",
        "\n",
        "#generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\n",
        "\n",
        "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "generator = Seq2SeqGenerator(model_name_or_path=\"gpt2-medium\")\n",
        "#generator = Seq2SeqGenerator(model_name_or_path=\"facebook/opt-125m\")\n",
        "#generator = pipeline('text-generation', model='anas-awadalla/gpt2-span-head-finetuned-squad')\n",
        "\n",
        "#generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n",
        "\n",
        "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"anas-awadalla/gpt2-span-head-finetuned-squad\")\n",
        "\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"anas-awadalla/gpt2-span-head-finetuned-squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ527asle7qI"
      },
      "source": [
        "#### TransformersReader\n",
        "\n",
        "Alternative:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIOWg8VJe7qK"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import TransformersReader\n",
        "# reader = TransformersReader(model_name_or_path=\"anas-awadalla/gpt2-span-head-finetuned-squad\", tokenizer=\"distilbert-base-uncased\", use_gpu=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_t4PgD8e7qL"
      },
      "source": [
        "### Pipeline\n",
        "\n",
        "With a Haystack `Pipeline` you can stick together your building blocks to a search pipeline.\n",
        "Under the hood, `Pipelines` are Directed Acyclic Graphs (DAGs) that you can easily customize for your own use cases.\n",
        "To speed things up, Haystack also comes with a few predefined Pipelines. One of them is the `ExtractiveQAPipeline` that combines a retriever and a reader to answer our questions.\n",
        "You can learn more about `Pipelines` in the [docs](https://haystack.deepset.ai/docs/latest/pipelinesmd)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "VRvBYCS4e7qL"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import GenerativeQAPipeline\n",
        "\n",
        "pipe = GenerativeQAPipeline(generator, retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgUx7CQPe7qM"
      },
      "source": [
        "## Voilà! Ask a question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "BtWkh_1Ze7qN"
      },
      "outputs": [],
      "source": [
        "# You can configure how many candidates the Reader and Retriever shall return\n",
        "# The higher top_k_retriever, the better (but also the slower) your answers.\n",
        "prediction = pipe.run(\n",
        "    query=\"what is biology?\", params={\"Retriever\": {\"top_k\": 2}}\n",
        "    #query= \"what is python ?\", params={\"Retriever\": {\"top_k\": 3}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Dx6ache7qR"
      },
      "outputs": [],
      "source": [
        "# prediction = pipe.run(query=\"Who created the Dothraki vocabulary?\", params={\"Reader\": {\"top_k\": 5}})\n",
        "# prediction = pipe.run(query=\"Who is the sister of Sansa?\", params={\"Reader\": {\"top_k\": 5}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Fl8Pv-e7qS"
      },
      "source": [
        "Now you can either print the object directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBo2aDdLe7qS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0446f4fb-3e71-46f0-bbf1-619cd986a6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answers': [<Answer {'answer': 'Biology is the study of biology.', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['1cbe810b32647cc5dd8987abf6e55224', 'a8cfcc71c86494259ef8cd56e4976e16'], 'doc_scores': [0.7764225701857133, 0.6631352049025421], 'content': ['this in computational biology or social network analysis and other domains or you could find ', 'And this is the point that is get captured in what is offend, what is really described '], 'titles': ['Raw Data_3.txt', 'Raw Data_3.txt']}}>],\n",
            " 'documents': [<Document: {'content': 'this in computational biology or social network analysis and other domains or you could find ', 'content_type': 'text', 'score': 0.7764225701857133, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': '1cbe810b32647cc5dd8987abf6e55224'}>,\n",
            "               <Document: {'content': 'And this is the point that is get captured in what is offend, what is really described ', 'content_type': 'text', 'score': 0.6631352049025421, 'meta': {'name': 'Raw Data_3.txt'}, 'embedding': None, 'id': 'a8cfcc71c86494259ef8cd56e4976e16'}>],\n",
            " 'node_id': 'Generator',\n",
            " 'params': {'Retriever': {'top_k': 2}},\n",
            " 'query': 'what is biology?',\n",
            " 'root_node': 'Query'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(prediction)\n",
        "\n",
        "# Sample output:\n",
        "# {\n",
        "#     'answers': [ <Answer: answer='Eddard', type='extractive', score=0.9919578731060028, offsets_in_document=[{'start': 608, 'end': 615}], offsets_in_context=[{'start': 72, 'end': 79}], document_id='cc75f739897ecbf8c14657b13dda890e', meta={'name': '454_Music_of_Game_of_Thrones.txt'}}, context='...' >,\n",
        "#                  <Answer: answer='Ned', type='extractive', score=0.9767240881919861, offsets_in_document=[{'start': 3687, 'end': 3801}], offsets_in_context=[{'start': 18, 'end': 132}], document_id='9acf17ec9083c4022f69eb4a37187080', meta={'name': '454_Music_of_Game_of_Thrones.txt'}}, context='...' >,\n",
        "#                  ...\n",
        "#                ]\n",
        "#     'documents': [ <Document: content_type='text', score=0.8034909798951382, meta={'name': '332_Sansa_Stark.txt'}, embedding=None, id=d1f36ec7170e4c46cde65787fe125dfe', content='\\n===\\'\\'A Game of Thrones\\'\\'===\\nSansa Stark begins the novel by being betrothed to Crown ...'>,\n",
        "#                    <Document: content_type='text', score=0.8002150354529785, meta={'name': '191_Gendry.txt'}, embedding=None, id='dd4e070a22896afa81748d6510006d2', 'content='\\n===Season 2===\\nGendry travels North with Yoren and other Night's Watch recruits, including Arya ...'>,\n",
        "#                    ...\n",
        "#                  ],\n",
        "#     'no_ans_gap':  11.688868522644043,\n",
        "#     'node_id': 'Reader',\n",
        "#     'params': {'Reader': {'top_k': 5}, 'Retriever': {'top_k': 5}},\n",
        "#     'query': 'Who is the father of Arya Stark?',\n",
        "#     'root_node': 'Query'\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QzJnUe_m57A5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_sLWGJ9e7qU"
      },
      "source": [
        "\\Or use a util to simplify the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "M6yfW-COe7qV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c57e8e-87a1-4c1f-bce0-72577fdbb951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: what is support vector machine?\n",
            "Answers:\n",
            "[   {   'answer': 'A support vector machine is a machine learning algorithm '\n",
            "                  'that can be used to solve a problem. For example, if you '\n",
            "                  'want to solve an image problem, you can use a vector '\n",
            "                  'machine to solve it.'}]\n"
          ]
        }
      ],
      "source": [
        "from haystack.utils import print_answers\n",
        "\n",
        "# Change `minimum` to `medium` or `all` to raise the level of detail\n",
        "print_answers(prediction, details=\"minimum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8ZubzCJ5e7qW"
      },
      "source": [
        "## About us\n",
        "\n",
        "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
        "\n",
        "We bring NLP to the industry via open source!  \n",
        "Our focus: Industry specific language models & large scale QA systems.  \n",
        "  \n",
        "Some of our other work: \n",
        "- [German BERT](https://deepset.ai/german-bert)\n",
        "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
        "- [FARM](https://github.com/deepset-ai/FARM)\n",
        "\n",
        "Get in touch:\n",
        "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
        "\n",
        "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8ZubzCJ5e7qW"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e109722d15f4ab6b283f0d9be1bcf62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_064e3a168f70491683520794e38ce77c",
              "IPY_MODEL_a1fcfb7d2ccc467786b6cc620d2a2eae",
              "IPY_MODEL_bdaeafc7586f4b0aa76b11b5bcd0a752"
            ],
            "layout": "IPY_MODEL_21045411d0934ab287b74ae4e7136535"
          }
        },
        "064e3a168f70491683520794e38ce77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc238fb51f34e73ae6b0f3d69d04a29",
            "placeholder": "​",
            "style": "IPY_MODEL_29b10325735e47c2b39a6bb027b03533",
            "value": "Downloading config.json: 100%"
          }
        },
        "a1fcfb7d2ccc467786b6cc620d2a2eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c92b1ed407347d69686be5860de463e",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44bb6005947e463398e82b1417015131",
            "value": 718
          }
        },
        "bdaeafc7586f4b0aa76b11b5bcd0a752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_176534704bc842b6a40614ed0e70d945",
            "placeholder": "​",
            "style": "IPY_MODEL_3f412f5344a14e31959631f71bcef044",
            "value": " 718/718 [00:00&lt;00:00, 20.1kB/s]"
          }
        },
        "21045411d0934ab287b74ae4e7136535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc238fb51f34e73ae6b0f3d69d04a29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b10325735e47c2b39a6bb027b03533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c92b1ed407347d69686be5860de463e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bb6005947e463398e82b1417015131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "176534704bc842b6a40614ed0e70d945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f412f5344a14e31959631f71bcef044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e06f12624e274ad5ad68de626e4faca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaf9939160bf4085bfa4781be9a0e394",
              "IPY_MODEL_105e33b2c68f49f48e14b31359596755",
              "IPY_MODEL_6a3137a954da4d24bd9b62e6ce6f5b73"
            ],
            "layout": "IPY_MODEL_cd66f2dbc3894247ba92e015d822592a"
          }
        },
        "aaf9939160bf4085bfa4781be9a0e394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92a6fd733aae4ae0ae5ee2c208065748",
            "placeholder": "​",
            "style": "IPY_MODEL_8710a9932e334925ae6f7865cee0a882",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "105e33b2c68f49f48e14b31359596755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3cbe683fc9f4bd8bca5f5549a0dce90",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d527a94a52347b58e7f4515a7e69b17",
            "value": 1042301
          }
        },
        "6a3137a954da4d24bd9b62e6ce6f5b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a62f5c70cb034f3b8a174f48c88acf00",
            "placeholder": "​",
            "style": "IPY_MODEL_6869b75271b24b4cbf5b021ea86e3f61",
            "value": " 0.99M/0.99M [00:01&lt;00:00, 901kB/s]"
          }
        },
        "cd66f2dbc3894247ba92e015d822592a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a6fd733aae4ae0ae5ee2c208065748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8710a9932e334925ae6f7865cee0a882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3cbe683fc9f4bd8bca5f5549a0dce90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d527a94a52347b58e7f4515a7e69b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a62f5c70cb034f3b8a174f48c88acf00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6869b75271b24b4cbf5b021ea86e3f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d5ef232646f415ca80f1f977ffa0a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84736337d4aa4420833e9aed597d5c85",
              "IPY_MODEL_ea9b613a94dd4adcb62d9f63e996d4fd",
              "IPY_MODEL_7f307c985bd74127934b7f20ae649e01"
            ],
            "layout": "IPY_MODEL_ce95f85f785d4196898742b6423cc817"
          }
        },
        "84736337d4aa4420833e9aed597d5c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862477e421b94e589ddb69b55cdba918",
            "placeholder": "​",
            "style": "IPY_MODEL_d116bf76e5ce4849ad58565b289d71b1",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "ea9b613a94dd4adcb62d9f63e996d4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43c1b8a0266a483c82636a43385a295e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c4a25f22d2a47edb2ac592bd03fd4d8",
            "value": 456318
          }
        },
        "7f307c985bd74127934b7f20ae649e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00c5803dd4c40c5a3bf8b1357ba980d",
            "placeholder": "​",
            "style": "IPY_MODEL_149bcfc62d664120abbbb7136c955a7c",
            "value": " 446k/446k [00:01&lt;00:00, 501kB/s]"
          }
        },
        "ce95f85f785d4196898742b6423cc817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862477e421b94e589ddb69b55cdba918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d116bf76e5ce4849ad58565b289d71b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c1b8a0266a483c82636a43385a295e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4a25f22d2a47edb2ac592bd03fd4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c00c5803dd4c40c5a3bf8b1357ba980d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149bcfc62d664120abbbb7136c955a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a9ff3f91d86404f8ced2bcb0c1d7cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_012a1d1e472949768b6018c7618c480a",
              "IPY_MODEL_3e291dab6c8f4cefb088c5438691130b",
              "IPY_MODEL_f9f761e51b884c3c877a60cc61c88098"
            ],
            "layout": "IPY_MODEL_b0d361674e7e49678067e4cdab734143"
          }
        },
        "012a1d1e472949768b6018c7618c480a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b2367d1b0f456d85fa70aff2cae17b",
            "placeholder": "​",
            "style": "IPY_MODEL_2432e9dd65104dda88760d137fc4460e",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "3e291dab6c8f4cefb088c5438691130b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d8eacc884c417ca1b91773b6eb6342",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41a798c0222848e7a9e45a0b6aaa0fc2",
            "value": 1355256
          }
        },
        "f9f761e51b884c3c877a60cc61c88098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8fd505913846a3a76e17b5d0299626",
            "placeholder": "​",
            "style": "IPY_MODEL_97604979479d4d4484bdb463487d5bec",
            "value": " 1.29M/1.29M [00:01&lt;00:00, 1.16MB/s]"
          }
        },
        "b0d361674e7e49678067e4cdab734143": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b2367d1b0f456d85fa70aff2cae17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2432e9dd65104dda88760d137fc4460e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08d8eacc884c417ca1b91773b6eb6342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a798c0222848e7a9e45a0b6aaa0fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f8fd505913846a3a76e17b5d0299626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97604979479d4d4484bdb463487d5bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}